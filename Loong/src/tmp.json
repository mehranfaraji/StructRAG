[
    {
        "level": 3,
        "set": 2,
        "length": 80092,
        "type": "paper",
        "language": "en",
        "question": "CHAIN-OF-TABLE: EVOLVING TABLES IN THE REASONING CHAIN FOR TABLE UNDERSTANDING",
        "instruction": "We hope you will carefully study the provided papers and determine the citation relationships between them. Please follow the instructions below strictly to complete the task:\n\n#Specific Requirements:\n1. Reference: When a given paper mentions other provided papers, those other papers are considered as \"references\" for the given paper. To summarize in this specific context, references are about what the given paper is using.\n2. Citation: Conversely, when other provided papers mention the given paper in their works, the given paper is being \"cited\" by those other papers. To summarize in this specific context, citations are about who is using the given paper.\n3. Given a paper, you need to determine the citation or reference relationship between this paper and the other papers. Do not consider papers that are not provided.\n3. Please present the paper titles in a json format as follows: {{\"Reference\":[\"Reference Title 1\", \"Reference Title 2\", ..., \"Reference Title n\"], \"Citation\":[\"Citation Title 1\", \"Citation Title 2\", ..., \"Citation Title n\"]}}.\n4. If a paper does not have any references or citations, please leave the corresponding list empty, e.g.{{\"Refernce\":[]}}, {{\"Citation\":[]}}.",
        "prompt_template": "#Papers:\n{docs}\n\n{instruction}\n\n#The paper you need to analyze:\n{question}",
        "doc": [
            "2401.04398.md",
            "2404.14215.md",
            "2404.15676.md",
            "2305.14992.md"
        ],
        "answer": {
            "Reference": [
                "# Reasoning with Language Model is Planning with World Model "
            ],
            "Citation": [
                "# Text-Tuple-Table: Towards Information Integration in Text-to-Table Generation via Global Tuple Extraction ",
                "# Beyond Chain-of-Thought: A Survey of Chain-of-X Paradigms for LLMs "
            ]
        },
        "shuffle_doc": true,
        "id": "28a022e5-a896-4700-9c50-9750e2bcdacc",
        "prompt": "[Question]\n#Papers:\n{docs}\n\nWe hope you will carefully study the provided papers and determine the citation relationships between them. Please follow the instructions below strictly to complete the task:\n\n#Specific Requirements:\n1. Reference: When a given paper mentions other provided papers, those other papers are considered as \"references\" for the given paper. To summarize in this specific context, references are about what the given paper is using.\n2. Citation: Conversely, when other provided papers mention the given paper in their works, the given paper is being \"cited\" by those other papers. To summarize in this specific context, citations are about who is using the given paper.\n3. Given a paper, you need to determine the citation or reference relationship between this paper and the other papers. Do not consider papers that are not provided.\n3. Please present the paper titles in a json format as follows: {{\"Reference\":[\"Reference Title 1\", \"Reference Title 2\", ..., \"Reference Title n\"], \"Citation\":[\"Citation Title 1\", \"Citation Title 2\", ..., \"Citation Title n\"]}}.\n4. If a paper does not have any references or citations, please leave the corresponding list empty, e.g.{{\"Refernce\":[]}}, {{\"Citation\":[]}}.\n\n#The paper you need to analyze:\nCHAIN-OF-TABLE: EVOLVING TABLES IN THE REASONING CHAIN FOR TABLE UNDERSTANDING\n\n[Gold Answer]\n{'Reference': ['# Reasoning with Language Model is Planning with World Model '], 'Citation': ['# Text-Tuple-Table: Towards Information Integration in Text-to-Table Generation via Global Tuple Extraction ', '# Beyond Chain-of-Thought: A Survey of Chain-of-X Paradigms for LLMs ']}\n\n[The Start of Assistant's Predicted Answer]\n{\"Citation\": [\"Beyond Chain-of-Thought: A Survey of Chain-of-X Paradigms for LLMs\"], \"Reference\": [\"Fine-tuning Language Model for Table Understanding\", \"Prompting Language Model for Table Understanding\", \"End-to-End QA\", \"Few-Shot QA\", \"Chain-of-Thought (Wei et al., 2022)\", \"Text-to-SQL (Rajkumar et al., 2022)\", \"Binder (Cheng et al., 2022)\", \"Dater (Ye et al., 2023)\", \"TaPas (Herzig et al., 2020)\", \"Pasta (Gu et al., 2022)\", \"TUTA (Wang et al., 2021)\", \"TAPEX (Liu et al., 2021)\", \"DynamicPlan (T, Q, chain)\", \"GenerateArgs (T, Q, f)\", \"Query (T, Q)\", \"Chain-of-Thought\", \"Text-to-SQL (Rajkumar et al., 2022)\", \"Binder (Cheng et al., 2022)\", \"Dater (Ye et al., 2023)\", \"Table-to-text generation and pre-training with TabT5\", \"Palm 2 technical report\", \"Bert: Pre-training of deep bidirectional transformers for language understanding\", \"TaPas: Weakly supervised table parsing via pre-training\", \"PASTA: Tableoperations aware fact verification via sentence-table cloze pre-training\", \"Reasoning with language model is planning with world model\", \"A survey on table question answering: recent advances\", \"OmniTab: Pretraining with natural and synthetic data for few-shot table-based question answering\", \"Table pre-training via learning a neural sql executor\", \"Lever: Learning to verify language-to-code generation with execution\", \"Gpt-4 technical report\", \"Compositional semantic parsing on semi-structured tables\", \"Llama 2: Open foundation and fine-tuned chat models\", \"Chain-of-thought prompting elicits reasoning in large language models\", \"Tree of thoughts: Deliberate problem solving with large language models\", \"Large language models are versatile decomposers: Decompose evidence and questions for table-based reasoning\", \"Least-to-most prompting enables complex reasoning in large language models\"]}}\n[The End of Assistant's Predicted Answer]\n\n[System]\nWe would like to request your feedback on the performance of the AI assistant in response to the user question displayed above according to the gold answer. Please use the following listed aspects and their descriptions as evaluation criteria:\n    - Accuracy and Hallucinations: The assistant's answer is semantically consistent with the gold answer; The numerical value and order need to be accurate, and there should be no hallucinations.\n    - Completeness: Referring to the reference answers, the assistant's answer should contain all the key points needed to answer the user's question; further elaboration on these key points can be omitted.\nPlease rate whether this answer is suitable for the question. Please note that the gold answer can be considered as a correct answer to the question.\n\nThe assistant receives an overall score on a scale of 1 to 100, where a higher score indicates better overall performance.\nPlease note that if the assistant's answer and the gold answer fully meet the above criteria, its overall rating should be the full marks (100).\nPlease first provide a comprehensive explanation of your evaluation, avoiding any potential bias.\nThen, output a line indicating the score of the Assistant.\n\nPLEASE OUTPUT WITH THE FOLLOWING FORMAT, WHERE THE SCORE IS A SCALE OF 1 TO 100 BY STRICTLY FOLLOWING THIS FORMAT: \"[[score]]\", FOR EXAMPLE \"Rating: [[100]]\":\n<start output>\nEvaluation evidence: your evluation explanation here, no more than 100 words\nRating: [[score]]\n<end output>\n\nNow, start your evaluation:",
        "generate_response": "{\"Citation\": [\"Beyond Chain-of-Thought: A Survey of Chain-of-X Paradigms for LLMs\"], \"Reference\": [\"Fine-tuning Language Model for Table Understanding\", \"Prompting Language Model for Table Understanding\", \"End-to-End QA\", \"Few-Shot QA\", \"Chain-of-Thought (Wei et al., 2022)\", \"Text-to-SQL (Rajkumar et al., 2022)\", \"Binder (Cheng et al., 2022)\", \"Dater (Ye et al., 2023)\", \"TaPas (Herzig et al., 2020)\", \"Pasta (Gu et al., 2022)\", \"TUTA (Wang et al., 2021)\", \"TAPEX (Liu et al., 2021)\", \"DynamicPlan (T, Q, chain)\", \"GenerateArgs (T, Q, f)\", \"Query (T, Q)\", \"Chain-of-Thought\", \"Text-to-SQL (Rajkumar et al., 2022)\", \"Binder (Cheng et al., 2022)\", \"Dater (Ye et al., 2023)\", \"Table-to-text generation and pre-training with TabT5\", \"Palm 2 technical report\", \"Bert: Pre-training of deep bidirectional transformers for language understanding\", \"TaPas: Weakly supervised table parsing via pre-training\", \"PASTA: Tableoperations aware fact verification via sentence-table cloze pre-training\", \"Reasoning with language model is planning with world model\", \"A survey on table question answering: recent advances\", \"OmniTab: Pretraining with natural and synthetic data for few-shot table-based question answering\", \"Table pre-training via learning a neural sql executor\", \"Lever: Learning to verify language-to-code generation with execution\", \"Gpt-4 technical report\", \"Compositional semantic parsing on semi-structured tables\", \"Llama 2: Open foundation and fine-tuned chat models\", \"Chain-of-thought prompting elicits reasoning in large language models\", \"Tree of thoughts: Deliberate problem solving with large language models\", \"Large language models are versatile decomposers: Decompose evidence and questions for table-based reasoning\", \"Least-to-most prompting enables complex reasoning in large language models\"]}}",
        "eval_response": "Evaluation evidence: The assistant's answer shows significant hallucinations and inaccuracies. It lists numerous references that are not mentioned in the gold answer, indicating a misunderstanding or misinterpretation of the source material or task. The citation list partially matches the gold answer but omits one crucial citation. This discrepancy and the extensive list of incorrect references demonstrate a lack of accuracy and completeness in relation to the task requirements.\n\nRating: [[20]]",
        "score": 20
    },
    {
        "level": 3,
        "set": 3,
        "length": 106066,
        "type": "paper",
        "language": "en",
        "question": "Adapting Large Language Models for Document-Level Machine Translation",
        "instruction": "We hope you will carefully study the provided papers and determine the citation relationships between them. Please follow the instructions below strictly to complete the task:\n\n#Specific Requirements:\n1. Reference: When a given paper mentions other provided papers, those other papers are considered as \"references\" for the given paper. To summarize in this specific context, references are about what the given paper is using.\n2. Citation: Conversely, when other provided papers mention the given paper in their works, the given paper is being \"cited\" by those other papers. To summarize in this specific context, citations are about who is using the given paper.\n3. Given a paper, you need to determine the citation or reference relationship between this paper and the other papers. Do not consider papers that are not provided.\n3. Please present the paper titles in a json format as follows: {{\"Reference\":[\"Reference Title 1\", \"Reference Title 2\", ..., \"Reference Title n\"], \"Citation\":[\"Citation Title 1\", \"Citation Title 2\", ..., \"Citation Title n\"]}}.\n4. If a paper does not have any references or citations, please leave the corresponding list empty, e.g.{{\"Refernce\":[]}}, {{\"Citation\":[]}}.",
        "prompt_template": "#Papers:\n{docs}\n\n{instruction}\n\n#The paper you need to analyze:\n{question}",
        "doc": [
            "2401.06468.md",
            "2404.19505.md",
            "2405.11804.md",
            "2405.12819.md"
        ],
        "answer": {
            "Reference": [],
            "Citation": [
                "# Context-Aware Machine Translation with Source Coreference Explanation ",
                "# (PERHAPs) BEYOND Human TranSlation: HarNESSING MULTI-AGENT COLLABORATION FOR TRANSLATING ULTRA-LONG LITERARY TEXTS ",
                "# Large Language Models Meet NLP: A Survey "
            ]
        },
        "shuffle_doc": true,
        "id": "5a73f0db-932a-4f28-b716-d0f63564d564",
        "prompt": "[Question]\n#Papers:\n{docs}\n\nWe hope you will carefully study the provided papers and determine the citation relationships between them. Please follow the instructions below strictly to complete the task:\n\n#Specific Requirements:\n1. Reference: When a given paper mentions other provided papers, those other papers are considered as \"references\" for the given paper. To summarize in this specific context, references are about what the given paper is using.\n2. Citation: Conversely, when other provided papers mention the given paper in their works, the given paper is being \"cited\" by those other papers. To summarize in this specific context, citations are about who is using the given paper.\n3. Given a paper, you need to determine the citation or reference relationship between this paper and the other papers. Do not consider papers that are not provided.\n3. Please present the paper titles in a json format as follows: {{\"Reference\":[\"Reference Title 1\", \"Reference Title 2\", ..., \"Reference Title n\"], \"Citation\":[\"Citation Title 1\", \"Citation Title 2\", ..., \"Citation Title n\"]}}.\n4. If a paper does not have any references or citations, please leave the corresponding list empty, e.g.{{\"Refernce\":[]}}, {{\"Citation\":[]}}.\n\n#The paper you need to analyze:\nAdapting Large Language Models for Document-Level Machine Translation\n\n[Gold Answer]\n{'Reference': [], 'Citation': ['# Context-Aware Machine Translation with Source Coreference Explanation ', '# (PERHAPs) BEYOND Human TranSlation: HarNESSING MULTI-AGENT COLLABORATION FOR TRANSLATING ULTRA-LONG LITERARY TEXTS ', '# Large Language Models Meet NLP: A Survey ']}\n\n[The Start of Assistant's Predicted Answer]\n{\"Reference\": [\"A simple and effective unified encoder for document-level neural machine translation\", \"Using whole document context in neural machine translation\", \"Document-level neural machine translation with hierarchical attention networks\", \"Document context neural machine translation with memory networks\", \"A neuronal mechanism for translation with large language models\", \"Llemma: An open language model for mathematics\", \"Overview of the IWSLT 2017 evaluation campaign\", \"Monolingual or multilingual instruction tuning: Which makes a better alpaca\", \"Learn to remember: Transformer with recurrent memory for document-level neural machine translation\", \"Results of WMT22 metrics shared task: Stop using BLEU - neural metrics are better and more robust\"], \"Citation\": [\"Large Language Models Meet NLP: A Survey\", \"(PERHAPs) BEYOND Human TranSlation: HarNESSING MULTI-AGENT COLLABORATION FOR TRANSLATING ULTRA-LONG LITERARY TEXTS\"]}\n[The End of Assistant's Predicted Answer]\n\n[System]\nWe would like to request your feedback on the performance of the AI assistant in response to the user question displayed above according to the gold answer. Please use the following listed aspects and their descriptions as evaluation criteria:\n    - Accuracy and Hallucinations: The assistant's answer is semantically consistent with the gold answer; The numerical value and order need to be accurate, and there should be no hallucinations.\n    - Completeness: Referring to the reference answers, the assistant's answer should contain all the key points needed to answer the user's question; further elaboration on these key points can be omitted.\nPlease rate whether this answer is suitable for the question. Please note that the gold answer can be considered as a correct answer to the question.\n\nThe assistant receives an overall score on a scale of 1 to 100, where a higher score indicates better overall performance.\nPlease note that if the assistant's answer and the gold answer fully meet the above criteria, its overall rating should be the full marks (100).\nPlease first provide a comprehensive explanation of your evaluation, avoiding any potential bias.\nThen, output a line indicating the score of the Assistant.\n\nPLEASE OUTPUT WITH THE FOLLOWING FORMAT, WHERE THE SCORE IS A SCALE OF 1 TO 100 BY STRICTLY FOLLOWING THIS FORMAT: \"[[score]]\", FOR EXAMPLE \"Rating: [[100]]\":\n<start output>\nEvaluation evidence: your evluation explanation here, no more than 100 words\nRating: [[score]]\n<end output>\n\nNow, start your evaluation:",
        "generate_response": "{\"Reference\": [\"A simple and effective unified encoder for document-level neural machine translation\", \"Using whole document context in neural machine translation\", \"Document-level neural machine translation with hierarchical attention networks\", \"Document context neural machine translation with memory networks\", \"A neuronal mechanism for translation with large language models\", \"Llemma: An open language model for mathematics\", \"Overview of the IWSLT 2017 evaluation campaign\", \"Monolingual or multilingual instruction tuning: Which makes a better alpaca\", \"Learn to remember: Transformer with recurrent memory for document-level neural machine translation\", \"Results of WMT22 metrics shared task: Stop using BLEU - neural metrics are better and more robust\"], \"Citation\": [\"Large Language Models Meet NLP: A Survey\", \"(PERHAPs) BEYOND Human TranSlation: HarNESSING MULTI-AGENT COLLABORATION FOR TRANSLATING ULTRA-LONG LITERARY TEXTS\"]}",
        "eval_response": "Evaluation evidence: The assistant's answer inaccurately lists multiple references that are not mentioned in the gold answer, indicating a significant hallucination issue. The citations, while partially correct, also include an entry not present in the gold answer. This discrepancy in both references and citations demonstrates issues with accuracy and hallucinations. The answer fails to meet the completeness criterion as it does not accurately reflect the provided gold answer, which is considered correct.\n\nRating: [[20]]",
        "score": 20
    },
    {
        "level": 3,
        "set": 1,
        "length": 43353,
        "type": "paper",
        "language": "en",
        "question": "MedusA: Simple LLM Inference Acceleration Framework with Multiple Decoding Heads",
        "instruction": "We hope you will carefully study the provided papers and determine the citation relationships between them. Please follow the instructions below strictly to complete the task:\n\n#Specific Requirements:\n1. Reference: When a given paper mentions other provided papers, those other papers are considered as \"references\" for the given paper. To summarize in this specific context, references are about what the given paper is using.\n2. Citation: Conversely, when other provided papers mention the given paper in their works, the given paper is being \"cited\" by those other papers. To summarize in this specific context, citations are about who is using the given paper.\n3. Given a paper, you need to determine the citation or reference relationship between this paper and the other papers. Do not consider papers that are not provided.\n3. Please present the paper titles in a json format as follows: {{\"Reference\":[\"Reference Title 1\", \"Reference Title 2\", ..., \"Reference Title n\"], \"Citation\":[\"Citation Title 1\", \"Citation Title 2\", ..., \"Citation Title n\"]}}.\n4. If a paper does not have any references or citations, please leave the corresponding list empty, e.g.{{\"Refernce\":[]}}, {{\"Citation\":[]}}.",
        "prompt_template": "#Papers:\n{docs}\n\n{instruction}\n\n#The paper you need to analyze:\n{question}",
        "doc": [
            "2401.10774.md",
            "2405.17741.md",
            "2310.07177.md"
        ],
        "answer": {
            "Reference": [
                "# ONLINE SPECULATIVE DECODING "
            ],
            "Citation": [
                "# LoRA-Switch: Boosting the Efficiency of Dynamic LLM Adapters via System-Algorithm Co-design "
            ]
        },
        "shuffle_doc": true,
        "id": "71d0fb39-d315-4e33-beec-c53271663f7f",
        "prompt": "[Question]\n#Papers:\n{docs}\n\nWe hope you will carefully study the provided papers and determine the citation relationships between them. Please follow the instructions below strictly to complete the task:\n\n#Specific Requirements:\n1. Reference: When a given paper mentions other provided papers, those other papers are considered as \"references\" for the given paper. To summarize in this specific context, references are about what the given paper is using.\n2. Citation: Conversely, when other provided papers mention the given paper in their works, the given paper is being \"cited\" by those other papers. To summarize in this specific context, citations are about who is using the given paper.\n3. Given a paper, you need to determine the citation or reference relationship between this paper and the other papers. Do not consider papers that are not provided.\n3. Please present the paper titles in a json format as follows: {{\"Reference\":[\"Reference Title 1\", \"Reference Title 2\", ..., \"Reference Title n\"], \"Citation\":[\"Citation Title 1\", \"Citation Title 2\", ..., \"Citation Title n\"]}}.\n4. If a paper does not have any references or citations, please leave the corresponding list empty, e.g.{{\"Refernce\":[]}}, {{\"Citation\":[]}}.\n\n#The paper you need to analyze:\nMedusA: Simple LLM Inference Acceleration Framework with Multiple Decoding Heads\n\n[Gold Answer]\n{'Reference': ['# ONLINE SPECULATIVE DECODING '], 'Citation': ['# LoRA-Switch: Boosting the Efficiency of Dynamic LLM Adapters via System-Algorithm Co-design ']}\n\n[The Start of Assistant's Predicted Answer]\n{\"Reference\": [\"A Survey on Evaluation of Large Language Models\", \"Vicuna: An Open-Source Chatbot Impressing GPT-4 with $90 \\% *$ ChatGPT Quality\", \"ChatLaw: Open-Source Legal Large Language Model with Integrated External Knowledge Bases\", \"Compositional Semantic Parsing with Large Language Models\", \"Open LLM Leaderboard\", \"How Close is ChatGPT to Human Experts? Comparison Corpus, Evaluation, and Detection\", \"REALM: Retrieval-Augmented Language Model PreTraining\", \"Rethinking with Retrieval: Faithful Large Language Model Inference\", \"Measuring Massive Multitask Language Understanding\", \"Atlas: Few-shot Learning with Retrieval Augmented Language Models\", \"GQA: Training Generalized Multi-Query Transformer Models from Multi-Head Checkpoints\", \"ALPACA-farm: A Simulation Framework for Methods that Learn from Human Feedback\", \"The Curious Case of Neural Text Degeneration\", \"LOLA: Low-Rank Adaptation of Large Language Models\", \"SqueezeLLM: Dense-and-Sparse Quantization\", \"Fast Inference from Transformers via Speculative Decoding\", \"MAUVE: Measuring the Gap between Neural Text and Human Text using Divergence Frontiers\", \"Efficiently Scaling Transformer Inference\", \"SpecInfer: Accelerating Generative LLM Serving with Speculative Inference and Token Tree Verification\", \"Training Language Models to Follow Instructions with Human Feedback\", \"SmoothQuant: Accurate and Efficient Post-Training Quantization for Large Language Models\", \"A Survey on Non-Autoregressive Generation for Neural Machine Translation and Beyond\", \"Do Transformers Really Perform Badly for Graph Representation?\", \"OPT: Open Pre-trained Transformer Language Models\", \"H _2 O: Heavy-Hitter Oracle for Efficient Generative Inference of Large Language Models\", \"Judging LLM-as-a-Judge with MT-Bench and Chatbot Arena\"], \"Citation\": []}\n[The End of Assistant's Predicted Answer]\n\n[System]\nWe would like to request your feedback on the performance of the AI assistant in response to the user question displayed above according to the gold answer. Please use the following listed aspects and their descriptions as evaluation criteria:\n    - Accuracy and Hallucinations: The assistant's answer is semantically consistent with the gold answer; The numerical value and order need to be accurate, and there should be no hallucinations.\n    - Completeness: Referring to the reference answers, the assistant's answer should contain all the key points needed to answer the user's question; further elaboration on these key points can be omitted.\nPlease rate whether this answer is suitable for the question. Please note that the gold answer can be considered as a correct answer to the question.\n\nThe assistant receives an overall score on a scale of 1 to 100, where a higher score indicates better overall performance.\nPlease note that if the assistant's answer and the gold answer fully meet the above criteria, its overall rating should be the full marks (100).\nPlease first provide a comprehensive explanation of your evaluation, avoiding any potential bias.\nThen, output a line indicating the score of the Assistant.\n\nPLEASE OUTPUT WITH THE FOLLOWING FORMAT, WHERE THE SCORE IS A SCALE OF 1 TO 100 BY STRICTLY FOLLOWING THIS FORMAT: \"[[score]]\", FOR EXAMPLE \"Rating: [[100]]\":\n<start output>\nEvaluation evidence: your evluation explanation here, no more than 100 words\nRating: [[score]]\n<end output>\n\nNow, start your evaluation:",
        "generate_response": "{\"Reference\": [\"A Survey on Evaluation of Large Language Models\", \"Vicuna: An Open-Source Chatbot Impressing GPT-4 with $90 \\% *$ ChatGPT Quality\", \"ChatLaw: Open-Source Legal Large Language Model with Integrated External Knowledge Bases\", \"Compositional Semantic Parsing with Large Language Models\", \"Open LLM Leaderboard\", \"How Close is ChatGPT to Human Experts? Comparison Corpus, Evaluation, and Detection\", \"REALM: Retrieval-Augmented Language Model PreTraining\", \"Rethinking with Retrieval: Faithful Large Language Model Inference\", \"Measuring Massive Multitask Language Understanding\", \"Atlas: Few-shot Learning with Retrieval Augmented Language Models\", \"GQA: Training Generalized Multi-Query Transformer Models from Multi-Head Checkpoints\", \"ALPACA-farm: A Simulation Framework for Methods that Learn from Human Feedback\", \"The Curious Case of Neural Text Degeneration\", \"LOLA: Low-Rank Adaptation of Large Language Models\", \"SqueezeLLM: Dense-and-Sparse Quantization\", \"Fast Inference from Transformers via Speculative Decoding\", \"MAUVE: Measuring the Gap between Neural Text and Human Text using Divergence Frontiers\", \"Efficiently Scaling Transformer Inference\", \"SpecInfer: Accelerating Generative LLM Serving with Speculative Inference and Token Tree Verification\", \"Training Language Models to Follow Instructions with Human Feedback\", \"SmoothQuant: Accurate and Efficient Post-Training Quantization for Large Language Models\", \"A Survey on Non-Autoregressive Generation for Neural Machine Translation and Beyond\", \"Do Transformers Really Perform Badly for Graph Representation?\", \"OPT: Open Pre-trained Transformer Language Models\", \"H _2 O: Heavy-Hitter Oracle for Efficient Generative Inference of Large Language Models\", \"Judging LLM-as-a-Judge with MT-Bench and Chatbot Arena\"], \"Citation\": []}",
        "eval_response": "Evaluation evidence: The assistant's answer does not match the gold answer in terms of accuracy. The reference paper listed in the gold answer is missing, and instead, a long list of unrelated papers is provided. Additionally, the citation mentioned in the gold answer is absent in the assistant's response. This indicates issues with both accuracy and completeness, as the assistant failed to identify the correct references and citations for the given paper.\n\nRating: [[10]]",
        "score": 10
    },
    {
        "level": 3,
        "set": 1,
        "length": 49739,
        "type": "paper",
        "language": "en",
        "question": "OpenMoE: An Early Effort on Open Mixture-of-Experts Language Models",
        "instruction": "We hope you will carefully study the provided papers and determine the citation relationships between them. Please follow the instructions below strictly to complete the task:\n\n#Specific Requirements:\n1. Reference: When a given paper mentions other provided papers, those other papers are considered as \"references\" for the given paper. To summarize in this specific context, references are about what the given paper is using.\n2. Citation: Conversely, when other provided papers mention the given paper in their works, the given paper is being \"cited\" by those other papers. To summarize in this specific context, citations are about who is using the given paper.\n3. Given a paper, you need to determine the citation or reference relationship between this paper and the other papers. Do not consider papers that are not provided.\n3. Please present the paper titles in a json format as follows: {{\"Reference\":[\"Reference Title 1\", \"Reference Title 2\", ..., \"Reference Title n\"], \"Citation\":[\"Citation Title 1\", \"Citation Title 2\", ..., \"Citation Title n\"]}}.\n4. If a paper does not have any references or citations, please leave the corresponding list empty, e.g.{{\"Refernce\":[]}}, {{\"Citation\":[]}}.",
        "prompt_template": "#Papers:\n{docs}\n\n{instruction}\n\n#The paper you need to analyze:\n{question}",
        "doc": [
            "2402.01739.md",
            "2405.03133.md",
            "2401.02385.md"
        ],
        "answer": {
            "Reference": [
                "# TinyLlama: An Open-Source Small Language Model "
            ],
            "Citation": [
                "# Lory: Fully Differentiable Mixture-of-Experts for Autoregressive Language Model Pre-training "
            ]
        },
        "shuffle_doc": true,
        "id": "13a4a371-6339-4c9d-82cf-fc9ab2bb017d",
        "prompt": "[Question]\n#Papers:\n{docs}\n\nWe hope you will carefully study the provided papers and determine the citation relationships between them. Please follow the instructions below strictly to complete the task:\n\n#Specific Requirements:\n1. Reference: When a given paper mentions other provided papers, those other papers are considered as \"references\" for the given paper. To summarize in this specific context, references are about what the given paper is using.\n2. Citation: Conversely, when other provided papers mention the given paper in their works, the given paper is being \"cited\" by those other papers. To summarize in this specific context, citations are about who is using the given paper.\n3. Given a paper, you need to determine the citation or reference relationship between this paper and the other papers. Do not consider papers that are not provided.\n3. Please present the paper titles in a json format as follows: {{\"Reference\":[\"Reference Title 1\", \"Reference Title 2\", ..., \"Reference Title n\"], \"Citation\":[\"Citation Title 1\", \"Citation Title 2\", ..., \"Citation Title n\"]}}.\n4. If a paper does not have any references or citations, please leave the corresponding list empty, e.g.{{\"Refernce\":[]}}, {{\"Citation\":[]}}.\n\n#The paper you need to analyze:\nOpenMoE: An Early Effort on Open Mixture-of-Experts Language Models\n\n[Gold Answer]\n{'Reference': ['# TinyLlama: An Open-Source Small Language Model '], 'Citation': ['# Lory: Fully Differentiable Mixture-of-Experts for Autoregressive Language Model Pre-training ']}\n\n[The Start of Assistant's Predicted Answer]\n{\"Reference\": [\"Palm 2 technical report\", \"(inthe)wildchat: 570k chatGPT interaction logs in the wild\", \"Beyond the imitation game: Quantifying and extrapolating the capabilities of language models\", \"Efficient training of language models to fill in the middle\", \"Findings of the 2016 conference on machine translation\", \"Language models are few-shot learners\", \"Evaluating large language models trained on code\", \"Palm: Scaling language modeling with pathways\", \"Switch transformers: Scaling to trillion parameter models with simple and efficient sparsity\", \"How does gpt obtain its ability? tracing emergent abilities of language models to their sources\", \"Megablocks: Efficient sparse training with mixture-of-experts\", \"A framework for few-shot language model evaluation, version v0.4.0\", \"Openllama: An open reproduction of llama\", \"Measuring massive multitask language understanding\", \"Training compute-optimal large language models\", \"Mixtral of experts\", \"Base layers: Simplifying training of large, sparse models\", \"Self-prompting large language models for open-domain qa\", \"Starcoder: May the source be with you!\", \"Roberta: A robustly optimized bert pretraining approach\", \"Cross-token modeling with conditional computation\", \"Multimodal contrastive learning with limoe: The language-image mixture of experts\", \"Xgen-7b technical report\", \"From sparse to soft mixtures of experts\", \"Scaling language models: Methods, analysis \\u0026 insights from training gopher\", \"Exploring the limits of transfer learning with a unified text-to-text transformer\", \"Scaling vision with sparse mixture of experts\", \"Hash layers for large sparse models\", \"Code llama: Open foundation models for code\", \"Glu variants improve transformer\", \"Outrageously large neural networks: The sparsely-gated mixture-of-experts layer\", \"Megatron-lm: Training multi-billion parameter language models using model parallelism\", \"Dolma: An Open Corpus of Three Trillion Tokens for Language Model Pretraining Research\", \"Roformer: Enhanced transformer with rotary position embedding\", \"U12: Unifying language learning paradigms\", \"Unifying language learning paradigms\", \"Llama: Open and efficient foundation language models\", \"GPT-J-6B: A 6 Billion Parameter Autoregressive Language Model\", \"CCNet: Extracting high quality monolingual datasets from web crawl data\", \"Gspmd: General and scalable parallelization for ml computation graphs\", \"One student knows all experts know: From sparse to dense\", \"Go wider instead of deeper\", \"Efficient language modeling with sparse all-mlp\", \"Tinyllama: An open-source small language model\", \"Deep long-tailed learning: A survey\", \"Judging llm-as-a-judge with mt-bench and chatbot arena\", \"Brainformers: Trading simplicity for efficiency\", \"Mixture-of-experts with expert choice routing\", \"St-moe: Designing stable and transferable sparse expert models\"], \"Citation\": []}\n[The End of Assistant's Predicted Answer]\n\n[System]\nWe would like to request your feedback on the performance of the AI assistant in response to the user question displayed above according to the gold answer. Please use the following listed aspects and their descriptions as evaluation criteria:\n    - Accuracy and Hallucinations: The assistant's answer is semantically consistent with the gold answer; The numerical value and order need to be accurate, and there should be no hallucinations.\n    - Completeness: Referring to the reference answers, the assistant's answer should contain all the key points needed to answer the user's question; further elaboration on these key points can be omitted.\nPlease rate whether this answer is suitable for the question. Please note that the gold answer can be considered as a correct answer to the question.\n\nThe assistant receives an overall score on a scale of 1 to 100, where a higher score indicates better overall performance.\nPlease note that if the assistant's answer and the gold answer fully meet the above criteria, its overall rating should be the full marks (100).\nPlease first provide a comprehensive explanation of your evaluation, avoiding any potential bias.\nThen, output a line indicating the score of the Assistant.\n\nPLEASE OUTPUT WITH THE FOLLOWING FORMAT, WHERE THE SCORE IS A SCALE OF 1 TO 100 BY STRICTLY FOLLOWING THIS FORMAT: \"[[score]]\", FOR EXAMPLE \"Rating: [[100]]\":\n<start output>\nEvaluation evidence: your evluation explanation here, no more than 100 words\nRating: [[score]]\n<end output>\n\nNow, start your evaluation:",
        "generate_response": "{\"Reference\": [\"Palm 2 technical report\", \"(inthe)wildchat: 570k chatGPT interaction logs in the wild\", \"Beyond the imitation game: Quantifying and extrapolating the capabilities of language models\", \"Efficient training of language models to fill in the middle\", \"Findings of the 2016 conference on machine translation\", \"Language models are few-shot learners\", \"Evaluating large language models trained on code\", \"Palm: Scaling language modeling with pathways\", \"Switch transformers: Scaling to trillion parameter models with simple and efficient sparsity\", \"How does gpt obtain its ability? tracing emergent abilities of language models to their sources\", \"Megablocks: Efficient sparse training with mixture-of-experts\", \"A framework for few-shot language model evaluation, version v0.4.0\", \"Openllama: An open reproduction of llama\", \"Measuring massive multitask language understanding\", \"Training compute-optimal large language models\", \"Mixtral of experts\", \"Base layers: Simplifying training of large, sparse models\", \"Self-prompting large language models for open-domain qa\", \"Starcoder: May the source be with you!\", \"Roberta: A robustly optimized bert pretraining approach\", \"Cross-token modeling with conditional computation\", \"Multimodal contrastive learning with limoe: The language-image mixture of experts\", \"Xgen-7b technical report\", \"From sparse to soft mixtures of experts\", \"Scaling language models: Methods, analysis \\u0026 insights from training gopher\", \"Exploring the limits of transfer learning with a unified text-to-text transformer\", \"Scaling vision with sparse mixture of experts\", \"Hash layers for large sparse models\", \"Code llama: Open foundation models for code\", \"Glu variants improve transformer\", \"Outrageously large neural networks: The sparsely-gated mixture-of-experts layer\", \"Megatron-lm: Training multi-billion parameter language models using model parallelism\", \"Dolma: An Open Corpus of Three Trillion Tokens for Language Model Pretraining Research\", \"Roformer: Enhanced transformer with rotary position embedding\", \"U12: Unifying language learning paradigms\", \"Unifying language learning paradigms\", \"Llama: Open and efficient foundation language models\", \"GPT-J-6B: A 6 Billion Parameter Autoregressive Language Model\", \"CCNet: Extracting high quality monolingual datasets from web crawl data\", \"Gspmd: General and scalable parallelization for ml computation graphs\", \"One student knows all experts know: From sparse to dense\", \"Go wider instead of deeper\", \"Efficient language modeling with sparse all-mlp\", \"Tinyllama: An open-source small language model\", \"Deep long-tailed learning: A survey\", \"Judging llm-as-a-judge with mt-bench and chatbot arena\", \"Brainformers: Trading simplicity for efficiency\", \"Mixture-of-experts with expert choice routing\", \"St-moe: Designing stable and transferable sparse expert models\"], \"Citation\": []}",
        "eval_response": "Evaluation evidence: The assistant's answer does not match the gold answer in terms of accuracy. The reference paper listed in the gold answer is missing from the assistant's response, and the citation paper mentioned in the gold answer is also absent. Instead, the assistant provided a long list of references that are not verified against the provided documents, indicating potential hallucinations. The response fails to accurately identify the specific references and citations for the paper in question, leading to a significant deviation from the required task.\n\nRating: [[10]]",
        "score": 10
    },
    {
        "level": 3,
        "set": 2,
        "length": 75168,
        "type": "paper",
        "language": "en",
        "question": "AUtoAcT: Automatic Agent Learning from Scratch for QA via Self-Planning",
        "instruction": "We hope you will carefully study the provided papers and determine the citation relationships between them. Please follow the instructions below strictly to complete the task:\n\n#Specific Requirements:\n1. Reference: When a given paper mentions other provided papers, those other papers are considered as \"references\" for the given paper. To summarize in this specific context, references are about what the given paper is using.\n2. Citation: Conversely, when other provided papers mention the given paper in their works, the given paper is being \"cited\" by those other papers. To summarize in this specific context, citations are about who is using the given paper.\n3. Given a paper, you need to determine the citation or reference relationship between this paper and the other papers. Do not consider papers that are not provided.\n3. Please present the paper titles in a json format as follows: {{\"Reference\":[\"Reference Title 1\", \"Reference Title 2\", ..., \"Reference Title n\"], \"Citation\":[\"Citation Title 1\", \"Citation Title 2\", ..., \"Citation Title n\"]}}.\n4. If a paper does not have any references or citations, please leave the corresponding list empty, e.g.{{\"Refernce\":[]}}, {{\"Citation\":[]}}.",
        "prompt_template": "#Papers:\n{docs}\n\n{instruction}\n\n#The paper you need to analyze:\n{question}",
        "doc": [
            "2401.05268.md",
            "2403.19154.md",
            "2403.19962.md",
            "2402.02716.md",
            "2403.08140.md"
        ],
        "answer": {
            "Reference": [
                "# Understanding the planning of LLM agents: A survey ",
                "# BAGEL: Bootstrapping Agents by Guiding Exploration with Language "
            ],
            "Citation": [
                "# STaR-GATE: <br> Teaching Language Models to Ask Clarifying Questions ",
                "# Enhancing the General Agent Capabilities of Low-Parameter LLMs through Tuning and Multi-Branch Reasoning "
            ]
        },
        "shuffle_doc": true,
        "id": "0d0f9c93-df34-4e88-86a3-93afd635f603",
        "prompt": "[Question]\n#Papers:\n{docs}\n\nWe hope you will carefully study the provided papers and determine the citation relationships between them. Please follow the instructions below strictly to complete the task:\n\n#Specific Requirements:\n1. Reference: When a given paper mentions other provided papers, those other papers are considered as \"references\" for the given paper. To summarize in this specific context, references are about what the given paper is using.\n2. Citation: Conversely, when other provided papers mention the given paper in their works, the given paper is being \"cited\" by those other papers. To summarize in this specific context, citations are about who is using the given paper.\n3. Given a paper, you need to determine the citation or reference relationship between this paper and the other papers. Do not consider papers that are not provided.\n3. Please present the paper titles in a json format as follows: {{\"Reference\":[\"Reference Title 1\", \"Reference Title 2\", ..., \"Reference Title n\"], \"Citation\":[\"Citation Title 1\", \"Citation Title 2\", ..., \"Citation Title n\"]}}.\n4. If a paper does not have any references or citations, please leave the corresponding list empty, e.g.{{\"Refernce\":[]}}, {{\"Citation\":[]}}.\n\n#The paper you need to analyze:\nAUtoAcT: Automatic Agent Learning from Scratch for QA via Self-Planning\n\n[Gold Answer]\n{'Reference': ['# Understanding the planning of LLM agents: A survey ', '# BAGEL: Bootstrapping Agents by Guiding Exploration with Language '], 'Citation': ['# STaR-GATE: <br> Teaching Language Models to Ask Clarifying Questions ', '# Enhancing the General Agent Capabilities of Low-Parameter LLMs through Tuning and Multi-Branch Reasoning ']}\n\n[The Start of Assistant's Predicted Answer]\n{\"Reference\":[\"A Survey on Evaluation of Large Language Models\", \"Vicuna: An Open-Source Chatbot Impressing GPT-4 with $90 \\% *$ ChatGPT Quality\", \"ChatLaw: Open-Source Legal Large Language Model with Integrated External Knowledge Bases\", \"Compositional Semantic Parsing with Large Language Models\", \"Open LLM Leaderboard\", \"How Close is ChatGPT to Human Experts? Comparison Corpus, Evaluation, and Detection\", \"REALM: Retrieval-Augmented Language Model PreTraining\", \"Rethinking with Retrieval: Faithful Large Language Model Inference\", \"Measuring Massive Multitask Language Understanding\", \"Atlas: Few-shot Learning with Retrieval Augmented Language Models\", \"Rest meets react: Self-improvement for multistep reasoning llm agent\", \"Fireact: Toward language agent fine-tuning\", \"Autoagents: A framework for automatic agent generation\", \"Reconcile: Round-table conference improves reasoning via consensus among diverse llms\", \"Agentverse: Facilitating multiagent collaboration and exploring emergent behaviors in agents\", \"Hugginggpt: Solving AI tasks with chatgpt and its friends in huggingface\", \"Reflexion: language agents with verbal reinforcement learning\", \"Multiagent collaboration: Harnessing the power of intelligent LLM agents\", \"Medagents: Large language models as collaborators for zero-shot medical reasoning\", \"Stanford alpaca: An instruction-following llama model\", \"Xagent: An autonomous agent for complex task solving\", \"Autogpt: build \\& use ai agents\", \"Llama 2: Open foundation and fine-tuned chat models\", \"A survey on large language model based autonomous agents\", \"Self-instruct: Aligning language models with self-generated instructions\", \"Describe, explain, plan and select: interactive planning with llms enables open-world multi-task agents\", \"Jarvis-1: Open-world multi-task agents with memoryaugmented multimodal language models\", \"Chain-of-thought prompting elicits reasoning in large language models\", \"The rise and potential of large language model based agents: A survey\", \"Rewoo: Decoupling reasoning from observations for efficient augmented language models\", \"Openagents: An open platform for language agents in the wild\", \"Webshop: Towards scalable realworld web interaction with grounded language agents\", \"React: Synergizing reasoning and acting in language models\", \"Lumos: Learning agents with unified data, modular design, and open-source llms\", \"Star: Bootstrapping reasoning with reasoning\", \"Agenttuning: Enabling generalized agent abilities for llms\", \"Igniting language intelligence: The hitchhiker's guide from chain-of-thought reasoning to language agents\", \"Judging llm-as-a-judge with mt-bench and chatbot arena\", \"Webarena: A realistic web environment for building autonomous agents\", \"Agents: An open-source framework for autonomous language agents\"], \"Citation\":[\"Enhancing the General Agent Capabilities of Low-Parameter LLMs through Tuning and Multi-Branch Reasoning\"]}\n[The End of Assistant's Predicted Answer]\n\n[System]\nWe would like to request your feedback on the performance of the AI assistant in response to the user question displayed above according to the gold answer. Please use the following listed aspects and their descriptions as evaluation criteria:\n    - Accuracy and Hallucinations: The assistant's answer is semantically consistent with the gold answer; The numerical value and order need to be accurate, and there should be no hallucinations.\n    - Completeness: Referring to the reference answers, the assistant's answer should contain all the key points needed to answer the user's question; further elaboration on these key points can be omitted.\nPlease rate whether this answer is suitable for the question. Please note that the gold answer can be considered as a correct answer to the question.\n\nThe assistant receives an overall score on a scale of 1 to 100, where a higher score indicates better overall performance.\nPlease note that if the assistant's answer and the gold answer fully meet the above criteria, its overall rating should be the full marks (100).\nPlease first provide a comprehensive explanation of your evaluation, avoiding any potential bias.\nThen, output a line indicating the score of the Assistant.\n\nPLEASE OUTPUT WITH THE FOLLOWING FORMAT, WHERE THE SCORE IS A SCALE OF 1 TO 100 BY STRICTLY FOLLOWING THIS FORMAT: \"[[score]]\", FOR EXAMPLE \"Rating: [[100]]\":\n<start output>\nEvaluation evidence: your evluation explanation here, no more than 100 words\nRating: [[score]]\n<end output>\n\nNow, start your evaluation:",
        "generate_response": "{\"Reference\":[\"A Survey on Evaluation of Large Language Models\", \"Vicuna: An Open-Source Chatbot Impressing GPT-4 with $90 \\% *$ ChatGPT Quality\", \"ChatLaw: Open-Source Legal Large Language Model with Integrated External Knowledge Bases\", \"Compositional Semantic Parsing with Large Language Models\", \"Open LLM Leaderboard\", \"How Close is ChatGPT to Human Experts? Comparison Corpus, Evaluation, and Detection\", \"REALM: Retrieval-Augmented Language Model PreTraining\", \"Rethinking with Retrieval: Faithful Large Language Model Inference\", \"Measuring Massive Multitask Language Understanding\", \"Atlas: Few-shot Learning with Retrieval Augmented Language Models\", \"Rest meets react: Self-improvement for multistep reasoning llm agent\", \"Fireact: Toward language agent fine-tuning\", \"Autoagents: A framework for automatic agent generation\", \"Reconcile: Round-table conference improves reasoning via consensus among diverse llms\", \"Agentverse: Facilitating multiagent collaboration and exploring emergent behaviors in agents\", \"Hugginggpt: Solving AI tasks with chatgpt and its friends in huggingface\", \"Reflexion: language agents with verbal reinforcement learning\", \"Multiagent collaboration: Harnessing the power of intelligent LLM agents\", \"Medagents: Large language models as collaborators for zero-shot medical reasoning\", \"Stanford alpaca: An instruction-following llama model\", \"Xagent: An autonomous agent for complex task solving\", \"Autogpt: build \\& use ai agents\", \"Llama 2: Open foundation and fine-tuned chat models\", \"A survey on large language model based autonomous agents\", \"Self-instruct: Aligning language models with self-generated instructions\", \"Describe, explain, plan and select: interactive planning with llms enables open-world multi-task agents\", \"Jarvis-1: Open-world multi-task agents with memoryaugmented multimodal language models\", \"Chain-of-thought prompting elicits reasoning in large language models\", \"The rise and potential of large language model based agents: A survey\", \"Rewoo: Decoupling reasoning from observations for efficient augmented language models\", \"Openagents: An open platform for language agents in the wild\", \"Webshop: Towards scalable realworld web interaction with grounded language agents\", \"React: Synergizing reasoning and acting in language models\", \"Lumos: Learning agents with unified data, modular design, and open-source llms\", \"Star: Bootstrapping reasoning with reasoning\", \"Agenttuning: Enabling generalized agent abilities for llms\", \"Igniting language intelligence: The hitchhiker's guide from chain-of-thought reasoning to language agents\", \"Judging llm-as-a-judge with mt-bench and chatbot arena\", \"Webarena: A realistic web environment for building autonomous agents\", \"Agents: An open-source framework for autonomous language agents\"], \"Citation\":[\"Enhancing the General Agent Capabilities of Low-Parameter LLMs through Tuning and Multi-Branch Reasoning\"]}",
        "eval_response": "Evaluation evidence: The assistant's answer does not match the gold answer in terms of accuracy and completeness. The references and citations listed by the assistant do not align with those in the gold answer, indicating a significant deviation and possible hallucination of data. The assistant's response includes a large number of unrelated papers, which were not mentioned in the gold answer, suggesting a misunderstanding of the task requirements.\n\nRating: [[10]]",
        "score": 10
    },
    {
        "level": 3,
        "set": 2,
        "length": 90335,
        "type": "paper",
        "language": "en",
        "question": "Investigating Data Contamination for Pre-training Language Models",
        "instruction": "We hope you will carefully study the provided papers and determine the citation relationships between them. Please follow the instructions below strictly to complete the task:\n\n#Specific Requirements:\n1. Reference: When a given paper mentions other provided papers, those other papers are considered as \"references\" for the given paper. To summarize in this specific context, references are about what the given paper is using.\n2. Citation: Conversely, when other provided papers mention the given paper in their works, the given paper is being \"cited\" by those other papers. To summarize in this specific context, citations are about who is using the given paper.\n3. Given a paper, you need to determine the citation or reference relationship between this paper and the other papers. Do not consider papers that are not provided.\n3. Please present the paper titles in a json format as follows: {{\"Reference\":[\"Reference Title 1\", \"Reference Title 2\", ..., \"Reference Title n\"], \"Citation\":[\"Citation Title 1\", \"Citation Title 2\", ..., \"Citation Title n\"]}}.\n4. If a paper does not have any references or citations, please leave the corresponding list empty, e.g.{{\"Refernce\":[]}}, {{\"Citation\":[]}}.",
        "prompt_template": "#Papers:\n{docs}\n\n{instruction}\n\n#The paper you need to analyze:\n{question}",
        "doc": [
            "2401.06059.md",
            "2404.18824.md",
            "2405.10632.md"
        ],
        "answer": {
            "Reference": [],
            "Citation": [
                "# Benchmarking Benchmark Leakage in Large Language Models ",
                "# Beyond Static AI Evaluations: Advancing Human Interaction Evaluations for LLM Harms and Risks "
            ]
        },
        "shuffle_doc": true,
        "id": "a62d977d-2b61-4bc7-ba0e-22ed1ae176d1",
        "prompt": "[Question]\n#Papers:\n{docs}\n\nWe hope you will carefully study the provided papers and determine the citation relationships between them. Please follow the instructions below strictly to complete the task:\n\n#Specific Requirements:\n1. Reference: When a given paper mentions other provided papers, those other papers are considered as \"references\" for the given paper. To summarize in this specific context, references are about what the given paper is using.\n2. Citation: Conversely, when other provided papers mention the given paper in their works, the given paper is being \"cited\" by those other papers. To summarize in this specific context, citations are about who is using the given paper.\n3. Given a paper, you need to determine the citation or reference relationship between this paper and the other papers. Do not consider papers that are not provided.\n3. Please present the paper titles in a json format as follows: {{\"Reference\":[\"Reference Title 1\", \"Reference Title 2\", ..., \"Reference Title n\"], \"Citation\":[\"Citation Title 1\", \"Citation Title 2\", ..., \"Citation Title n\"]}}.\n4. If a paper does not have any references or citations, please leave the corresponding list empty, e.g.{{\"Refernce\":[]}}, {{\"Citation\":[]}}.\n\n#The paper you need to analyze:\nInvestigating Data Contamination for Pre-training Language Models\n\n[Gold Answer]\n{'Reference': [], 'Citation': ['# Benchmarking Benchmark Leakage in Large Language Models ', '# Beyond Static AI Evaluations: Advancing Human Interaction Evaluations for LLM Harms and Risks ']}\n\n[The Start of Assistant's Predicted Answer]\n{\"Reference\":[\"A Survey on Evaluation of Large Language Models\", \"Vicuna: An Open-Source Chatbot Impressing GPT-4 with $90 \\% *$ ChatGPT Quality\", \"ChatLaw: Open-Source Legal Large Language Model with Integrated External Knowledge Bases\", \"Compositional Semantic Parsing with Large Language Models\", \"Open LLM Leaderboard\", \"How Close is ChatGPT to Human Experts? Comparison Corpus, Evaluation, and Detection\", \"REALM: Retrieval-Augmented Language Model PreTraining\", \"Rethinking with Retrieval: Faithful Large Language Model Inference\", \"Measuring Massive Multitask Language Understanding\", \"Towards a unified multi-dimensional evaluator for text generation\", \"Language models are few-shot learners\", \"Membership inference attacks from first principles\", \"Quantifying memorization across neural language models\", \"Extracting training data from large language models\", \"Palm: Scaling language modeling with pathways\", \"BERT: Pre-training of deep bidirectional transformers for language understanding\", \"Does learning require memorization? a short tale about a long tail\", \"The pile: An $800 \\mathrm{gb}$ dataset of diverse text for language modeling\", \"Time travel in llms: Tracing data contamination in large language models\", \"Textbooks are all you need\", \"Preventing generation of verbatim memorization in language models gives a false sense of privacy\", \"Membership inference attack susceptibility of clinical language models\", \"Pretraining language models with human preferences\", \"Textbooks are all you need ii: phi-1.5 technical report\", \"Holistic evaluation of language models\", \"ROUGE: A package for automatic evaluation of summaries\", \"Data contamination: From memorization to exploitation\", \"Membership inference on word embedding and beyond\", \"Membership inference attacks against language models via neighbourhood comparison\", \"Quantifying privacy risks of masked language models using membership inference attacks\", \"Abstractive text summarization using sequence-to-sequence RNNs and beyond\", \"Scalable extraction of training data from (production) language models\", \"Modern neural networks generalize on small data sets\", \"Gpt-4 technical report\", \"Proving test set contamination in black box language models\", \"Language models are unsupervised multitask learners\", \"Squad: 100,000+ questions for machine comprehension of text\", \"Recursive deep models for semantic compositionality over a sentiment treebank\", \"Llama: Open and efficient foundation language models\", \"Llama 2: Open foundation and fine-tuned chat models\", \"How far can camels go? exploring the state of instruction tuning on open resources\", \"Rethinking benchmark and contamination for language models with rephrased samples\", \"Towards a unified multi-dimensional evaluator for text generation\"], \"Citation\":[]}\n[The End of Assistant's Predicted Answer]\n\n[System]\nWe would like to request your feedback on the performance of the AI assistant in response to the user question displayed above according to the gold answer. Please use the following listed aspects and their descriptions as evaluation criteria:\n    - Accuracy and Hallucinations: The assistant's answer is semantically consistent with the gold answer; The numerical value and order need to be accurate, and there should be no hallucinations.\n    - Completeness: Referring to the reference answers, the assistant's answer should contain all the key points needed to answer the user's question; further elaboration on these key points can be omitted.\nPlease rate whether this answer is suitable for the question. Please note that the gold answer can be considered as a correct answer to the question.\n\nThe assistant receives an overall score on a scale of 1 to 100, where a higher score indicates better overall performance.\nPlease note that if the assistant's answer and the gold answer fully meet the above criteria, its overall rating should be the full marks (100).\nPlease first provide a comprehensive explanation of your evaluation, avoiding any potential bias.\nThen, output a line indicating the score of the Assistant.\n\nPLEASE OUTPUT WITH THE FOLLOWING FORMAT, WHERE THE SCORE IS A SCALE OF 1 TO 100 BY STRICTLY FOLLOWING THIS FORMAT: \"[[score]]\", FOR EXAMPLE \"Rating: [[100]]\":\n<start output>\nEvaluation evidence: your evluation explanation here, no more than 100 words\nRating: [[score]]\n<end output>\n\nNow, start your evaluation:",
        "generate_response": "{\"Reference\":[\"A Survey on Evaluation of Large Language Models\", \"Vicuna: An Open-Source Chatbot Impressing GPT-4 with $90 \\% *$ ChatGPT Quality\", \"ChatLaw: Open-Source Legal Large Language Model with Integrated External Knowledge Bases\", \"Compositional Semantic Parsing with Large Language Models\", \"Open LLM Leaderboard\", \"How Close is ChatGPT to Human Experts? Comparison Corpus, Evaluation, and Detection\", \"REALM: Retrieval-Augmented Language Model PreTraining\", \"Rethinking with Retrieval: Faithful Large Language Model Inference\", \"Measuring Massive Multitask Language Understanding\", \"Towards a unified multi-dimensional evaluator for text generation\", \"Language models are few-shot learners\", \"Membership inference attacks from first principles\", \"Quantifying memorization across neural language models\", \"Extracting training data from large language models\", \"Palm: Scaling language modeling with pathways\", \"BERT: Pre-training of deep bidirectional transformers for language understanding\", \"Does learning require memorization? a short tale about a long tail\", \"The pile: An $800 \\mathrm{gb}$ dataset of diverse text for language modeling\", \"Time travel in llms: Tracing data contamination in large language models\", \"Textbooks are all you need\", \"Preventing generation of verbatim memorization in language models gives a false sense of privacy\", \"Membership inference attack susceptibility of clinical language models\", \"Pretraining language models with human preferences\", \"Textbooks are all you need ii: phi-1.5 technical report\", \"Holistic evaluation of language models\", \"ROUGE: A package for automatic evaluation of summaries\", \"Data contamination: From memorization to exploitation\", \"Membership inference on word embedding and beyond\", \"Membership inference attacks against language models via neighbourhood comparison\", \"Quantifying privacy risks of masked language models using membership inference attacks\", \"Abstractive text summarization using sequence-to-sequence RNNs and beyond\", \"Scalable extraction of training data from (production) language models\", \"Modern neural networks generalize on small data sets\", \"Gpt-4 technical report\", \"Proving test set contamination in black box language models\", \"Language models are unsupervised multitask learners\", \"Squad: 100,000+ questions for machine comprehension of text\", \"Recursive deep models for semantic compositionality over a sentiment treebank\", \"Llama: Open and efficient foundation language models\", \"Llama 2: Open foundation and fine-tuned chat models\", \"How far can camels go? exploring the state of instruction tuning on open resources\", \"Rethinking benchmark and contamination for language models with rephrased samples\", \"Towards a unified multi-dimensional evaluator for text generation\"], \"Citation\":[]}",
        "eval_response": "Evaluation evidence: The assistant's answer inaccurately lists numerous references that are not mentioned in the gold answer, indicating a significant hallucination issue. The gold answer specifies no references and two citations, which the assistant fails to include, showing a lack of accuracy and completeness. The assistant's response does not align with the gold answer's citation details and incorrectly populates the reference section, leading to a substantial deviation from the task requirements.\n\nRating: [[10]]",
        "score": 10
    },
    {
        "level": 4,
        "set": 1,
        "length": 48608,
        "type": "paper",
        "language": "en",
        "question": "",
        "instruction": "We kindly ask you to thoroughly review the provided papers and construct a citation chain from them. Please adhere to the following instructions strictly while completing the task:\n\n#Task Instructions:\nGiven several papers, you are required to identify and list the longest citation chain, which demonstrates the citation relationship among the provided papers.\n\n#Specific Requirements:\n1.Please present the titles of the papers in the form of a list, as follows: [\"Title of Paper 1\", \"Title of Paper 2\", ..., \"Title of Paper n\"].\n2.Ensure that the citation chain in the list is linear and continuous, meaning that the first paper title in the list (Paper 1) should not cite any other works. Instead, it should be cited by the next paper in the list (Paper 2); subsequently, each paper should then be cited by the next one in the list, continuing up to the last paper (Paper n).\n3.Consider only the citation relationships within the supplied collection of papers, and ensure that the citation chain accurately reflects the sequential citation order among these documents.\n4.Do not take into account any articles not provided, and disregard other non-linear citation relationships.",
        "prompt_template": "{instruction}\n\n#Papers Provided:\n{docs}",
        "doc": [
            "2005.10084.md",
            "2305.00857.md",
            "2405.08582.md"
        ],
        "answer": [
            "# Context-Aware Learning to Rank with Self-Attention ",
            "# On the Impact of Outlier Bias on User Clicks ",
            "# Treatment Effect Estimation for User Interest Exploration on Recommender Systems "
        ],
        "shuffle_doc": true,
        "id": "c8d80aee-3b85-434f-a348-2b40bb7d12dd",
        "prompt": "[Question]\nWe kindly ask you to thoroughly review the provided papers and construct a citation chain from them. Please adhere to the following instructions strictly while completing the task:\n\n#Task Instructions:\nGiven several papers, you are required to identify and list the longest citation chain, which demonstrates the citation relationship among the provided papers.\n\n#Specific Requirements:\n1.Please present the titles of the papers in the form of a list, as follows: [\"Title of Paper 1\", \"Title of Paper 2\", ..., \"Title of Paper n\"].\n2.Ensure that the citation chain in the list is linear and continuous, meaning that the first paper title in the list (Paper 1) should not cite any other works. Instead, it should be cited by the next paper in the list (Paper 2); subsequently, each paper should then be cited by the next one in the list, continuing up to the last paper (Paper n).\n3.Consider only the citation relationships within the supplied collection of papers, and ensure that the citation chain accurately reflects the sequential citation order among these documents.\n4.Do not take into account any articles not provided, and disregard other non-linear citation relationships.\n\n#Papers Provided:\n{docs}\n\n[Gold Answer]\n['# Context-Aware Learning to Rank with Self-Attention ', '# On the Impact of Outlier Bias on User Clicks ', '# Treatment Effect Estimation for User Interest Exploration on Recommender Systems ']\n\n[The Start of Assistant's Predicted Answer]\n[]\n[The End of Assistant's Predicted Answer]\n\n[System]\nWe would like to request your feedback on the performance of the AI assistant in response to the user question displayed above according to the gold answer. Please use the following listed aspects and their descriptions as evaluation criteria:\n    - Accuracy and Hallucinations: The assistant's answer is semantically consistent with the gold answer; The numerical value and order need to be accurate, and there should be no hallucinations.\n    - Completeness: Referring to the reference answers, the assistant's answer should contain all the key points needed to answer the user's question; further elaboration on these key points can be omitted.\nPlease rate whether this answer is suitable for the question. Please note that the gold answer can be considered as a correct answer to the question.\n\nThe assistant receives an overall score on a scale of 1 to 100, where a higher score indicates better overall performance.\nPlease note that if the assistant's answer and the gold answer fully meet the above criteria, its overall rating should be the full marks (100).\nPlease first provide a comprehensive explanation of your evaluation, avoiding any potential bias.\nThen, output a line indicating the score of the Assistant.\n\nPLEASE OUTPUT WITH THE FOLLOWING FORMAT, WHERE THE SCORE IS A SCALE OF 1 TO 100 BY STRICTLY FOLLOWING THIS FORMAT: \"[[score]]\", FOR EXAMPLE \"Rating: [[100]]\":\n<start output>\nEvaluation evidence: your evluation explanation here, no more than 100 words\nRating: [[score]]\n<end output>\n\nNow, start your evaluation:",
        "generate_response": "[]",
        "eval_response": "Evaluation evidence: The assistant's response is missing, as indicated by the empty list in the predicted answer section. This results in a failure to meet the task requirements of identifying and listing the longest citation chain among the provided papers. The gold answer lists a specific citation chain, which the assistant should have replicated or provided a valid alternative to, based on the papers provided. The absence of any response leads to a score of zero due to a lack of accuracy, completeness, and any relevant content.\n\nRating: [[0]]",
        "score": 0
    },
    {
        "level": 4,
        "set": 3,
        "length": 109781,
        "type": "paper",
        "language": "en",
        "question": "",
        "instruction": "We kindly ask you to thoroughly review the provided papers and construct a citation chain from them. Please adhere to the following instructions strictly while completing the task:\n\n#Task Instructions:\nGiven several papers, you are required to identify and list the longest citation chain, which demonstrates the citation relationship among the provided papers.\n\n#Specific Requirements:\n1.Please present the titles of the papers in the form of a list, as follows: [\"Title of Paper 1\", \"Title of Paper 2\", ..., \"Title of Paper n\"].\n2.Ensure that the citation chain in the list is linear and continuous, meaning that the first paper title in the list (Paper 1) should not cite any other works. Instead, it should be cited by the next paper in the list (Paper 2); subsequently, each paper should then be cited by the next one in the list, continuing up to the last paper (Paper n).\n3.Consider only the citation relationships within the supplied collection of papers, and ensure that the citation chain accurately reflects the sequential citation order among these documents.\n4.Do not take into account any articles not provided, and disregard other non-linear citation relationships.",
        "prompt_template": "{instruction}\n\n#Papers Provided:\n{docs}",
        "doc": [
            "2303.08774.md",
            "2303.07678.md",
            "2305.03653.md",
            "2405.07764.md"
        ],
        "answer": [
            "# GPT-4 Technical Report ",
            "# Query2doc: Query Expansion with Large Language Models ",
            "# Query Expansion by Prompting Large Language Models ",
            "# LGDE: Local Graph-based Dictionary Expansion "
        ],
        "shuffle_doc": true,
        "id": "f1600139-fdb1-4b1f-8658-8d2529e24f9d",
        "prompt": "[Question]\nWe kindly ask you to thoroughly review the provided papers and construct a citation chain from them. Please adhere to the following instructions strictly while completing the task:\n\n#Task Instructions:\nGiven several papers, you are required to identify and list the longest citation chain, which demonstrates the citation relationship among the provided papers.\n\n#Specific Requirements:\n1.Please present the titles of the papers in the form of a list, as follows: [\"Title of Paper 1\", \"Title of Paper 2\", ..., \"Title of Paper n\"].\n2.Ensure that the citation chain in the list is linear and continuous, meaning that the first paper title in the list (Paper 1) should not cite any other works. Instead, it should be cited by the next paper in the list (Paper 2); subsequently, each paper should then be cited by the next one in the list, continuing up to the last paper (Paper n).\n3.Consider only the citation relationships within the supplied collection of papers, and ensure that the citation chain accurately reflects the sequential citation order among these documents.\n4.Do not take into account any articles not provided, and disregard other non-linear citation relationships.\n\n#Papers Provided:\n{docs}\n\n[Gold Answer]\n['# GPT-4 Technical Report ', '# Query2doc: Query Expansion with Large Language Models ', '# Query Expansion by Prompting Large Language Models ', '# LGDE: Local Graph-based Dictionary Expansion ']\n\n[The Start of Assistant's Predicted Answer]\n[\"GPT-4 Technical Report\", \"Language models are few-shot learners\", \"BERT: Pre-training of deep bidirectional transformers for language understanding\"]\n[The End of Assistant's Predicted Answer]\n\n[System]\nWe would like to request your feedback on the performance of the AI assistant in response to the user question displayed above according to the gold answer. Please use the following listed aspects and their descriptions as evaluation criteria:\n    - Accuracy and Hallucinations: The assistant's answer is semantically consistent with the gold answer; The numerical value and order need to be accurate, and there should be no hallucinations.\n    - Completeness: Referring to the reference answers, the assistant's answer should contain all the key points needed to answer the user's question; further elaboration on these key points can be omitted.\nPlease rate whether this answer is suitable for the question. Please note that the gold answer can be considered as a correct answer to the question.\n\nThe assistant receives an overall score on a scale of 1 to 100, where a higher score indicates better overall performance.\nPlease note that if the assistant's answer and the gold answer fully meet the above criteria, its overall rating should be the full marks (100).\nPlease first provide a comprehensive explanation of your evaluation, avoiding any potential bias.\nThen, output a line indicating the score of the Assistant.\n\nPLEASE OUTPUT WITH THE FOLLOWING FORMAT, WHERE THE SCORE IS A SCALE OF 1 TO 100 BY STRICTLY FOLLOWING THIS FORMAT: \"[[score]]\", FOR EXAMPLE \"Rating: [[100]]\":\n<start output>\nEvaluation evidence: your evluation explanation here, no more than 100 words\nRating: [[score]]\n<end output>\n\nNow, start your evaluation:",
        "generate_response": "[\"GPT-4 Technical Report\", \"Language models are few-shot learners\", \"BERT: Pre-training of deep bidirectional transformers for language understanding\"]",
        "eval_response": "Evaluation evidence: The assistant's answer does not match the gold answer in terms of the titles of the papers listed in the citation chain. The assistant's response includes titles that are not present in the gold answer, indicating a failure in accurately identifying the correct citation chain among the provided papers. This discrepancy suggests inaccuracies and hallucinations in the response, as it introduces elements that are not part of the correct answer. Additionally, the assistant's answer is incomplete as it does not include any of the titles from the correct citation chain provided in the gold answer.\n\nRating: [[10]]",
        "score": 10
    },
    {
        "level": 3,
        "set": 4,
        "length": 247520,
        "type": "paper",
        "language": "en",
        "question": "Extreme Compression of Large Language Models via Additive Quantization",
        "instruction": "We hope you will carefully study the provided papers and determine the citation relationships between them. Please follow the instructions below strictly to complete the task:\n\n#Specific Requirements:\n1. Reference: When a given paper mentions other provided papers, those other papers are considered as \"references\" for the given paper. To summarize in this specific context, references are about what the given paper is using.\n2. Citation: Conversely, when other provided papers mention the given paper in their works, the given paper is being \"cited\" by those other papers. To summarize in this specific context, citations are about who is using the given paper.\n3. Given a paper, you need to determine the citation or reference relationship between this paper and the other papers. Do not consider papers that are not provided.\n3. Please present the paper titles in a json format as follows: {{\"Reference\":[\"Reference Title 1\", \"Reference Title 2\", ..., \"Reference Title n\"], \"Citation\":[\"Citation Title 1\", \"Citation Title 2\", ..., \"Citation Title n\"]}}.\n4. If a paper does not have any references or citations, please leave the corresponding list empty, e.g.{{\"Refernce\":[]}}, {{\"Citation\":[]}}.",
        "prompt_template": "#Papers:\n{docs}\n\n{instruction}\n\n#The paper you need to analyze:\n{question}",
        "doc": [
            "2401.06118.md",
            "2405.15756.md",
            "2405.16406.md",
            "2405.16528.md",
            "2405.18137.md",
            "2405.18886.md",
            "2306.03078.md",
            "2306.07629.md",
            "2310.06825.md",
            "2310.16795.md",
            "2401.04088.md"
        ],
        "answer": {
            "Reference": [
                "# SpQR: A Sparse-Quantized Representation for Near-Lossless LLM Weight Compression ",
                "# SqueezeLLM: Dense-and-Sparse Quantization ",
                "# Mistral 7B ",
                "# QMoE: Practical Sub-1-Bit Compression of Trillion-Parameter Models ",
                "# Mixtral of Experts "
            ],
            "Citation": [
                "# Sparse Expansion and Neuronal Disentanglement ",
                "# SpinQuant: LLM Quantization with Learned Rotations ",
                "# LoQT: Low Rank Adapters for Quantized Training ",
                "# Exploiting LLM Quantization ",
                "# Compressing Large Language Models using Low Rank and Low Precision Decomposition "
            ]
        },
        "shuffle_doc": true,
        "id": "04e1a10d-d1c0-4502-a132-1de73ecace02",
        "prompt": "[Question]\n#Papers:\n{docs}\n\nWe hope you will carefully study the provided papers and determine the citation relationships between them. Please follow the instructions below strictly to complete the task:\n\n#Specific Requirements:\n1. Reference: When a given paper mentions other provided papers, those other papers are considered as \"references\" for the given paper. To summarize in this specific context, references are about what the given paper is using.\n2. Citation: Conversely, when other provided papers mention the given paper in their works, the given paper is being \"cited\" by those other papers. To summarize in this specific context, citations are about who is using the given paper.\n3. Given a paper, you need to determine the citation or reference relationship between this paper and the other papers. Do not consider papers that are not provided.\n3. Please present the paper titles in a json format as follows: {{\"Reference\":[\"Reference Title 1\", \"Reference Title 2\", ..., \"Reference Title n\"], \"Citation\":[\"Citation Title 1\", \"Citation Title 2\", ..., \"Citation Title n\"]}}.\n4. If a paper does not have any references or citations, please leave the corresponding list empty, e.g.{{\"Refernce\":[]}}, {{\"Citation\":[]}}.\n\n#The paper you need to analyze:\nExtreme Compression of Large Language Models via Additive Quantization\n\n[Gold Answer]\n{'Reference': ['# SpQR: A Sparse-Quantized Representation for Near-Lossless LLM Weight Compression ', '# SqueezeLLM: Dense-and-Sparse Quantization ', '# Mistral 7B ', '# QMoE: Practical Sub-1-Bit Compression of Trillion-Parameter Models ', '# Mixtral of Experts '], 'Citation': ['# Sparse Expansion and Neuronal Disentanglement ', '# SpinQuant: LLM Quantization with Learned Rotations ', '# LoQT: Low Rank Adapters for Quantized Training ', '# Exploiting LLM Quantization ', '# Compressing Large Language Models using Low Rank and Low Precision Decomposition ']}\n\n[The Start of Assistant's Predicted Answer]\n{\"Reference\": [\"QuIP: 2-Bit Quantization of Large Language Models With Guarantees\", \"Think you have Solved Question Answering? Try ARC, the AI2 Reasoning Challenge\", \"QLoRA: Efficient Finetuning of Quantized LLMs\", \"Documenting Large Webtext Corpora: A Case Study on the Colossal Clean Crawled Corpus\", \"OPTQ: Accurate Quantization for Generative Pre-trained Transformers\", \"A framework for few-shot language model evaluation\", \"LQ-LoRA: Low-rank Plus Quantized Matrix Decomposition for Efficient Language Model Finetuning\", \"How Good Are Low-bit Quantized LLaMA3 Models? An Empirical Study\", \"LORD: Low Rank Decomposition Of Monolingual Code LLMs For One-Shot Compression\", \"WinoGrande: An Adversarial Winograd Schema Challenge at Scale\", \"Dora: Weight-decomposed low-rank adaptation\", \"The era of 1-bit llms: All large language models are in 1.58 bits\", \"Pointer Sentinel Mixture Models\", \"Introducing Meta Llama 3: The most capable openly available LLM to date\", \"Up or Down? Adaptive Rounding for Post-Training Quantization\", \"Zero: Memory optimizations toward training trillion parameter models\", \"Deepspeed: System optimizations enable training deep learning models with over 100 billion parameters\", \"Matrix Compression via Randomized Low Rank and Low Precision Factorization\", \"Dither Signals and Their Effect on Quantization Noise\", \"The Truth is in There: Improving Reasoning in Language Models with Layer-Selective Rank Reduction\", \"Redpajama: an open dataset for training large language models\", \"Llama 2: Open Foundation and Fine-Tuned Chat Models\", \"QuIP#: Even Better LLM Quantization with Hadamard Incoherence and Lattice Codebooks\", \"Why are big data matrices approximately low rank?\", \"High-Dimensional Probability: An Introduction with Applications in Data Science\", \"The sphere packing problem in dimension 8\", \"GLUE: A Multi-Task Benchmark and Analysis Platform for Natural Language Understanding\", \"Optimal exact least squares rank minimization\", \"SmoothQuant: Accurate and Efficient Post-Training Quantization for Large Language Models\", \"ZeroQuant-V2: Exploring Post-training Quantization in LLMs from Comprehensive Study to Low Rank Compensation\", \"ASVD: Activation-aware Singular Value Decomposition for Compressing Large Language Models\", \"LQER: Low-Rank Quantization Error Reconstruction for LLMs\"], \"Citation\": [\"Sparse Expansion and Neuronal Disentanglement\", \"QMoE: Practical Sub-1-Bit Compression of Trillion-Parameter Models\"]}\n[The End of Assistant's Predicted Answer]\n\n[System]\nWe would like to request your feedback on the performance of the AI assistant in response to the user question displayed above according to the gold answer. Please use the following listed aspects and their descriptions as evaluation criteria:\n    - Accuracy and Hallucinations: The assistant's answer is semantically consistent with the gold answer; The numerical value and order need to be accurate, and there should be no hallucinations.\n    - Completeness: Referring to the reference answers, the assistant's answer should contain all the key points needed to answer the user's question; further elaboration on these key points can be omitted.\nPlease rate whether this answer is suitable for the question. Please note that the gold answer can be considered as a correct answer to the question.\n\nThe assistant receives an overall score on a scale of 1 to 100, where a higher score indicates better overall performance.\nPlease note that if the assistant's answer and the gold answer fully meet the above criteria, its overall rating should be the full marks (100).\nPlease first provide a comprehensive explanation of your evaluation, avoiding any potential bias.\nThen, output a line indicating the score of the Assistant.\n\nPLEASE OUTPUT WITH THE FOLLOWING FORMAT, WHERE THE SCORE IS A SCALE OF 1 TO 100 BY STRICTLY FOLLOWING THIS FORMAT: \"[[score]]\", FOR EXAMPLE \"Rating: [[100]]\":\n<start output>\nEvaluation evidence: your evluation explanation here, no more than 100 words\nRating: [[score]]\n<end output>\n\nNow, start your evaluation:",
        "generate_response": "{\"Reference\": [\"QuIP: 2-Bit Quantization of Large Language Models With Guarantees\", \"Think you have Solved Question Answering? Try ARC, the AI2 Reasoning Challenge\", \"QLoRA: Efficient Finetuning of Quantized LLMs\", \"Documenting Large Webtext Corpora: A Case Study on the Colossal Clean Crawled Corpus\", \"OPTQ: Accurate Quantization for Generative Pre-trained Transformers\", \"A framework for few-shot language model evaluation\", \"LQ-LoRA: Low-rank Plus Quantized Matrix Decomposition for Efficient Language Model Finetuning\", \"How Good Are Low-bit Quantized LLaMA3 Models? An Empirical Study\", \"LORD: Low Rank Decomposition Of Monolingual Code LLMs For One-Shot Compression\", \"WinoGrande: An Adversarial Winograd Schema Challenge at Scale\", \"Dora: Weight-decomposed low-rank adaptation\", \"The era of 1-bit llms: All large language models are in 1.58 bits\", \"Pointer Sentinel Mixture Models\", \"Introducing Meta Llama 3: The most capable openly available LLM to date\", \"Up or Down? Adaptive Rounding for Post-Training Quantization\", \"Zero: Memory optimizations toward training trillion parameter models\", \"Deepspeed: System optimizations enable training deep learning models with over 100 billion parameters\", \"Matrix Compression via Randomized Low Rank and Low Precision Factorization\", \"Dither Signals and Their Effect on Quantization Noise\", \"The Truth is in There: Improving Reasoning in Language Models with Layer-Selective Rank Reduction\", \"Redpajama: an open dataset for training large language models\", \"Llama 2: Open Foundation and Fine-Tuned Chat Models\", \"QuIP#: Even Better LLM Quantization with Hadamard Incoherence and Lattice Codebooks\", \"Why are big data matrices approximately low rank?\", \"High-Dimensional Probability: An Introduction with Applications in Data Science\", \"The sphere packing problem in dimension 8\", \"GLUE: A Multi-Task Benchmark and Analysis Platform for Natural Language Understanding\", \"Optimal exact least squares rank minimization\", \"SmoothQuant: Accurate and Efficient Post-Training Quantization for Large Language Models\", \"ZeroQuant-V2: Exploring Post-training Quantization in LLMs from Comprehensive Study to Low Rank Compensation\", \"ASVD: Activation-aware Singular Value Decomposition for Compressing Large Language Models\", \"LQER: Low-Rank Quantization Error Reconstruction for LLMs\"], \"Citation\": [\"Sparse Expansion and Neuronal Disentanglement\", \"QMoE: Practical Sub-1-Bit Compression of Trillion-Parameter Models\"]}",
        "eval_response": "Evaluation evidence: The assistant's answer does not match the gold answer in terms of accuracy and completeness. The references and citations listed are entirely different from those in the gold answer, indicating a significant deviation and possible hallucination of data. The assistant's response fails to accurately identify the correct papers that reference or cite the given paper, which is crucial for the task.\n\nRating: [[10]]",
        "score": 10
    },
    {
        "level": 3,
        "set": 2,
        "length": 78034,
        "type": "paper",
        "language": "en",
        "question": "DistServe: Disaggregating Prefill and Decoding for Goodput-optimized Large Language Model Serving",
        "instruction": "We hope you will carefully study the provided papers and determine the citation relationships between them. Please follow the instructions below strictly to complete the task:\n\n#Specific Requirements:\n1. Reference: When a given paper mentions other provided papers, those other papers are considered as \"references\" for the given paper. To summarize in this specific context, references are about what the given paper is using.\n2. Citation: Conversely, when other provided papers mention the given paper in their works, the given paper is being \"cited\" by those other papers. To summarize in this specific context, citations are about who is using the given paper.\n3. Given a paper, you need to determine the citation or reference relationship between this paper and the other papers. Do not consider papers that are not provided.\n3. Please present the paper titles in a json format as follows: {{\"Reference\":[\"Reference Title 1\", \"Reference Title 2\", ..., \"Reference Title n\"], \"Citation\":[\"Citation Title 1\", \"Citation Title 2\", ..., \"Citation Title n\"]}}.\n4. If a paper does not have any references or citations, please leave the corresponding list empty, e.g.{{\"Refernce\":[]}}, {{\"Citation\":[]}}.",
        "prompt_template": "#Papers:\n{docs}\n\n{instruction}\n\n#The paper you need to analyze:\n{question}",
        "doc": [
            "2401.09670.md",
            "2405.06856.md",
            "2405.19888.md",
            "2309.06180.md"
        ],
        "answer": {
            "Reference": [
                "# Efficient Memory Management for Large Language Model Serving with PagedAttention "
            ],
            "Citation": [
                "# Aladdin: Joint Placement and Scaling for SLO-Aware LLM Serving ",
                "# Parrot: Efficient Serving of LLM-based Applications with Semantic Variable "
            ]
        },
        "shuffle_doc": true,
        "id": "4b31bbec-cc18-4222-b027-7fd29081a55d",
        "prompt": "[Question]\n#Papers:\n{docs}\n\nWe hope you will carefully study the provided papers and determine the citation relationships between them. Please follow the instructions below strictly to complete the task:\n\n#Specific Requirements:\n1. Reference: When a given paper mentions other provided papers, those other papers are considered as \"references\" for the given paper. To summarize in this specific context, references are about what the given paper is using.\n2. Citation: Conversely, when other provided papers mention the given paper in their works, the given paper is being \"cited\" by those other papers. To summarize in this specific context, citations are about who is using the given paper.\n3. Given a paper, you need to determine the citation or reference relationship between this paper and the other papers. Do not consider papers that are not provided.\n3. Please present the paper titles in a json format as follows: {{\"Reference\":[\"Reference Title 1\", \"Reference Title 2\", ..., \"Reference Title n\"], \"Citation\":[\"Citation Title 1\", \"Citation Title 2\", ..., \"Citation Title n\"]}}.\n4. If a paper does not have any references or citations, please leave the corresponding list empty, e.g.{{\"Refernce\":[]}}, {{\"Citation\":[]}}.\n\n#The paper you need to analyze:\nDistServe: Disaggregating Prefill and Decoding for Goodput-optimized Large Language Model Serving\n\n[Gold Answer]\n{'Reference': ['# Efficient Memory Management for Large Language Model Serving with PagedAttention '], 'Citation': ['# Aladdin: Joint Placement and Scaling for SLO-Aware LLM Serving ', '# Parrot: Efficient Serving of LLM-based Applications with Semantic Variable ']}\n\n[The Start of Assistant's Predicted Answer]\n{\"Reference\":[\"Introducing chatgpt\", \"Bard, an experiment by google\", \"Inflection tech memo\", \"Lanchain usecase: Summarization\", \"Nvidia collective communications library (nccl)\", \"Serve, optimize and scale pytorch models in production\", \"Sharegpt teams\", \"Sarathi: Efficient llm inference by piggybacking decodes with chunked prefills\", \"Gqa: Training generalized multi-query transformer models from multi-head checkpoints\", \"A case for disaggregation of ml data processing\", \"Longbench: A bilingual, multitask benchmark for long context understanding\", \"Evaluating large language models trained on code\", \"Vicuna: An open-source chatbot impressing gpt-4 with $90 \\% *$ chatgpt quality\", \"Compute Express Link Consortium\", \"Fastertransformer\", \"Triton inference server: An optimized cloud and edge inferencing solution.\", \"Flashattention: Fast and memoryefficient exact attention with io-awareness\", \"Turbotransformers: an efficient gpu serving system for transformer models\", \"Serving DNNs like clockwork: Performance predictability from the bottom up\", \"Mira: A program-behavior-guided far memory system\", \"Microsecond-scale preemption for concurrent GPU-accelerated DNN inferences\", \"Inference without interference: Disaggregate 11 m inference for mixed downstream workloads\", \"Gpipe: Efficient training of giant neural networks using pipeline parallelism\", \"Sia: Heterogeneity-aware, goodput-optimized ml-cluster scheduling\", \"Efficient memory management for large language model serving with pagedattention\", \"Alpaserve: Statistical multiplexing with model parallelism for deep learning serving\", \"Ray: A distributed framework for emerging AI applications\", \"Pipedream: Generalized pipeline parallelism for dnn training\", \"Gpt-4 technical report\", \"Splitwise: Efficient generative llm inference using phase splitting\", \"Pollux: Co-adaptive cluster scheduling for goodput-optimized deep learning\", \"Zero: Memory optimizations toward training trillion parameter models\", \"Fundamentals of queueing theory\", \"Djvu: Kv-cache streaming for fast, fault-tolerant generative llm serving\", \"Hotgpt: How to make software documentation more useful with a large language model?\", \"Llama: Open and efficient foundation language models\", \"Fast distributed inference serving for large language models\", \"Orca: A distributed serving system for Transformer-Based generative models\", \"Shepherd: Serving dnns in the wild\", \"Opt: Open pre-trained transformer language models\", \"Make it real: An end-to-end implementation of a physically disaggregated data center\", \"Ft-cnn: Algorithm-based fault tolerance for convolutional neural networks\", \"Alpa: Automating inter- and Intra-Operator parallelism for distributed deep learning\", \"PetS: A unified framework for ParameterEfficient transformers serving\"], \"Citation\":[]}\n[The End of Assistant's Predicted Answer]\n\n[System]\nWe would like to request your feedback on the performance of the AI assistant in response to the user question displayed above according to the gold answer. Please use the following listed aspects and their descriptions as evaluation criteria:\n    - Accuracy and Hallucinations: The assistant's answer is semantically consistent with the gold answer; The numerical value and order need to be accurate, and there should be no hallucinations.\n    - Completeness: Referring to the reference answers, the assistant's answer should contain all the key points needed to answer the user's question; further elaboration on these key points can be omitted.\nPlease rate whether this answer is suitable for the question. Please note that the gold answer can be considered as a correct answer to the question.\n\nThe assistant receives an overall score on a scale of 1 to 100, where a higher score indicates better overall performance.\nPlease note that if the assistant's answer and the gold answer fully meet the above criteria, its overall rating should be the full marks (100).\nPlease first provide a comprehensive explanation of your evaluation, avoiding any potential bias.\nThen, output a line indicating the score of the Assistant.\n\nPLEASE OUTPUT WITH THE FOLLOWING FORMAT, WHERE THE SCORE IS A SCALE OF 1 TO 100 BY STRICTLY FOLLOWING THIS FORMAT: \"[[score]]\", FOR EXAMPLE \"Rating: [[100]]\":\n<start output>\nEvaluation evidence: your evluation explanation here, no more than 100 words\nRating: [[score]]\n<end output>\n\nNow, start your evaluation:",
        "generate_response": "{\"Reference\":[\"Introducing chatgpt\", \"Bard, an experiment by google\", \"Inflection tech memo\", \"Lanchain usecase: Summarization\", \"Nvidia collective communications library (nccl)\", \"Serve, optimize and scale pytorch models in production\", \"Sharegpt teams\", \"Sarathi: Efficient llm inference by piggybacking decodes with chunked prefills\", \"Gqa: Training generalized multi-query transformer models from multi-head checkpoints\", \"A case for disaggregation of ml data processing\", \"Longbench: A bilingual, multitask benchmark for long context understanding\", \"Evaluating large language models trained on code\", \"Vicuna: An open-source chatbot impressing gpt-4 with $90 \\% *$ chatgpt quality\", \"Compute Express Link Consortium\", \"Fastertransformer\", \"Triton inference server: An optimized cloud and edge inferencing solution.\", \"Flashattention: Fast and memoryefficient exact attention with io-awareness\", \"Turbotransformers: an efficient gpu serving system for transformer models\", \"Serving DNNs like clockwork: Performance predictability from the bottom up\", \"Mira: A program-behavior-guided far memory system\", \"Microsecond-scale preemption for concurrent GPU-accelerated DNN inferences\", \"Inference without interference: Disaggregate 11 m inference for mixed downstream workloads\", \"Gpipe: Efficient training of giant neural networks using pipeline parallelism\", \"Sia: Heterogeneity-aware, goodput-optimized ml-cluster scheduling\", \"Efficient memory management for large language model serving with pagedattention\", \"Alpaserve: Statistical multiplexing with model parallelism for deep learning serving\", \"Ray: A distributed framework for emerging AI applications\", \"Pipedream: Generalized pipeline parallelism for dnn training\", \"Gpt-4 technical report\", \"Splitwise: Efficient generative llm inference using phase splitting\", \"Pollux: Co-adaptive cluster scheduling for goodput-optimized deep learning\", \"Zero: Memory optimizations toward training trillion parameter models\", \"Fundamentals of queueing theory\", \"Djvu: Kv-cache streaming for fast, fault-tolerant generative llm serving\", \"Hotgpt: How to make software documentation more useful with a large language model?\", \"Llama: Open and efficient foundation language models\", \"Fast distributed inference serving for large language models\", \"Orca: A distributed serving system for Transformer-Based generative models\", \"Shepherd: Serving dnns in the wild\", \"Opt: Open pre-trained transformer language models\", \"Make it real: An end-to-end implementation of a physically disaggregated data center\", \"Ft-cnn: Algorithm-based fault tolerance for convolutional neural networks\", \"Alpa: Automating inter- and Intra-Operator parallelism for distributed deep learning\", \"PetS: A unified framework for ParameterEfficient transformers serving\"], \"Citation\":[]}",
        "eval_response": "Evaluation evidence: The assistant's answer does not match the gold answer in terms of accuracy and completeness. The assistant lists a large number of references that are not mentioned in the gold answer, indicating hallucinations. Additionally, the assistant fails to list any citations, whereas the gold answer lists two specific citations. This discrepancy shows a lack of accuracy and completeness in the assistant's response.\n\nRating: [[10]]",
        "score": 10
    },
    {
        "level": 3,
        "set": 3,
        "length": 143805,
        "type": "paper",
        "language": "en",
        "question": "LLM4Vuln: A Unified Evaluation Framework for Decoupling and Enhancing LLMs' Vulnerability Reasoning",
        "instruction": "We hope you will carefully study the provided papers and determine the citation relationships between them. Please follow the instructions below strictly to complete the task:\n\n#Specific Requirements:\n1. Reference: When a given paper mentions other provided papers, those other papers are considered as \"references\" for the given paper. To summarize in this specific context, references are about what the given paper is using.\n2. Citation: Conversely, when other provided papers mention the given paper in their works, the given paper is being \"cited\" by those other papers. To summarize in this specific context, citations are about who is using the given paper.\n3. Given a paper, you need to determine the citation or reference relationship between this paper and the other papers. Do not consider papers that are not provided.\n3. Please present the paper titles in a json format as follows: {{\"Reference\":[\"Reference Title 1\", \"Reference Title 2\", ..., \"Reference Title n\"], \"Citation\":[\"Citation Title 1\", \"Citation Title 2\", ..., \"Citation Title n\"]}}.\n4. If a paper does not have any references or citations, please leave the corresponding list empty, e.g.{{\"Refernce\":[]}}, {{\"Citation\":[]}}.",
        "prompt_template": "#Papers:\n{docs}\n\n{instruction}\n\n#The paper you need to analyze:\n{question}",
        "doc": [
            "2401.16185.md",
            "2405.03644.md",
            "2405.12750.md"
        ],
        "answer": {
            "Reference": [],
            "Citation": [
                "# WHEN LLMs MEET CYberSECURITY: A SYStEMATIC LITERATURE REVIEW ",
                "# Generative AI and Large Language Models for Cyber Security: All Insights You Need "
            ]
        },
        "shuffle_doc": true,
        "id": "aebd9505-2044-4893-b47d-47e4208119ef",
        "prompt": "[Question]\n#Papers:\n{docs}\n\nWe hope you will carefully study the provided papers and determine the citation relationships between them. Please follow the instructions below strictly to complete the task:\n\n#Specific Requirements:\n1. Reference: When a given paper mentions other provided papers, those other papers are considered as \"references\" for the given paper. To summarize in this specific context, references are about what the given paper is using.\n2. Citation: Conversely, when other provided papers mention the given paper in their works, the given paper is being \"cited\" by those other papers. To summarize in this specific context, citations are about who is using the given paper.\n3. Given a paper, you need to determine the citation or reference relationship between this paper and the other papers. Do not consider papers that are not provided.\n3. Please present the paper titles in a json format as follows: {{\"Reference\":[\"Reference Title 1\", \"Reference Title 2\", ..., \"Reference Title n\"], \"Citation\":[\"Citation Title 1\", \"Citation Title 2\", ..., \"Citation Title n\"]}}.\n4. If a paper does not have any references or citations, please leave the corresponding list empty, e.g.{{\"Refernce\":[]}}, {{\"Citation\":[]}}.\n\n#The paper you need to analyze:\nLLM4Vuln: A Unified Evaluation Framework for Decoupling and Enhancing LLMs' Vulnerability Reasoning\n\n[Gold Answer]\n{'Reference': [], 'Citation': ['# WHEN LLMs MEET CYberSECURITY: A SYStEMATIC LITERATURE REVIEW ', '# Generative AI and Large Language Models for Cyber Security: All Insights You Need ']}\n\n[The Start of Assistant's Predicted Answer]\n{\"Reference\":[\"A Survey on Evaluation of Large Language Models\", \"Vicuna: An Open-Source Chatbot Impressing GPT-4 with $90 \\% *$ ChatGPT Quality\", \"ChatLaw: Open-Source Legal Large Language Model with Integrated External Knowledge Bases\", \"Compositional Semantic Parsing with Large Language Models\", \"Open LLM Leaderboard\", \"How Close is ChatGPT to Human Experts? Comparison Corpus, Evaluation, and Detection\", \"REALM: Retrieval-Augmented Language Model PreTraining\", \"Rethinking with Retrieval: Faithful Large Language Model Inference\", \"Measuring Massive Multitask Language Understanding\", \"Atlas: Few-shot Learning with Retrieval Augmented Language Models\", \"Apebond_final_Secure3_Audit_Report.pdf\", \"TitanFuzz\", \"FuzzGPT\", \"Fuzz4All\", \"ChatAFL\", \"GPTScan\", \"GPTLens\", \"LLift\", \"LATTE\", \"Deep Learning Based Vulnerability Detection\", \"Low level source code vulnerability detection using advanced bert language model\", \"FlowDroid: precise context, flow, field, object-sensitive and lifecycle-aware taint analysis for Android apps\", \"KLEE Unassisted and Automatic Generation of High-Coverage Tests for Complex Systems Programs\", \"Vuldeepecker: A deep learning-based system for vulnerability detection\", \"Split and Merge: Aligning Position Biases in Large Language Model based Evaluators\", \"Len or index or count, anything but v1\": Predicting Variable Names in Decompilation Output with Transfer Learning\", \"Examining Zero-Shot Vulnerability Repair with Large Language Models\", \"Symmetry-Preserving Program Representations for Learning Code Semantics\", \"Code Llama: Open Foundation Models for Code\", \"Toolformer: Language Models Can Teach Themselves to Use Tools\", \"Neuzz: Efficient fuzzing with neural program smoothing\", \"SOK: (State of) The Art of War: Offensive Techniques in Binary Analysis\", \"GPTScan: Detecting Logic Vulnerabilities in Smart Contracts by Combining GPT with Program Analysis\", \"CSGVD: A deep learning approach combining sequence and graph embedding for source code vulnerability detection\", \"Transformer-Based Language Models for Software Vulnerability Detection\", \"LLama 2: Open Foundation and Fine-Tuned Chat Models\", \"Can Large Language Models Identify And Reason About Security Vulnerabilities? Not Yet\", \"Attention Is All You Need\", \"TaintScope: A ChecksumAware Directed Fuzzing Tool for Automatic Software Vulnerability Detection\", \"Chain-of-Thought Prompting Elicits Reasoning in Large Language Models\", \"When Program Analysis Meets Bytecode Search: Targeted and Efficient Inter-procedural Analysis of Modern Android Apps in BackDroid\", \"Fuzz4All: Universal Fuzzing with Large Language Models\", \"Neural Network-based Graph Embedding for Cross-Platform Binary Code Similarity Detection\", \"BlockScope: Detecting and Investigating Propagated Vulnerabilities in Forked Blockchain Projects\", \"Devign: Effective Vulnerability Identification by Learning Comprehensive Program Semantics via Graph Neural Networks\", \"kTrans: Knowledge-Aware Transformer for Binary Code Embedding\"], \"Citation\":[]}\n[The End of Assistant's Predicted Answer]\n\n[System]\nWe would like to request your feedback on the performance of the AI assistant in response to the user question displayed above according to the gold answer. Please use the following listed aspects and their descriptions as evaluation criteria:\n    - Accuracy and Hallucinations: The assistant's answer is semantically consistent with the gold answer; The numerical value and order need to be accurate, and there should be no hallucinations.\n    - Completeness: Referring to the reference answers, the assistant's answer should contain all the key points needed to answer the user's question; further elaboration on these key points can be omitted.\nPlease rate whether this answer is suitable for the question. Please note that the gold answer can be considered as a correct answer to the question.\n\nThe assistant receives an overall score on a scale of 1 to 100, where a higher score indicates better overall performance.\nPlease note that if the assistant's answer and the gold answer fully meet the above criteria, its overall rating should be the full marks (100).\nPlease first provide a comprehensive explanation of your evaluation, avoiding any potential bias.\nThen, output a line indicating the score of the Assistant.\n\nPLEASE OUTPUT WITH THE FOLLOWING FORMAT, WHERE THE SCORE IS A SCALE OF 1 TO 100 BY STRICTLY FOLLOWING THIS FORMAT: \"[[score]]\", FOR EXAMPLE \"Rating: [[100]]\":\n<start output>\nEvaluation evidence: your evluation explanation here, no more than 100 words\nRating: [[score]]\n<end output>\n\nNow, start your evaluation:",
        "generate_response": "{\"Reference\":[\"A Survey on Evaluation of Large Language Models\", \"Vicuna: An Open-Source Chatbot Impressing GPT-4 with $90 \\% *$ ChatGPT Quality\", \"ChatLaw: Open-Source Legal Large Language Model with Integrated External Knowledge Bases\", \"Compositional Semantic Parsing with Large Language Models\", \"Open LLM Leaderboard\", \"How Close is ChatGPT to Human Experts? Comparison Corpus, Evaluation, and Detection\", \"REALM: Retrieval-Augmented Language Model PreTraining\", \"Rethinking with Retrieval: Faithful Large Language Model Inference\", \"Measuring Massive Multitask Language Understanding\", \"Atlas: Few-shot Learning with Retrieval Augmented Language Models\", \"Apebond_final_Secure3_Audit_Report.pdf\", \"TitanFuzz\", \"FuzzGPT\", \"Fuzz4All\", \"ChatAFL\", \"GPTScan\", \"GPTLens\", \"LLift\", \"LATTE\", \"Deep Learning Based Vulnerability Detection\", \"Low level source code vulnerability detection using advanced bert language model\", \"FlowDroid: precise context, flow, field, object-sensitive and lifecycle-aware taint analysis for Android apps\", \"KLEE Unassisted and Automatic Generation of High-Coverage Tests for Complex Systems Programs\", \"Vuldeepecker: A deep learning-based system for vulnerability detection\", \"Split and Merge: Aligning Position Biases in Large Language Model based Evaluators\", \"Len or index or count, anything but v1\": Predicting Variable Names in Decompilation Output with Transfer Learning\", \"Examining Zero-Shot Vulnerability Repair with Large Language Models\", \"Symmetry-Preserving Program Representations for Learning Code Semantics\", \"Code Llama: Open Foundation Models for Code\", \"Toolformer: Language Models Can Teach Themselves to Use Tools\", \"Neuzz: Efficient fuzzing with neural program smoothing\", \"SOK: (State of) The Art of War: Offensive Techniques in Binary Analysis\", \"GPTScan: Detecting Logic Vulnerabilities in Smart Contracts by Combining GPT with Program Analysis\", \"CSGVD: A deep learning approach combining sequence and graph embedding for source code vulnerability detection\", \"Transformer-Based Language Models for Software Vulnerability Detection\", \"LLama 2: Open Foundation and Fine-Tuned Chat Models\", \"Can Large Language Models Identify And Reason About Security Vulnerabilities? Not Yet\", \"Attention Is All You Need\", \"TaintScope: A ChecksumAware Directed Fuzzing Tool for Automatic Software Vulnerability Detection\", \"Chain-of-Thought Prompting Elicits Reasoning in Large Language Models\", \"When Program Analysis Meets Bytecode Search: Targeted and Efficient Inter-procedural Analysis of Modern Android Apps in BackDroid\", \"Fuzz4All: Universal Fuzzing with Large Language Models\", \"Neural Network-based Graph Embedding for Cross-Platform Binary Code Similarity Detection\", \"BlockScope: Detecting and Investigating Propagated Vulnerabilities in Forked Blockchain Projects\", \"Devign: Effective Vulnerability Identification by Learning Comprehensive Program Semantics via Graph Neural Networks\", \"kTrans: Knowledge-Aware Transformer for Binary Code Embedding\"], \"Citation\":[]}",
        "eval_response": "Evaluation evidence: The assistant's answer does not align with the gold answer. The gold answer specifies that there are no references and two citations for the paper \"LLM4Vuln: A Unified Evaluation Framework for Decoupling and Enhancing LLMs' Vulnerability Reasoning.\" However, the assistant's answer lists numerous references that are not mentioned in the gold answer and no citations, which contradicts the gold answer. This indicates inaccuracies and hallucinations in the assistant's response.\n\nRating: [[10]]",
        "score": 10
    },
    {
        "level": 3,
        "set": 3,
        "length": 101716,
        "type": "paper",
        "language": "en",
        "question": "LLaMA Beyond English: An Empirical Study on Language Capability Transfer",
        "instruction": "We hope you will carefully study the provided papers and determine the citation relationships between them. Please follow the instructions below strictly to complete the task:\n\n#Specific Requirements:\n1. Reference: When a given paper mentions other provided papers, those other papers are considered as \"references\" for the given paper. To summarize in this specific context, references are about what the given paper is using.\n2. Citation: Conversely, when other provided papers mention the given paper in their works, the given paper is being \"cited\" by those other papers. To summarize in this specific context, citations are about who is using the given paper.\n3. Given a paper, you need to determine the citation or reference relationship between this paper and the other papers. Do not consider papers that are not provided.\n3. Please present the paper titles in a json format as follows: {{\"Reference\":[\"Reference Title 1\", \"Reference Title 2\", ..., \"Reference Title n\"], \"Citation\":[\"Citation Title 1\", \"Citation Title 2\", ..., \"Citation Title n\"]}}.\n4. If a paper does not have any references or citations, please leave the corresponding list empty, e.g.{{\"Refernce\":[]}}, {{\"Citation\":[]}}.",
        "prompt_template": "#Papers:\n{docs}\n\n{instruction}\n\n#The paper you need to analyze:\n{question}",
        "doc": [
            "2401.01055.md",
            "2405.13816.md",
            "2405.15032.md",
            "2304.08177.md",
            "2305.07004.md"
        ],
        "answer": {
            "Reference": [
                "# EFFICIENT AND EFFECTIVE TEXT ENCODING FOR CHINESE LLAMA AND ALPACA ",
                "# Not All Languages Are Created Equal in LLMs: Improving Multilingual Capability by Cross-Lingual-Thought Prompting "
            ],
            "Citation": [
                "# Large Language Models are Good Spontaneous Multilingual Learners: Is the Multilingual Annotated Data Necessary? ",
                "# 8-Aya 23: Open Weight Releases to Further Multilingual Progress "
            ]
        },
        "shuffle_doc": true,
        "id": "4632239d-9db4-4ddf-bce0-18f8f34a3bd6",
        "prompt": "[Question]\n#Papers:\n{docs}\n\nWe hope you will carefully study the provided papers and determine the citation relationships between them. Please follow the instructions below strictly to complete the task:\n\n#Specific Requirements:\n1. Reference: When a given paper mentions other provided papers, those other papers are considered as \"references\" for the given paper. To summarize in this specific context, references are about what the given paper is using.\n2. Citation: Conversely, when other provided papers mention the given paper in their works, the given paper is being \"cited\" by those other papers. To summarize in this specific context, citations are about who is using the given paper.\n3. Given a paper, you need to determine the citation or reference relationship between this paper and the other papers. Do not consider papers that are not provided.\n3. Please present the paper titles in a json format as follows: {{\"Reference\":[\"Reference Title 1\", \"Reference Title 2\", ..., \"Reference Title n\"], \"Citation\":[\"Citation Title 1\", \"Citation Title 2\", ..., \"Citation Title n\"]}}.\n4. If a paper does not have any references or citations, please leave the corresponding list empty, e.g.{{\"Refernce\":[]}}, {{\"Citation\":[]}}.\n\n#The paper you need to analyze:\nLLaMA Beyond English: An Empirical Study on Language Capability Transfer\n\n[Gold Answer]\n{'Reference': ['# EFFICIENT AND EFFECTIVE TEXT ENCODING FOR CHINESE LLAMA AND ALPACA ', '# Not All Languages Are Created Equal in LLMs: Improving Multilingual Capability by Cross-Lingual-Thought Prompting '], 'Citation': ['# Large Language Models are Good Spontaneous Multilingual Learners: Is the Multilingual Annotated Data Necessary? ', '# 8-Aya 23: Open Weight Releases to Further Multilingual Progress ']}\n\n[The Start of Assistant's Predicted Answer]\n{\"Reference\":[\"A Survey on Evaluation of Large Language Models\", \"Vicuna: An Open-Source Chatbot Impressing GPT-4 with $90 \\% *$ ChatGPT Quality\", \"Compositional Semantic Parsing with Large Language Models\", \"Open LLM Leaderboard\", \"How Close is ChatGPT to Human Experts? Comparison Corpus, Evaluation, and Detection\", \"REALM: Retrieval-Augmented Language Model PreTraining\", \"Rethinking with Retrieval: Faithful Large Language Model Inference\", \"Measuring Massive Multitask Language Understanding\", \"PaLM 2 Technical Report\", \"On the Cross-lingual Transferability of Monolingual Representations\", \"Sparks of Artificial General Intelligence: Early experiments with GPT-4\", \"NusaCrowd: Open Source Initiative for Indonesian NLP Resources\", \"Multilingual Alignment of Contextual Word Representations\", \"Towards Making the Most of Multilingual Pretraining for Zero-Shot Neural Machine Translation\", \"Finding Universal Grammatical Relations in Multilingual BERT\", \"Training Verifiers to Solve Math Word Problems\", \"Unsupervised Cross-lingual Representation Learning at Scale\", \"Emerging Cross-lingual Structure in Pretrained Language Models\", \"Free Dolly: Introducing the World's First Truly Open Instruction-Tuned LLM\", \"Chinese LLaMA and Alpaca Large Language Models\", \"Efficient and Effective Text Encoding for Chinese LLaMA and Alpaca\", \"BERT: Pre-training of Deep Bidirectional Transformers for Language Understanding\", \"A Survey on In-context Learning\", \"Identifying Elements Essential for BERT's Multilinguality\", \"Not All Languages Are Created Equal in LLMs: Improving Multilingual Capability by Cross-Lingual-Thought Prompting\", \"Language Models as Zero-Shot Planners: Extracting Actionable Knowledge for Embodied Agents\", \"C-Eval: A Multi-Level Multi-Discipline Chinese Evaluation Suite for Foundation Models\", \"BELLE: Be Everyone's Large Language model Engine\", \"X-FACTR: Multilingual Factual Knowledge Retrieval from Pretrained Language Models\", \"The State and Fate of Linguistic Diversity and Inclusion in the NLP World\", \"Gpt-4 passes the bar exam\", \"GLUECoS: An Evaluation Benchmark for Code-Switched NLP\", \"Multilingual Code-Switching for ZeroShot Cross-Lingual Intent Prediction and Slot Filling\", \"Bactrian-X : A Multilingual Replicable Instruction-Following Model with Low-Rank Adaptation\", \"Few-shot Learning with Multilingual Language Models\", \"Democratizing LLMs for Low-Resource Languages by Leveraging their English Dominant Abilities with Linguistically-Diverse Prompts\", \"Introducing ChatGPT\", \"Open-Chinese-LLaMA\", \"The RefinedWeb Dataset for Falcon LLM: Outperforming Curated Corpora with Web Data, and Web Data Only\", \"How Multilingual is Multilingual BERT?\", \"Linguistic Diversity in Natural Language Processing\", \"BLOOM: A 176BParameter Open-Access Multilingual Language Model\", \"Announcing StableCode\", \"Code-Mixing on Sesame Street: Dawn of the Adversarial Polyglots\", \"Alpaca: A Strong, Replicable Instruction-Following Model\", \"Internlm: A multilingual language model with progressively enhanced capabilities\", \"LLaMA: Open and Efficient Foundation Language Models\", \"Language Models are Few-shot Multilingual Learners\", \"Beto, Bentz, Becas: The Surprising Cross-Lingual Effectiveness of BERT\", \"Oolong: Investigating What Makes Crosslingual Transfer Hard with Controlled Studies\", \"LLMEVAL-1 Chinese Large Language Model Evaluation Phase 1\", \"Evaluating the Performance of Large Language Models on GAOKAO Benchmark\", \"AGIEval: A Human-Centric Benchmark for Evaluating Foundation Models\", \"Multilingual Machine Translation with Large Language Models: Empirical Results and Analysis\"], \"Citation\":[\"Large Language Models are Good Spontaneous Multilingual Learners: Is the Multilingual Annotated Data Necessary?\"]}\n[The End of Assistant's Predicted Answer]\n\n[System]\nWe would like to request your feedback on the performance of the AI assistant in response to the user question displayed above according to the gold answer. Please use the following listed aspects and their descriptions as evaluation criteria:\n    - Accuracy and Hallucinations: The assistant's answer is semantically consistent with the gold answer; The numerical value and order need to be accurate, and there should be no hallucinations.\n    - Completeness: Referring to the reference answers, the assistant's answer should contain all the key points needed to answer the user's question; further elaboration on these key points can be omitted.\nPlease rate whether this answer is suitable for the question. Please note that the gold answer can be considered as a correct answer to the question.\n\nThe assistant receives an overall score on a scale of 1 to 100, where a higher score indicates better overall performance.\nPlease note that if the assistant's answer and the gold answer fully meet the above criteria, its overall rating should be the full marks (100).\nPlease first provide a comprehensive explanation of your evaluation, avoiding any potential bias.\nThen, output a line indicating the score of the Assistant.\n\nPLEASE OUTPUT WITH THE FOLLOWING FORMAT, WHERE THE SCORE IS A SCALE OF 1 TO 100 BY STRICTLY FOLLOWING THIS FORMAT: \"[[score]]\", FOR EXAMPLE \"Rating: [[100]]\":\n<start output>\nEvaluation evidence: your evluation explanation here, no more than 100 words\nRating: [[score]]\n<end output>\n\nNow, start your evaluation:",
        "generate_response": "{\"Reference\":[\"A Survey on Evaluation of Large Language Models\", \"Vicuna: An Open-Source Chatbot Impressing GPT-4 with $90 \\% *$ ChatGPT Quality\", \"Compositional Semantic Parsing with Large Language Models\", \"Open LLM Leaderboard\", \"How Close is ChatGPT to Human Experts? Comparison Corpus, Evaluation, and Detection\", \"REALM: Retrieval-Augmented Language Model PreTraining\", \"Rethinking with Retrieval: Faithful Large Language Model Inference\", \"Measuring Massive Multitask Language Understanding\", \"PaLM 2 Technical Report\", \"On the Cross-lingual Transferability of Monolingual Representations\", \"Sparks of Artificial General Intelligence: Early experiments with GPT-4\", \"NusaCrowd: Open Source Initiative for Indonesian NLP Resources\", \"Multilingual Alignment of Contextual Word Representations\", \"Towards Making the Most of Multilingual Pretraining for Zero-Shot Neural Machine Translation\", \"Finding Universal Grammatical Relations in Multilingual BERT\", \"Training Verifiers to Solve Math Word Problems\", \"Unsupervised Cross-lingual Representation Learning at Scale\", \"Emerging Cross-lingual Structure in Pretrained Language Models\", \"Free Dolly: Introducing the World's First Truly Open Instruction-Tuned LLM\", \"Chinese LLaMA and Alpaca Large Language Models\", \"Efficient and Effective Text Encoding for Chinese LLaMA and Alpaca\", \"BERT: Pre-training of Deep Bidirectional Transformers for Language Understanding\", \"A Survey on In-context Learning\", \"Identifying Elements Essential for BERT's Multilinguality\", \"Not All Languages Are Created Equal in LLMs: Improving Multilingual Capability by Cross-Lingual-Thought Prompting\", \"Language Models as Zero-Shot Planners: Extracting Actionable Knowledge for Embodied Agents\", \"C-Eval: A Multi-Level Multi-Discipline Chinese Evaluation Suite for Foundation Models\", \"BELLE: Be Everyone's Large Language model Engine\", \"X-FACTR: Multilingual Factual Knowledge Retrieval from Pretrained Language Models\", \"The State and Fate of Linguistic Diversity and Inclusion in the NLP World\", \"Gpt-4 passes the bar exam\", \"GLUECoS: An Evaluation Benchmark for Code-Switched NLP\", \"Multilingual Code-Switching for ZeroShot Cross-Lingual Intent Prediction and Slot Filling\", \"Bactrian-X : A Multilingual Replicable Instruction-Following Model with Low-Rank Adaptation\", \"Few-shot Learning with Multilingual Language Models\", \"Democratizing LLMs for Low-Resource Languages by Leveraging their English Dominant Abilities with Linguistically-Diverse Prompts\", \"Introducing ChatGPT\", \"Open-Chinese-LLaMA\", \"The RefinedWeb Dataset for Falcon LLM: Outperforming Curated Corpora with Web Data, and Web Data Only\", \"How Multilingual is Multilingual BERT?\", \"Linguistic Diversity in Natural Language Processing\", \"BLOOM: A 176BParameter Open-Access Multilingual Language Model\", \"Announcing StableCode\", \"Code-Mixing on Sesame Street: Dawn of the Adversarial Polyglots\", \"Alpaca: A Strong, Replicable Instruction-Following Model\", \"Internlm: A multilingual language model with progressively enhanced capabilities\", \"LLaMA: Open and Efficient Foundation Language Models\", \"Language Models are Few-shot Multilingual Learners\", \"Beto, Bentz, Becas: The Surprising Cross-Lingual Effectiveness of BERT\", \"Oolong: Investigating What Makes Crosslingual Transfer Hard with Controlled Studies\", \"LLMEVAL-1 Chinese Large Language Model Evaluation Phase 1\", \"Evaluating the Performance of Large Language Models on GAOKAO Benchmark\", \"AGIEval: A Human-Centric Benchmark for Evaluating Foundation Models\", \"Multilingual Machine Translation with Large Language Models: Empirical Results and Analysis\"], \"Citation\":[\"Large Language Models are Good Spontaneous Multilingual Learners: Is the Multilingual Annotated Data Necessary?\"]}",
        "eval_response": "Evaluation evidence: The assistant's answer does not match the gold answer in terms of accuracy and completeness. The references and citations listed by the assistant are largely hallucinated and do not correspond to those in the gold answer. The assistant's response includes a long list of unrelated paper titles, which indicates a significant deviation from the required task. This results in both inaccurate content and a lack of completeness relative to the gold answer's specific citations and references.\n\nRating: [[10]]",
        "score": 10
    },
    {
        "level": 4,
        "set": 2,
        "length": 83370,
        "type": "paper",
        "language": "en",
        "question": "",
        "instruction": "We kindly ask you to thoroughly review the provided papers and construct a citation chain from them. Please adhere to the following instructions strictly while completing the task:\n\n#Task Instructions:\nGiven several papers, you are required to identify and list the longest citation chain, which demonstrates the citation relationship among the provided papers.\n\n#Specific Requirements:\n1.Please present the titles of the papers in the form of a list, as follows: [\"Title of Paper 1\", \"Title of Paper 2\", ..., \"Title of Paper n\"].\n2.Ensure that the citation chain in the list is linear and continuous, meaning that the first paper title in the list (Paper 1) should not cite any other works. Instead, it should be cited by the next paper in the list (Paper 2); subsequently, each paper should then be cited by the next one in the list, continuing up to the last paper (Paper n).\n3.Consider only the citation relationships within the supplied collection of papers, and ensure that the citation chain accurately reflects the sequential citation order among these documents.\n4.Do not take into account any articles not provided, and disregard other non-linear citation relationships.",
        "prompt_template": "{instruction}\n\n#Papers Provided:\n{docs}",
        "doc": [
            "2003.10555.md",
            "1910.10683.md",
            "1909.11942.md",
            "2005.12592.md",
            "2405.09711.md"
        ],
        "answer": [
            "# ELECTRA: PRE-TRAINING TEXT ENCODERS AS DISCRIMINATORS RATHER THAN GENERATORS ",
            "# Exploring the Limits of Transfer Learning with a Unified Text-to-Text Transformer ",
            "# ALBERT: A LITE BERT FOR SELF-SUPERVISED LEARNING OF LANGUAGE REPRESENTATIONS ",
            "# GECToR - Grammatical Error Correction: Tag, Not Rewrite ",
            "# STAR: A Benchmark for Situated Reasoning in Real-World Videos "
        ],
        "shuffle_doc": true,
        "id": "7784d7a8-0b8a-4eb8-ae0d-4fc0f665698a",
        "prompt": "[Question]\nWe kindly ask you to thoroughly review the provided papers and construct a citation chain from them. Please adhere to the following instructions strictly while completing the task:\n\n#Task Instructions:\nGiven several papers, you are required to identify and list the longest citation chain, which demonstrates the citation relationship among the provided papers.\n\n#Specific Requirements:\n1.Please present the titles of the papers in the form of a list, as follows: [\"Title of Paper 1\", \"Title of Paper 2\", ..., \"Title of Paper n\"].\n2.Ensure that the citation chain in the list is linear and continuous, meaning that the first paper title in the list (Paper 1) should not cite any other works. Instead, it should be cited by the next paper in the list (Paper 2); subsequently, each paper should then be cited by the next one in the list, continuing up to the last paper (Paper n).\n3.Consider only the citation relationships within the supplied collection of papers, and ensure that the citation chain accurately reflects the sequential citation order among these documents.\n4.Do not take into account any articles not provided, and disregard other non-linear citation relationships.\n\n#Papers Provided:\n{docs}\n\n[Gold Answer]\n['# ELECTRA: PRE-TRAINING TEXT ENCODERS AS DISCRIMINATORS RATHER THAN GENERATORS ', '# Exploring the Limits of Transfer Learning with a Unified Text-to-Text Transformer ', '# ALBERT: A LITE BERT FOR SELF-SUPERVISED LEARNING OF LANGUAGE REPRESENTATIONS ', '# GECToR - Grammatical Error Correction: Tag, Not Rewrite ', '# STAR: A Benchmark for Situated Reasoning in Real-World Videos ']\n\n[The Start of Assistant's Predicted Answer]\n[]\n[The End of Assistant's Predicted Answer]\n\n[System]\nWe would like to request your feedback on the performance of the AI assistant in response to the user question displayed above according to the gold answer. Please use the following listed aspects and their descriptions as evaluation criteria:\n    - Accuracy and Hallucinations: The assistant's answer is semantically consistent with the gold answer; The numerical value and order need to be accurate, and there should be no hallucinations.\n    - Completeness: Referring to the reference answers, the assistant's answer should contain all the key points needed to answer the user's question; further elaboration on these key points can be omitted.\nPlease rate whether this answer is suitable for the question. Please note that the gold answer can be considered as a correct answer to the question.\n\nThe assistant receives an overall score on a scale of 1 to 100, where a higher score indicates better overall performance.\nPlease note that if the assistant's answer and the gold answer fully meet the above criteria, its overall rating should be the full marks (100).\nPlease first provide a comprehensive explanation of your evaluation, avoiding any potential bias.\nThen, output a line indicating the score of the Assistant.\n\nPLEASE OUTPUT WITH THE FOLLOWING FORMAT, WHERE THE SCORE IS A SCALE OF 1 TO 100 BY STRICTLY FOLLOWING THIS FORMAT: \"[[score]]\", FOR EXAMPLE \"Rating: [[100]]\":\n<start output>\nEvaluation evidence: your evluation explanation here, no more than 100 words\nRating: [[score]]\n<end output>\n\nNow, start your evaluation:",
        "generate_response": "[]",
        "eval_response": "Evaluation evidence: The assistant's response is incomplete as it provided an empty list, which does not align with the gold answer or the task requirements. The task was to identify and list the longest citation chain among the provided papers, which the assistant failed to do. Therefore, the response lacks accuracy and completeness, as it does not contain any of the required elements from the task instructions.\n\nRating: [[0]]",
        "score": 0
    },
    {
        "level": 3,
        "set": 2,
        "length": 74879,
        "type": "paper",
        "language": "en",
        "question": "The Power of Noise: Redefining Retrieval for RAG Systems",
        "instruction": "We hope you will carefully study the provided papers and determine the citation relationships between them. Please follow the instructions below strictly to complete the task:\n\n#Specific Requirements:\n1. Reference: When a given paper mentions other provided papers, those other papers are considered as \"references\" for the given paper. To summarize in this specific context, references are about what the given paper is using.\n2. Citation: Conversely, when other provided papers mention the given paper in their works, the given paper is being \"cited\" by those other papers. To summarize in this specific context, citations are about who is using the given paper.\n3. Given a paper, you need to determine the citation or reference relationship between this paper and the other papers. Do not consider papers that are not provided.\n3. Please present the paper titles in a json format as follows: {{\"Reference\":[\"Reference Title 1\", \"Reference Title 2\", ..., \"Reference Title n\"], \"Citation\":[\"Citation Title 1\", \"Citation Title 2\", ..., \"Citation Title n\"]}}.\n4. If a paper does not have any references or citations, please leave the corresponding list empty, e.g.{{\"Refernce\":[]}}, {{\"Citation\":[]}}.",
        "prompt_template": "#Papers:\n{docs}\n\n{instruction}\n\n#The paper you need to analyze:\n{question}",
        "doc": [
            "2401.14887.md",
            "2405.18111.md",
            "2309.05463.md",
            "2310.12956.md"
        ],
        "answer": {
            "Reference": [
                "# Textbooks Are All You Need II: phi-1.5 technical report ",
                "# EUREKA-MOMENTS IN TRANSFORMERS: MULTI-STEP TASKS REVEAL SOFTMAX INDUCED OPTIMIZATION PROBLEMS "
            ],
            "Citation": [
                "# ATM: Adversarial Tuning Multi-agent System Makes a Robust Retrieval-Augmented GENERATOR "
            ]
        },
        "shuffle_doc": true,
        "id": "7d51d9cf-8276-4611-979f-dc1ae144502f",
        "prompt": "[Question]\n#Papers:\n{docs}\n\nWe hope you will carefully study the provided papers and determine the citation relationships between them. Please follow the instructions below strictly to complete the task:\n\n#Specific Requirements:\n1. Reference: When a given paper mentions other provided papers, those other papers are considered as \"references\" for the given paper. To summarize in this specific context, references are about what the given paper is using.\n2. Citation: Conversely, when other provided papers mention the given paper in their works, the given paper is being \"cited\" by those other papers. To summarize in this specific context, citations are about who is using the given paper.\n3. Given a paper, you need to determine the citation or reference relationship between this paper and the other papers. Do not consider papers that are not provided.\n3. Please present the paper titles in a json format as follows: {{\"Reference\":[\"Reference Title 1\", \"Reference Title 2\", ..., \"Reference Title n\"], \"Citation\":[\"Citation Title 1\", \"Citation Title 2\", ..., \"Citation Title n\"]}}.\n4. If a paper does not have any references or citations, please leave the corresponding list empty, e.g.{{\"Refernce\":[]}}, {{\"Citation\":[]}}.\n\n#The paper you need to analyze:\nThe Power of Noise: Redefining Retrieval for RAG Systems\n\n[Gold Answer]\n{'Reference': ['# Textbooks Are All You Need II: phi-1.5 technical report ', '# EUREKA-MOMENTS IN TRANSFORMERS: MULTI-STEP TASKS REVEAL SOFTMAX INDUCED OPTIMIZATION PROBLEMS '], 'Citation': ['# ATM: Adversarial Tuning Multi-agent System Makes a Robust Retrieval-Augmented GENERATOR ']}\n\n[The Start of Assistant's Predicted Answer]\n{\"Reference\": [\"The Falcon Series of Open Language Models\", \"Self-RAG: Learning to Retrieve, Generate, and Critique through Self-Reflection\", \"Entropybased attention regularization frees unintended bias mitigation from lists\", \"RRAML: Reinforced Retrieval Augmented Machine Learning\", \"Fauno: The Italian Large Language Model that will leave you senza parole!\", \"Open LLM Leaderboard\", \"Can Retriever-Augmented Language Models Reason? The Blame Game Between the Retriever and the Language Model\", \"Improving language models by retrieving from trillions of tokens\", \"Language models are few-shot learners\", \"Efficient and Effective Text Encoding for Chinese LLaMA and Alpaca\", \"The Faiss library\", \"LlamaDos\", \"Retrieval augmented language model pre-training\", \"Eureka-Moments in Transformers: Multi-Step Tasks Reveal Softmax Induced Optimization Problems\", \"Unsupervised dense information retrieval with contrastive learning\", \"Phi-2: The surprising power of small language models\", \"Llama-2-13b-chat-german\", \"Large language models struggle to learn long-tail knowledge\", \"Dense passage retrieval for opendomain question answering\", \"Dense Passage Retrieval for OpenDomain Question Answering\", \"Bridging the Preference Gap between Retrievers and LLMs\", \"BERT: Pre-training of deep bidirectional transformers for language understanding\", \"Sharp nearby, fuzzy far away: How neural language models use context\", \"Colbert: Efficient and effective passage search via contextualized late interaction over bert\", \"Dr ChatGPT tell me what I want to hear: How different prompts impact health answer correctness\", \"Natural Questions: A Benchmark for Question Answering Research\", \"Latent Retrieval for Weakly Supervised Open Domain Question Answering\", \"Retrieval-augmented generation for knowledge-intensive nlp tasks\", \"Textbooks are all you need ii: phi-1.5 technical report\", \"Lost in the middle: How language models use long contexts\", \"Fantastically Ordered Prompts and Where to Find Them: Overcoming Few-Shot Prompt Order Sensitivity\", \"Term weighting, and the vector space model\", \"Augmented language models: a survey\", \"AmbigQA: Answering Ambiguous Open-domain Questions\", \"The RefinedWeb Dataset for Falcon LLM: Outperforming Curated Corpora with Web Data, and Web Data Only\", \"Train Short, Test Long: Attention with Linear Biases Enables Input Length Extrapolation\", \"Improving language understanding by generative pre-training\", \"Language models are unsupervised multitask learners\", \"In-context retrieval-augmented language models\", \"The probabilistic relevance framework: BM25 and beyond\", \"Recipes for Building an Open-Domain Chatbot\", \"Introduction to modern information retrieval\", \"Camoscio: an Italian Instructiontuned LLaMA\", \"On the Role of Relevance in Natural Language Processing Tasks\", \"Fast Transformer Decoding: One Write-Head is All You Need\", \"Do long-range language models actually use long-range context?\", \"Introducing mpt-7b: A new standard for opensource, ly usable llms\", \"Prompt-to-OS (P2OS): Revolutionizing Operating Systems and Human-Computer Interaction with Integrated AI Generative Models\", \"Llama: Open and efficient foundation language models\", \"Llama 2: Open foundation and fine-tuned chat models\", \"Multimodal Neural Databases\", \"Attention is All you Need\", \"TL;DR: Mining Reddit to Learn Automatic Summarization\", \"OOP: Object-Oriented Programming Evaluation Benchmark for Large Language Models\", \"Adaptive chameleon or stubborn sloth: Revealing the behavior of large language models in knowledge conflicts\", \"OSWorld: Benchmarking Multimodal Agents for Open-Ended Tasks in Real Computer Environments\", \"Pretrained Transformers for Text Ranking: BERT and Beyond\", \"Stabilizing transformer training by preventing attention entropy collapse\", \"Optimizing Dense Retrieval Model Training with Hard Negatives\", \"ChatGPT Hallucinates when Attributing Answers\"], \"Citation\": []}\n[The End of Assistant's Predicted Answer]\n\n[System]\nWe would like to request your feedback on the performance of the AI assistant in response to the user question displayed above according to the gold answer. Please use the following listed aspects and their descriptions as evaluation criteria:\n    - Accuracy and Hallucinations: The assistant's answer is semantically consistent with the gold answer; The numerical value and order need to be accurate, and there should be no hallucinations.\n    - Completeness: Referring to the reference answers, the assistant's answer should contain all the key points needed to answer the user's question; further elaboration on these key points can be omitted.\nPlease rate whether this answer is suitable for the question. Please note that the gold answer can be considered as a correct answer to the question.\n\nThe assistant receives an overall score on a scale of 1 to 100, where a higher score indicates better overall performance.\nPlease note that if the assistant's answer and the gold answer fully meet the above criteria, its overall rating should be the full marks (100).\nPlease first provide a comprehensive explanation of your evaluation, avoiding any potential bias.\nThen, output a line indicating the score of the Assistant.\n\nPLEASE OUTPUT WITH THE FOLLOWING FORMAT, WHERE THE SCORE IS A SCALE OF 1 TO 100 BY STRICTLY FOLLOWING THIS FORMAT: \"[[score]]\", FOR EXAMPLE \"Rating: [[100]]\":\n<start output>\nEvaluation evidence: your evluation explanation here, no more than 100 words\nRating: [[score]]\n<end output>\n\nNow, start your evaluation:",
        "generate_response": "{\"Reference\": [\"The Falcon Series of Open Language Models\", \"Self-RAG: Learning to Retrieve, Generate, and Critique through Self-Reflection\", \"Entropybased attention regularization frees unintended bias mitigation from lists\", \"RRAML: Reinforced Retrieval Augmented Machine Learning\", \"Fauno: The Italian Large Language Model that will leave you senza parole!\", \"Open LLM Leaderboard\", \"Can Retriever-Augmented Language Models Reason? The Blame Game Between the Retriever and the Language Model\", \"Improving language models by retrieving from trillions of tokens\", \"Language models are few-shot learners\", \"Efficient and Effective Text Encoding for Chinese LLaMA and Alpaca\", \"The Faiss library\", \"LlamaDos\", \"Retrieval augmented language model pre-training\", \"Eureka-Moments in Transformers: Multi-Step Tasks Reveal Softmax Induced Optimization Problems\", \"Unsupervised dense information retrieval with contrastive learning\", \"Phi-2: The surprising power of small language models\", \"Llama-2-13b-chat-german\", \"Large language models struggle to learn long-tail knowledge\", \"Dense passage retrieval for opendomain question answering\", \"Dense Passage Retrieval for OpenDomain Question Answering\", \"Bridging the Preference Gap between Retrievers and LLMs\", \"BERT: Pre-training of deep bidirectional transformers for language understanding\", \"Sharp nearby, fuzzy far away: How neural language models use context\", \"Colbert: Efficient and effective passage search via contextualized late interaction over bert\", \"Dr ChatGPT tell me what I want to hear: How different prompts impact health answer correctness\", \"Natural Questions: A Benchmark for Question Answering Research\", \"Latent Retrieval for Weakly Supervised Open Domain Question Answering\", \"Retrieval-augmented generation for knowledge-intensive nlp tasks\", \"Textbooks are all you need ii: phi-1.5 technical report\", \"Lost in the middle: How language models use long contexts\", \"Fantastically Ordered Prompts and Where to Find Them: Overcoming Few-Shot Prompt Order Sensitivity\", \"Term weighting, and the vector space model\", \"Augmented language models: a survey\", \"AmbigQA: Answering Ambiguous Open-domain Questions\", \"The RefinedWeb Dataset for Falcon LLM: Outperforming Curated Corpora with Web Data, and Web Data Only\", \"Train Short, Test Long: Attention with Linear Biases Enables Input Length Extrapolation\", \"Improving language understanding by generative pre-training\", \"Language models are unsupervised multitask learners\", \"In-context retrieval-augmented language models\", \"The probabilistic relevance framework: BM25 and beyond\", \"Recipes for Building an Open-Domain Chatbot\", \"Introduction to modern information retrieval\", \"Camoscio: an Italian Instructiontuned LLaMA\", \"On the Role of Relevance in Natural Language Processing Tasks\", \"Fast Transformer Decoding: One Write-Head is All You Need\", \"Do long-range language models actually use long-range context?\", \"Introducing mpt-7b: A new standard for opensource, ly usable llms\", \"Prompt-to-OS (P2OS): Revolutionizing Operating Systems and Human-Computer Interaction with Integrated AI Generative Models\", \"Llama: Open and efficient foundation language models\", \"Llama 2: Open foundation and fine-tuned chat models\", \"Multimodal Neural Databases\", \"Attention is All you Need\", \"TL;DR: Mining Reddit to Learn Automatic Summarization\", \"OOP: Object-Oriented Programming Evaluation Benchmark for Large Language Models\", \"Adaptive chameleon or stubborn sloth: Revealing the behavior of large language models in knowledge conflicts\", \"OSWorld: Benchmarking Multimodal Agents for Open-Ended Tasks in Real Computer Environments\", \"Pretrained Transformers for Text Ranking: BERT and Beyond\", \"Stabilizing transformer training by preventing attention entropy collapse\", \"Optimizing Dense Retrieval Model Training with Hard Negatives\", \"ChatGPT Hallucinates when Attributing Answers\"], \"Citation\": []}",
        "eval_response": "Evaluation evidence: The assistant's answer does not match the gold answer in terms of accuracy and completeness. The assistant listed numerous papers as references which are not mentioned in the gold answer, indicating a significant hallucination issue. Additionally, the assistant failed to list any citations, whereas the gold answer specifies one citation. This discrepancy shows a lack of accuracy and completeness in the assistant's response.\n\nRating: [[10]]",
        "score": 10
    },
    {
        "level": 3,
        "set": 3,
        "length": 113681,
        "type": "paper",
        "language": "en",
        "question": "LLMs Can't Plan, But Can Help Planning in LLM-Modulo Frameworks",
        "instruction": "We hope you will carefully study the provided papers and determine the citation relationships between them. Please follow the instructions below strictly to complete the task:\n\n#Specific Requirements:\n1. Reference: When a given paper mentions other provided papers, those other papers are considered as \"references\" for the given paper. To summarize in this specific context, references are about what the given paper is using.\n2. Citation: Conversely, when other provided papers mention the given paper in their works, the given paper is being \"cited\" by those other papers. To summarize in this specific context, citations are about who is using the given paper.\n3. Given a paper, you need to determine the citation or reference relationship between this paper and the other papers. Do not consider papers that are not provided.\n3. Please present the paper titles in a json format as follows: {{\"Reference\":[\"Reference Title 1\", \"Reference Title 2\", ..., \"Reference Title n\"], \"Citation\":[\"Citation Title 1\", \"Citation Title 2\", ..., \"Citation Title n\"]}}.\n4. If a paper does not have any references or citations, please leave the corresponding list empty, e.g.{{\"Refernce\":[]}}, {{\"Citation\":[]}}.",
        "prompt_template": "#Papers:\n{docs}\n\n{instruction}\n\n#The paper you need to analyze:\n{question}",
        "doc": [
            "2402.01817.md",
            "2405.05885.md",
            "2405.16661.md",
            "2310.12931.md",
            "2401.05302.md"
        ],
        "answer": {
            "Reference": [
                "# EUREKA: HUMAN-LEVEL REWARD DESIGN VIA CODING LARGE LANGUAGE MODELS ",
                "# Theory of Mind abilities of Large Language Models in Human-Robot Interaction : An Illusion? "
            ],
            "Citation": [
                "# Co-driver: VLM-based Autonomous Driving Assistant with Human-like Behavior and Understanding for Complex Road Scenes ",
                "# RLSF: Reinforcement Learning via Symbolic Feedback "
            ]
        },
        "shuffle_doc": true,
        "id": "e070475f-fca4-4cf0-9fdb-28b8da896966",
        "prompt": "[Question]\n#Papers:\n{docs}\n\nWe hope you will carefully study the provided papers and determine the citation relationships between them. Please follow the instructions below strictly to complete the task:\n\n#Specific Requirements:\n1. Reference: When a given paper mentions other provided papers, those other papers are considered as \"references\" for the given paper. To summarize in this specific context, references are about what the given paper is using.\n2. Citation: Conversely, when other provided papers mention the given paper in their works, the given paper is being \"cited\" by those other papers. To summarize in this specific context, citations are about who is using the given paper.\n3. Given a paper, you need to determine the citation or reference relationship between this paper and the other papers. Do not consider papers that are not provided.\n3. Please present the paper titles in a json format as follows: {{\"Reference\":[\"Reference Title 1\", \"Reference Title 2\", ..., \"Reference Title n\"], \"Citation\":[\"Citation Title 1\", \"Citation Title 2\", ..., \"Citation Title n\"]}}.\n4. If a paper does not have any references or citations, please leave the corresponding list empty, e.g.{{\"Refernce\":[]}}, {{\"Citation\":[]}}.\n\n#The paper you need to analyze:\nLLMs Can't Plan, But Can Help Planning in LLM-Modulo Frameworks\n\n[Gold Answer]\n{'Reference': ['# EUREKA: HUMAN-LEVEL REWARD DESIGN VIA CODING LARGE LANGUAGE MODELS ', '# Theory of Mind abilities of Large Language Models in Human-Robot Interaction : An Illusion? '], 'Citation': ['# Co-driver: VLM-based Autonomous Driving Assistant with Human-like Behavior and Understanding for Complex Road Scenes ', '# RLSF: Reinforcement Learning via Symbolic Feedback ']}\n\n[The Start of Assistant's Predicted Answer]\n{\\n  \\\"Citation\\\": [],\\n  \\\"Reference\\\": [\\n    \\\"Bairi et al., 2023\\\",\\n    \\\"Yao et al., 2023b\\\",\\n    \\\"Shinn et al., 2023\\\",\\n    \\\"Huang et al., 2022\\\",\\n    \\\"Hao et al., 2023\\\",\\n    \\\"Valmeekam et al., 2023c\\\",\\n    \\\"Liu et al., 2023\\\",\\n    \\\"Silver et al., 2022\\\",\\n    \\\"Russell & Norvig, 2010\\\",\\n    \\\"Yao et al., 2023a\\\",\\n    \\\"Guan et al., 2023\\\",\\n    \\\"Pallagani et al., 2023\\\",\\n    \\\"Romera-Paredes et al., 2023\\\",\\n    \\\"Trinh et al., 2024\\\",\\n    \\\"Valmeekam et al., 2023a\\\",\\n    \\\"Bresina et al., 2004\\\",\\n    \\\"Valmeekam et al., 2023b\\\",\\n    \\\"Ahn et al., 2022\\\",\\n    \\\"Bubeck et al., 2023\\\",\\n    \\\"Gendron et al., 2023\\\",\\n    \\\"Guan et al., 2022\\\",\\n    \\\"Hoffmann and Nebel, 2001\\\",\\n    \\\"Howey et al., 2004\\\",\\n    \\\"Liang et al., 2023\\\",\\n    \\\"Ma et al., 2023\\\",\\n    \\\"McCoy et al., 2023\\\",\\n    \\\"McDermott et al., 1998\\\",\\n    \\\"Nieuwenhuis and Oliveras, 2006\\\",\\n    \\\"Olmo et al., 2021\\\",\\n    \\\"OpenAI, 2022\\\",\\n    \\\"OpenAI, 2023\\\",\\n    \\\"Ouyang et al., 2022\\\",\\n    \\\"Pan et al., 2023\\\",\\n    \\\"Rajvanshi et al., 2023\\\",\\n    \\\"Shridhar et al., 2021\\\",\\n    \\\"Stechly et al., 2023\\\",\\n    \\\"Ullman, 2023\\\",\\n    \\\"Wang et al., 2023\\\",\\n    \\\"Wang et al., 2022\\\",\\n    \\\"Weng et al., 2023\\\",\\n    \\\"Xie et al., 2023\\\",\\n    \\\"Yao et al., 2023c\\\",\\n    \\\"Kwon et al., 2022\\\",\\n    \\\"Ghallab et al., 2004\\\",\\n    \\\"Doyle & Patil, 1991\\\"\\n  ]\\n}\"\n[The End of Assistant's Predicted Answer]\n\n[System]\nWe would like to request your feedback on the performance of the AI assistant in response to the user question displayed above according to the gold answer. Please use the following listed aspects and their descriptions as evaluation criteria:\n    - Accuracy and Hallucinations: The assistant's answer is semantically consistent with the gold answer; The numerical value and order need to be accurate, and there should be no hallucinations.\n    - Completeness: Referring to the reference answers, the assistant's answer should contain all the key points needed to answer the user's question; further elaboration on these key points can be omitted.\nPlease rate whether this answer is suitable for the question. Please note that the gold answer can be considered as a correct answer to the question.\n\nThe assistant receives an overall score on a scale of 1 to 100, where a higher score indicates better overall performance.\nPlease note that if the assistant's answer and the gold answer fully meet the above criteria, its overall rating should be the full marks (100).\nPlease first provide a comprehensive explanation of your evaluation, avoiding any potential bias.\nThen, output a line indicating the score of the Assistant.\n\nPLEASE OUTPUT WITH THE FOLLOWING FORMAT, WHERE THE SCORE IS A SCALE OF 1 TO 100 BY STRICTLY FOLLOWING THIS FORMAT: \"[[score]]\", FOR EXAMPLE \"Rating: [[100]]\":\n<start output>\nEvaluation evidence: your evluation explanation here, no more than 100 words\nRating: [[score]]\n<end output>\n\nNow, start your evaluation:",
        "generate_response": "{\\n  \\\"Citation\\\": [],\\n  \\\"Reference\\\": [\\n    \\\"Bairi et al., 2023\\\",\\n    \\\"Yao et al., 2023b\\\",\\n    \\\"Shinn et al., 2023\\\",\\n    \\\"Huang et al., 2022\\\",\\n    \\\"Hao et al., 2023\\\",\\n    \\\"Valmeekam et al., 2023c\\\",\\n    \\\"Liu et al., 2023\\\",\\n    \\\"Silver et al., 2022\\\",\\n    \\\"Russell & Norvig, 2010\\\",\\n    \\\"Yao et al., 2023a\\\",\\n    \\\"Guan et al., 2023\\\",\\n    \\\"Pallagani et al., 2023\\\",\\n    \\\"Romera-Paredes et al., 2023\\\",\\n    \\\"Trinh et al., 2024\\\",\\n    \\\"Valmeekam et al., 2023a\\\",\\n    \\\"Bresina et al., 2004\\\",\\n    \\\"Valmeekam et al., 2023b\\\",\\n    \\\"Ahn et al., 2022\\\",\\n    \\\"Bubeck et al., 2023\\\",\\n    \\\"Gendron et al., 2023\\\",\\n    \\\"Guan et al., 2022\\\",\\n    \\\"Hoffmann and Nebel, 2001\\\",\\n    \\\"Howey et al., 2004\\\",\\n    \\\"Liang et al., 2023\\\",\\n    \\\"Ma et al., 2023\\\",\\n    \\\"McCoy et al., 2023\\\",\\n    \\\"McDermott et al., 1998\\\",\\n    \\\"Nieuwenhuis and Oliveras, 2006\\\",\\n    \\\"Olmo et al., 2021\\\",\\n    \\\"OpenAI, 2022\\\",\\n    \\\"OpenAI, 2023\\\",\\n    \\\"Ouyang et al., 2022\\\",\\n    \\\"Pan et al., 2023\\\",\\n    \\\"Rajvanshi et al., 2023\\\",\\n    \\\"Shridhar et al., 2021\\\",\\n    \\\"Stechly et al., 2023\\\",\\n    \\\"Ullman, 2023\\\",\\n    \\\"Wang et al., 2023\\\",\\n    \\\"Wang et al., 2022\\\",\\n    \\\"Weng et al., 2023\\\",\\n    \\\"Xie et al., 2023\\\",\\n    \\\"Yao et al., 2023c\\\",\\n    \\\"Kwon et al., 2022\\\",\\n    \\\"Ghallab et al., 2004\\\",\\n    \\\"Doyle & Patil, 1991\\\"\\n  ]\\n}\"",
        "eval_response": "Evaluation evidence: The assistant's answer does not match the gold answer in terms of accuracy and completeness. The assistant lists a large number of references that are not mentioned in the gold answer, indicating a significant hallucination issue. Additionally, the assistant fails to identify any citations, whereas the gold answer lists two specific citations. This discrepancy shows a lack of accuracy in identifying the correct citation and reference relationships as per the task requirements.\n\nRating: [[10]]",
        "score": 10
    },
    {
        "level": 3,
        "set": 4,
        "length": 207337,
        "type": "paper",
        "language": "en",
        "question": "Break the Sequential Dependency of LLM Inference Using LOOKAHEAD DECODING",
        "instruction": "We hope you will carefully study the provided papers and determine the citation relationships between them. Please follow the instructions below strictly to complete the task:\n\n#Specific Requirements:\n1. Reference: When a given paper mentions other provided papers, those other papers are considered as \"references\" for the given paper. To summarize in this specific context, references are about what the given paper is using.\n2. Citation: Conversely, when other provided papers mention the given paper in their works, the given paper is being \"cited\" by those other papers. To summarize in this specific context, citations are about who is using the given paper.\n3. Given a paper, you need to determine the citation or reference relationship between this paper and the other papers. Do not consider papers that are not provided.\n3. Please present the paper titles in a json format as follows: {{\"Reference\":[\"Reference Title 1\", \"Reference Title 2\", ..., \"Reference Title n\"], \"Citation\":[\"Citation Title 1\", \"Citation Title 2\", ..., \"Citation Title n\"]}}.\n4. If a paper does not have any references or citations, please leave the corresponding list empty, e.g.{{\"Refernce\":[]}}, {{\"Citation\":[]}}.",
        "prompt_template": "#Papers:\n{docs}\n\n{instruction}\n\n#The paper you need to analyze:\n{question}",
        "doc": [
            "2402.02057.md",
            "2404.18911.md",
            "2405.00263.md",
            "2405.18628.md",
            "2405.19715.md",
            "2405.20314.md",
            "2308.01861.md",
            "2308.12950.md",
            "2310.06825.md",
            "2311.08252.md",
            "2401.10774.md"
        ],
        "answer": {
            "Reference": [
                "# ClassEval: A Manually-Crafted Benchmark for Evaluating LLMs on Class-level Code Generation ",
                "# Code Llama: Open Foundation Models for Code ",
                "# Mistral 7B ",
                "# REST: Retrieval-Based Speculative Decoding ",
                "# MedusA: Simple LLM Inference Acceleration Framework with Multiple Decoding Heads "
            ],
            "Citation": [
                "# Kangaroo: Lossless Self-Speculative Decoding via Double Early Exiting ",
                "# Clover: Regressive Lightweight Speculative Decoding with Sequential Knowledge ",
                "# Hardware-Aware Parallel Prompt Decoding for Memory-Efficient Acceleration of LLM Inference ",
                "# SpecDec++: Boosting Speculative Decoding via Adaptive Candidate Lengths ",
                "# S3D: A Simple and Cost-Effective Self-Speculative Decoding Scheme for Low-Memory GPUs "
            ]
        },
        "shuffle_doc": true,
        "id": "6d6c49ef-245c-4d64-9ae1-60038df4f22f",
        "prompt": "[Question]\n#Papers:\n{docs}\n\nWe hope you will carefully study the provided papers and determine the citation relationships between them. Please follow the instructions below strictly to complete the task:\n\n#Specific Requirements:\n1. Reference: When a given paper mentions other provided papers, those other papers are considered as \"references\" for the given paper. To summarize in this specific context, references are about what the given paper is using.\n2. Citation: Conversely, when other provided papers mention the given paper in their works, the given paper is being \"cited\" by those other papers. To summarize in this specific context, citations are about who is using the given paper.\n3. Given a paper, you need to determine the citation or reference relationship between this paper and the other papers. Do not consider papers that are not provided.\n3. Please present the paper titles in a json format as follows: {{\"Reference\":[\"Reference Title 1\", \"Reference Title 2\", ..., \"Reference Title n\"], \"Citation\":[\"Citation Title 1\", \"Citation Title 2\", ..., \"Citation Title n\"]}}.\n4. If a paper does not have any references or citations, please leave the corresponding list empty, e.g.{{\"Refernce\":[]}}, {{\"Citation\":[]}}.\n\n#The paper you need to analyze:\nBreak the Sequential Dependency of LLM Inference Using LOOKAHEAD DECODING\n\n[Gold Answer]\n{'Reference': ['# ClassEval: A Manually-Crafted Benchmark for Evaluating LLMs on Class-level Code Generation ', '# Code Llama: Open Foundation Models for Code ', '# Mistral 7B ', '# REST: Retrieval-Based Speculative Decoding ', '# MedusA: Simple LLM Inference Acceleration Framework with Multiple Decoding Heads '], 'Citation': ['# Kangaroo: Lossless Self-Speculative Decoding via Double Early Exiting ', '# Clover: Regressive Lightweight Speculative Decoding with Sequential Knowledge ', '# Hardware-Aware Parallel Prompt Decoding for Memory-Efficient Acceleration of LLM Inference ', '# SpecDec++: Boosting Speculative Decoding via Adaptive Candidate Lengths ', '# S3D: A Simple and Cost-Effective Self-Speculative Decoding Scheme for Low-Memory GPUs ']}\n\n[The Start of Assistant's Predicted Answer]\n{\"Reference\":[\"A Survey on Evaluation of Large Language Models\", \"Vicuna: An Open-Source Chatbot Impressing GPT-4 with $90 \\% *$ ChatGPT Quality\", \"ChatLaw: Open-Source Legal Large Language Model with Integrated External Knowledge Bases\", \"Compositional Semantic Parsing with Large Language Models\", \"Open LLM Leaderboard\", \"How Close is ChatGPT to Human Experts? Comparison Corpus, Evaluation, and Detection\", \"REALM: Retrieval-Augmented Language Model PreTraining\", \"Rethinking with Retrieval: Faithful Large Language Model Inference\", \"Measuring Massive Multitask Language Understanding\", \"Atlas: Few-shot Learning with Retrieval Augmented Language Models\", \"Don't give me the details, just the summary! Topic-aware convolutional neural networks for extreme summarization\", \"Efficient large-scale language model training on gpu clusters using megatron-lm\", \"Gpt-4 technical report\", \"Training language models to follow instructions with human feedback\", \"Code llama: Open foundation models for code\", \"Best prompting practices for using the llama 2 chat llm through amazon sagemaker jumpstart\", \"Accelerating transformer inference for translation via parallel decoding\", \"Prompt lookup decoding\", \"Attention is all you need\", \"Transformers: State-of-the-art natural language processing\", \"Inference with reference: Lossless acceleration of large language models\", \"Judging llm-as-a-judge with mt-bench and chatbot arena\", \"Layer normalization\", \"Root mean square layer normalization\", \"Accelerating large language model decoding with speculative sampling\", \"Fast inference from transformers via speculative decoding\", \"Eagle: Lossless acceleration of llm decoding by feature extrapolation\", \"Specinfer: Accelerating generative large language model serving with speculative inference and token tree verification\", \"Get to the point: Summarization with pointer-generator networks\", \"Blockwise parallel decoding for deep autoregressive models\", \"Gemini: a family of highly capable multimodal models\", \"Llama: Open and efficient foundation language models\", \"Llama 2: Open foundation and finetuned chat models\", \"Accelerating feedforward computation via parallel nonlinear equation solving\", \"The curious case of neural text degeneration\", \"A framework for the evaluation of code generation models\", \"A manuallycrafted benchmark for evaluating llms on class-level code generation\", \"Automatic tensor parallelism for huggingface models\", \"Accelerate: Training and inference at scale made simple, efficient and adaptable\", \"Fast and memory-efficient exact attention with IO-awareness\"], \"Citation\":[]}\n[The End of Assistant's Predicted Answer]\n\n[System]\nWe would like to request your feedback on the performance of the AI assistant in response to the user question displayed above according to the gold answer. Please use the following listed aspects and their descriptions as evaluation criteria:\n    - Accuracy and Hallucinations: The assistant's answer is semantically consistent with the gold answer; The numerical value and order need to be accurate, and there should be no hallucinations.\n    - Completeness: Referring to the reference answers, the assistant's answer should contain all the key points needed to answer the user's question; further elaboration on these key points can be omitted.\nPlease rate whether this answer is suitable for the question. Please note that the gold answer can be considered as a correct answer to the question.\n\nThe assistant receives an overall score on a scale of 1 to 100, where a higher score indicates better overall performance.\nPlease note that if the assistant's answer and the gold answer fully meet the above criteria, its overall rating should be the full marks (100).\nPlease first provide a comprehensive explanation of your evaluation, avoiding any potential bias.\nThen, output a line indicating the score of the Assistant.\n\nPLEASE OUTPUT WITH THE FOLLOWING FORMAT, WHERE THE SCORE IS A SCALE OF 1 TO 100 BY STRICTLY FOLLOWING THIS FORMAT: \"[[score]]\", FOR EXAMPLE \"Rating: [[100]]\":\n<start output>\nEvaluation evidence: your evluation explanation here, no more than 100 words\nRating: [[score]]\n<end output>\n\nNow, start your evaluation:",
        "generate_response": "{\"Reference\":[\"A Survey on Evaluation of Large Language Models\", \"Vicuna: An Open-Source Chatbot Impressing GPT-4 with $90 \\% *$ ChatGPT Quality\", \"ChatLaw: Open-Source Legal Large Language Model with Integrated External Knowledge Bases\", \"Compositional Semantic Parsing with Large Language Models\", \"Open LLM Leaderboard\", \"How Close is ChatGPT to Human Experts? Comparison Corpus, Evaluation, and Detection\", \"REALM: Retrieval-Augmented Language Model PreTraining\", \"Rethinking with Retrieval: Faithful Large Language Model Inference\", \"Measuring Massive Multitask Language Understanding\", \"Atlas: Few-shot Learning with Retrieval Augmented Language Models\", \"Don't give me the details, just the summary! Topic-aware convolutional neural networks for extreme summarization\", \"Efficient large-scale language model training on gpu clusters using megatron-lm\", \"Gpt-4 technical report\", \"Training language models to follow instructions with human feedback\", \"Code llama: Open foundation models for code\", \"Best prompting practices for using the llama 2 chat llm through amazon sagemaker jumpstart\", \"Accelerating transformer inference for translation via parallel decoding\", \"Prompt lookup decoding\", \"Attention is all you need\", \"Transformers: State-of-the-art natural language processing\", \"Inference with reference: Lossless acceleration of large language models\", \"Judging llm-as-a-judge with mt-bench and chatbot arena\", \"Layer normalization\", \"Root mean square layer normalization\", \"Accelerating large language model decoding with speculative sampling\", \"Fast inference from transformers via speculative decoding\", \"Eagle: Lossless acceleration of llm decoding by feature extrapolation\", \"Specinfer: Accelerating generative large language model serving with speculative inference and token tree verification\", \"Get to the point: Summarization with pointer-generator networks\", \"Blockwise parallel decoding for deep autoregressive models\", \"Gemini: a family of highly capable multimodal models\", \"Llama: Open and efficient foundation language models\", \"Llama 2: Open foundation and finetuned chat models\", \"Accelerating feedforward computation via parallel nonlinear equation solving\", \"The curious case of neural text degeneration\", \"A framework for the evaluation of code generation models\", \"A manuallycrafted benchmark for evaluating llms on class-level code generation\", \"Automatic tensor parallelism for huggingface models\", \"Accelerate: Training and inference at scale made simple, efficient and adaptable\", \"Fast and memory-efficient exact attention with IO-awareness\"], \"Citation\":[]}",
        "eval_response": "Evaluation evidence: The assistant's answer does not match the gold answer in terms of accuracy. The references listed in the assistant's answer do not align with those in the gold answer, indicating a significant deviation and hallucination of content. Additionally, the assistant's answer fails to list any citations, whereas the gold answer provides specific citations. This discrepancy shows a lack of completeness in addressing the user's question about both references and citations for the specified paper.\n\nRating: [[10]]",
        "score": 10
    },
    {
        "level": 3,
        "set": 4,
        "length": 205379,
        "type": "paper",
        "language": "en",
        "question": "LLM Maybe LongLM: SelfExtend LLM Context Window Without Tuning",
        "instruction": "We hope you will carefully study the provided papers and determine the citation relationships between them. Please follow the instructions below strictly to complete the task:\n\n#Specific Requirements:\n1. Reference: When a given paper mentions other provided papers, those other papers are considered as \"references\" for the given paper. To summarize in this specific context, references are about what the given paper is using.\n2. Citation: Conversely, when other provided papers mention the given paper in their works, the given paper is being \"cited\" by those other papers. To summarize in this specific context, citations are about who is using the given paper.\n3. Given a paper, you need to determine the citation or reference relationship between this paper and the other papers. Do not consider papers that are not provided.\n3. Please present the paper titles in a json format as follows: {{\"Reference\":[\"Reference Title 1\", \"Reference Title 2\", ..., \"Reference Title n\"], \"Citation\":[\"Citation Title 1\", \"Citation Title 2\", ..., \"Citation Title n\"]}}.\n4. If a paper does not have any references or citations, please leave the corresponding list empty, e.g.{{\"Refernce\":[]}}, {{\"Citation\":[]}}.",
        "prompt_template": "#Papers:\n{docs}\n\n{instruction}\n\n#The paper you need to analyze:\n{question}",
        "doc": [
            "2401.01325.md",
            "2404.18311.md",
            "2405.03547.md",
            "2405.04028.md",
            "2405.14722.md",
            "2405.17602.md",
            "2309.16039.md",
            "2309.17453.md",
            "2310.06825.md",
            "2310.16450.md",
            "2312.15166.md"
        ],
        "answer": {
            "Reference": [
                "# Effective Long-Context Scaling of Foundation Models ",
                "# EFFICIENT STREAMING LANGUAGE MoDELS WITH ATTENTION SINKS ",
                "# Mistral 7B ",
                "# CLEX: CONTINUOUS LENGTH EXTRAPOLATION FOR LARGE LANGUAGE MODELS ",
                "# SOLAR 10.7B: Scaling Large Language Models with Simple yet Effective Depth Up-Scaling "
            ],
            "Citation": [
                "# Towards Incremental Learning in Large Language Models: A Critical Review ",
                "# Position: Leverage Foundational Models for Black-Box Optimization ",
                "# Masked Graph Transformer for Large-Scale Recommendation ",
                "# CAPE: Context-Adaptive Positional Encoding for Length Extrapolation ",
                "# Augmenting Textual Generation via Topology Aware Retrieval "
            ]
        },
        "shuffle_doc": true,
        "id": "b38a322a-57ba-4285-a0d8-f11195c1c269",
        "prompt": "[Question]\n#Papers:\n{docs}\n\nWe hope you will carefully study the provided papers and determine the citation relationships between them. Please follow the instructions below strictly to complete the task:\n\n#Specific Requirements:\n1. Reference: When a given paper mentions other provided papers, those other papers are considered as \"references\" for the given paper. To summarize in this specific context, references are about what the given paper is using.\n2. Citation: Conversely, when other provided papers mention the given paper in their works, the given paper is being \"cited\" by those other papers. To summarize in this specific context, citations are about who is using the given paper.\n3. Given a paper, you need to determine the citation or reference relationship between this paper and the other papers. Do not consider papers that are not provided.\n3. Please present the paper titles in a json format as follows: {{\"Reference\":[\"Reference Title 1\", \"Reference Title 2\", ..., \"Reference Title n\"], \"Citation\":[\"Citation Title 1\", \"Citation Title 2\", ..., \"Citation Title n\"]}}.\n4. If a paper does not have any references or citations, please leave the corresponding list empty, e.g.{{\"Refernce\":[]}}, {{\"Citation\":[]}}.\n\n#The paper you need to analyze:\nLLM Maybe LongLM: SelfExtend LLM Context Window Without Tuning\n\n[Gold Answer]\n{'Reference': ['# Effective Long-Context Scaling of Foundation Models ', '# EFFICIENT STREAMING LANGUAGE MoDELS WITH ATTENTION SINKS ', '# Mistral 7B ', '# CLEX: CONTINUOUS LENGTH EXTRAPOLATION FOR LARGE LANGUAGE MODELS ', '# SOLAR 10.7B: Scaling Large Language Models with Simple yet Effective Depth Up-Scaling '], 'Citation': ['# Towards Incremental Learning in Large Language Models: A Critical Review ', '# Position: Leverage Foundational Models for Black-Box Optimization ', '# Masked Graph Transformer for Large-Scale Recommendation ', '# CAPE: Context-Adaptive Positional Encoding for Length Extrapolation ', '# Augmenting Textual Generation via Topology Aware Retrieval ']}\n\n[The Start of Assistant's Predicted Answer]\n{\"Reference\":[\"A Survey on Evaluation of Large Language Models\", \"Vicuna: An Open-Source Chatbot Impressing GPT-4 with $90 \\% *$ ChatGPT Quality\", \"ChatLaw: Open-Source Legal Large Language Model with Integrated External Knowledge Bases\", \"Compositional Semantic Parsing with Large Language Models\", \"Open LLM Leaderboard\", \"How Close is ChatGPT to Human Experts? Comparison Corpus, Evaluation, and Detection\", \"REALM: Retrieval-Augmented Language Model PreTraining\", \"Rethinking with Retrieval: Faithful Large Language Model Inference\", \"Measuring Massive Multitask Language Understanding\", \"Extending context window of large language models via positional interpolation\", \"LLMTest-NeedleInAHaystack: Doing simple retrieval from llm models\", \"LM-Infinite: Simple on-the-fly length generalization for large language models\", \"Measuring massive multitask language understanding\", \"Phi-2: The surprising power of small language models\", \"Mistral 7b\", \"Sparsebert: Rethinking the importance analysis in self-attention\", \"Rectified rotary position embeddings\", \"RoFormer: Enhanced transformer with rotary position embedding\", \"A length-extrapolatable transformer\", \"Llama 2: Open foundation and finetuned chat models\", \"Attention is all you need\", \"Rethinking and improving relative position encoding for vision transformer\", \"Efficient streaming language models with attention sinks\", \"Effective long-context scaling of foundation models\", \"mt5: A massively multilingual pre-trained text-to-text transformer\", \"Harnessing the power of llms in practice: A survey on chatgpt and beyond\", \"amazon/MistralLite\", \"Big bird: Transformers for longer sequences\", \"Hellaswag: Can a machine really finish your sentence?\", \"When neural networks fail to generalize? a model sensitivity perspective\", \"Opt: Open pre-trained transformer language models\", \"A survey of large language models\"], \"Citation\":[]}\n[The End of Assistant's Predicted Answer]\n\n[System]\nWe would like to request your feedback on the performance of the AI assistant in response to the user question displayed above according to the gold answer. Please use the following listed aspects and their descriptions as evaluation criteria:\n    - Accuracy and Hallucinations: The assistant's answer is semantically consistent with the gold answer; The numerical value and order need to be accurate, and there should be no hallucinations.\n    - Completeness: Referring to the reference answers, the assistant's answer should contain all the key points needed to answer the user's question; further elaboration on these key points can be omitted.\nPlease rate whether this answer is suitable for the question. Please note that the gold answer can be considered as a correct answer to the question.\n\nThe assistant receives an overall score on a scale of 1 to 100, where a higher score indicates better overall performance.\nPlease note that if the assistant's answer and the gold answer fully meet the above criteria, its overall rating should be the full marks (100).\nPlease first provide a comprehensive explanation of your evaluation, avoiding any potential bias.\nThen, output a line indicating the score of the Assistant.\n\nPLEASE OUTPUT WITH THE FOLLOWING FORMAT, WHERE THE SCORE IS A SCALE OF 1 TO 100 BY STRICTLY FOLLOWING THIS FORMAT: \"[[score]]\", FOR EXAMPLE \"Rating: [[100]]\":\n<start output>\nEvaluation evidence: your evluation explanation here, no more than 100 words\nRating: [[score]]\n<end output>\n\nNow, start your evaluation:",
        "generate_response": "{\"Reference\":[\"A Survey on Evaluation of Large Language Models\", \"Vicuna: An Open-Source Chatbot Impressing GPT-4 with $90 \\% *$ ChatGPT Quality\", \"ChatLaw: Open-Source Legal Large Language Model with Integrated External Knowledge Bases\", \"Compositional Semantic Parsing with Large Language Models\", \"Open LLM Leaderboard\", \"How Close is ChatGPT to Human Experts? Comparison Corpus, Evaluation, and Detection\", \"REALM: Retrieval-Augmented Language Model PreTraining\", \"Rethinking with Retrieval: Faithful Large Language Model Inference\", \"Measuring Massive Multitask Language Understanding\", \"Extending context window of large language models via positional interpolation\", \"LLMTest-NeedleInAHaystack: Doing simple retrieval from llm models\", \"LM-Infinite: Simple on-the-fly length generalization for large language models\", \"Measuring massive multitask language understanding\", \"Phi-2: The surprising power of small language models\", \"Mistral 7b\", \"Sparsebert: Rethinking the importance analysis in self-attention\", \"Rectified rotary position embeddings\", \"RoFormer: Enhanced transformer with rotary position embedding\", \"A length-extrapolatable transformer\", \"Llama 2: Open foundation and finetuned chat models\", \"Attention is all you need\", \"Rethinking and improving relative position encoding for vision transformer\", \"Efficient streaming language models with attention sinks\", \"Effective long-context scaling of foundation models\", \"mt5: A massively multilingual pre-trained text-to-text transformer\", \"Harnessing the power of llms in practice: A survey on chatgpt and beyond\", \"amazon/MistralLite\", \"Big bird: Transformers for longer sequences\", \"Hellaswag: Can a machine really finish your sentence?\", \"When neural networks fail to generalize? a model sensitivity perspective\", \"Opt: Open pre-trained transformer language models\", \"A survey of large language models\"], \"Citation\":[]}",
        "eval_response": "Evaluation evidence: The assistant's answer does not match the gold answer in terms of the references listed. The gold answer specifies five specific papers as references, none of which appear in the assistant's response. Instead, the assistant lists a variety of unrelated titles, indicating a significant deviation from the required task. Additionally, the assistant fails to identify any citations, which are present in the gold answer. This discrepancy shows a lack of accuracy and completeness in the assistant's response.\n\nRating: [[10]]",
        "score": 10
    },
    {
        "level": 4,
        "set": 1,
        "length": 45765,
        "type": "paper",
        "language": "en",
        "question": "",
        "instruction": "We kindly ask you to thoroughly review the provided papers and construct a citation chain from them. Please adhere to the following instructions strictly while completing the task:\n\n#Task Instructions:\nGiven several papers, you are required to identify and list the longest citation chain, which demonstrates the citation relationship among the provided papers.\n\n#Specific Requirements:\n1.Please present the titles of the papers in the form of a list, as follows: [\"Title of Paper 1\", \"Title of Paper 2\", ..., \"Title of Paper n\"].\n2.Ensure that the citation chain in the list is linear and continuous, meaning that the first paper title in the list (Paper 1) should not cite any other works. Instead, it should be cited by the next paper in the list (Paper 2); subsequently, each paper should then be cited by the next one in the list, continuing up to the last paper (Paper n).\n3.Consider only the citation relationships within the supplied collection of papers, and ensure that the citation chain accurately reflects the sequential citation order among these documents.\n4.Do not take into account any articles not provided, and disregard other non-linear citation relationships.",
        "prompt_template": "{instruction}\n\n#Papers Provided:\n{docs}",
        "doc": [
            "2304.06793.md",
            "2310.00564.md"
        ],
        "answer": [
            "# Speck: A Smart event-based Vision Sensor with a low latency 327K Neuron Convolutional Neuronal Network Processing Pipeline ",
            "# DYNAP-SE2: a scalable multi-core dynamic neuromorphic asynchronous spiking neural network processor "
        ],
        "shuffle_doc": true,
        "id": "014ae33b-22e7-4fb7-ae34-56f64160ce74",
        "prompt": "[Question]\nWe kindly ask you to thoroughly review the provided papers and construct a citation chain from them. Please adhere to the following instructions strictly while completing the task:\n\n#Task Instructions:\nGiven several papers, you are required to identify and list the longest citation chain, which demonstrates the citation relationship among the provided papers.\n\n#Specific Requirements:\n1.Please present the titles of the papers in the form of a list, as follows: [\"Title of Paper 1\", \"Title of Paper 2\", ..., \"Title of Paper n\"].\n2.Ensure that the citation chain in the list is linear and continuous, meaning that the first paper title in the list (Paper 1) should not cite any other works. Instead, it should be cited by the next paper in the list (Paper 2); subsequently, each paper should then be cited by the next one in the list, continuing up to the last paper (Paper n).\n3.Consider only the citation relationships within the supplied collection of papers, and ensure that the citation chain accurately reflects the sequential citation order among these documents.\n4.Do not take into account any articles not provided, and disregard other non-linear citation relationships.\n\n#Papers Provided:\n{docs}\n\n[Gold Answer]\n['# Speck: A Smart event-based Vision Sensor with a low latency 327K Neuron Convolutional Neuronal Network Processing Pipeline ', '# DYNAP-SE2: a scalable multi-core dynamic neuromorphic asynchronous spiking neural network processor ']\n\n[The Start of Assistant's Predicted Answer]\n[]\n[The End of Assistant's Predicted Answer]\n\n[System]\nWe would like to request your feedback on the performance of the AI assistant in response to the user question displayed above according to the gold answer. Please use the following listed aspects and their descriptions as evaluation criteria:\n    - Accuracy and Hallucinations: The assistant's answer is semantically consistent with the gold answer; The numerical value and order need to be accurate, and there should be no hallucinations.\n    - Completeness: Referring to the reference answers, the assistant's answer should contain all the key points needed to answer the user's question; further elaboration on these key points can be omitted.\nPlease rate whether this answer is suitable for the question. Please note that the gold answer can be considered as a correct answer to the question.\n\nThe assistant receives an overall score on a scale of 1 to 100, where a higher score indicates better overall performance.\nPlease note that if the assistant's answer and the gold answer fully meet the above criteria, its overall rating should be the full marks (100).\nPlease first provide a comprehensive explanation of your evaluation, avoiding any potential bias.\nThen, output a line indicating the score of the Assistant.\n\nPLEASE OUTPUT WITH THE FOLLOWING FORMAT, WHERE THE SCORE IS A SCALE OF 1 TO 100 BY STRICTLY FOLLOWING THIS FORMAT: \"[[score]]\", FOR EXAMPLE \"Rating: [[100]]\":\n<start output>\nEvaluation evidence: your evluation explanation here, no more than 100 words\nRating: [[score]]\n<end output>\n\nNow, start your evaluation:",
        "generate_response": "[]",
        "eval_response": "Evaluation evidence: The assistant failed to provide any answer to the task of identifying and listing the longest citation chain among the provided papers. This results in a complete lack of accuracy and completeness as there is no content to evaluate against the gold answer or the task requirements. The assistant's response does not meet the necessary criteria to be considered a valid attempt at answering the question.\n\nRating: [[0]]",
        "score": 0
    },
    {
        "level": 3,
        "set": 2,
        "length": 65157,
        "type": "paper",
        "language": "en",
        "question": "EasyTool: Enhancing LLM-based Agents with Concise Tool Instruction",
        "instruction": "We hope you will carefully study the provided papers and determine the citation relationships between them. Please follow the instructions below strictly to complete the task:\n\n#Specific Requirements:\n1. Reference: When a given paper mentions other provided papers, those other papers are considered as \"references\" for the given paper. To summarize in this specific context, references are about what the given paper is using.\n2. Citation: Conversely, when other provided papers mention the given paper in their works, the given paper is being \"cited\" by those other papers. To summarize in this specific context, citations are about who is using the given paper.\n3. Given a paper, you need to determine the citation or reference relationship between this paper and the other papers. Do not consider papers that are not provided.\n3. Please present the paper titles in a json format as follows: {{\"Reference\":[\"Reference Title 1\", \"Reference Title 2\", ..., \"Reference Title n\"], \"Citation\":[\"Citation Title 1\", \"Citation Title 2\", ..., \"Citation Title n\"]}}.\n4. If a paper does not have any references or citations, please leave the corresponding list empty, e.g.{{\"Refernce\":[]}}, {{\"Citation\":[]}}.",
        "prompt_template": "#Papers:\n{docs}\n\n{instruction}\n\n#The paper you need to analyze:\n{question}",
        "doc": [
            "2401.06201.md",
            "2405.05955.md",
            "2405.16533.md",
            "2310.07075.md"
        ],
        "answer": {
            "Reference": [
                "# Don't Fine-Tune, Decode: Syntax Error-Free Tool Use via Constrained Decoding "
            ],
            "Citation": [
                "# Smurfs: Leveraging Multiple Proficiency Agents with Context-Efficiency for Tool Planning ",
                "# Chain of Tools: Large Language Model is an Automatic Multi-tool Learner "
            ]
        },
        "shuffle_doc": true,
        "id": "f947493a-511c-4cce-bd2d-7bb7c3d51ddd",
        "prompt": "[Question]\n#Papers:\n{docs}\n\nWe hope you will carefully study the provided papers and determine the citation relationships between them. Please follow the instructions below strictly to complete the task:\n\n#Specific Requirements:\n1. Reference: When a given paper mentions other provided papers, those other papers are considered as \"references\" for the given paper. To summarize in this specific context, references are about what the given paper is using.\n2. Citation: Conversely, when other provided papers mention the given paper in their works, the given paper is being \"cited\" by those other papers. To summarize in this specific context, citations are about who is using the given paper.\n3. Given a paper, you need to determine the citation or reference relationship between this paper and the other papers. Do not consider papers that are not provided.\n3. Please present the paper titles in a json format as follows: {{\"Reference\":[\"Reference Title 1\", \"Reference Title 2\", ..., \"Reference Title n\"], \"Citation\":[\"Citation Title 1\", \"Citation Title 2\", ..., \"Citation Title n\"]}}.\n4. If a paper does not have any references or citations, please leave the corresponding list empty, e.g.{{\"Refernce\":[]}}, {{\"Citation\":[]}}.\n\n#The paper you need to analyze:\nEasyTool: Enhancing LLM-based Agents with Concise Tool Instruction\n\n[Gold Answer]\n{'Reference': [\"# Don't Fine-Tune, Decode: Syntax Error-Free Tool Use via Constrained Decoding \"], 'Citation': ['# Smurfs: Leveraging Multiple Proficiency Agents with Context-Efficiency for Tool Planning ', '# Chain of Tools: Large Language Model is an Automatic Multi-tool Learner ']}\n\n[The Start of Assistant's Predicted Answer]\n{\"Reference\":[\"A Survey on Evaluation of Large Language Models\", \"Vicuna: An Open-Source Chatbot Impressing GPT-4 with $90 \\% *$ ChatGPT Quality\", \"ChatLaw: Open-Source Legal Large Language Model with Integrated External Knowledge Bases\", \"Compositional Semantic Parsing with Large Language Models\", \"Open LLM Leaderboard\", \"How Close is ChatGPT to Human Experts? Comparison Corpus, Evaluation, and Detection\", \"REALM: Retrieval-Augmented Language Model PreTraining\", \"Rethinking with Retrieval: Faithful Large Language Model Inference\", \"Measuring Massive Multitask Language Understanding\", \"Atlas: Few-shot Learning with Retrieval Augmented Language Models\", \"RAG: Retrieval-Augmented Generation\"], \"Citation\":[]}\n[The End of Assistant's Predicted Answer]\n\n[System]\nWe would like to request your feedback on the performance of the AI assistant in response to the user question displayed above according to the gold answer. Please use the following listed aspects and their descriptions as evaluation criteria:\n    - Accuracy and Hallucinations: The assistant's answer is semantically consistent with the gold answer; The numerical value and order need to be accurate, and there should be no hallucinations.\n    - Completeness: Referring to the reference answers, the assistant's answer should contain all the key points needed to answer the user's question; further elaboration on these key points can be omitted.\nPlease rate whether this answer is suitable for the question. Please note that the gold answer can be considered as a correct answer to the question.\n\nThe assistant receives an overall score on a scale of 1 to 100, where a higher score indicates better overall performance.\nPlease note that if the assistant's answer and the gold answer fully meet the above criteria, its overall rating should be the full marks (100).\nPlease first provide a comprehensive explanation of your evaluation, avoiding any potential bias.\nThen, output a line indicating the score of the Assistant.\n\nPLEASE OUTPUT WITH THE FOLLOWING FORMAT, WHERE THE SCORE IS A SCALE OF 1 TO 100 BY STRICTLY FOLLOWING THIS FORMAT: \"[[score]]\", FOR EXAMPLE \"Rating: [[100]]\":\n<start output>\nEvaluation evidence: your evluation explanation here, no more than 100 words\nRating: [[score]]\n<end output>\n\nNow, start your evaluation:",
        "generate_response": "{\"Reference\":[\"A Survey on Evaluation of Large Language Models\", \"Vicuna: An Open-Source Chatbot Impressing GPT-4 with $90 \\% *$ ChatGPT Quality\", \"ChatLaw: Open-Source Legal Large Language Model with Integrated External Knowledge Bases\", \"Compositional Semantic Parsing with Large Language Models\", \"Open LLM Leaderboard\", \"How Close is ChatGPT to Human Experts? Comparison Corpus, Evaluation, and Detection\", \"REALM: Retrieval-Augmented Language Model PreTraining\", \"Rethinking with Retrieval: Faithful Large Language Model Inference\", \"Measuring Massive Multitask Language Understanding\", \"Atlas: Few-shot Learning with Retrieval Augmented Language Models\", \"RAG: Retrieval-Augmented Generation\"], \"Citation\":[]}",
        "eval_response": "Evaluation evidence: The assistant's answer does not match the gold answer in terms of accuracy and completeness. The references listed in the assistant's answer do not include the one mentioned in the gold answer, and it incorrectly omits citations that should have been included as per the gold answer. This indicates both inaccuracies and hallucinations, as the titles listed neither reference nor cite the paper in question according to the gold standard provided.\n\nRating: [[10]]",
        "score": 10
    },
    {
        "level": 3,
        "set": 4,
        "length": 228420,
        "type": "paper",
        "language": "en",
        "question": "TravelPlanner: A Benchmark for Real-World Planning with Language Agents",
        "instruction": "We hope you will carefully study the provided papers and determine the citation relationships between them. Please follow the instructions below strictly to complete the task:\n\n#Specific Requirements:\n1. Reference: When a given paper mentions other provided papers, those other papers are considered as \"references\" for the given paper. To summarize in this specific context, references are about what the given paper is using.\n2. Citation: Conversely, when other provided papers mention the given paper in their works, the given paper is being \"cited\" by those other papers. To summarize in this specific context, citations are about who is using the given paper.\n3. Given a paper, you need to determine the citation or reference relationship between this paper and the other papers. Do not consider papers that are not provided.\n3. Please present the paper titles in a json format as follows: {{\"Reference\":[\"Reference Title 1\", \"Reference Title 2\", ..., \"Reference Title n\"], \"Citation\":[\"Citation Title 1\", \"Citation Title 2\", ..., \"Citation Title n\"]}}.\n4. If a paper does not have any references or citations, please leave the corresponding list empty, e.g.{{\"Refernce\":[]}}, {{\"Citation\":[]}}.",
        "prompt_template": "#Papers:\n{docs}\n\n{instruction}\n\n#The paper you need to analyze:\n{question}",
        "doc": [
            "2402.01622.md",
            "2404.18231.md",
            "2405.02714.md",
            "2405.16510.md",
            "2405.18208.md",
            "2405.20625.md",
            "2310.06825.md",
            "2310.10634.md",
            "2310.13227.md",
            "2401.01614.md",
            "2401.04088.md"
        ],
        "answer": {
            "Reference": [
                "# Mistral 7B ",
                "# OpenAgents: AN OPEN PlATFORM FOR LANGUAGE AGENTS IN THE WILD ",
                "# ToolChaIN*: EFFiCIENT ACtion SpaCe NAVIGATION in Large Language ModelS With A* Search ",
                "# GPT-4V(ision) is a Generalist Web Agent, if Grounded ",
                "# Mixtral of Experts "
            ],
            "Citation": [
                "# From Persona to Personalization: A Survey on Role-Playing Language Agents ",
                "# Beyond Relevance: Evaluate and Improve Retrievers on Perspective Awareness ",
                "# Meta-Task Planning for Language Agents ",
                "# A Human-Like Reasoning Framework for Multi-Phases Planning Task with Large Language Models ",
                "# Robust Planning with LLM-Modulo Framework: Case Study in Travel Planning "
            ]
        },
        "shuffle_doc": true,
        "id": "0bf88a67-cbf2-49ac-b170-84b506392356",
        "prompt": "[Question]\n#Papers:\n{docs}\n\nWe hope you will carefully study the provided papers and determine the citation relationships between them. Please follow the instructions below strictly to complete the task:\n\n#Specific Requirements:\n1. Reference: When a given paper mentions other provided papers, those other papers are considered as \"references\" for the given paper. To summarize in this specific context, references are about what the given paper is using.\n2. Citation: Conversely, when other provided papers mention the given paper in their works, the given paper is being \"cited\" by those other papers. To summarize in this specific context, citations are about who is using the given paper.\n3. Given a paper, you need to determine the citation or reference relationship between this paper and the other papers. Do not consider papers that are not provided.\n3. Please present the paper titles in a json format as follows: {{\"Reference\":[\"Reference Title 1\", \"Reference Title 2\", ..., \"Reference Title n\"], \"Citation\":[\"Citation Title 1\", \"Citation Title 2\", ..., \"Citation Title n\"]}}.\n4. If a paper does not have any references or citations, please leave the corresponding list empty, e.g.{{\"Refernce\":[]}}, {{\"Citation\":[]}}.\n\n#The paper you need to analyze:\nTravelPlanner: A Benchmark for Real-World Planning with Language Agents\n\n[Gold Answer]\n{'Reference': ['# Mistral 7B ', '# OpenAgents: AN OPEN PlATFORM FOR LANGUAGE AGENTS IN THE WILD ', '# ToolChaIN*: EFFiCIENT ACtion SpaCe NAVIGATION in Large Language ModelS With A* Search ', '# GPT-4V(ision) is a Generalist Web Agent, if Grounded ', '# Mixtral of Experts '], 'Citation': ['# From Persona to Personalization: A Survey on Role-Playing Language Agents ', '# Beyond Relevance: Evaluate and Improve Retrievers on Perspective Awareness ', '# Meta-Task Planning for Language Agents ', '# A Human-Like Reasoning Framework for Multi-Phases Planning Task with Large Language Models ', '# Robust Planning with LLM-Modulo Framework: Case Study in Travel Planning ']}\n\n[The Start of Assistant's Predicted Answer]\n{\"Reference\":[\"Robot planning in the real world: Research challenges and opportunities\", \"Language models as agent models\", \"Autogpt\", \"Graph of thoughts: Solving elaborate problems with large language models\", \"Language models are few-shot learners\", \"Deep blue\", \"Walking down the memory maze: Beyond context limit through interactive reading\", \"Training verifiers to solve math word problems\", \"Dart: an example of accelerated evolutionary development\", \"Mind2web: Towards a generalist agent for the web\", \"Gemini: a family of highly capable multimodal models\", \"Openagi: When llm meets domain experts\", \"HTN planning: Overview, comparison, and beyond\", \"Planning and the brain\", \"A cognitive model of planning\", \"Planning with theory of mind\", \"Language models as zero-shot planners: Extracting actionable knowledge for embodied agents\", \"Automated planning for robotics\", \"Solving quantitative reasoning problems with language models\", \"API-Bank: A comprehensive benchmark for tool-augmented llms\", \"Unleashing infinite-length input capacity for large-scale language models with self-controlled memory system\", \"LLM+P: Empowering large language models with optimal planning proficiency\", \"Lost in the middle: How language models use long contexts\", \"AgentBench: Evaluating llms as agents\", \"Chameleon: Plug-and-play compositional reasoning with large language models\", \"Planning in the brain\", \"Robot planning\", \"Task-driven autonomous agent utilizing gpt-4, pinecone, and langchain for diverse applications\", \"Webgpt: Browser-assisted question-answering with human feedback\", \"Chatgpt\", \"Gpt-4 technical report\", \"Generative agents: Interactive simulacra of human behavior\", \"Are NLP models really able to solve simple math word problems?\", \"Gorilla: Large language model connected with massive apis\", \"Planning and scheduling in manufacturing and services\", \"Toolllm: Facilitating large language models to master 16000+ real-world apis\", \"Solving general arithmetic word problems\", \"Artificial intelligence a modern approach\", \"Toolformer: Language models can teach themselves to use tools\", \"HuggingGPT: Solving AI tasks with chatGPT and its friends in hugging face\", \"Reflexion: Language agents with verbal reinforcement learning\", \"Mastering the game of go with deep neural networks and tree search\", \"The predictron: End-to-end learning and planning\", \"LLM-Planner: Few-shot grounded planning for embodied agents with large language models\", \"Language agents: a critical evolutionary step of artificial intelligence\", \"Cognitive architectures for language agents\", \"Llama: Open and efficient foundation language models\", \"Llama 2: Open foundation and finetuned chat models\", \"Planbench: An extensible benchmark for evaluating large language models on planning and reasoning about change\", \"Voyager: An openended embodied agent with large language models\", \"Chain-of-thought prompting elicits reasoning in large language models\", \"LLM-powered autonomous agents\", \"On the tool manipulation capability of open-source large language models\", \"React: Synergizing reasoning and acting in language models\", \"Tree of thoughts: Deliberate problem solving with large language models\", \"Distilling script knowledge from large language models for constrained language planning\", \"Gpt-4v(ision) is a generalist web agent, if grounded\", \"Memorybank: Enhancing large language models with long-term memory\", \"Webarena: A realistic web environment for building autonomous agents\", \"Recurrentgpt: Interactive generation of (arbitrarily) long text\", \"ToolQA: A dataset for LLM question answering with external tools\", \"Toolchain*: Efficient action space navigation in large language models with a* search\"], \"Citation\":[\"A Human-Like Reasoning Framework for Multi-Phases Planning Task with Large Language Models\"]}\n[The End of Assistant's Predicted Answer]\n\n[System]\nWe would like to request your feedback on the performance of the AI assistant in response to the user question displayed above according to the gold answer. Please use the following listed aspects and their descriptions as evaluation criteria:\n    - Accuracy and Hallucinations: The assistant's answer is semantically consistent with the gold answer; The numerical value and order need to be accurate, and there should be no hallucinations.\n    - Completeness: Referring to the reference answers, the assistant's answer should contain all the key points needed to answer the user's question; further elaboration on these key points can be omitted.\nPlease rate whether this answer is suitable for the question. Please note that the gold answer can be considered as a correct answer to the question.\n\nThe assistant receives an overall score on a scale of 1 to 100, where a higher score indicates better overall performance.\nPlease note that if the assistant's answer and the gold answer fully meet the above criteria, its overall rating should be the full marks (100).\nPlease first provide a comprehensive explanation of your evaluation, avoiding any potential bias.\nThen, output a line indicating the score of the Assistant.\n\nPLEASE OUTPUT WITH THE FOLLOWING FORMAT, WHERE THE SCORE IS A SCALE OF 1 TO 100 BY STRICTLY FOLLOWING THIS FORMAT: \"[[score]]\", FOR EXAMPLE \"Rating: [[100]]\":\n<start output>\nEvaluation evidence: your evluation explanation here, no more than 100 words\nRating: [[score]]\n<end output>\n\nNow, start your evaluation:",
        "generate_response": "{\"Reference\":[\"Robot planning in the real world: Research challenges and opportunities\", \"Language models as agent models\", \"Autogpt\", \"Graph of thoughts: Solving elaborate problems with large language models\", \"Language models are few-shot learners\", \"Deep blue\", \"Walking down the memory maze: Beyond context limit through interactive reading\", \"Training verifiers to solve math word problems\", \"Dart: an example of accelerated evolutionary development\", \"Mind2web: Towards a generalist agent for the web\", \"Gemini: a family of highly capable multimodal models\", \"Openagi: When llm meets domain experts\", \"HTN planning: Overview, comparison, and beyond\", \"Planning and the brain\", \"A cognitive model of planning\", \"Planning with theory of mind\", \"Language models as zero-shot planners: Extracting actionable knowledge for embodied agents\", \"Automated planning for robotics\", \"Solving quantitative reasoning problems with language models\", \"API-Bank: A comprehensive benchmark for tool-augmented llms\", \"Unleashing infinite-length input capacity for large-scale language models with self-controlled memory system\", \"LLM+P: Empowering large language models with optimal planning proficiency\", \"Lost in the middle: How language models use long contexts\", \"AgentBench: Evaluating llms as agents\", \"Chameleon: Plug-and-play compositional reasoning with large language models\", \"Planning in the brain\", \"Robot planning\", \"Task-driven autonomous agent utilizing gpt-4, pinecone, and langchain for diverse applications\", \"Webgpt: Browser-assisted question-answering with human feedback\", \"Chatgpt\", \"Gpt-4 technical report\", \"Generative agents: Interactive simulacra of human behavior\", \"Are NLP models really able to solve simple math word problems?\", \"Gorilla: Large language model connected with massive apis\", \"Planning and scheduling in manufacturing and services\", \"Toolllm: Facilitating large language models to master 16000+ real-world apis\", \"Solving general arithmetic word problems\", \"Artificial intelligence a modern approach\", \"Toolformer: Language models can teach themselves to use tools\", \"HuggingGPT: Solving AI tasks with chatGPT and its friends in hugging face\", \"Reflexion: Language agents with verbal reinforcement learning\", \"Mastering the game of go with deep neural networks and tree search\", \"The predictron: End-to-end learning and planning\", \"LLM-Planner: Few-shot grounded planning for embodied agents with large language models\", \"Language agents: a critical evolutionary step of artificial intelligence\", \"Cognitive architectures for language agents\", \"Llama: Open and efficient foundation language models\", \"Llama 2: Open foundation and finetuned chat models\", \"Planbench: An extensible benchmark for evaluating large language models on planning and reasoning about change\", \"Voyager: An openended embodied agent with large language models\", \"Chain-of-thought prompting elicits reasoning in large language models\", \"LLM-powered autonomous agents\", \"On the tool manipulation capability of open-source large language models\", \"React: Synergizing reasoning and acting in language models\", \"Tree of thoughts: Deliberate problem solving with large language models\", \"Distilling script knowledge from large language models for constrained language planning\", \"Gpt-4v(ision) is a generalist web agent, if grounded\", \"Memorybank: Enhancing large language models with long-term memory\", \"Webarena: A realistic web environment for building autonomous agents\", \"Recurrentgpt: Interactive generation of (arbitrarily) long text\", \"ToolQA: A dataset for LLM question answering with external tools\", \"Toolchain*: Efficient action space navigation in large language models with a* search\"], \"Citation\":[\"A Human-Like Reasoning Framework for Multi-Phases Planning Task with Large Language Models\"]}",
        "eval_response": "Evaluation evidence: The assistant's answer shows significant hallucinations and inaccuracies. The references listed in the assistant's answer do not match any from the gold answer, indicating a complete deviation from the correct data. Additionally, the citations mentioned are also incorrect, with only one partially matching the gold answer, showing a lack of completeness and accuracy. The assistant's response fails to accurately reflect the relationships between the papers as required by the task.\n\nRating: [[10]]",
        "score": 10
    },
    {
        "level": 3,
        "set": 3,
        "length": 141452,
        "type": "paper",
        "language": "en",
        "question": "A Comprehensive Study of Knowledge Editing for Large Language Models",
        "instruction": "We hope you will carefully study the provided papers and determine the citation relationships between them. Please follow the instructions below strictly to complete the task:\n\n#Specific Requirements:\n1. Reference: When a given paper mentions other provided papers, those other papers are considered as \"references\" for the given paper. To summarize in this specific context, references are about what the given paper is using.\n2. Citation: Conversely, when other provided papers mention the given paper in their works, the given paper is being \"cited\" by those other papers. To summarize in this specific context, citations are about who is using the given paper.\n3. Given a paper, you need to determine the citation or reference relationship between this paper and the other papers. Do not consider papers that are not provided.\n3. Please present the paper titles in a json format as follows: {{\"Reference\":[\"Reference Title 1\", \"Reference Title 2\", ..., \"Reference Title n\"], \"Citation\":[\"Citation Title 1\", \"Citation Title 2\", ..., \"Citation Title n\"]}}.\n4. If a paper does not have any references or citations, please leave the corresponding list empty, e.g.{{\"Refernce\":[]}}, {{\"Citation\":[]}}.",
        "prompt_template": "#Papers:\n{docs}\n\n{instruction}\n\n#The paper you need to analyze:\n{question}",
        "doc": [
            "2401.01286.md",
            "2405.17969.md",
            "2405.19010.md",
            "2312.10997.md",
            "2403.13355.md"
        ],
        "answer": {
            "Reference": [
                "# Retrieval-Augmented Generation for Large Language Models: A Survey ",
                "# BADEDIT: BACKDOORING LARGE LANGUAGE MODELS BY MODEL EDITING "
            ],
            "Citation": [
                "# Knowledge Circuits in Pretrained Transformers ",
                "# Evaluating the External and Parametric Knowledge Fusion of Large Language Models "
            ]
        },
        "shuffle_doc": true,
        "id": "4e74c662-1c00-4af9-ab57-25486612c256",
        "prompt": "[Question]\n#Papers:\n{docs}\n\nWe hope you will carefully study the provided papers and determine the citation relationships between them. Please follow the instructions below strictly to complete the task:\n\n#Specific Requirements:\n1. Reference: When a given paper mentions other provided papers, those other papers are considered as \"references\" for the given paper. To summarize in this specific context, references are about what the given paper is using.\n2. Citation: Conversely, when other provided papers mention the given paper in their works, the given paper is being \"cited\" by those other papers. To summarize in this specific context, citations are about who is using the given paper.\n3. Given a paper, you need to determine the citation or reference relationship between this paper and the other papers. Do not consider papers that are not provided.\n3. Please present the paper titles in a json format as follows: {{\"Reference\":[\"Reference Title 1\", \"Reference Title 2\", ..., \"Reference Title n\"], \"Citation\":[\"Citation Title 1\", \"Citation Title 2\", ..., \"Citation Title n\"]}}.\n4. If a paper does not have any references or citations, please leave the corresponding list empty, e.g.{{\"Refernce\":[]}}, {{\"Citation\":[]}}.\n\n#The paper you need to analyze:\nA Comprehensive Study of Knowledge Editing for Large Language Models\n\n[Gold Answer]\n{'Reference': ['# Retrieval-Augmented Generation for Large Language Models: A Survey ', '# BADEDIT: BACKDOORING LARGE LANGUAGE MODELS BY MODEL EDITING '], 'Citation': ['# Knowledge Circuits in Pretrained Transformers ', '# Evaluating the External and Parametric Knowledge Fusion of Large Language Models ']}\n\n[The Start of Assistant's Predicted Answer]\n{\"Reference\":[],\"Citation\":[]}\n[The End of Assistant's Predicted Answer]\n\n[System]\nWe would like to request your feedback on the performance of the AI assistant in response to the user question displayed above according to the gold answer. Please use the following listed aspects and their descriptions as evaluation criteria:\n    - Accuracy and Hallucinations: The assistant's answer is semantically consistent with the gold answer; The numerical value and order need to be accurate, and there should be no hallucinations.\n    - Completeness: Referring to the reference answers, the assistant's answer should contain all the key points needed to answer the user's question; further elaboration on these key points can be omitted.\nPlease rate whether this answer is suitable for the question. Please note that the gold answer can be considered as a correct answer to the question.\n\nThe assistant receives an overall score on a scale of 1 to 100, where a higher score indicates better overall performance.\nPlease note that if the assistant's answer and the gold answer fully meet the above criteria, its overall rating should be the full marks (100).\nPlease first provide a comprehensive explanation of your evaluation, avoiding any potential bias.\nThen, output a line indicating the score of the Assistant.\n\nPLEASE OUTPUT WITH THE FOLLOWING FORMAT, WHERE THE SCORE IS A SCALE OF 1 TO 100 BY STRICTLY FOLLOWING THIS FORMAT: \"[[score]]\", FOR EXAMPLE \"Rating: [[100]]\":\n<start output>\nEvaluation evidence: your evluation explanation here, no more than 100 words\nRating: [[score]]\n<end output>\n\nNow, start your evaluation:",
        "generate_response": "{\"Reference\":[],\"Citation\":[]}",
        "eval_response": "Evaluation evidence: The assistant's answer is completely incorrect as it lists both \"Reference\" and \"Citation\" as empty arrays, which contradicts the gold answer. The gold answer specifies two references and two citations for the paper in question, indicating that the assistant failed to identify any of the relationships correctly. This shows a lack of accuracy and completeness in the response.\n\nRating: [[0]]",
        "score": 0
    },
    {
        "level": 4,
        "set": 2,
        "length": 85800,
        "type": "paper",
        "language": "en",
        "question": "",
        "instruction": "We kindly ask you to thoroughly review the provided papers and construct a citation chain from them. Please adhere to the following instructions strictly while completing the task:\n\n#Task Instructions:\nGiven several papers, you are required to identify and list the longest citation chain, which demonstrates the citation relationship among the provided papers.\n\n#Specific Requirements:\n1.Please present the titles of the papers in the form of a list, as follows: [\"Title of Paper 1\", \"Title of Paper 2\", ..., \"Title of Paper n\"].\n2.Ensure that the citation chain in the list is linear and continuous, meaning that the first paper title in the list (Paper 1) should not cite any other works. Instead, it should be cited by the next paper in the list (Paper 2); subsequently, each paper should then be cited by the next one in the list, continuing up to the last paper (Paper n).\n3.Consider only the citation relationships within the supplied collection of papers, and ensure that the citation chain accurately reflects the sequential citation order among these documents.\n4.Do not take into account any articles not provided, and disregard other non-linear citation relationships.",
        "prompt_template": "{instruction}\n\n#Papers Provided:\n{docs}",
        "doc": [
            "2309.15088.md",
            "2310.08319.md",
            "2401.00368.md",
            "2310.20329.md",
            "2401.04621.md"
        ],
        "answer": [
            "# RankVicuna: Zero-Shot Listwise Document Reranking with Open-Source Large Language Models ",
            "# Fine-Tuning LLaMA for Multi-Stage Text Retrieval ",
            "# Improving Text Embeddings with Large Language Models ",
            "# InstructCoder: Instruction Tuning Large Language Models for Code Editing ",
            "#  DebugBench: <br> Evaluating Debugging Capability of Large Language Models "
        ],
        "shuffle_doc": true,
        "id": "4e20a21e-b90c-455c-abbf-c62583e2d4c4",
        "prompt": "[Question]\nWe kindly ask you to thoroughly review the provided papers and construct a citation chain from them. Please adhere to the following instructions strictly while completing the task:\n\n#Task Instructions:\nGiven several papers, you are required to identify and list the longest citation chain, which demonstrates the citation relationship among the provided papers.\n\n#Specific Requirements:\n1.Please present the titles of the papers in the form of a list, as follows: [\"Title of Paper 1\", \"Title of Paper 2\", ..., \"Title of Paper n\"].\n2.Ensure that the citation chain in the list is linear and continuous, meaning that the first paper title in the list (Paper 1) should not cite any other works. Instead, it should be cited by the next paper in the list (Paper 2); subsequently, each paper should then be cited by the next one in the list, continuing up to the last paper (Paper n).\n3.Consider only the citation relationships within the supplied collection of papers, and ensure that the citation chain accurately reflects the sequential citation order among these documents.\n4.Do not take into account any articles not provided, and disregard other non-linear citation relationships.\n\n#Papers Provided:\n{docs}\n\n[Gold Answer]\n['# RankVicuna: Zero-Shot Listwise Document Reranking with Open-Source Large Language Models ', '# Fine-Tuning LLaMA for Multi-Stage Text Retrieval ', '# Improving Text Embeddings with Large Language Models ', '# InstructCoder: Instruction Tuning Large Language Models for Code Editing ', '#  DebugBench: <br> Evaluating Debugging Capability of Large Language Models ']\n\n[The Start of Assistant's Predicted Answer]\n[\"InstructCoder: Instruction Tuning Large Language Models for Code Editing\", \"Improving Text Embeddings with Large Language Models\"]\n[The End of Assistant's Predicted Answer]\n\n[System]\nWe would like to request your feedback on the performance of the AI assistant in response to the user question displayed above according to the gold answer. Please use the following listed aspects and their descriptions as evaluation criteria:\n    - Accuracy and Hallucinations: The assistant's answer is semantically consistent with the gold answer; The numerical value and order need to be accurate, and there should be no hallucinations.\n    - Completeness: Referring to the reference answers, the assistant's answer should contain all the key points needed to answer the user's question; further elaboration on these key points can be omitted.\nPlease rate whether this answer is suitable for the question. Please note that the gold answer can be considered as a correct answer to the question.\n\nThe assistant receives an overall score on a scale of 1 to 100, where a higher score indicates better overall performance.\nPlease note that if the assistant's answer and the gold answer fully meet the above criteria, its overall rating should be the full marks (100).\nPlease first provide a comprehensive explanation of your evaluation, avoiding any potential bias.\nThen, output a line indicating the score of the Assistant.\n\nPLEASE OUTPUT WITH THE FOLLOWING FORMAT, WHERE THE SCORE IS A SCALE OF 1 TO 100 BY STRICTLY FOLLOWING THIS FORMAT: \"[[score]]\", FOR EXAMPLE \"Rating: [[100]]\":\n<start output>\nEvaluation evidence: your evluation explanation here, no more than 100 words\nRating: [[score]]\n<end output>\n\nNow, start your evaluation:",
        "generate_response": "[\"InstructCoder: Instruction Tuning Large Language Models for Code Editing\", \"Improving Text Embeddings with Large Language Models\"]",
        "eval_response": "Evaluation evidence: The assistant's answer does not match the gold answer in terms of the length and content of the citation chain. The gold answer lists a chain of five papers, while the assistant's answer only includes two papers, which are not in the correct order nor are they part of the longest citation chain as per the gold answer. This indicates issues with both accuracy and completeness.\n\nRating: [[20]]",
        "score": 20
    },
    {
        "level": 3,
        "set": 2,
        "length": 63238,
        "type": "paper",
        "language": "en",
        "question": "Multilingual Instruction Tuning With Just a Pinch of Multilinguality",
        "instruction": "We hope you will carefully study the provided papers and determine the citation relationships between them. Please follow the instructions below strictly to complete the task:\n\n#Specific Requirements:\n1. Reference: When a given paper mentions other provided papers, those other papers are considered as \"references\" for the given paper. To summarize in this specific context, references are about what the given paper is using.\n2. Citation: Conversely, when other provided papers mention the given paper in their works, the given paper is being \"cited\" by those other papers. To summarize in this specific context, citations are about who is using the given paper.\n3. Given a paper, you need to determine the citation or reference relationship between this paper and the other papers. Do not consider papers that are not provided.\n3. Please present the paper titles in a json format as follows: {{\"Reference\":[\"Reference Title 1\", \"Reference Title 2\", ..., \"Reference Title n\"], \"Citation\":[\"Citation Title 1\", \"Citation Title 2\", ..., \"Citation Title n\"]}}.\n4. If a paper does not have any references or citations, please leave the corresponding list empty, e.g.{{\"Refernce\":[]}}, {{\"Citation\":[]}}.",
        "prompt_template": "#Papers:\n{docs}\n\n{instruction}\n\n#The paper you need to analyze:\n{question}",
        "doc": [
            "2401.01854.md",
            "2404.04850.md",
            "2405.15604.md"
        ],
        "answer": {
            "Reference": [],
            "Citation": [
                "# Lucky 52: How Many Languages Are Needed to Instruction Fine-Tune Large Language Models? ",
                "# Text Generation: A Systematic Literature Review of Tasks, Evaluation, and Challenges "
            ]
        },
        "shuffle_doc": true,
        "id": "b65b3f80-820a-4c10-874e-0af23139e143",
        "prompt": "[Question]\n#Papers:\n{docs}\n\nWe hope you will carefully study the provided papers and determine the citation relationships between them. Please follow the instructions below strictly to complete the task:\n\n#Specific Requirements:\n1. Reference: When a given paper mentions other provided papers, those other papers are considered as \"references\" for the given paper. To summarize in this specific context, references are about what the given paper is using.\n2. Citation: Conversely, when other provided papers mention the given paper in their works, the given paper is being \"cited\" by those other papers. To summarize in this specific context, citations are about who is using the given paper.\n3. Given a paper, you need to determine the citation or reference relationship between this paper and the other papers. Do not consider papers that are not provided.\n3. Please present the paper titles in a json format as follows: {{\"Reference\":[\"Reference Title 1\", \"Reference Title 2\", ..., \"Reference Title n\"], \"Citation\":[\"Citation Title 1\", \"Citation Title 2\", ..., \"Citation Title n\"]}}.\n4. If a paper does not have any references or citations, please leave the corresponding list empty, e.g.{{\"Refernce\":[]}}, {{\"Citation\":[]}}.\n\n#The paper you need to analyze:\nMultilingual Instruction Tuning With Just a Pinch of Multilinguality\n\n[Gold Answer]\n{'Reference': [], 'Citation': ['# Lucky 52: How Many Languages Are Needed to Instruction Fine-Tune Large Language Models? ', '# Text Generation: A Systematic Literature Review of Tasks, Evaluation, and Challenges ']}\n\n[The Start of Assistant's Predicted Answer]\n{\"Reference\":[\"Attention is all you need\", \"Self-instruct: Aligning language models with self-generated instructions\", \"Finetuned language models are zero-shot learners\", \"Beto, bentz, becas: The surprising cross-lingual effectiveness of BERT\", \"mT5: A massively multilingual pre-trained text-to-text transformer\", \"Language versatilists vs. specialists: An empirical revisiting on multilingual transfer ability\", \"Plug: Leveraging pivot language in crosslingual instruction tuning\", \"Judging llm-as-a-judge with mt-bench and chatbot arena\", \"A Survey on Evaluation of Large Language Models\", \"Vicuna: An Open-Source Chatbot Impressing GPT-4 with $90 \\% *$ ChatGPT Quality\", \"ChatLaw: Open-Source Legal Large Language Model with Integrated External Knowledge Bases\", \"Compositional Semantic Parsing with Large Language Models\", \"Open LLM Leaderboard\", \"How Close is ChatGPT to Human Experts? Comparison Corpus, Evaluation, and Detection\", \"REALM: Retrieval-Augmented Language Model PreTraining\", \"Rethinking with Retrieval: Faithful Large Language Model Inference\", \"Measuring Massive Multitask Language Understanding\", \"Atlas: Few-shot Learning with Retrieval Augmented Language Models\", \"How multilingual is multilingual llm?\"], \"Citation\":[]}\n[The End of Assistant's Predicted Answer]\n\n[System]\nWe would like to request your feedback on the performance of the AI assistant in response to the user question displayed above according to the gold answer. Please use the following listed aspects and their descriptions as evaluation criteria:\n    - Accuracy and Hallucinations: The assistant's answer is semantically consistent with the gold answer; The numerical value and order need to be accurate, and there should be no hallucinations.\n    - Completeness: Referring to the reference answers, the assistant's answer should contain all the key points needed to answer the user's question; further elaboration on these key points can be omitted.\nPlease rate whether this answer is suitable for the question. Please note that the gold answer can be considered as a correct answer to the question.\n\nThe assistant receives an overall score on a scale of 1 to 100, where a higher score indicates better overall performance.\nPlease note that if the assistant's answer and the gold answer fully meet the above criteria, its overall rating should be the full marks (100).\nPlease first provide a comprehensive explanation of your evaluation, avoiding any potential bias.\nThen, output a line indicating the score of the Assistant.\n\nPLEASE OUTPUT WITH THE FOLLOWING FORMAT, WHERE THE SCORE IS A SCALE OF 1 TO 100 BY STRICTLY FOLLOWING THIS FORMAT: \"[[score]]\", FOR EXAMPLE \"Rating: [[100]]\":\n<start output>\nEvaluation evidence: your evluation explanation here, no more than 100 words\nRating: [[score]]\n<end output>\n\nNow, start your evaluation:",
        "generate_response": "{\"Reference\":[\"Attention is all you need\", \"Self-instruct: Aligning language models with self-generated instructions\", \"Finetuned language models are zero-shot learners\", \"Beto, bentz, becas: The surprising cross-lingual effectiveness of BERT\", \"mT5: A massively multilingual pre-trained text-to-text transformer\", \"Language versatilists vs. specialists: An empirical revisiting on multilingual transfer ability\", \"Plug: Leveraging pivot language in crosslingual instruction tuning\", \"Judging llm-as-a-judge with mt-bench and chatbot arena\", \"A Survey on Evaluation of Large Language Models\", \"Vicuna: An Open-Source Chatbot Impressing GPT-4 with $90 \\% *$ ChatGPT Quality\", \"ChatLaw: Open-Source Legal Large Language Model with Integrated External Knowledge Bases\", \"Compositional Semantic Parsing with Large Language Models\", \"Open LLM Leaderboard\", \"How Close is ChatGPT to Human Experts? Comparison Corpus, Evaluation, and Detection\", \"REALM: Retrieval-Augmented Language Model PreTraining\", \"Rethinking with Retrieval: Faithful Large Language Model Inference\", \"Measuring Massive Multitask Language Understanding\", \"Atlas: Few-shot Learning with Retrieval Augmented Language Models\", \"How multilingual is multilingual llm?\"], \"Citation\":[]}",
        "eval_response": "Evaluation evidence: The assistant's answer inaccurately lists numerous references that are not mentioned in the gold answer, indicating a significant hallucination issue. The gold answer specifies that there are no references for the analyzed paper, yet the assistant lists many, showing a clear deviation from the task requirements. Additionally, the citations listed in the gold answer are missing from the assistant's response, further highlighting inaccuracies and incompleteness in addressing the task.\n\nRating: [[10]]",
        "score": 10
    },
    {
        "level": 3,
        "set": 1,
        "length": 37587,
        "type": "paper",
        "language": "en",
        "question": "TravelPlanner: A Benchmark for Real-World Planning with Language Agents",
        "instruction": "We hope you will carefully study the provided papers and determine the citation relationships between them. Please follow the instructions below strictly to complete the task:\n\n#Specific Requirements:\n1. Reference: When a given paper mentions other provided papers, those other papers are considered as \"references\" for the given paper. To summarize in this specific context, references are about what the given paper is using.\n2. Citation: Conversely, when other provided papers mention the given paper in their works, the given paper is being \"cited\" by those other papers. To summarize in this specific context, citations are about who is using the given paper.\n3. Given a paper, you need to determine the citation or reference relationship between this paper and the other papers. Do not consider papers that are not provided.\n3. Please present the paper titles in a json format as follows: {{\"Reference\":[\"Reference Title 1\", \"Reference Title 2\", ..., \"Reference Title n\"], \"Citation\":[\"Citation Title 1\", \"Citation Title 2\", ..., \"Citation Title n\"]}}.\n4. If a paper does not have any references or citations, please leave the corresponding list empty, e.g.{{\"Refernce\":[]}}, {{\"Citation\":[]}}.",
        "prompt_template": "#Papers:\n{docs}\n\n{instruction}\n\n#The paper you need to analyze:\n{question}",
        "doc": [
            "2402.01622.md",
            "2405.20625.md",
            "2310.06825.md"
        ],
        "answer": {
            "Reference": [
                "# Mistral 7B "
            ],
            "Citation": [
                "# Robust Planning with LLM-Modulo Framework: Case Study in Travel Planning "
            ]
        },
        "shuffle_doc": true,
        "id": "c2eb0280-3201-4791-9ffb-1503841ce9ff",
        "prompt": "[Question]\n#Papers:\n{docs}\n\nWe hope you will carefully study the provided papers and determine the citation relationships between them. Please follow the instructions below strictly to complete the task:\n\n#Specific Requirements:\n1. Reference: When a given paper mentions other provided papers, those other papers are considered as \"references\" for the given paper. To summarize in this specific context, references are about what the given paper is using.\n2. Citation: Conversely, when other provided papers mention the given paper in their works, the given paper is being \"cited\" by those other papers. To summarize in this specific context, citations are about who is using the given paper.\n3. Given a paper, you need to determine the citation or reference relationship between this paper and the other papers. Do not consider papers that are not provided.\n3. Please present the paper titles in a json format as follows: {{\"Reference\":[\"Reference Title 1\", \"Reference Title 2\", ..., \"Reference Title n\"], \"Citation\":[\"Citation Title 1\", \"Citation Title 2\", ..., \"Citation Title n\"]}}.\n4. If a paper does not have any references or citations, please leave the corresponding list empty, e.g.{{\"Refernce\":[]}}, {{\"Citation\":[]}}.\n\n#The paper you need to analyze:\nTravelPlanner: A Benchmark for Real-World Planning with Language Agents\n\n[Gold Answer]\n{'Reference': ['# Mistral 7B '], 'Citation': ['# Robust Planning with LLM-Modulo Framework: Case Study in Travel Planning ']}\n\n[The Start of Assistant's Predicted Answer]\n{\"Reference\": [\"Robot planning in the real world: Research challenges and opportunities\", \"Language models as agent models\", \"Autogpt\", \"Graph of thoughts: Solving elaborate problems with large language models\", \"Language models are few-shot learners\", \"Deep blue\", \"Walking down the memory maze: Beyond context limit through interactive reading\", \"Training verifiers to solve math word problems\", \"Dart: an example of accelerated evolutionary development\", \"Mind2web: Towards a generalist agent for the web\", \"Gemini: a family of highly capable multimodal models\", \"Openagi: When llm meets domain experts\", \"HTN planning: Overview, comparison, and beyond\", \"Planning and the brain\", \"A cognitive model of planning\", \"Planning with theory of mind\", \"Language models as zero-shot planners: Extracting actionable knowledge for embodied agents\", \"Do as I can, not as I say: Grounding language in robotic affordances\", \"Mistral 7b\", \"Mixtral of experts\", \"Automated planning for robotics\", \"Solving quantitative reasoning problems with language models\", \"API-Bank: A comprehensive benchmark for tool-augmented llms\", \"Unleashing infinite-length input capacity for large-scale language models with self-controlled memory system\", \"LLM+P: Empowering large language models with optimal planning proficiency\", \"Lost in the middle: How language models use long contexts\", \"AgentBench: Evaluating llms as agents\", \"Chameleon: Plug-and-play compositional reasoning with large language models\", \"Planning in the brain\", \"Robot planning\", \"Task-driven autonomous agent utilizing gpt-4, pinecone, and langchain for diverse applications\", \"Webgpt: Browser-assisted question-answering with human feedback\", \"Chatgpt\", \"Gpt-4 technical report\", \"Generative agents: Interactive simulacra of human behavior\", \"Are NLP models really able to solve simple math word problems?\", \"Gorilla: Large language model connected with massive apis\", \"Planning and scheduling in manufacturing and services\", \"ToolLLM: Facilitating large language models to master $16000+$ real-world apis\", \"Solving general arithmetic word problems\", \"Artificial intelligence a modern approach\", \"Toolformer: Language models can teach themselves to use tools\", \"HuggingGPT: Solving AI tasks with chatGPT and its friends in hugging face\", \"Reflexion: Language agents with verbal reinforcement learning\", \"Mastering the game of go with deep neural networks and tree search\", \"The predictron: End-to-end learning and planning\", \"LLM-Planner: Few-shot grounded planning for embodied agents with large language models\", \"Language agents: a critical evolutionary step of artificial intelligence\", \"Cognitive architectures for language agents\", \"Llama: Open and efficient foundation language models\", \"Llama 2: Open foundation and finetuned chat models\", \"PlanBench: An extensible benchmark for evaluating large language models on planning and reasoning about change\", \"Voyager: An open-ended embodied agent with large language models\", \"Chain-of-thought prompting elicits reasoning in large language models\", \"LLM-powered autonomous agents\", \"On the tool manipulation capability of open-source large language models\", \"React: Synergizing reasoning and acting in language models\", \"Tree of thoughts: Deliberate problem solving with large language models\", \"Distilling script knowledge from large language models for constrained language planning\", \"GPT-4V(ision) is a generalist web agent, if grounded\", \"Memorybank: Enhancing large language models with long-term memory\", \"WebArena: A realistic web environment for building autonomous agents\", \"RecurrentGPT: Interactive generation of (arbitrarily) long text\", \"ToolQA: A dataset for LLM question answering with external tools\", \"Toolchain*: Efficient action space navigation in large language models with $\\mathrm{a}^{*}$ search\"], \"Citation\": [\"Robust Planning with LLM-Modulo Framework: Case Study in Travel Planning\"]}\n[The End of Assistant's Predicted Answer]\n\n[System]\nWe would like to request your feedback on the performance of the AI assistant in response to the user question displayed above according to the gold answer. Please use the following listed aspects and their descriptions as evaluation criteria:\n    - Accuracy and Hallucinations: The assistant's answer is semantically consistent with the gold answer; The numerical value and order need to be accurate, and there should be no hallucinations.\n    - Completeness: Referring to the reference answers, the assistant's answer should contain all the key points needed to answer the user's question; further elaboration on these key points can be omitted.\nPlease rate whether this answer is suitable for the question. Please note that the gold answer can be considered as a correct answer to the question.\n\nThe assistant receives an overall score on a scale of 1 to 100, where a higher score indicates better overall performance.\nPlease note that if the assistant's answer and the gold answer fully meet the above criteria, its overall rating should be the full marks (100).\nPlease first provide a comprehensive explanation of your evaluation, avoiding any potential bias.\nThen, output a line indicating the score of the Assistant.\n\nPLEASE OUTPUT WITH THE FOLLOWING FORMAT, WHERE THE SCORE IS A SCALE OF 1 TO 100 BY STRICTLY FOLLOWING THIS FORMAT: \"[[score]]\", FOR EXAMPLE \"Rating: [[100]]\":\n<start output>\nEvaluation evidence: your evluation explanation here, no more than 100 words\nRating: [[score]]\n<end output>\n\nNow, start your evaluation:",
        "generate_response": "{\"Reference\": [\"Robot planning in the real world: Research challenges and opportunities\", \"Language models as agent models\", \"Autogpt\", \"Graph of thoughts: Solving elaborate problems with large language models\", \"Language models are few-shot learners\", \"Deep blue\", \"Walking down the memory maze: Beyond context limit through interactive reading\", \"Training verifiers to solve math word problems\", \"Dart: an example of accelerated evolutionary development\", \"Mind2web: Towards a generalist agent for the web\", \"Gemini: a family of highly capable multimodal models\", \"Openagi: When llm meets domain experts\", \"HTN planning: Overview, comparison, and beyond\", \"Planning and the brain\", \"A cognitive model of planning\", \"Planning with theory of mind\", \"Language models as zero-shot planners: Extracting actionable knowledge for embodied agents\", \"Do as I can, not as I say: Grounding language in robotic affordances\", \"Mistral 7b\", \"Mixtral of experts\", \"Automated planning for robotics\", \"Solving quantitative reasoning problems with language models\", \"API-Bank: A comprehensive benchmark for tool-augmented llms\", \"Unleashing infinite-length input capacity for large-scale language models with self-controlled memory system\", \"LLM+P: Empowering large language models with optimal planning proficiency\", \"Lost in the middle: How language models use long contexts\", \"AgentBench: Evaluating llms as agents\", \"Chameleon: Plug-and-play compositional reasoning with large language models\", \"Planning in the brain\", \"Robot planning\", \"Task-driven autonomous agent utilizing gpt-4, pinecone, and langchain for diverse applications\", \"Webgpt: Browser-assisted question-answering with human feedback\", \"Chatgpt\", \"Gpt-4 technical report\", \"Generative agents: Interactive simulacra of human behavior\", \"Are NLP models really able to solve simple math word problems?\", \"Gorilla: Large language model connected with massive apis\", \"Planning and scheduling in manufacturing and services\", \"ToolLLM: Facilitating large language models to master $16000+$ real-world apis\", \"Solving general arithmetic word problems\", \"Artificial intelligence a modern approach\", \"Toolformer: Language models can teach themselves to use tools\", \"HuggingGPT: Solving AI tasks with chatGPT and its friends in hugging face\", \"Reflexion: Language agents with verbal reinforcement learning\", \"Mastering the game of go with deep neural networks and tree search\", \"The predictron: End-to-end learning and planning\", \"LLM-Planner: Few-shot grounded planning for embodied agents with large language models\", \"Language agents: a critical evolutionary step of artificial intelligence\", \"Cognitive architectures for language agents\", \"Llama: Open and efficient foundation language models\", \"Llama 2: Open foundation and finetuned chat models\", \"PlanBench: An extensible benchmark for evaluating large language models on planning and reasoning about change\", \"Voyager: An open-ended embodied agent with large language models\", \"Chain-of-thought prompting elicits reasoning in large language models\", \"LLM-powered autonomous agents\", \"On the tool manipulation capability of open-source large language models\", \"React: Synergizing reasoning and acting in language models\", \"Tree of thoughts: Deliberate problem solving with large language models\", \"Distilling script knowledge from large language models for constrained language planning\", \"GPT-4V(ision) is a generalist web agent, if grounded\", \"Memorybank: Enhancing large language models with long-term memory\", \"WebArena: A realistic web environment for building autonomous agents\", \"RecurrentGPT: Interactive generation of (arbitrarily) long text\", \"ToolQA: A dataset for LLM question answering with external tools\", \"Toolchain*: Efficient action space navigation in large language models with $\\mathrm{a}^{*}$ search\"], \"Citation\": [\"Robust Planning with LLM-Modulo Framework: Case Study in Travel Planning\"]}",
        "eval_response": "Evaluation evidence: The assistant's answer includes a significant number of hallucinated references that are not mentioned in the gold answer, indicating a lack of accuracy and the presence of hallucinations. The only correct reference mentioned is \"Mistral 7b\". The citation \"Robust Planning with LLM-Modulo Framework: Case Study in Travel Planning\" is correctly identified. However, the extensive list of incorrect references overshadows the correct elements, leading to a low score.\n\nRating: [[20]]",
        "score": 20
    },
    {
        "level": 3,
        "set": 1,
        "length": 41848,
        "type": "paper",
        "language": "en",
        "question": "VisualWebArena: EvalUATING MULTIMODAL AGENTS ON REALISTIC VISUAL WEB TASKS",
        "instruction": "We hope you will carefully study the provided papers and determine the citation relationships between them. Please follow the instructions below strictly to complete the task:\n\n#Specific Requirements:\n1. Reference: When a given paper mentions other provided papers, those other papers are considered as \"references\" for the given paper. To summarize in this specific context, references are about what the given paper is using.\n2. Citation: Conversely, when other provided papers mention the given paper in their works, the given paper is being \"cited\" by those other papers. To summarize in this specific context, citations are about who is using the given paper.\n3. Given a paper, you need to determine the citation or reference relationship between this paper and the other papers. Do not consider papers that are not provided.\n3. Please present the paper titles in a json format as follows: {{\"Reference\":[\"Reference Title 1\", \"Reference Title 2\", ..., \"Reference Title n\"], \"Citation\":[\"Citation Title 1\", \"Citation Title 2\", ..., \"Citation Title n\"]}}.\n4. If a paper does not have any references or citations, please leave the corresponding list empty, e.g.{{\"Refernce\":[]}}, {{\"Citation\":[]}}.",
        "prompt_template": "#Papers:\n{docs}\n\n{instruction}\n\n#The paper you need to analyze:\n{question}",
        "doc": [
            "2401.13649.md",
            "2405.15341.md",
            "2312.13771.md"
        ],
        "answer": {
            "Reference": [
                "# AppAgent: Multimodal Agents as Smartphone Users "
            ],
            "Citation": [
                "# V-Zen: Efficient GUI Understanding and Precise Grounding With A Novel Multimodal LLM "
            ]
        },
        "shuffle_doc": true,
        "id": "21437cb8-574d-4713-ab25-2f1667ac4ab8",
        "prompt": "[Question]\n#Papers:\n{docs}\n\nWe hope you will carefully study the provided papers and determine the citation relationships between them. Please follow the instructions below strictly to complete the task:\n\n#Specific Requirements:\n1. Reference: When a given paper mentions other provided papers, those other papers are considered as \"references\" for the given paper. To summarize in this specific context, references are about what the given paper is using.\n2. Citation: Conversely, when other provided papers mention the given paper in their works, the given paper is being \"cited\" by those other papers. To summarize in this specific context, citations are about who is using the given paper.\n3. Given a paper, you need to determine the citation or reference relationship between this paper and the other papers. Do not consider papers that are not provided.\n3. Please present the paper titles in a json format as follows: {{\"Reference\":[\"Reference Title 1\", \"Reference Title 2\", ..., \"Reference Title n\"], \"Citation\":[\"Citation Title 1\", \"Citation Title 2\", ..., \"Citation Title n\"]}}.\n4. If a paper does not have any references or citations, please leave the corresponding list empty, e.g.{{\"Refernce\":[]}}, {{\"Citation\":[]}}.\n\n#The paper you need to analyze:\nVisualWebArena: EvalUATING MULTIMODAL AGENTS ON REALISTIC VISUAL WEB TASKS\n\n[Gold Answer]\n{'Reference': ['# AppAgent: Multimodal Agents as Smartphone Users '], 'Citation': ['# V-Zen: Efficient GUI Understanding and Precise Grounding With A Novel Multimodal LLM ']}\n\n[The Start of Assistant's Predicted Answer]\n{\"Reference\": [\"A Survey on Evaluation of Large Language Models\", \"Vicuna: An Open-Source Chatbot Impressing GPT-4 with $90 \\% *$ ChatGPT Quality\", \"ChatLaw: Open-Source Legal Large Language Model with Integrated External Knowledge Bases\", \"Compositional Semantic Parsing with Large Language Models\", \"Open LLM Leaderboard\", \"How Close is ChatGPT to Human Experts? Comparison Corpus, Evaluation, and Detection\", \"REALM: Retrieval-Augmented Language Model PreTraining\", \"Rethinking with Retrieval: Faithful Large Language Model Inference\", \"Measuring Massive Multitask Language Understanding\", \"Atlas: Few-shot Learning with Retrieval Augmented Language Models\", \"Flamingo: a visual language model for few-shot learning\", \"Vqa: Visual question answering\", \"Unlimiformer: Long-range transformers with unlimited length input\", \"Graph of thoughts: Solving elaborate problems with large language models\", \"Mind2web: Towards a generalist agent for the web\", \"Gemini: a family of highly capable multimodal models\", \"A real-world webagent with planning, long context understanding, and program synthesis\", \"A Survey on large language model based autonomous agents\", \"Self-consistency improves chain of thought reasoning in language models\", \"Image quality assessment: from error visibility to structural similarity\", \"The dawn of lmms: Preliminary explorations with gpt-4v (ision)\", \"Webshop: Towards scalable real-world web interaction with grounded language agents\", \"Tree of thoughts: Deliberate problem solving with large language models\", \"Agenttuning: Enabling generalized agent abilities for llms\", \"Gpt-4v (ision) is a generalist web agent, if grounded\", \"Gemini: a family of highly capable multimodal models\", \"Mobilevlm: A fast, reproducible and strong vision language assistant for mobile devices\", \"Long range arena: A benchmark for efficient transformers\", \"Bolaa: Benchmarking and orchestrating llm-augmented autonomous agents\", \"Agentbench: Evaluating llms as agents\", \"Gaia: a benchmark for general ai assistants\", \"Gpt-4 technical report\", \"Training language models to follow instructions with human feedback\", \"Scaling language models: Methods, analysis \\& insights from training gopher\", \"World of bits: An open-domain platform for web-based agents\", \"Multimodal few-shot learning with frozen language models\", \"Llama: Open and efficient foundation language models\", \"Llama 2: Open foundation and fine-tuned chat models\", \"Focused transformer: Contrastive training for context scaling\", \"Attention is all you need\", \"Show and tell: A neural image caption generator\", \"The rise and potential of large language model based agents: A survey\", \"Gpt-4v in wonderland: Large multimodal models for zero-shot smartphone gui navigation\", \"You only look at screens: Multimodal chain-of-action agents\", \"Appagent: Multimodal agents as smartphone users\", \"Mmmu: A massive multi-discipline multimodal understanding and reasoning benchmark for expert agi\", \"Opt: Open pre-trained transformer language models\", \"Palm: Scaling language modeling with pathways\", \"Scaling instruction-finetuned language models\", \"Microsoft coco: Common objects in context\", \"Improved baselines with visual instruction tuning\", \"Visual instruction tuning\", \"Agentbench: Evaluating llms as agents\", \"Mobilevlm: A fast, reproducible and strong vision language assistant for mobile devices\", \"Long range arena: A benchmark for efficient transformers\", \"Bolaa: Benchmarking and orchestrating llm-augmented autonomous agents\", \"Eyes wide shut? exploring the visual shortcomings of multimodal llms\", \"The dawn of lmms: Preliminary explorations with gpt-4v (ision)\"], \"Citation\": []}\n[The End of Assistant's Predicted Answer]\n\n[System]\nWe would like to request your feedback on the performance of the AI assistant in response to the user question displayed above according to the gold answer. Please use the following listed aspects and their descriptions as evaluation criteria:\n    - Accuracy and Hallucinations: The assistant's answer is semantically consistent with the gold answer; The numerical value and order need to be accurate, and there should be no hallucinations.\n    - Completeness: Referring to the reference answers, the assistant's answer should contain all the key points needed to answer the user's question; further elaboration on these key points can be omitted.\nPlease rate whether this answer is suitable for the question. Please note that the gold answer can be considered as a correct answer to the question.\n\nThe assistant receives an overall score on a scale of 1 to 100, where a higher score indicates better overall performance.\nPlease note that if the assistant's answer and the gold answer fully meet the above criteria, its overall rating should be the full marks (100).\nPlease first provide a comprehensive explanation of your evaluation, avoiding any potential bias.\nThen, output a line indicating the score of the Assistant.\n\nPLEASE OUTPUT WITH THE FOLLOWING FORMAT, WHERE THE SCORE IS A SCALE OF 1 TO 100 BY STRICTLY FOLLOWING THIS FORMAT: \"[[score]]\", FOR EXAMPLE \"Rating: [[100]]\":\n<start output>\nEvaluation evidence: your evluation explanation here, no more than 100 words\nRating: [[score]]\n<end output>\n\nNow, start your evaluation:",
        "generate_response": "{\"Reference\": [\"A Survey on Evaluation of Large Language Models\", \"Vicuna: An Open-Source Chatbot Impressing GPT-4 with $90 \\% *$ ChatGPT Quality\", \"ChatLaw: Open-Source Legal Large Language Model with Integrated External Knowledge Bases\", \"Compositional Semantic Parsing with Large Language Models\", \"Open LLM Leaderboard\", \"How Close is ChatGPT to Human Experts? Comparison Corpus, Evaluation, and Detection\", \"REALM: Retrieval-Augmented Language Model PreTraining\", \"Rethinking with Retrieval: Faithful Large Language Model Inference\", \"Measuring Massive Multitask Language Understanding\", \"Atlas: Few-shot Learning with Retrieval Augmented Language Models\", \"Flamingo: a visual language model for few-shot learning\", \"Vqa: Visual question answering\", \"Unlimiformer: Long-range transformers with unlimited length input\", \"Graph of thoughts: Solving elaborate problems with large language models\", \"Mind2web: Towards a generalist agent for the web\", \"Gemini: a family of highly capable multimodal models\", \"A real-world webagent with planning, long context understanding, and program synthesis\", \"A Survey on large language model based autonomous agents\", \"Self-consistency improves chain of thought reasoning in language models\", \"Image quality assessment: from error visibility to structural similarity\", \"The dawn of lmms: Preliminary explorations with gpt-4v (ision)\", \"Webshop: Towards scalable real-world web interaction with grounded language agents\", \"Tree of thoughts: Deliberate problem solving with large language models\", \"Agenttuning: Enabling generalized agent abilities for llms\", \"Gpt-4v (ision) is a generalist web agent, if grounded\", \"Gemini: a family of highly capable multimodal models\", \"Mobilevlm: A fast, reproducible and strong vision language assistant for mobile devices\", \"Long range arena: A benchmark for efficient transformers\", \"Bolaa: Benchmarking and orchestrating llm-augmented autonomous agents\", \"Agentbench: Evaluating llms as agents\", \"Gaia: a benchmark for general ai assistants\", \"Gpt-4 technical report\", \"Training language models to follow instructions with human feedback\", \"Scaling language models: Methods, analysis \\& insights from training gopher\", \"World of bits: An open-domain platform for web-based agents\", \"Multimodal few-shot learning with frozen language models\", \"Llama: Open and efficient foundation language models\", \"Llama 2: Open foundation and fine-tuned chat models\", \"Focused transformer: Contrastive training for context scaling\", \"Attention is all you need\", \"Show and tell: A neural image caption generator\", \"The rise and potential of large language model based agents: A survey\", \"Gpt-4v in wonderland: Large multimodal models for zero-shot smartphone gui navigation\", \"You only look at screens: Multimodal chain-of-action agents\", \"Appagent: Multimodal agents as smartphone users\", \"Mmmu: A massive multi-discipline multimodal understanding and reasoning benchmark for expert agi\", \"Opt: Open pre-trained transformer language models\", \"Palm: Scaling language modeling with pathways\", \"Scaling instruction-finetuned language models\", \"Microsoft coco: Common objects in context\", \"Improved baselines with visual instruction tuning\", \"Visual instruction tuning\", \"Agentbench: Evaluating llms as agents\", \"Mobilevlm: A fast, reproducible and strong vision language assistant for mobile devices\", \"Long range arena: A benchmark for efficient transformers\", \"Bolaa: Benchmarking and orchestrating llm-augmented autonomous agents\", \"Eyes wide shut? exploring the visual shortcomings of multimodal llms\", \"The dawn of lmms: Preliminary explorations with gpt-4v (ision)\"], \"Citation\": []}",
        "eval_response": "Evaluation evidence: The assistant's answer does not match the gold answer in terms of accuracy. The gold answer lists one reference and one citation, while the assistant's answer lists numerous references and no citations, none of which match the gold answer. This indicates a significant deviation from the required answer, suggesting issues with accuracy and hallucinations. The assistant's response also fails in completeness as it does not correctly identify the specific papers that reference or cite the given paper.\n\nRating: [[10]]",
        "score": 10
    },
    {
        "level": 3,
        "set": 4,
        "length": 246310,
        "type": "paper",
        "language": "en",
        "question": "Seven Failure Points When Engineering a Retrieval Augmented Generation System",
        "instruction": "We hope you will carefully study the provided papers and determine the citation relationships between them. Please follow the instructions below strictly to complete the task:\n\n#Specific Requirements:\n1. Reference: When a given paper mentions other provided papers, those other papers are considered as \"references\" for the given paper. To summarize in this specific context, references are about what the given paper is using.\n2. Citation: Conversely, when other provided papers mention the given paper in their works, the given paper is being \"cited\" by those other papers. To summarize in this specific context, citations are about who is using the given paper.\n3. Given a paper, you need to determine the citation or reference relationship between this paper and the other papers. Do not consider papers that are not provided.\n3. Please present the paper titles in a json format as follows: {{\"Reference\":[\"Reference Title 1\", \"Reference Title 2\", ..., \"Reference Title n\"], \"Citation\":[\"Citation Title 1\", \"Citation Title 2\", ..., \"Citation Title n\"]}}.\n4. If a paper does not have any references or citations, please leave the corresponding list empty, e.g.{{\"Refernce\":[]}}, {{\"Citation\":[]}}.",
        "prompt_template": "#Papers:\n{docs}\n\n{instruction}\n\n#The paper you need to analyze:\n{question}",
        "doc": [
            "2401.05856.md",
            "2404.19705.md",
            "2405.01868.md",
            "2405.07437.md",
            "2405.15007.md",
            "2405.20362.md",
            "2212.04356.md",
            "2305.13691.md",
            "2307.03172.md",
            "2308.07107.md",
            "2309.01431.md"
        ],
        "answer": {
            "Reference": [
                "# Robust Speech Recognition via Large-Scale Weak Supervision ",
                "# Few-Shot Data Synthesis for Open Domain Multi-Hop Question Answering ",
                "# Lost in the Middle: How Language Models Use Long Contexts ",
                "# Large Language Models for Information Retrieval: A Survey ",
                "# Benchmarking Large Language Models in Retrieval-Augmented Generation "
            ],
            "Citation": [
                "# When to Retrieve: Teaching LLMs to Utilize Information Retrieval Effectively ",
                "# Incorporating External Knowledge and Goal Guidance for LLM-based Conversational Recommender Systems ",
                "# Evaluation of Retrieval-Augmented Generation: A Survey ",
                "# RE-Adapt: Reverse Engineered Adaptation of Large Language Models ",
                "# Hallucination-Free? Assessing the Reliability of Leading AI Legal Research Tools "
            ]
        },
        "shuffle_doc": true,
        "id": "3fe4146b-97ed-44b6-ba27-76fcda016763",
        "prompt": "[Question]\n#Papers:\n{docs}\n\nWe hope you will carefully study the provided papers and determine the citation relationships between them. Please follow the instructions below strictly to complete the task:\n\n#Specific Requirements:\n1. Reference: When a given paper mentions other provided papers, those other papers are considered as \"references\" for the given paper. To summarize in this specific context, references are about what the given paper is using.\n2. Citation: Conversely, when other provided papers mention the given paper in their works, the given paper is being \"cited\" by those other papers. To summarize in this specific context, citations are about who is using the given paper.\n3. Given a paper, you need to determine the citation or reference relationship between this paper and the other papers. Do not consider papers that are not provided.\n3. Please present the paper titles in a json format as follows: {{\"Reference\":[\"Reference Title 1\", \"Reference Title 2\", ..., \"Reference Title n\"], \"Citation\":[\"Citation Title 1\", \"Citation Title 2\", ..., \"Citation Title n\"]}}.\n4. If a paper does not have any references or citations, please leave the corresponding list empty, e.g.{{\"Refernce\":[]}}, {{\"Citation\":[]}}.\n\n#The paper you need to analyze:\nSeven Failure Points When Engineering a Retrieval Augmented Generation System\n\n[Gold Answer]\n{'Reference': ['# Robust Speech Recognition via Large-Scale Weak Supervision ', '# Few-Shot Data Synthesis for Open Domain Multi-Hop Question Answering ', '# Lost in the Middle: How Language Models Use Long Contexts ', '# Large Language Models for Information Retrieval: A Survey ', '# Benchmarking Large Language Models in Retrieval-Augmented Generation '], 'Citation': ['# When to Retrieve: Teaching LLMs to Utilize Information Retrieval Effectively ', '# Incorporating External Knowledge and Goal Guidance for LLM-based Conversational Recommender Systems ', '# Evaluation of Retrieval-Augmented Generation: A Survey ', '# RE-Adapt: Reverse Engineered Adaptation of Large Language Models ', '# Hallucination-Free? Assessing the Reliability of Leading AI Legal Research Tools ']}\n\n[The Start of Assistant's Predicted Answer]\n{\"Reference\": [\"GPTCache: An Open-Source Semantic Cache for LLM Applications Enabling Faster Answers and Cost Savings\", \"Self-adaptive Machine Learning Systems: Research Challenges and Opportunities\", \"Benchmarking Large Language Models in Retrieval-Augmented Generation\", \"Efficient Open Domain Multi-Hop Question Answering with Few-Shot Data Synthesis\", \"Threshy: Supporting safe usage of intelligent web services\", \"Beware the evolving 'intelligent'web service! An integration architecture tactic to guard AI-first components\", \"Retrieval augmented language model pre-training\", \"Fidlight: Efficient and effective retrieval-augmented text generation\", \"Leveraging passage retrieval with generative models for open domain question answering\", \"BioASQ-QA: A manually curated corpus for biomedical question answering\", \"LoRA Fine-tuning Efficiently Undoes Safety Training in Llama 2-Chat 70B\", \"Lost in the middle: How language models use long contexts\", \"G-eval: Nlg evaluation using gpt-4 with better human alignment\", \"Retrieval-based prompt selection for code-related few-shot learning\", \"GPT-4 Technical Report\", \"Robust speech recognition via large-scale weak supervision\", \"Improving the domain adaptation of retrieval augmented generation (RAG) models for open domain question answering\"], \"Citation\": []}\n[The End of Assistant's Predicted Answer]\n\n[System]\nWe would like to request your feedback on the performance of the AI assistant in response to the user question displayed above according to the gold answer. Please use the following listed aspects and their descriptions as evaluation criteria:\n    - Accuracy and Hallucinations: The assistant's answer is semantically consistent with the gold answer; The numerical value and order need to be accurate, and there should be no hallucinations.\n    - Completeness: Referring to the reference answers, the assistant's answer should contain all the key points needed to answer the user's question; further elaboration on these key points can be omitted.\nPlease rate whether this answer is suitable for the question. Please note that the gold answer can be considered as a correct answer to the question.\n\nThe assistant receives an overall score on a scale of 1 to 100, where a higher score indicates better overall performance.\nPlease note that if the assistant's answer and the gold answer fully meet the above criteria, its overall rating should be the full marks (100).\nPlease first provide a comprehensive explanation of your evaluation, avoiding any potential bias.\nThen, output a line indicating the score of the Assistant.\n\nPLEASE OUTPUT WITH THE FOLLOWING FORMAT, WHERE THE SCORE IS A SCALE OF 1 TO 100 BY STRICTLY FOLLOWING THIS FORMAT: \"[[score]]\", FOR EXAMPLE \"Rating: [[100]]\":\n<start output>\nEvaluation evidence: your evluation explanation here, no more than 100 words\nRating: [[score]]\n<end output>\n\nNow, start your evaluation:",
        "generate_response": "{\"Reference\": [\"GPTCache: An Open-Source Semantic Cache for LLM Applications Enabling Faster Answers and Cost Savings\", \"Self-adaptive Machine Learning Systems: Research Challenges and Opportunities\", \"Benchmarking Large Language Models in Retrieval-Augmented Generation\", \"Efficient Open Domain Multi-Hop Question Answering with Few-Shot Data Synthesis\", \"Threshy: Supporting safe usage of intelligent web services\", \"Beware the evolving 'intelligent'web service! An integration architecture tactic to guard AI-first components\", \"Retrieval augmented language model pre-training\", \"Fidlight: Efficient and effective retrieval-augmented text generation\", \"Leveraging passage retrieval with generative models for open domain question answering\", \"BioASQ-QA: A manually curated corpus for biomedical question answering\", \"LoRA Fine-tuning Efficiently Undoes Safety Training in Llama 2-Chat 70B\", \"Lost in the middle: How language models use long contexts\", \"G-eval: Nlg evaluation using gpt-4 with better human alignment\", \"Retrieval-based prompt selection for code-related few-shot learning\", \"GPT-4 Technical Report\", \"Robust speech recognition via large-scale weak supervision\", \"Improving the domain adaptation of retrieval augmented generation (RAG) models for open domain question answering\"], \"Citation\": []}",
        "eval_response": "Evaluation evidence: The assistant's answer shows significant discrepancies from the gold answer. It includes multiple references that are not mentioned in the gold answer, indicating hallucinations. Additionally, it fails to list any citations, whereas the gold answer lists several. This shows a lack of accuracy and completeness in understanding and presenting the citation relationships as per the task requirements.\n\nRating: [[20]]",
        "score": 20
    },
    {
        "level": 3,
        "set": 2,
        "length": 59991,
        "type": "paper",
        "language": "en",
        "question": "TOFU: A Task of Fictitious Unlearning for LLMs",
        "instruction": "We hope you will carefully study the provided papers and determine the citation relationships between them. Please follow the instructions below strictly to complete the task:\n\n#Specific Requirements:\n1. Reference: When a given paper mentions other provided papers, those other papers are considered as \"references\" for the given paper. To summarize in this specific context, references are about what the given paper is using.\n2. Citation: Conversely, when other provided papers mention the given paper in their works, the given paper is being \"cited\" by those other papers. To summarize in this specific context, citations are about who is using the given paper.\n3. Given a paper, you need to determine the citation or reference relationship between this paper and the other papers. Do not consider papers that are not provided.\n3. Please present the paper titles in a json format as follows: {{\"Reference\":[\"Reference Title 1\", \"Reference Title 2\", ..., \"Reference Title n\"], \"Citation\":[\"Citation Title 1\", \"Citation Title 2\", ..., \"Citation Title n\"]}}.\n4. If a paper does not have any references or citations, please leave the corresponding list empty, e.g.{{\"Refernce\":[]}}, {{\"Citation\":[]}}.",
        "prompt_template": "#Papers:\n{docs}\n\n{instruction}\n\n#The paper you need to analyze:\n{question}",
        "doc": [
            "2401.06121.md",
            "2405.03097.md",
            "2405.17438.md"
        ],
        "answer": {
            "Reference": [],
            "Citation": [
                "# To Each (Textual Sequence) Its Own: Improving Memorized-Data Unlearning in Large Language Models ",
                "# An LLM-Tool Compiler for Fused Parallel Function Calling "
            ]
        },
        "shuffle_doc": true,
        "id": "21c7b1f8-e610-4cbd-aa12-2f2291cfd8d0",
        "prompt": "[Question]\n#Papers:\n{docs}\n\nWe hope you will carefully study the provided papers and determine the citation relationships between them. Please follow the instructions below strictly to complete the task:\n\n#Specific Requirements:\n1. Reference: When a given paper mentions other provided papers, those other papers are considered as \"references\" for the given paper. To summarize in this specific context, references are about what the given paper is using.\n2. Citation: Conversely, when other provided papers mention the given paper in their works, the given paper is being \"cited\" by those other papers. To summarize in this specific context, citations are about who is using the given paper.\n3. Given a paper, you need to determine the citation or reference relationship between this paper and the other papers. Do not consider papers that are not provided.\n3. Please present the paper titles in a json format as follows: {{\"Reference\":[\"Reference Title 1\", \"Reference Title 2\", ..., \"Reference Title n\"], \"Citation\":[\"Citation Title 1\", \"Citation Title 2\", ..., \"Citation Title n\"]}}.\n4. If a paper does not have any references or citations, please leave the corresponding list empty, e.g.{{\"Refernce\":[]}}, {{\"Citation\":[]}}.\n\n#The paper you need to analyze:\nTOFU: A Task of Fictitious Unlearning for LLMs\n\n[Gold Answer]\n{'Reference': [], 'Citation': ['# To Each (Textual Sequence) Its Own: Improving Memorized-Data Unlearning in Large Language Models ', '# An LLM-Tool Compiler for Fused Parallel Function Calling ']}\n\n[The Start of Assistant's Predicted Answer]\n{\"Reference\": [\"Unlearn what you want to forget: Efficient unlearning for llms\", \"Who's harry potter? approximate unlearning in llms\", \"Towards adversarial evaluations for inexact machine unlearning\", \"Extracting training data from large language models\", \"Membership inference attacks from first principles\", \"Unlearn what you want to forget: Efficient unlearning for llms\", \"Who's harry potter? approximate unlearning in llms\", \"Towards adversarial evaluations for inexact machine unlearning\", \"Certified data removal from machine learning models\", \"Separate the wheat from the chaff: Model deficiency unlearning via parameter-efficient module operation\", \"Are large pre-trained language models leaking your personal information?\", \"Auditing differentially private machine learning: How private is private sgd?\", \"Knowledge unlearning for mitigating privacy risks in language models\", \"Propile: Probing privacy leakage in large language models\", \"The brainy student: Scalable unlearning by selectively disobeying the teacher\", \"Towards unbounded machine unlearning\", \"Textbooks are all you need ii: phi-1.5 technical report\", \"Rouge: A package for automatic evaluation of summaries\", \"Quark: Controllable text generation with reinforced unlearning\", \"Locating and editing factual associations in gpt\", \"Adversary instantiation: Lower bounds for differentially private machine learning\", \"Ccpa regulations: Final regulation text\", \"Can sensitive information be deleted from llms? objectives for defending against extraction attacks\", \"In-context unlearning: Language models as few shot unlearners\", \"Direct preference optimization: Your language model is secretly a reward model\", \"Remember what you want to forget: Algorithms for machine unlearning\", \"Detecting pretraining data from large language models\", \"Membership inference attacks against machine learning models\", \"Privacy auditing with one (1) training run\", \"Llama 2: Open foundation and fine-tuned chat models\", \"The eu general data protection regulation (gdpr). a practical guide\", \"Jailbroken: How does llm safety training fail?\", \"Large language model unlearning\", \"Right to be forgotten in the era of large language models: Implications, challenges, and solutions\", \"A comprehensive study of knowledge editing for large language models\", \"Universal and transferable adversarial attacks on aligned language models\"], \"Citation\": []}\n[The End of Assistant's Predicted Answer]\n\n[System]\nWe would like to request your feedback on the performance of the AI assistant in response to the user question displayed above according to the gold answer. Please use the following listed aspects and their descriptions as evaluation criteria:\n    - Accuracy and Hallucinations: The assistant's answer is semantically consistent with the gold answer; The numerical value and order need to be accurate, and there should be no hallucinations.\n    - Completeness: Referring to the reference answers, the assistant's answer should contain all the key points needed to answer the user's question; further elaboration on these key points can be omitted.\nPlease rate whether this answer is suitable for the question. Please note that the gold answer can be considered as a correct answer to the question.\n\nThe assistant receives an overall score on a scale of 1 to 100, where a higher score indicates better overall performance.\nPlease note that if the assistant's answer and the gold answer fully meet the above criteria, its overall rating should be the full marks (100).\nPlease first provide a comprehensive explanation of your evaluation, avoiding any potential bias.\nThen, output a line indicating the score of the Assistant.\n\nPLEASE OUTPUT WITH THE FOLLOWING FORMAT, WHERE THE SCORE IS A SCALE OF 1 TO 100 BY STRICTLY FOLLOWING THIS FORMAT: \"[[score]]\", FOR EXAMPLE \"Rating: [[100]]\":\n<start output>\nEvaluation evidence: your evluation explanation here, no more than 100 words\nRating: [[score]]\n<end output>\n\nNow, start your evaluation:",
        "generate_response": "{\"Reference\": [\"Unlearn what you want to forget: Efficient unlearning for llms\", \"Who's harry potter? approximate unlearning in llms\", \"Towards adversarial evaluations for inexact machine unlearning\", \"Extracting training data from large language models\", \"Membership inference attacks from first principles\", \"Unlearn what you want to forget: Efficient unlearning for llms\", \"Who's harry potter? approximate unlearning in llms\", \"Towards adversarial evaluations for inexact machine unlearning\", \"Certified data removal from machine learning models\", \"Separate the wheat from the chaff: Model deficiency unlearning via parameter-efficient module operation\", \"Are large pre-trained language models leaking your personal information?\", \"Auditing differentially private machine learning: How private is private sgd?\", \"Knowledge unlearning for mitigating privacy risks in language models\", \"Propile: Probing privacy leakage in large language models\", \"The brainy student: Scalable unlearning by selectively disobeying the teacher\", \"Towards unbounded machine unlearning\", \"Textbooks are all you need ii: phi-1.5 technical report\", \"Rouge: A package for automatic evaluation of summaries\", \"Quark: Controllable text generation with reinforced unlearning\", \"Locating and editing factual associations in gpt\", \"Adversary instantiation: Lower bounds for differentially private machine learning\", \"Ccpa regulations: Final regulation text\", \"Can sensitive information be deleted from llms? objectives for defending against extraction attacks\", \"In-context unlearning: Language models as few shot unlearners\", \"Direct preference optimization: Your language model is secretly a reward model\", \"Remember what you want to forget: Algorithms for machine unlearning\", \"Detecting pretraining data from large language models\", \"Membership inference attacks against machine learning models\", \"Privacy auditing with one (1) training run\", \"Llama 2: Open foundation and fine-tuned chat models\", \"The eu general data protection regulation (gdpr). a practical guide\", \"Jailbroken: How does llm safety training fail?\", \"Large language model unlearning\", \"Right to be forgotten in the era of large language models: Implications, challenges, and solutions\", \"A comprehensive study of knowledge editing for large language models\", \"Universal and transferable adversarial attacks on aligned language models\"], \"Citation\": []}",
        "eval_response": "Evaluation evidence: The assistant's answer contains a significant number of hallucinations, listing numerous references that are not mentioned in the gold answer. The gold answer specifies that there are no references and two citations for the paper \"TOFU: A Task of Fictitious Unlearning for LLMs.\" The assistant incorrectly lists a large number of references and no citations, which is directly contrary to the gold answer. This indicates issues with both accuracy and completeness.\n\nRating: [[10]]",
        "score": 10
    },
    {
        "level": 3,
        "set": 2,
        "length": 54609,
        "type": "paper",
        "language": "en",
        "question": "Adapting Large Language Models for Document-Level Machine Translation",
        "instruction": "We hope you will carefully study the provided papers and determine the citation relationships between them. Please follow the instructions below strictly to complete the task:\n\n#Specific Requirements:\n1. Reference: When a given paper mentions other provided papers, those other papers are considered as \"references\" for the given paper. To summarize in this specific context, references are about what the given paper is using.\n2. Citation: Conversely, when other provided papers mention the given paper in their works, the given paper is being \"cited\" by those other papers. To summarize in this specific context, citations are about who is using the given paper.\n3. Given a paper, you need to determine the citation or reference relationship between this paper and the other papers. Do not consider papers that are not provided.\n3. Please present the paper titles in a json format as follows: {{\"Reference\":[\"Reference Title 1\", \"Reference Title 2\", ..., \"Reference Title n\"], \"Citation\":[\"Citation Title 1\", \"Citation Title 2\", ..., \"Citation Title n\"]}}.\n4. If a paper does not have any references or citations, please leave the corresponding list empty, e.g.{{\"Refernce\":[]}}, {{\"Citation\":[]}}.",
        "prompt_template": "#Papers:\n{docs}\n\n{instruction}\n\n#The paper you need to analyze:\n{question}",
        "doc": [
            "2401.06468.md",
            "2405.18922.md",
            "2310.13988.md"
        ],
        "answer": {
            "Reference": [
                "# GEMBA-MQM: Detecting Translation Quality Error Spans with GPT-4 "
            ],
            "Citation": [
                "# Understanding and Addressing the Under-Translation Problem from the Perspective of Decoding Objective "
            ]
        },
        "shuffle_doc": true,
        "id": "b0163974-b1bb-4cb9-9dd3-1b4be1a11a3e",
        "prompt": "[Question]\n#Papers:\n{docs}\n\nWe hope you will carefully study the provided papers and determine the citation relationships between them. Please follow the instructions below strictly to complete the task:\n\n#Specific Requirements:\n1. Reference: When a given paper mentions other provided papers, those other papers are considered as \"references\" for the given paper. To summarize in this specific context, references are about what the given paper is using.\n2. Citation: Conversely, when other provided papers mention the given paper in their works, the given paper is being \"cited\" by those other papers. To summarize in this specific context, citations are about who is using the given paper.\n3. Given a paper, you need to determine the citation or reference relationship between this paper and the other papers. Do not consider papers that are not provided.\n3. Please present the paper titles in a json format as follows: {{\"Reference\":[\"Reference Title 1\", \"Reference Title 2\", ..., \"Reference Title n\"], \"Citation\":[\"Citation Title 1\", \"Citation Title 2\", ..., \"Citation Title n\"]}}.\n4. If a paper does not have any references or citations, please leave the corresponding list empty, e.g.{{\"Refernce\":[]}}, {{\"Citation\":[]}}.\n\n#The paper you need to analyze:\nAdapting Large Language Models for Document-Level Machine Translation\n\n[Gold Answer]\n{'Reference': ['# GEMBA-MQM: Detecting Translation Quality Error Spans with GPT-4 '], 'Citation': ['# Understanding and Addressing the Under-Translation Problem from the Perspective of Decoding Objective ']}\n\n[The Start of Assistant's Predicted Answer]\n{\"Reference\":[\"Palm 2 technical report\", \"Llemma: An open language model for mathematics\", \"Evaluating discourse phenomena in neural machine translation\", \"Enriching word vectors with subword information\", \"Language models are few-shot learners\", \"Multidimensional quality metrics: A flexible system for assessing translation quality\", \"Overview of the IWSLT 2017 evaluation campaign\", \"Monolingual or multilingual instruction tuning: Which makes a better alpaca\", \"A large-scale test set for the evaluation of context-aware pronoun translation in neural machine translation\", \"Culturax: A cleaned, enormous, and multilingual dataset for large language models in 167 languages\", \"GPT-4 technical report\", \"Training language models to follow instructions with human feedback\", \"Bleu: a method for automatic evaluation of machine translation\", \"A call for clarity in reporting BLEU scores\", \"COMET: A neural framework for MT evaluation\", \"ChatGPT MT: Competitive for high- (but not low-) resource languages\", \"Multitask prompted training enables zero-shot task generalization\", \"BLOOM: A 176b-parameter open-access multilingual language model\", \"Flan-moe: Scaling instruction-finetuned language models with sparse mixture of experts\", \"Rethinking document-level neural machine translation\", \"When a good translation is wrong in context: Context-aware machine translation improves on deixis, ellipsis, and lexical cohesion\", \"A survey on document-level neural machine translation: Methods and evaluation\", \"Document-level machine translation with large language models\", \"A paradigm shift in machine translation: Boosting translation performance of large language models\", \"mT5: A massively multilingual pre-trained text-to-text transformer\", \"Bigtrans: Augmenting large language models with multilingual translation capability over 100 languages\", \"Multilingual document-level translation enables zero-shot transfer from sentences to documents\", \"Improving the transformer translation model with document-level context\", \"Long-short term masking transformer: A simple but effective baseline for document-level neural machine translation\", \"Bayling: Bridging cross-lingual alignment and instruction following through interactive translation for large language models\", \"Multilingual machine translation with large language models: Empirical results and analysis\"], \"Citation\":[\"GEMBA-MQM: Detecting Translation Quality Error Spans with GPT-4\"]}\n[The End of Assistant's Predicted Answer]\n\n[System]\nWe would like to request your feedback on the performance of the AI assistant in response to the user question displayed above according to the gold answer. Please use the following listed aspects and their descriptions as evaluation criteria:\n    - Accuracy and Hallucinations: The assistant's answer is semantically consistent with the gold answer; The numerical value and order need to be accurate, and there should be no hallucinations.\n    - Completeness: Referring to the reference answers, the assistant's answer should contain all the key points needed to answer the user's question; further elaboration on these key points can be omitted.\nPlease rate whether this answer is suitable for the question. Please note that the gold answer can be considered as a correct answer to the question.\n\nThe assistant receives an overall score on a scale of 1 to 100, where a higher score indicates better overall performance.\nPlease note that if the assistant's answer and the gold answer fully meet the above criteria, its overall rating should be the full marks (100).\nPlease first provide a comprehensive explanation of your evaluation, avoiding any potential bias.\nThen, output a line indicating the score of the Assistant.\n\nPLEASE OUTPUT WITH THE FOLLOWING FORMAT, WHERE THE SCORE IS A SCALE OF 1 TO 100 BY STRICTLY FOLLOWING THIS FORMAT: \"[[score]]\", FOR EXAMPLE \"Rating: [[100]]\":\n<start output>\nEvaluation evidence: your evluation explanation here, no more than 100 words\nRating: [[score]]\n<end output>\n\nNow, start your evaluation:",
        "generate_response": "{\"Reference\":[\"Palm 2 technical report\", \"Llemma: An open language model for mathematics\", \"Evaluating discourse phenomena in neural machine translation\", \"Enriching word vectors with subword information\", \"Language models are few-shot learners\", \"Multidimensional quality metrics: A flexible system for assessing translation quality\", \"Overview of the IWSLT 2017 evaluation campaign\", \"Monolingual or multilingual instruction tuning: Which makes a better alpaca\", \"A large-scale test set for the evaluation of context-aware pronoun translation in neural machine translation\", \"Culturax: A cleaned, enormous, and multilingual dataset for large language models in 167 languages\", \"GPT-4 technical report\", \"Training language models to follow instructions with human feedback\", \"Bleu: a method for automatic evaluation of machine translation\", \"A call for clarity in reporting BLEU scores\", \"COMET: A neural framework for MT evaluation\", \"ChatGPT MT: Competitive for high- (but not low-) resource languages\", \"Multitask prompted training enables zero-shot task generalization\", \"BLOOM: A 176b-parameter open-access multilingual language model\", \"Flan-moe: Scaling instruction-finetuned language models with sparse mixture of experts\", \"Rethinking document-level neural machine translation\", \"When a good translation is wrong in context: Context-aware machine translation improves on deixis, ellipsis, and lexical cohesion\", \"A survey on document-level neural machine translation: Methods and evaluation\", \"Document-level machine translation with large language models\", \"A paradigm shift in machine translation: Boosting translation performance of large language models\", \"mT5: A massively multilingual pre-trained text-to-text transformer\", \"Bigtrans: Augmenting large language models with multilingual translation capability over 100 languages\", \"Multilingual document-level translation enables zero-shot transfer from sentences to documents\", \"Improving the transformer translation model with document-level context\", \"Long-short term masking transformer: A simple but effective baseline for document-level neural machine translation\", \"Bayling: Bridging cross-lingual alignment and instruction following through interactive translation for large language models\", \"Multilingual machine translation with large language models: Empirical results and analysis\"], \"Citation\":[\"GEMBA-MQM: Detecting Translation Quality Error Spans with GPT-4\"]}",
        "eval_response": "Evaluation evidence: The assistant's answer does not match the gold answer in terms of accuracy and completeness. The assistant lists numerous references that are not mentioned in the gold answer, indicating a significant hallucination issue. Additionally, the citation provided in the assistant's answer matches the gold answer, but the overwhelming number of incorrect references overshadows this correct point. The assistant's response fails to accurately reflect the specific relationships requested in the task.\n\nRating: [[20]]",
        "score": 20
    },
    {
        "level": 4,
        "set": 3,
        "length": 117964,
        "type": "paper",
        "language": "en",
        "question": "",
        "instruction": "We kindly ask you to thoroughly review the provided papers and construct a citation chain from them. Please adhere to the following instructions strictly while completing the task:\n\n#Task Instructions:\nGiven several papers, you are required to identify and list the longest citation chain, which demonstrates the citation relationship among the provided papers.\n\n#Specific Requirements:\n1.Please present the titles of the papers in the form of a list, as follows: [\"Title of Paper 1\", \"Title of Paper 2\", ..., \"Title of Paper n\"].\n2.Ensure that the citation chain in the list is linear and continuous, meaning that the first paper title in the list (Paper 1) should not cite any other works. Instead, it should be cited by the next paper in the list (Paper 2); subsequently, each paper should then be cited by the next one in the list, continuing up to the last paper (Paper n).\n3.Consider only the citation relationships within the supplied collection of papers, and ensure that the citation chain accurately reflects the sequential citation order among these documents.\n4.Do not take into account any articles not provided, and disregard other non-linear citation relationships.",
        "prompt_template": "{instruction}\n\n#Papers Provided:\n{docs}",
        "doc": [
            "1911.00837.md",
            "2009.02235.md",
            "1910.10434.md",
            "2007.01560.md",
            "2405.08135.md"
        ],
        "answer": [
            "# RCC: Resilient Concurrent Consensus for High-Throughput Secure Transaction Processing ",
            "# FnF-BFT: Exploring Performance Limits of BFT Protocols ",
            "# Divide \\& Scale: <br> Formalization and Roadmap to Robust Sharding ",
            "# GRANDPA: a Byzantine Finality Gadget ",
            "# An Optimal Multilevel Quorum System for Probabilistic Consensus "
        ],
        "shuffle_doc": true,
        "id": "dc4a0c93-5ae1-43ad-947b-b4099d2e9001",
        "prompt": "[Question]\nWe kindly ask you to thoroughly review the provided papers and construct a citation chain from them. Please adhere to the following instructions strictly while completing the task:\n\n#Task Instructions:\nGiven several papers, you are required to identify and list the longest citation chain, which demonstrates the citation relationship among the provided papers.\n\n#Specific Requirements:\n1.Please present the titles of the papers in the form of a list, as follows: [\"Title of Paper 1\", \"Title of Paper 2\", ..., \"Title of Paper n\"].\n2.Ensure that the citation chain in the list is linear and continuous, meaning that the first paper title in the list (Paper 1) should not cite any other works. Instead, it should be cited by the next paper in the list (Paper 2); subsequently, each paper should then be cited by the next one in the list, continuing up to the last paper (Paper n).\n3.Consider only the citation relationships within the supplied collection of papers, and ensure that the citation chain accurately reflects the sequential citation order among these documents.\n4.Do not take into account any articles not provided, and disregard other non-linear citation relationships.\n\n#Papers Provided:\n{docs}\n\n[Gold Answer]\n['# RCC: Resilient Concurrent Consensus for High-Throughput Secure Transaction Processing ', '# FnF-BFT: Exploring Performance Limits of BFT Protocols ', '# Divide \\\\& Scale: <br> Formalization and Roadmap to Robust Sharding ', '# GRANDPA: a Byzantine Finality Gadget ', '# An Optimal Multilevel Quorum System for Probabilistic Consensus ']\n\n[The Start of Assistant's Predicted Answer]\n[\"Sok: Sharding on blockchain\", \"Divide & Scale: Formalization and Roadmap to Robust Sharding\"]\n[The End of Assistant's Predicted Answer]\n\n[System]\nWe would like to request your feedback on the performance of the AI assistant in response to the user question displayed above according to the gold answer. Please use the following listed aspects and their descriptions as evaluation criteria:\n    - Accuracy and Hallucinations: The assistant's answer is semantically consistent with the gold answer; The numerical value and order need to be accurate, and there should be no hallucinations.\n    - Completeness: Referring to the reference answers, the assistant's answer should contain all the key points needed to answer the user's question; further elaboration on these key points can be omitted.\nPlease rate whether this answer is suitable for the question. Please note that the gold answer can be considered as a correct answer to the question.\n\nThe assistant receives an overall score on a scale of 1 to 100, where a higher score indicates better overall performance.\nPlease note that if the assistant's answer and the gold answer fully meet the above criteria, its overall rating should be the full marks (100).\nPlease first provide a comprehensive explanation of your evaluation, avoiding any potential bias.\nThen, output a line indicating the score of the Assistant.\n\nPLEASE OUTPUT WITH THE FOLLOWING FORMAT, WHERE THE SCORE IS A SCALE OF 1 TO 100 BY STRICTLY FOLLOWING THIS FORMAT: \"[[score]]\", FOR EXAMPLE \"Rating: [[100]]\":\n<start output>\nEvaluation evidence: your evluation explanation here, no more than 100 words\nRating: [[score]]\n<end output>\n\nNow, start your evaluation:",
        "generate_response": "[\"Sok: Sharding on blockchain\", \"Divide & Scale: Formalization and Roadmap to Robust Sharding\"]",
        "eval_response": "Evaluation evidence: The assistant's answer does not match the gold answer in terms of the number of papers listed and the sequence of the citation chain. The gold answer lists a chain of five papers, while the assistant's answer only includes two papers, which are not in the correct order or complete as per the gold answer. This indicates issues with both accuracy and completeness.\n\nRating: [[20]]",
        "score": 20
    },
    {
        "level": 3,
        "set": 3,
        "length": 126748,
        "type": "paper",
        "language": "en",
        "question": "CharacterEval: A Chinese Benchmark for Role-Playing Conversational Agent Evaluation",
        "instruction": "We hope you will carefully study the provided papers and determine the citation relationships between them. Please follow the instructions below strictly to complete the task:\n\n#Specific Requirements:\n1. Reference: When a given paper mentions other provided papers, those other papers are considered as \"references\" for the given paper. To summarize in this specific context, references are about what the given paper is using.\n2. Citation: Conversely, when other provided papers mention the given paper in their works, the given paper is being \"cited\" by those other papers. To summarize in this specific context, citations are about who is using the given paper.\n3. Given a paper, you need to determine the citation or reference relationship between this paper and the other papers. Do not consider papers that are not provided.\n3. Please present the paper titles in a json format as follows: {{\"Reference\":[\"Reference Title 1\", \"Reference Title 2\", ..., \"Reference Title n\"], \"Citation\":[\"Citation Title 1\", \"Citation Title 2\", ..., \"Citation Title n\"]}}.\n4. If a paper does not have any references or citations, please leave the corresponding list empty, e.g.{{\"Refernce\":[]}}, {{\"Citation\":[]}}.",
        "prompt_template": "#Papers:\n{docs}\n\n{instruction}\n\n#The paper you need to analyze:\n{question}",
        "doc": [
            "2401.01275.md",
            "2405.10150.md",
            "2405.18027.md",
            "2309.05922.md",
            "2310.17976.md"
        ],
        "answer": {
            "Reference": [
                "# A Survey of Hallucination in \"Large\" Foundation Models ",
                "# InCharacter: Evaluating Personality Fidelity in Role-Playing Agents through Psychological Interviews "
            ],
            "Citation": [
                "# Speaker Verification in Agent-generated Conversations ",
                "# TimeChara: Evaluating Point-in-Time Character Hallucination of Role-Playing Large Language Models "
            ]
        },
        "shuffle_doc": true,
        "id": "09b79128-4dcd-4d6b-8293-d931ae0ff8c3",
        "prompt": "[Question]\n#Papers:\n{docs}\n\nWe hope you will carefully study the provided papers and determine the citation relationships between them. Please follow the instructions below strictly to complete the task:\n\n#Specific Requirements:\n1. Reference: When a given paper mentions other provided papers, those other papers are considered as \"references\" for the given paper. To summarize in this specific context, references are about what the given paper is using.\n2. Citation: Conversely, when other provided papers mention the given paper in their works, the given paper is being \"cited\" by those other papers. To summarize in this specific context, citations are about who is using the given paper.\n3. Given a paper, you need to determine the citation or reference relationship between this paper and the other papers. Do not consider papers that are not provided.\n3. Please present the paper titles in a json format as follows: {{\"Reference\":[\"Reference Title 1\", \"Reference Title 2\", ..., \"Reference Title n\"], \"Citation\":[\"Citation Title 1\", \"Citation Title 2\", ..., \"Citation Title n\"]}}.\n4. If a paper does not have any references or citations, please leave the corresponding list empty, e.g.{{\"Refernce\":[]}}, {{\"Citation\":[]}}.\n\n#The paper you need to analyze:\nCharacterEval: A Chinese Benchmark for Role-Playing Conversational Agent Evaluation\n\n[Gold Answer]\n{'Reference': ['# A Survey of Hallucination in \"Large\" Foundation Models ', '# InCharacter: Evaluating Personality Fidelity in Role-Playing Agents through Psychological Interviews '], 'Citation': ['# Speaker Verification in Agent-generated Conversations ', '# TimeChara: Evaluating Point-in-Time Character Hallucination of Role-Playing Large Language Models ']}\n\n[The Start of Assistant's Predicted Answer]\n{\"Reference\":[\"A Survey on Evaluation of Large Language Models\", \"S3: Social-network simulation system with large language model-empowered agents\", \"OpenAGI: When $11 m$ meets domain experts\", \"ChatGPT an ENFJ, BARD an ISTJ: Empirical Study on Personalities of Large Language Models\", \"Chatharuhi: Reviving anime character in reality via large language model\", \"Zero-resource knowledge-grounded dialogue generation\", \"A Survey on Empathetic Dialogue Systems\", \"Improving Factual Consistency Between a Response and Persona Facts\", \"The Myers-Briggs Type Indicator: Manual (1962)\", \"Attention is all you need\", \"A Survey on Large Language Model Based Autonomous Agents\", \"Does role-playing chatbots capture the character personalities? Assessing personality traits for role-playing chatbots\", \"Self-consistency improves chain of thought reasoning in language models\", \"RoleLLM: Benchmarking, Eliciting, and Enhancing Role-Playing Abilities of Large Language Models\", \"Chain-of-Thought Prompting Elicits Reasoning in Large Language Models\", \"Improving Factual Consistency for Knowledge-Grounded Dialogue Systems via Knowledge Enhancement and Alignment\", \"Deep Learning for Dialogue Systems: Chit-Chat and Beyond\", \"DynAEval: Unifying Turn and Dialogue Level Evaluation\", \"Siren's Song in the AI Ocean: A Survey on Hallucination in Large Language Models\", \"A Survey of Large Language Models\", \"Low-resource knowledge-grounded dialogue generation\", \"Judging LLM-as-a-Judge with MT-Bench and Chatbot Arena\", \"Personalized Dialogue Generation with Diversified Traits\", \"A Pre-Training Based Personalized Dialogue Generation Model with Persona-Sparse Data\", \"Less is more: Learning to refine dialogue history for personalized dialogue generation\", \"CharacterGLM: Customizing Chinese Conversational AI Characters with Large Language Models\"], \"Citation\":[]}\n[The End of Assistant's Predicted Answer]\n\n[System]\nWe would like to request your feedback on the performance of the AI assistant in response to the user question displayed above according to the gold answer. Please use the following listed aspects and their descriptions as evaluation criteria:\n    - Accuracy and Hallucinations: The assistant's answer is semantically consistent with the gold answer; The numerical value and order need to be accurate, and there should be no hallucinations.\n    - Completeness: Referring to the reference answers, the assistant's answer should contain all the key points needed to answer the user's question; further elaboration on these key points can be omitted.\nPlease rate whether this answer is suitable for the question. Please note that the gold answer can be considered as a correct answer to the question.\n\nThe assistant receives an overall score on a scale of 1 to 100, where a higher score indicates better overall performance.\nPlease note that if the assistant's answer and the gold answer fully meet the above criteria, its overall rating should be the full marks (100).\nPlease first provide a comprehensive explanation of your evaluation, avoiding any potential bias.\nThen, output a line indicating the score of the Assistant.\n\nPLEASE OUTPUT WITH THE FOLLOWING FORMAT, WHERE THE SCORE IS A SCALE OF 1 TO 100 BY STRICTLY FOLLOWING THIS FORMAT: \"[[score]]\", FOR EXAMPLE \"Rating: [[100]]\":\n<start output>\nEvaluation evidence: your evluation explanation here, no more than 100 words\nRating: [[score]]\n<end output>\n\nNow, start your evaluation:",
        "generate_response": "{\"Reference\":[\"A Survey on Evaluation of Large Language Models\", \"S3: Social-network simulation system with large language model-empowered agents\", \"OpenAGI: When $11 m$ meets domain experts\", \"ChatGPT an ENFJ, BARD an ISTJ: Empirical Study on Personalities of Large Language Models\", \"Chatharuhi: Reviving anime character in reality via large language model\", \"Zero-resource knowledge-grounded dialogue generation\", \"A Survey on Empathetic Dialogue Systems\", \"Improving Factual Consistency Between a Response and Persona Facts\", \"The Myers-Briggs Type Indicator: Manual (1962)\", \"Attention is all you need\", \"A Survey on Large Language Model Based Autonomous Agents\", \"Does role-playing chatbots capture the character personalities? Assessing personality traits for role-playing chatbots\", \"Self-consistency improves chain of thought reasoning in language models\", \"RoleLLM: Benchmarking, Eliciting, and Enhancing Role-Playing Abilities of Large Language Models\", \"Chain-of-Thought Prompting Elicits Reasoning in Large Language Models\", \"Improving Factual Consistency for Knowledge-Grounded Dialogue Systems via Knowledge Enhancement and Alignment\", \"Deep Learning for Dialogue Systems: Chit-Chat and Beyond\", \"DynAEval: Unifying Turn and Dialogue Level Evaluation\", \"Siren's Song in the AI Ocean: A Survey on Hallucination in Large Language Models\", \"A Survey of Large Language Models\", \"Low-resource knowledge-grounded dialogue generation\", \"Judging LLM-as-a-Judge with MT-Bench and Chatbot Arena\", \"Personalized Dialogue Generation with Diversified Traits\", \"A Pre-Training Based Personalized Dialogue Generation Model with Persona-Sparse Data\", \"Less is more: Learning to refine dialogue history for personalized dialogue generation\", \"CharacterGLM: Customizing Chinese Conversational AI Characters with Large Language Models\"], \"Citation\":[]}",
        "eval_response": "Evaluation evidence: The assistant's answer significantly deviates from the gold answer in terms of accuracy and completeness. The references listed in the assistant's response do not match any from the gold answer, indicating a misunderstanding or misinterpretation of the source material. Additionally, the assistant's response includes a large number of references that are not relevant to the specific task, suggesting hallucinations. The citation section is correctly empty, aligning with the gold answer, but this does not compensate for the inaccuracies in the references section.\n\nRating: [[20]]",
        "score": 20
    },
    {
        "level": 4,
        "set": 3,
        "length": 119375,
        "type": "paper",
        "language": "en",
        "question": "",
        "instruction": "We kindly ask you to thoroughly review the provided papers and construct a citation chain from them. Please adhere to the following instructions strictly while completing the task:\n\n#Task Instructions:\nGiven several papers, you are required to identify and list the longest citation chain, which demonstrates the citation relationship among the provided papers.\n\n#Specific Requirements:\n1.Please present the titles of the papers in the form of a list, as follows: [\"Title of Paper 1\", \"Title of Paper 2\", ..., \"Title of Paper n\"].\n2.Ensure that the citation chain in the list is linear and continuous, meaning that the first paper title in the list (Paper 1) should not cite any other works. Instead, it should be cited by the next paper in the list (Paper 2); subsequently, each paper should then be cited by the next one in the list, continuing up to the last paper (Paper n).\n3.Consider only the citation relationships within the supplied collection of papers, and ensure that the citation chain accurately reflects the sequential citation order among these documents.\n4.Do not take into account any articles not provided, and disregard other non-linear citation relationships.",
        "prompt_template": "{instruction}\n\n#Papers Provided:\n{docs}",
        "doc": [
            "2010.00453.md",
            "2107.04700.md",
            "2207.06731.md",
            "2405.07628.md"
        ],
        "answer": [
            "## Water is a radiation protection agent for ionised pyrrole",
            "# The unreasonable effectiveness of optimal transport in economics ",
            "# Monotone Comparative Statics for Equilibrium Problems ",
            "# Substitutability, equilibrium transport, and matching models "
        ],
        "shuffle_doc": true,
        "id": "c3bb0543-a769-49b0-96c4-5bea5f980c66",
        "prompt": "[Question]\nWe kindly ask you to thoroughly review the provided papers and construct a citation chain from them. Please adhere to the following instructions strictly while completing the task:\n\n#Task Instructions:\nGiven several papers, you are required to identify and list the longest citation chain, which demonstrates the citation relationship among the provided papers.\n\n#Specific Requirements:\n1.Please present the titles of the papers in the form of a list, as follows: [\"Title of Paper 1\", \"Title of Paper 2\", ..., \"Title of Paper n\"].\n2.Ensure that the citation chain in the list is linear and continuous, meaning that the first paper title in the list (Paper 1) should not cite any other works. Instead, it should be cited by the next paper in the list (Paper 2); subsequently, each paper should then be cited by the next one in the list, continuing up to the last paper (Paper n).\n3.Consider only the citation relationships within the supplied collection of papers, and ensure that the citation chain accurately reflects the sequential citation order among these documents.\n4.Do not take into account any articles not provided, and disregard other non-linear citation relationships.\n\n#Papers Provided:\n{docs}\n\n[Gold Answer]\n['## Water is a radiation protection agent for ionised pyrrole', '# The unreasonable effectiveness of optimal transport in economics ', '# Monotone Comparative Statics for Equilibrium Problems ', '# Substitutability, equilibrium transport, and matching models ']\n\n[The Start of Assistant's Predicted Answer]\n[\"The unreasonable effectiveness of optimal transport in economics\", \"Substitutability, equilibrium transport, and matching models\", \"Matching Models with Transferable Utility and Taxes\", \"Distance-to-Frontier Function in Matching Models\", \"Matching Equilibrium with Piecewise Linear Taxes\", \"Full Assignment Case in Matching Models with Transferability\"]\n[The End of Assistant's Predicted Answer]\n\n[System]\nWe would like to request your feedback on the performance of the AI assistant in response to the user question displayed above according to the gold answer. Please use the following listed aspects and their descriptions as evaluation criteria:\n    - Accuracy and Hallucinations: The assistant's answer is semantically consistent with the gold answer; The numerical value and order need to be accurate, and there should be no hallucinations.\n    - Completeness: Referring to the reference answers, the assistant's answer should contain all the key points needed to answer the user's question; further elaboration on these key points can be omitted.\nPlease rate whether this answer is suitable for the question. Please note that the gold answer can be considered as a correct answer to the question.\n\nThe assistant receives an overall score on a scale of 1 to 100, where a higher score indicates better overall performance.\nPlease note that if the assistant's answer and the gold answer fully meet the above criteria, its overall rating should be the full marks (100).\nPlease first provide a comprehensive explanation of your evaluation, avoiding any potential bias.\nThen, output a line indicating the score of the Assistant.\n\nPLEASE OUTPUT WITH THE FOLLOWING FORMAT, WHERE THE SCORE IS A SCALE OF 1 TO 100 BY STRICTLY FOLLOWING THIS FORMAT: \"[[score]]\", FOR EXAMPLE \"Rating: [[100]]\":\n<start output>\nEvaluation evidence: your evluation explanation here, no more than 100 words\nRating: [[score]]\n<end output>\n\nNow, start your evaluation:",
        "generate_response": "[\"The unreasonable effectiveness of optimal transport in economics\", \"Substitutability, equilibrium transport, and matching models\", \"Matching Models with Transferable Utility and Taxes\", \"Distance-to-Frontier Function in Matching Models\", \"Matching Equilibrium with Piecewise Linear Taxes\", \"Full Assignment Case in Matching Models with Transferability\"]",
        "eval_response": "Evaluation evidence: The assistant's answer does not match the gold answer in terms of the titles of the papers listed. The titles in the assistant's response do not appear in the gold answer, indicating a significant deviation from the provided correct citation chain. This discrepancy suggests inaccuracies and hallucinations in the assistant's response as it introduces titles not present in the gold answer. Therefore, the response fails to meet the accuracy and completeness criteria required for the task.\n\nRating: [[10]]",
        "score": 10
    },
    {
        "level": 3,
        "set": 3,
        "length": 117873,
        "type": "paper",
        "language": "en",
        "question": "MM-LLMs: Recent Advances in MultiModal Large Language Models",
        "instruction": "We hope you will carefully study the provided papers and determine the citation relationships between them. Please follow the instructions below strictly to complete the task:\n\n#Specific Requirements:\n1. Reference: When a given paper mentions other provided papers, those other papers are considered as \"references\" for the given paper. To summarize in this specific context, references are about what the given paper is using.\n2. Citation: Conversely, when other provided papers mention the given paper in their works, the given paper is being \"cited\" by those other papers. To summarize in this specific context, citations are about who is using the given paper.\n3. Given a paper, you need to determine the citation or reference relationship between this paper and the other papers. Do not consider papers that are not provided.\n3. Please present the paper titles in a json format as follows: {{\"Reference\":[\"Reference Title 1\", \"Reference Title 2\", ..., \"Reference Title n\"], \"Citation\":[\"Citation Title 1\", \"Citation Title 2\", ..., \"Citation Title n\"]}}.\n4. If a paper does not have any references or citations, please leave the corresponding list empty, e.g.{{\"Refernce\":[]}}, {{\"Citation\":[]}}.",
        "prompt_template": "#Papers:\n{docs}\n\n{instruction}\n\n#The paper you need to analyze:\n{question}",
        "doc": [
            "2401.13601.md",
            "2405.17927.md",
            "2405.20773.md",
            "2404.14688.md"
        ],
        "answer": {
            "Reference": [
                "# FMint: Bridging Human Designed and Data Pretrained Models for Differential Equation Foundation Model "
            ],
            "Citation": [
                "# The Evolution of Multimodal Model Architectures ",
                "# Visual-RolePlay: Universal Jailbreak Attack on MultiModal Large Language Models via Role-playing Image Character "
            ]
        },
        "shuffle_doc": true,
        "id": "8e1dafdf-9c1e-435d-a207-4e0d5c162f6b",
        "prompt": "[Question]\n#Papers:\n{docs}\n\nWe hope you will carefully study the provided papers and determine the citation relationships between them. Please follow the instructions below strictly to complete the task:\n\n#Specific Requirements:\n1. Reference: When a given paper mentions other provided papers, those other papers are considered as \"references\" for the given paper. To summarize in this specific context, references are about what the given paper is using.\n2. Citation: Conversely, when other provided papers mention the given paper in their works, the given paper is being \"cited\" by those other papers. To summarize in this specific context, citations are about who is using the given paper.\n3. Given a paper, you need to determine the citation or reference relationship between this paper and the other papers. Do not consider papers that are not provided.\n3. Please present the paper titles in a json format as follows: {{\"Reference\":[\"Reference Title 1\", \"Reference Title 2\", ..., \"Reference Title n\"], \"Citation\":[\"Citation Title 1\", \"Citation Title 2\", ..., \"Citation Title n\"]}}.\n4. If a paper does not have any references or citations, please leave the corresponding list empty, e.g.{{\"Refernce\":[]}}, {{\"Citation\":[]}}.\n\n#The paper you need to analyze:\nMM-LLMs: Recent Advances in MultiModal Large Language Models\n\n[Gold Answer]\n{'Reference': ['# FMint: Bridging Human Designed and Data Pretrained Models for Differential Equation Foundation Model '], 'Citation': ['# The Evolution of Multimodal Model Architectures ', '# Visual-RolePlay: Universal Jailbreak Attack on MultiModal Large Language Models via Role-playing Image Character ']}\n\n[The Start of Assistant's Predicted Answer]\n{\"Reference\":[\"A simple multimodal llm for better handling of text-rich visual questions\", \"Jointly Training Large Autoregressive Multimodal Models\", \"Vatt: Transformers for multimodal selfsupervised learning from raw video, audio and text\", \"Bliva: A simple multimodal llm for better handling of text-rich visual questions\", \"RL4F: Generating Natural Language Feedback with Reinforcement Learning for Repairing Model Outputs\", \"Flamingo: a visual language model for few-shot learning\", \"Retrieval-based language models and applications\", \"Openflamingo: An open-source framework for training large autoregressive vision-language models\", \"High-performance large-scale image recognition without normalization\", \"Language models are few-shot learners\", \"Coyo-700m: Image-text pair dataset\", \"Conceptual 12m: Pushing webscale image-text pre-training to recognize long-tail visual concepts\", \"Vlp: A survey on vision-language pre-training\", \"Xllm: Bootstrapping advanced large language models by treating multi-modalities as foreign languages\", \"LION: Empowering Multimodal Large Language Model with Dual-Level Visual Knowledge\", \"Minigpt-v2: large language model as a unified interface for vision-language multi-task learning\", \"Shikra: Unleashing Multimodal LLM's Referential Dialogue Magic\", \"Instructblip: Towards General-purpose Vision-Language Models with Instruction Tuning\", \"Pali-x: On Scaling up a Multilingual Vision and Language Model\", \"Laion-5b: An open large-scale dataset for training next generation imagetext models\", \"Vicuna: An OpenSource Chatbot Impressing GPT-4 with $90 \\% *$ ChatGPT Quality\", \"Palm: Scaling language modeling with pathways\", \"Mobilevlm: A fast, reproducible and strong vision language assistant for mobile devices\", \"MobileVLM V2: Faster and Stronger Baseline for Vision Language Model\", \"Qwen-audio: Advancing universal audio understanding via unified large-scale audiolanguage models\", \"Scaling instruction-finetuned language models\", \"Xtuner: A toolkit for efficiently fine-tuning llm\", \"A survey on multimodal large language models for autonomous driving\", \"Instructblip: Towards General-purpose Vision-Language Models with Instruction Tuning\", \"Qlora: Efficient finetuning of quantized llms\", \"No One Left Behind: Real-World Federated Class-Incremental Learning\", \"Heterogeneous forgetting compensation for class-incremental learning\", \"Federated classincremental learning\", \"Federated incremental semantic segmentation\", \"Cif: Continuous integrate-and-fire for end-to-end speech recognition\", \"In-context, interleaved, and interactive any-to-any generation\", \"Any-to-any generation via composable diffusion\", \"U12: Unifying language learning paradigms\", \"Gemini: a family of highly capable multimodal models\", \"Internlm: A multilingual language model with progressively enhanced capabilities\", \"Yi-VL\", \"Interleaved image-text generative modeling via multi-modal feature synchronizer\", \"Llama: Open and efficient foundation language models\", \"Llama 2: Open foundation and fine-tuned chat models\", \"Attention is all you need\", \"DecodingTrust: A Comprehensive Assessment of Trustworthiness in GPT Models\", \"Mobile-Agent: Autonomous Multi-Modal Mobile Device Agent with Visual Perception\", \"Ofa: Unifying architectures, tasks, and modalities through a simple sequence-to-sequence learning framework\", \"Image as a foreign language: Beit pretraining for all vision and vision-language tasks\", \"Image as a foreign language:beit pretraining for vision and vision-language tasks\", \"Finetuned language models are zero-shot learners\", \"Visual chatgpt: Talking, drawing and editing with visual foundation models\", \"Qbench: A benchmark for general-purpose foundation models on low-level vision\", \"Ai challenger: A largescale dataset for going deeper in image understanding\", \"Multimodal large language models: A survey\", \"Gpt4roi: Instruction tuning large language model on region-of-interest\", \"Opt: Open pre-trained transformer language models\", \"Egsde: Unpaired image-to-image translation via energy-guided stochastic differential equations\", \"A survey of large language models\", \"Bubogpt: Enabling visual grounding in multi-modal llms\", \"Beyond Anti-Forgetting: Multimodal Continual Instruction Tuning with Positive Forward Transfer\", \"Learn or Recall? Revisiting Incremental Learning with Pre-trained Language Models\", \"Interleaved vision-and-language generation via generative vokens\", \"Languagebind: Extending video-language pretraining to $\\mathrm{n}$-modality by language-based semantic alignment\", \"Minigpt-4: Enhancing vision-language understanding with advanced large language models\", \"Vl-GPT: A generative pre-trained transformer for vision and language understanding and generation\", \"Multimodal c4: An open, billion-scale corpus of images interleaved with text\"], \"Citation\":[]}\n[The End of Assistant's Predicted Answer]\n\n[System]\nWe would like to request your feedback on the performance of the AI assistant in response to the user question displayed above according to the gold answer. Please use the following listed aspects and their descriptions as evaluation criteria:\n    - Accuracy and Hallucinations: The assistant's answer is semantically consistent with the gold answer; The numerical value and order need to be accurate, and there should be no hallucinations.\n    - Completeness: Referring to the reference answers, the assistant's answer should contain all the key points needed to answer the user's question; further elaboration on these key points can be omitted.\nPlease rate whether this answer is suitable for the question. Please note that the gold answer can be considered as a correct answer to the question.\n\nThe assistant receives an overall score on a scale of 1 to 100, where a higher score indicates better overall performance.\nPlease note that if the assistant's answer and the gold answer fully meet the above criteria, its overall rating should be the full marks (100).\nPlease first provide a comprehensive explanation of your evaluation, avoiding any potential bias.\nThen, output a line indicating the score of the Assistant.\n\nPLEASE OUTPUT WITH THE FOLLOWING FORMAT, WHERE THE SCORE IS A SCALE OF 1 TO 100 BY STRICTLY FOLLOWING THIS FORMAT: \"[[score]]\", FOR EXAMPLE \"Rating: [[100]]\":\n<start output>\nEvaluation evidence: your evluation explanation here, no more than 100 words\nRating: [[score]]\n<end output>\n\nNow, start your evaluation:",
        "generate_response": "{\"Reference\":[\"A simple multimodal llm for better handling of text-rich visual questions\", \"Jointly Training Large Autoregressive Multimodal Models\", \"Vatt: Transformers for multimodal selfsupervised learning from raw video, audio and text\", \"Bliva: A simple multimodal llm for better handling of text-rich visual questions\", \"RL4F: Generating Natural Language Feedback with Reinforcement Learning for Repairing Model Outputs\", \"Flamingo: a visual language model for few-shot learning\", \"Retrieval-based language models and applications\", \"Openflamingo: An open-source framework for training large autoregressive vision-language models\", \"High-performance large-scale image recognition without normalization\", \"Language models are few-shot learners\", \"Coyo-700m: Image-text pair dataset\", \"Conceptual 12m: Pushing webscale image-text pre-training to recognize long-tail visual concepts\", \"Vlp: A survey on vision-language pre-training\", \"Xllm: Bootstrapping advanced large language models by treating multi-modalities as foreign languages\", \"LION: Empowering Multimodal Large Language Model with Dual-Level Visual Knowledge\", \"Minigpt-v2: large language model as a unified interface for vision-language multi-task learning\", \"Shikra: Unleashing Multimodal LLM's Referential Dialogue Magic\", \"Instructblip: Towards General-purpose Vision-Language Models with Instruction Tuning\", \"Pali-x: On Scaling up a Multilingual Vision and Language Model\", \"Laion-5b: An open large-scale dataset for training next generation imagetext models\", \"Vicuna: An OpenSource Chatbot Impressing GPT-4 with $90 \\% *$ ChatGPT Quality\", \"Palm: Scaling language modeling with pathways\", \"Mobilevlm: A fast, reproducible and strong vision language assistant for mobile devices\", \"MobileVLM V2: Faster and Stronger Baseline for Vision Language Model\", \"Qwen-audio: Advancing universal audio understanding via unified large-scale audiolanguage models\", \"Scaling instruction-finetuned language models\", \"Xtuner: A toolkit for efficiently fine-tuning llm\", \"A survey on multimodal large language models for autonomous driving\", \"Instructblip: Towards General-purpose Vision-Language Models with Instruction Tuning\", \"Qlora: Efficient finetuning of quantized llms\", \"No One Left Behind: Real-World Federated Class-Incremental Learning\", \"Heterogeneous forgetting compensation for class-incremental learning\", \"Federated classincremental learning\", \"Federated incremental semantic segmentation\", \"Cif: Continuous integrate-and-fire for end-to-end speech recognition\", \"In-context, interleaved, and interactive any-to-any generation\", \"Any-to-any generation via composable diffusion\", \"U12: Unifying language learning paradigms\", \"Gemini: a family of highly capable multimodal models\", \"Internlm: A multilingual language model with progressively enhanced capabilities\", \"Yi-VL\", \"Interleaved image-text generative modeling via multi-modal feature synchronizer\", \"Llama: Open and efficient foundation language models\", \"Llama 2: Open foundation and fine-tuned chat models\", \"Attention is all you need\", \"DecodingTrust: A Comprehensive Assessment of Trustworthiness in GPT Models\", \"Mobile-Agent: Autonomous Multi-Modal Mobile Device Agent with Visual Perception\", \"Ofa: Unifying architectures, tasks, and modalities through a simple sequence-to-sequence learning framework\", \"Image as a foreign language: Beit pretraining for all vision and vision-language tasks\", \"Image as a foreign language:beit pretraining for vision and vision-language tasks\", \"Finetuned language models are zero-shot learners\", \"Visual chatgpt: Talking, drawing and editing with visual foundation models\", \"Qbench: A benchmark for general-purpose foundation models on low-level vision\", \"Ai challenger: A largescale dataset for going deeper in image understanding\", \"Multimodal large language models: A survey\", \"Gpt4roi: Instruction tuning large language model on region-of-interest\", \"Opt: Open pre-trained transformer language models\", \"Egsde: Unpaired image-to-image translation via energy-guided stochastic differential equations\", \"A survey of large language models\", \"Bubogpt: Enabling visual grounding in multi-modal llms\", \"Beyond Anti-Forgetting: Multimodal Continual Instruction Tuning with Positive Forward Transfer\", \"Learn or Recall? Revisiting Incremental Learning with Pre-trained Language Models\", \"Interleaved vision-and-language generation via generative vokens\", \"Languagebind: Extending video-language pretraining to $\\mathrm{n}$-modality by language-based semantic alignment\", \"Minigpt-4: Enhancing vision-language understanding with advanced large language models\", \"Vl-GPT: A generative pre-trained transformer for vision and language understanding and generation\", \"Multimodal c4: An open, billion-scale corpus of images interleaved with text\"], \"Citation\":[]}",
        "eval_response": "Evaluation evidence: The assistant's answer does not align with the gold answer, indicating a significant accuracy issue and hallucination of content. The provided references and citations in the assistant's response do not match those in the gold answer, which lists specific papers as references and citations. The assistant's answer includes a long list of unrelated papers, showing a lack of focus and misunderstanding of the task requirements. This results in a failure to meet the completeness criterion as well.\n\nRating: [[10]]",
        "score": 10
    },
    {
        "level": 3,
        "set": 2,
        "length": 76151,
        "type": "paper",
        "language": "en",
        "question": "Scalable Pre-training of Large Autoregressive Image Models",
        "instruction": "We hope you will carefully study the provided papers and determine the citation relationships between them. Please follow the instructions below strictly to complete the task:\n\n#Specific Requirements:\n1. Reference: When a given paper mentions other provided papers, those other papers are considered as \"references\" for the given paper. To summarize in this specific context, references are about what the given paper is using.\n2. Citation: Conversely, when other provided papers mention the given paper in their works, the given paper is being \"cited\" by those other papers. To summarize in this specific context, citations are about who is using the given paper.\n3. Given a paper, you need to determine the citation or reference relationship between this paper and the other papers. Do not consider papers that are not provided.\n3. Please present the paper titles in a json format as follows: {{\"Reference\":[\"Reference Title 1\", \"Reference Title 2\", ..., \"Reference Title n\"], \"Citation\":[\"Citation Title 1\", \"Citation Title 2\", ..., \"Citation Title n\"]}}.\n4. If a paper does not have any references or citations, please leave the corresponding list empty, e.g.{{\"Refernce\":[]}}, {{\"Citation\":[]}}.",
        "prompt_template": "#Papers:\n{docs}\n\n{instruction}\n\n#The paper you need to analyze:\n{question}",
        "doc": [
            "2401.08541.md",
            "2405.16847.md",
            "2405.18320.md"
        ],
        "answer": {
            "Reference": [],
            "Citation": [
                "# TokenUnify: Scalable Autoregressive Visual Pre-training with Mixture Token Prediction ",
                "# Self-Supervised Learning Based Handwriting Verification "
            ]
        },
        "shuffle_doc": true,
        "id": "0238cb2a-155b-4a2f-b857-351b2daad687",
        "prompt": "[Question]\n#Papers:\n{docs}\n\nWe hope you will carefully study the provided papers and determine the citation relationships between them. Please follow the instructions below strictly to complete the task:\n\n#Specific Requirements:\n1. Reference: When a given paper mentions other provided papers, those other papers are considered as \"references\" for the given paper. To summarize in this specific context, references are about what the given paper is using.\n2. Citation: Conversely, when other provided papers mention the given paper in their works, the given paper is being \"cited\" by those other papers. To summarize in this specific context, citations are about who is using the given paper.\n3. Given a paper, you need to determine the citation or reference relationship between this paper and the other papers. Do not consider papers that are not provided.\n3. Please present the paper titles in a json format as follows: {{\"Reference\":[\"Reference Title 1\", \"Reference Title 2\", ..., \"Reference Title n\"], \"Citation\":[\"Citation Title 1\", \"Citation Title 2\", ..., \"Citation Title n\"]}}.\n4. If a paper does not have any references or citations, please leave the corresponding list empty, e.g.{{\"Refernce\":[]}}, {{\"Citation\":[]}}.\n\n#The paper you need to analyze:\nScalable Pre-training of Large Autoregressive Image Models\n\n[Gold Answer]\n{'Reference': [], 'Citation': ['# TokenUnify: Scalable Autoregressive Visual Pre-training with Mixture Token Prediction ', '# Self-Supervised Learning Based Handwriting Verification ']}\n\n[The Start of Assistant's Predicted Answer]\n{\"Reference\": [\"A Survey on Evaluation of Large Language Models\", \"Vicuna: An Open-Source Chatbot Impressing GPT-4 with $90 \\% *$ ChatGPT Quality\", \"ChatLaw: Open-Source Legal Large Language Model with Integrated External Knowledge Bases\", \"Compositional Semantic Parsing with Large Language Models\", \"Open LLM Leaderboard\", \"How Close is ChatGPT to Human Experts? Comparison Corpus, Evaluation, and Detection\", \"REALM: Retrieval-Augmented Language Model PreTraining\", \"Rethinking with Retrieval: Faithful Large Language Model Inference\", \"Measuring Massive Multitask Language Understanding\", \"Atlas: Few-shot Learning with Retrieval Augmented Language Models\", \"V-JEPA: Latent video prediction for visual representation learning\", \"Self-supervised learning from images with a joint-embedding predictive architecture\", \"Sequential modeling enables scalable learning for large vision models\", \"From detection of individual metastases to classification of lymph node status at the patient level: the camelyon17 challenge\", \"BEiT: Bert pretraining of image transformers\", \"Vicreg: Variance-invariance-covariance regularization for selfsupervised learning\", \"Cliquecnn: Deep unsupervised exemplar learning\", \"The iwildcam 2020 competition dataset\", \"A neural probabilistic language model\", \"Unsupervised learning by predicting noise\", \"Unsupervised pre-training of image features on non-curated data\", \"Unsupervised learning of visual features by contrasting cluster assignments\", \"Emerging properties in self-supervised vision transformers\", \"Generative pretraining from pixels\", \"A simple framework for contrastive learning of visual representations\", \"An empirical study of training self-supervised vision transformers\", \"Data filtering networks\", \"Datacomp: In search of the next generation of multimodal datasets\", \"Unsupervised representation learning by predicting image rotations\", \"Generative adversarial nets\", \"Scaling and benchmarking self-supervised visual representation learning\", \"Vision models are more robust and fair when pretrained on uncurated images without supervision\", \"Bootstrap your own latent-a new approach to self-supervised learning\", \"Mask r-cnn\", \"Momentum contrast for unsupervised visual representation learning\", \"Masked autoencoders are scalable vision learners\", \"Training compute-optimal large language models\", \"Lora: Low-rank adaptation of large language models\", \"Deep networks with stochastic depth\", \"Unsupervised learning of visual representations by solving jigsaw puzzles\", \"Wavenet: A generative model for raw audio\", \"Representation learning with contrastive predictive coding\", \"Dinov2: Learning robust visual features without supervision\", \"Zero-shot text-to-image generation\", \"Divide and contrast: Self-supervised learning from uncurated data\", \"Training data-efficient image transformers \\u0026 distillation through attention\", \"Going deeper with image transformers\", \"Llama: Open and efficient foundation language models\", \"Conditional image generation with pixelcnn decoders\", \"Pixel recurrent neural networks\", \"Rotation equivariant cnns for digital pathology\", \"Stacked denoising autoencoders: Learning useful representations in a deep network with a local denoising criterion\", \"Diffusion models as masked autoencoders\", \"ClusterFit: Improving Generalization of Visual Representations\", \"Coca: Contrastive captioners are image-text foundation models\", \"Barlow twins: Self-supervised learning via redundancy reduction\", \"mixup: Beyond empirical risk minimization\", \"Colorful image colorization\", \"ibot: Image bert pre-training with online tokenizer\"], \"Citation\": []}\n[The End of Assistant's Predicted Answer]\n\n[System]\nWe would like to request your feedback on the performance of the AI assistant in response to the user question displayed above according to the gold answer. Please use the following listed aspects and their descriptions as evaluation criteria:\n    - Accuracy and Hallucinations: The assistant's answer is semantically consistent with the gold answer; The numerical value and order need to be accurate, and there should be no hallucinations.\n    - Completeness: Referring to the reference answers, the assistant's answer should contain all the key points needed to answer the user's question; further elaboration on these key points can be omitted.\nPlease rate whether this answer is suitable for the question. Please note that the gold answer can be considered as a correct answer to the question.\n\nThe assistant receives an overall score on a scale of 1 to 100, where a higher score indicates better overall performance.\nPlease note that if the assistant's answer and the gold answer fully meet the above criteria, its overall rating should be the full marks (100).\nPlease first provide a comprehensive explanation of your evaluation, avoiding any potential bias.\nThen, output a line indicating the score of the Assistant.\n\nPLEASE OUTPUT WITH THE FOLLOWING FORMAT, WHERE THE SCORE IS A SCALE OF 1 TO 100 BY STRICTLY FOLLOWING THIS FORMAT: \"[[score]]\", FOR EXAMPLE \"Rating: [[100]]\":\n<start output>\nEvaluation evidence: your evluation explanation here, no more than 100 words\nRating: [[score]]\n<end output>\n\nNow, start your evaluation:",
        "generate_response": "{\"Reference\": [\"A Survey on Evaluation of Large Language Models\", \"Vicuna: An Open-Source Chatbot Impressing GPT-4 with $90 \\% *$ ChatGPT Quality\", \"ChatLaw: Open-Source Legal Large Language Model with Integrated External Knowledge Bases\", \"Compositional Semantic Parsing with Large Language Models\", \"Open LLM Leaderboard\", \"How Close is ChatGPT to Human Experts? Comparison Corpus, Evaluation, and Detection\", \"REALM: Retrieval-Augmented Language Model PreTraining\", \"Rethinking with Retrieval: Faithful Large Language Model Inference\", \"Measuring Massive Multitask Language Understanding\", \"Atlas: Few-shot Learning with Retrieval Augmented Language Models\", \"V-JEPA: Latent video prediction for visual representation learning\", \"Self-supervised learning from images with a joint-embedding predictive architecture\", \"Sequential modeling enables scalable learning for large vision models\", \"From detection of individual metastases to classification of lymph node status at the patient level: the camelyon17 challenge\", \"BEiT: Bert pretraining of image transformers\", \"Vicreg: Variance-invariance-covariance regularization for selfsupervised learning\", \"Cliquecnn: Deep unsupervised exemplar learning\", \"The iwildcam 2020 competition dataset\", \"A neural probabilistic language model\", \"Unsupervised learning by predicting noise\", \"Unsupervised pre-training of image features on non-curated data\", \"Unsupervised learning of visual features by contrasting cluster assignments\", \"Emerging properties in self-supervised vision transformers\", \"Generative pretraining from pixels\", \"A simple framework for contrastive learning of visual representations\", \"An empirical study of training self-supervised vision transformers\", \"Data filtering networks\", \"Datacomp: In search of the next generation of multimodal datasets\", \"Unsupervised representation learning by predicting image rotations\", \"Generative adversarial nets\", \"Scaling and benchmarking self-supervised visual representation learning\", \"Vision models are more robust and fair when pretrained on uncurated images without supervision\", \"Bootstrap your own latent-a new approach to self-supervised learning\", \"Mask r-cnn\", \"Momentum contrast for unsupervised visual representation learning\", \"Masked autoencoders are scalable vision learners\", \"Training compute-optimal large language models\", \"Lora: Low-rank adaptation of large language models\", \"Deep networks with stochastic depth\", \"Unsupervised learning of visual representations by solving jigsaw puzzles\", \"Wavenet: A generative model for raw audio\", \"Representation learning with contrastive predictive coding\", \"Dinov2: Learning robust visual features without supervision\", \"Zero-shot text-to-image generation\", \"Divide and contrast: Self-supervised learning from uncurated data\", \"Training data-efficient image transformers \\u0026 distillation through attention\", \"Going deeper with image transformers\", \"Llama: Open and efficient foundation language models\", \"Conditional image generation with pixelcnn decoders\", \"Pixel recurrent neural networks\", \"Rotation equivariant cnns for digital pathology\", \"Stacked denoising autoencoders: Learning useful representations in a deep network with a local denoising criterion\", \"Diffusion models as masked autoencoders\", \"ClusterFit: Improving Generalization of Visual Representations\", \"Coca: Contrastive captioners are image-text foundation models\", \"Barlow twins: Self-supervised learning via redundancy reduction\", \"mixup: Beyond empirical risk minimization\", \"Colorful image colorization\", \"ibot: Image bert pre-training with online tokenizer\"], \"Citation\": []}",
        "eval_response": "Evaluation evidence: The assistant's answer inaccurately lists numerous references that are not mentioned in the gold answer, indicating a significant hallucination issue. The gold answer specifies that there are no references and two citations for the paper in question, which the assistant fails to recognize, instead providing a long list of unrelated references and no citations. This discrepancy shows a lack of accuracy and completeness in understanding and presenting the citation relationships as required.\n\nRating: [[10]]",
        "score": 10
    },
    {
        "level": 3,
        "set": 4,
        "length": 208087,
        "type": "paper",
        "language": "en",
        "question": "KVQuant: Towards 10 Million Context Length LLM Inference with KV Cache Quantization",
        "instruction": "We hope you will carefully study the provided papers and determine the citation relationships between them. Please follow the instructions below strictly to complete the task:\n\n#Specific Requirements:\n1. Reference: When a given paper mentions other provided papers, those other papers are considered as \"references\" for the given paper. To summarize in this specific context, references are about what the given paper is using.\n2. Citation: Conversely, when other provided papers mention the given paper in their works, the given paper is being \"cited\" by those other papers. To summarize in this specific context, citations are about who is using the given paper.\n3. Given a paper, you need to determine the citation or reference relationship between this paper and the other papers. Do not consider papers that are not provided.\n3. Please present the paper titles in a json format as follows: {{\"Reference\":[\"Reference Title 1\", \"Reference Title 2\", ..., \"Reference Title n\"], \"Citation\":[\"Citation Title 1\", \"Citation Title 2\", ..., \"Citation Title n\"]}}.\n4. If a paper does not have any references or citations, please leave the corresponding list empty, e.g.{{\"Refernce\":[]}}, {{\"Citation\":[]}}.",
        "prompt_template": "#Papers:\n{docs}\n\n{instruction}\n\n#The paper you need to analyze:\n{question}",
        "doc": [
            "2401.18079.md",
            "2405.03917.md",
            "2405.04434.md",
            "2405.06219.md",
            "2405.14591.md",
            "2405.16444.md",
            "2310.01801.md",
            "2310.06825.md",
            "2310.19102.md",
            "2312.04985.md",
            "2403.14123.md"
        ],
        "answer": {
            "Reference": [
                "# MODEL TELLS You What TO Discard: ADAPTIVE KV CACHE COMPRESSION FOR LLMs ",
                "# Mistral 7B ",
                "# ATOM: LOW-BIT QUANTIZATION FOR EFFICIENT AND ACCURATE LLM SERVING ",
                "# SparQ Attention: Bandwidth-Efficient LLM Inference ",
                "# AI and Memory Wall "
            ],
            "Citation": [
                "# KV Cache is 1 Bit Per Channel: Efficient Large Language Model Inference with Coupled Quantization ",
                "# DeepSeek-V2: A Strong, Economical, and Efficient Mixture-of-Experts Language Model ",
                "# SKVQ: Sliding-window Key and Value Cache Quantization for Large Language Models ",
                "# Base of RoPE Bounds Context Length ",
                "# CacheBlend: Fast Large Language Model Serving for RAG with Cached Knowledge Fusion "
            ]
        },
        "shuffle_doc": true,
        "id": "e77b2fce-42c8-4184-ab03-b8efe610bfd3",
        "prompt": "[Question]\n#Papers:\n{docs}\n\nWe hope you will carefully study the provided papers and determine the citation relationships between them. Please follow the instructions below strictly to complete the task:\n\n#Specific Requirements:\n1. Reference: When a given paper mentions other provided papers, those other papers are considered as \"references\" for the given paper. To summarize in this specific context, references are about what the given paper is using.\n2. Citation: Conversely, when other provided papers mention the given paper in their works, the given paper is being \"cited\" by those other papers. To summarize in this specific context, citations are about who is using the given paper.\n3. Given a paper, you need to determine the citation or reference relationship between this paper and the other papers. Do not consider papers that are not provided.\n3. Please present the paper titles in a json format as follows: {{\"Reference\":[\"Reference Title 1\", \"Reference Title 2\", ..., \"Reference Title n\"], \"Citation\":[\"Citation Title 1\", \"Citation Title 2\", ..., \"Citation Title n\"]}}.\n4. If a paper does not have any references or citations, please leave the corresponding list empty, e.g.{{\"Refernce\":[]}}, {{\"Citation\":[]}}.\n\n#The paper you need to analyze:\nKVQuant: Towards 10 Million Context Length LLM Inference with KV Cache Quantization\n\n[Gold Answer]\n{'Reference': ['# MODEL TELLS You What TO Discard: ADAPTIVE KV CACHE COMPRESSION FOR LLMs ', '# Mistral 7B ', '# ATOM: LOW-BIT QUANTIZATION FOR EFFICIENT AND ACCURATE LLM SERVING ', '# SparQ Attention: Bandwidth-Efficient LLM Inference ', '# AI and Memory Wall '], 'Citation': ['# KV Cache is 1 Bit Per Channel: Efficient Large Language Model Inference with Coupled Quantization ', '# DeepSeek-V2: A Strong, Economical, and Efficient Mixture-of-Experts Language Model ', '# SKVQ: Sliding-window Key and Value Cache Quantization for Large Language Models ', '# Base of RoPE Bounds Context Length ', '# CacheBlend: Fast Large Language Model Serving for RAG with Cached Knowledge Fusion ']}\n\n[The Start of Assistant's Predicted Answer]\n{\"Reference\": [\"Introducing claude 2.1\", \"Understanding and overcoming the challenges of efficient transformer quantization\", \"Extending context window of large language models via positional interpolation\", \"Longlora: Efficient fine-tuning of long-context large language models\", \"Llm.int8 (): 8bit matrix multiplication for transformers at scale\", \"Qlora: Efficient finetuning of quantized llms\", \"Spqr: A sparse-quantized representation for near-lossless llm weight compression\", \"Hawq-v2: Hessian aware trace-weighted quantization of neural networks\", \"Hawq: Hessian aware quantization of neural networks with mixed-precision\", \"Balanced csr sparse matrix-vector product on graphics processors\", \"A framework for few-shot language model evaluation\", \"Model tells you what to discard: Adaptive kv cache compression for llms\", \"Ai and memory wall\", \"Lm-infinite: Simple on-the-fly length generalization for large language models\", \"Measuring massive multitask language understanding\", \"Rethinking channel dimensions to isolate outliers for low-bit weight quantization of large language models\", \"Mistral 7b\", \"Squeezellm: Dense-and-sparse quantization\", \"Full stack optimization of transformer inference: a survey\", \"How long can context length of opensource llms truly promise?\", \"Norm tweaking: Highperformance low-bit quantization of large language models\", \"Awq: Activation-aware weight quantization for llm compression and acceleration\", \"Intactkv: Improving large language model quantization by keeping pivot tokens intact\", \"Llmqat: Data-free quantization aware training for large language models\", \"Scissorhands: Exploiting the persistence of importance hypothesis for llm kv cache compression at test time\", \"Kivi: Plug-and-play 2bit kv cache quantization with streaming asymmetric quantization\", \"Landmark attention: Random-access infinite context length for transformers\", \"Sensitivity and generalization in neural networks: an empirical study\", \"New models and developer products announced at devday 2023\", \"Sparq attention: Bandwidth-efficient llm inference\", \"Omniquant: Omnidirectionally calibrated quantization for large language models\", \"Flexgen: Highthroughput generative inference of large language models with a single gpu\", \"Roformer: Enhanced transformer with rotary position embedding\", \"Llama: Open and efficient foundation language models\", \"Llama 2: Open foundation and fine-tuned chat models\", \"Outlier suppression+: Accurate quantization of large language models by equivalent and optimal shifting and scaling\", \"Outlier suppression: Pushing the limit of low-bit transformer language models\", \"Smoothquant: Accurate and efficient post-training quantization for large language models\", \"Efficient streaming language models with attention sinks\", \"H 2 o Heavy-hitter oracle for efficient generative inference of large language models\", \"Atom: Low-bit quantization for efficient and accurate $1 \\\\mathrm{~lm}$ serving\", \"Pose: Efficient context window extension of llms via positional skipwise training\"], \"Citation\": [\"Base of RoPE Bounds Context Length\", \"SparQ Attention: Bandwidth-Efficient LLM Inference\", \"SKVQ: Sliding-window Key and Value Cache Quantization for Large Language Models\"]}\n[The End of Assistant's Predicted Answer]\n\n[System]\nWe would like to request your feedback on the performance of the AI assistant in response to the user question displayed above according to the gold answer. Please use the following listed aspects and their descriptions as evaluation criteria:\n    - Accuracy and Hallucinations: The assistant's answer is semantically consistent with the gold answer; The numerical value and order need to be accurate, and there should be no hallucinations.\n    - Completeness: Referring to the reference answers, the assistant's answer should contain all the key points needed to answer the user's question; further elaboration on these key points can be omitted.\nPlease rate whether this answer is suitable for the question. Please note that the gold answer can be considered as a correct answer to the question.\n\nThe assistant receives an overall score on a scale of 1 to 100, where a higher score indicates better overall performance.\nPlease note that if the assistant's answer and the gold answer fully meet the above criteria, its overall rating should be the full marks (100).\nPlease first provide a comprehensive explanation of your evaluation, avoiding any potential bias.\nThen, output a line indicating the score of the Assistant.\n\nPLEASE OUTPUT WITH THE FOLLOWING FORMAT, WHERE THE SCORE IS A SCALE OF 1 TO 100 BY STRICTLY FOLLOWING THIS FORMAT: \"[[score]]\", FOR EXAMPLE \"Rating: [[100]]\":\n<start output>\nEvaluation evidence: your evluation explanation here, no more than 100 words\nRating: [[score]]\n<end output>\n\nNow, start your evaluation:",
        "generate_response": "{\"Reference\": [\"Introducing claude 2.1\", \"Understanding and overcoming the challenges of efficient transformer quantization\", \"Extending context window of large language models via positional interpolation\", \"Longlora: Efficient fine-tuning of long-context large language models\", \"Llm.int8 (): 8bit matrix multiplication for transformers at scale\", \"Qlora: Efficient finetuning of quantized llms\", \"Spqr: A sparse-quantized representation for near-lossless llm weight compression\", \"Hawq-v2: Hessian aware trace-weighted quantization of neural networks\", \"Hawq: Hessian aware quantization of neural networks with mixed-precision\", \"Balanced csr sparse matrix-vector product on graphics processors\", \"A framework for few-shot language model evaluation\", \"Model tells you what to discard: Adaptive kv cache compression for llms\", \"Ai and memory wall\", \"Lm-infinite: Simple on-the-fly length generalization for large language models\", \"Measuring massive multitask language understanding\", \"Rethinking channel dimensions to isolate outliers for low-bit weight quantization of large language models\", \"Mistral 7b\", \"Squeezellm: Dense-and-sparse quantization\", \"Full stack optimization of transformer inference: a survey\", \"How long can context length of opensource llms truly promise?\", \"Norm tweaking: Highperformance low-bit quantization of large language models\", \"Awq: Activation-aware weight quantization for llm compression and acceleration\", \"Intactkv: Improving large language model quantization by keeping pivot tokens intact\", \"Llmqat: Data-free quantization aware training for large language models\", \"Scissorhands: Exploiting the persistence of importance hypothesis for llm kv cache compression at test time\", \"Kivi: Plug-and-play 2bit kv cache quantization with streaming asymmetric quantization\", \"Landmark attention: Random-access infinite context length for transformers\", \"Sensitivity and generalization in neural networks: an empirical study\", \"New models and developer products announced at devday 2023\", \"Sparq attention: Bandwidth-efficient llm inference\", \"Omniquant: Omnidirectionally calibrated quantization for large language models\", \"Flexgen: Highthroughput generative inference of large language models with a single gpu\", \"Roformer: Enhanced transformer with rotary position embedding\", \"Llama: Open and efficient foundation language models\", \"Llama 2: Open foundation and fine-tuned chat models\", \"Outlier suppression+: Accurate quantization of large language models by equivalent and optimal shifting and scaling\", \"Outlier suppression: Pushing the limit of low-bit transformer language models\", \"Smoothquant: Accurate and efficient post-training quantization for large language models\", \"Efficient streaming language models with attention sinks\", \"H 2 o Heavy-hitter oracle for efficient generative inference of large language models\", \"Atom: Low-bit quantization for efficient and accurate $1 \\\\mathrm{~lm}$ serving\", \"Pose: Efficient context window extension of llms via positional skipwise training\"], \"Citation\": [\"Base of RoPE Bounds Context Length\", \"SparQ Attention: Bandwidth-Efficient LLM Inference\", \"SKVQ: Sliding-window Key and Value Cache Quantization for Large Language Models\"]}",
        "eval_response": "Evaluation evidence: The assistant's answer shows significant discrepancies from the gold answer. The references and citations listed do not match those in the gold answer, indicating inaccuracies and hallucinations. The assistant's response includes titles not present in the gold answer, and it misses several key references and citations that are crucial for a correct response. This demonstrates a lack of accuracy and completeness in understanding and presenting the citation relationships for the specified paper.\n\nRating: [[20]]",
        "score": 20
    },
    {
        "level": 3,
        "set": 3,
        "length": 128164,
        "type": "paper",
        "language": "en",
        "question": "Self-Play Fine-Tuning Converts Weak Language Models to Strong Language Models",
        "instruction": "We hope you will carefully study the provided papers and determine the citation relationships between them. Please follow the instructions below strictly to complete the task:\n\n#Specific Requirements:\n1. Reference: When a given paper mentions other provided papers, those other papers are considered as \"references\" for the given paper. To summarize in this specific context, references are about what the given paper is using.\n2. Citation: Conversely, when other provided papers mention the given paper in their works, the given paper is being \"cited\" by those other papers. To summarize in this specific context, citations are about who is using the given paper.\n3. Given a paper, you need to determine the citation or reference relationship between this paper and the other papers. Do not consider papers that are not provided.\n3. Please present the paper titles in a json format as follows: {{\"Reference\":[\"Reference Title 1\", \"Reference Title 2\", ..., \"Reference Title n\"], \"Citation\":[\"Citation Title 1\", \"Citation Title 2\", ..., \"Citation Title n\"]}}.\n4. If a paper does not have any references or citations, please leave the corresponding list empty, e.g.{{\"Refernce\":[]}}, {{\"Citation\":[]}}.",
        "prompt_template": "#Papers:\n{docs}\n\n{instruction}\n\n#The paper you need to analyze:\n{question}",
        "doc": [
            "2401.01335.md",
            "2405.19716.md",
            "2405.20215.md",
            "2312.09241.md",
            "2312.09390.md"
        ],
        "answer": {
            "Reference": [
                "# TinyGSM: achieving $>80 \\%$ on GSM8k with small language models ",
                "## WEAK-TO-STRONG GENERALIZATION: ELICITING STRONG CAPABILITIES WITH WEAK SUPERVISION"
            ],
            "Citation": [
                "# Enhancing Large Vision Language Models with Self-Training on Image Comprehension ",
                "# TS-Align: A Teacher-Student Collaborative Framework for Scalable Iterative Finetuning of Large Language Models "
            ]
        },
        "shuffle_doc": true,
        "id": "84bd6e6f-ad9e-4566-a349-501a5854e9ca",
        "prompt": "[Question]\n#Papers:\n{docs}\n\nWe hope you will carefully study the provided papers and determine the citation relationships between them. Please follow the instructions below strictly to complete the task:\n\n#Specific Requirements:\n1. Reference: When a given paper mentions other provided papers, those other papers are considered as \"references\" for the given paper. To summarize in this specific context, references are about what the given paper is using.\n2. Citation: Conversely, when other provided papers mention the given paper in their works, the given paper is being \"cited\" by those other papers. To summarize in this specific context, citations are about who is using the given paper.\n3. Given a paper, you need to determine the citation or reference relationship between this paper and the other papers. Do not consider papers that are not provided.\n3. Please present the paper titles in a json format as follows: {{\"Reference\":[\"Reference Title 1\", \"Reference Title 2\", ..., \"Reference Title n\"], \"Citation\":[\"Citation Title 1\", \"Citation Title 2\", ..., \"Citation Title n\"]}}.\n4. If a paper does not have any references or citations, please leave the corresponding list empty, e.g.{{\"Refernce\":[]}}, {{\"Citation\":[]}}.\n\n#The paper you need to analyze:\nSelf-Play Fine-Tuning Converts Weak Language Models to Strong Language Models\n\n[Gold Answer]\n{'Reference': ['# TinyGSM: achieving $>80 \\\\%$ on GSM8k with small language models ', '## WEAK-TO-STRONG GENERALIZATION: ELICITING STRONG CAPABILITIES WITH WEAK SUPERVISION'], 'Citation': ['# Enhancing Large Vision Language Models with Self-Training on Image Comprehension ', '# TS-Align: A Teacher-Student Collaborative Framework for Scalable Iterative Finetuning of Large Language Models ']}\n\n[The Start of Assistant's Predicted Answer]\n{\"Reference\": [\"A Survey on Evaluation of Large Language Models\", \"Vicuna: An Open-Source Chatbot Impressing GPT-4 with $90 \\% *$ ChatGPT Quality\", \"Ultrafeedback: Boosting language models with high-quality feedback\", \"Generative Adversarial Imitation Learning (GAIL)\", \"Sparks of artificial general intelligence: Early experiments with gpt-4\", \"Scaling instruction-finetuned language models\", \"Curriculum learning: A survey\", \"Ultrafeedback: Boosting language models with high-quality feedback\", \"How does semi-supervised learning with pseudo-labelers work? a case study\", \"A unified game-theoretic approach to multiagent reinforcement learning\", \"Pseudo-label: The simple and efficient semi-supervised learning method for deep neural networks\", \"Self-paced visual category discovery\", \"Competition-level code generation with alphacode\", \"Truthfulqa: Measuring how models mimic human falsehoods\", \"Tinygsm: achieving $>80 \\%$ on gsm8k with small language models\", \"Beyond human data: Scaling self-training for problem-solving with language models\", \"Learning to summarize with human feedback\", \"Stanford alpaca: An instruction-following llama model\", \"AlphaStar: Mastering the Real-Time Strategy Game StarCraft II\", \"Decoding data quality via synthetic corruptions: Embedding-guided pruning of code data\", \"Metamath: Bootstrap your own mathematical questions for large language models\", \"Scaling relationship on learning mathematical reasoning with large language models\", \"Judging llm-as-a-judge with mt-bench and chatbot arena\", \"Fine-tuning language models from human preferences\"], \"Citation\": []}\n[The End of Assistant's Predicted Answer]\n\n[System]\nWe would like to request your feedback on the performance of the AI assistant in response to the user question displayed above according to the gold answer. Please use the following listed aspects and their descriptions as evaluation criteria:\n    - Accuracy and Hallucinations: The assistant's answer is semantically consistent with the gold answer; The numerical value and order need to be accurate, and there should be no hallucinations.\n    - Completeness: Referring to the reference answers, the assistant's answer should contain all the key points needed to answer the user's question; further elaboration on these key points can be omitted.\nPlease rate whether this answer is suitable for the question. Please note that the gold answer can be considered as a correct answer to the question.\n\nThe assistant receives an overall score on a scale of 1 to 100, where a higher score indicates better overall performance.\nPlease note that if the assistant's answer and the gold answer fully meet the above criteria, its overall rating should be the full marks (100).\nPlease first provide a comprehensive explanation of your evaluation, avoiding any potential bias.\nThen, output a line indicating the score of the Assistant.\n\nPLEASE OUTPUT WITH THE FOLLOWING FORMAT, WHERE THE SCORE IS A SCALE OF 1 TO 100 BY STRICTLY FOLLOWING THIS FORMAT: \"[[score]]\", FOR EXAMPLE \"Rating: [[100]]\":\n<start output>\nEvaluation evidence: your evluation explanation here, no more than 100 words\nRating: [[score]]\n<end output>\n\nNow, start your evaluation:",
        "generate_response": "{\"Reference\": [\"A Survey on Evaluation of Large Language Models\", \"Vicuna: An Open-Source Chatbot Impressing GPT-4 with $90 \\% *$ ChatGPT Quality\", \"Ultrafeedback: Boosting language models with high-quality feedback\", \"Generative Adversarial Imitation Learning (GAIL)\", \"Sparks of artificial general intelligence: Early experiments with gpt-4\", \"Scaling instruction-finetuned language models\", \"Curriculum learning: A survey\", \"Ultrafeedback: Boosting language models with high-quality feedback\", \"How does semi-supervised learning with pseudo-labelers work? a case study\", \"A unified game-theoretic approach to multiagent reinforcement learning\", \"Pseudo-label: The simple and efficient semi-supervised learning method for deep neural networks\", \"Self-paced visual category discovery\", \"Competition-level code generation with alphacode\", \"Truthfulqa: Measuring how models mimic human falsehoods\", \"Tinygsm: achieving $>80 \\%$ on gsm8k with small language models\", \"Beyond human data: Scaling self-training for problem-solving with language models\", \"Learning to summarize with human feedback\", \"Stanford alpaca: An instruction-following llama model\", \"AlphaStar: Mastering the Real-Time Strategy Game StarCraft II\", \"Decoding data quality via synthetic corruptions: Embedding-guided pruning of code data\", \"Metamath: Bootstrap your own mathematical questions for large language models\", \"Scaling relationship on learning mathematical reasoning with large language models\", \"Judging llm-as-a-judge with mt-bench and chatbot arena\", \"Fine-tuning language models from human preferences\"], \"Citation\": []}",
        "eval_response": "Evaluation evidence: The assistant's answer does not match the gold answer in terms of accuracy and completeness. The references listed in the assistant's answer do not align with those in the gold answer, and it includes multiple irrelevant references. Additionally, the assistant's answer does not list any citations, whereas the gold answer specifies two citations. This discrepancy indicates a lack of accuracy and completeness in the assistant's response.\n\nRating: [[20]]",
        "score": 20
    },
    {
        "level": 3,
        "set": 3,
        "length": 101044,
        "type": "paper",
        "language": "en",
        "question": "AUTORT: EMBODIED FOUNDATION MODELS FOR LARGE SCALE ORCHESTRATION OF ROBOTIC AGENTS",
        "instruction": "We hope you will carefully study the provided papers and determine the citation relationships between them. Please follow the instructions below strictly to complete the task:\n\n#Specific Requirements:\n1. Reference: When a given paper mentions other provided papers, those other papers are considered as \"references\" for the given paper. To summarize in this specific context, references are about what the given paper is using.\n2. Citation: Conversely, when other provided papers mention the given paper in their works, the given paper is being \"cited\" by those other papers. To summarize in this specific context, citations are about who is using the given paper.\n3. Given a paper, you need to determine the citation or reference relationship between this paper and the other papers. Do not consider papers that are not provided.\n3. Please present the paper titles in a json format as follows: {{\"Reference\":[\"Reference Title 1\", \"Reference Title 2\", ..., \"Reference Title n\"], \"Citation\":[\"Citation Title 1\", \"Citation Title 2\", ..., \"Citation Title n\"]}}.\n4. If a paper does not have any references or citations, please leave the corresponding list empty, e.g.{{\"Refernce\":[]}}, {{\"Citation\":[]}}.",
        "prompt_template": "#Papers:\n{docs}\n\n{instruction}\n\n#The paper you need to analyze:\n{question}",
        "doc": [
            "2401.12963.md",
            "2403.13438.md",
            "2405.05824.md",
            "2307.06135.md",
            "2307.15818.md"
        ],
        "answer": {
            "Reference": [
                "# SayPlan: Grounding Large Language Models using 3D Scene Graphs for Scalable Robot Task Planning ",
                "# RT-2: Vision-Language-Action Models Transfer Web Knowledge to Robotic Control "
            ],
            "Citation": [
                "# See, Imagine, Plan: Discovering and Hallucinating Tasks from a Single Image ",
                "# Robots Can Feel: LLM-based Framework for Robot Ethical Reasoning "
            ]
        },
        "shuffle_doc": true,
        "id": "7d66d0d3-5c36-4b90-9c7f-27af0ea1c27c",
        "prompt": "[Question]\n#Papers:\n{docs}\n\nWe hope you will carefully study the provided papers and determine the citation relationships between them. Please follow the instructions below strictly to complete the task:\n\n#Specific Requirements:\n1. Reference: When a given paper mentions other provided papers, those other papers are considered as \"references\" for the given paper. To summarize in this specific context, references are about what the given paper is using.\n2. Citation: Conversely, when other provided papers mention the given paper in their works, the given paper is being \"cited\" by those other papers. To summarize in this specific context, citations are about who is using the given paper.\n3. Given a paper, you need to determine the citation or reference relationship between this paper and the other papers. Do not consider papers that are not provided.\n3. Please present the paper titles in a json format as follows: {{\"Reference\":[\"Reference Title 1\", \"Reference Title 2\", ..., \"Reference Title n\"], \"Citation\":[\"Citation Title 1\", \"Citation Title 2\", ..., \"Citation Title n\"]}}.\n4. If a paper does not have any references or citations, please leave the corresponding list empty, e.g.{{\"Refernce\":[]}}, {{\"Citation\":[]}}.\n\n#The paper you need to analyze:\nAUTORT: EMBODIED FOUNDATION MODELS FOR LARGE SCALE ORCHESTRATION OF ROBOTIC AGENTS\n\n[Gold Answer]\n{'Reference': ['# SayPlan: Grounding Large Language Models using 3D Scene Graphs for Scalable Robot Task Planning ', '# RT-2: Vision-Language-Action Models Transfer Web Knowledge to Robotic Control '], 'Citation': ['# See, Imagine, Plan: Discovering and Hallucinating Tasks from a Single Image ', '# Robots Can Feel: LLM-based Framework for Robot Ethical Reasoning ']}\n\n[The Start of Assistant's Predicted Answer]\n{\"Reference\":[\"Do as i can and not as i say: Grounding language in robotic affordances\", \"How to prompt your robot: A promptbook for manipulation skills with code as policies\", \"Runaround\", \"Constitutional ai: Harmlessness from ai feedback\", \"Robocat: A self-improving foundation agent for robotic manipulation\", \"RT-1: Robotics transformer for real-world control at scale\", \"RT2: Vision-language-action models transfer web knowledge to robotic control\", \"Pali: A jointly-scaled multilingual language-image model\", \"Leveraging procedural generation to benchmark reinforcement learning\", \"Robonet: Large-scale multi-robot learning\", \"Visual foresight: Model-based deep reinforcement learning for vision-based robotic control\", \"Physically grounded vision-language models for robotic manipulation\", \"Robot learning in homes: Improving generalization and reducing dataset bias\", \"Interactive robot fleet learning with scalable human supervision\", \"BC-z: Zero-shot task generalization with robotic imitation learning\", \"Qt-opt: Scalable deep reinforcement learning for vision-based robotic manipulation\", \"MT-opt: Continuous multi-task robotic reinforcement learning at scale\", \"Hgdagger: Interactive imitation learning with human experts\", \"Learning hand-eye coordination for robotic grasping with deep learning and large-scale data collection\", \"Interactive language: Talking to robots in real time\", \"Scaling robot supervision to hundreds of hours with roboturk: Robotic manipulation dataset through human reasoning and dexterity\", \"Grounding language with visual affordances over unstructured data\", \"Generative agents: Interactive simulacra of human behavior\", \"Supersizing self-supervision: Learning to grasp from 50k tries and 700 robot hours\", \"Grasp learning: Models, methods, and performance\", \"Sayplan: Grounding large language models using 3d scene graphs for scalable task planning\", \"Flexcap: Generating rich, localized, and flexible captions in images\", \"A reduction of imitation learning and structured prediction to no-regret online learning\", \"Multiple interactions made easy (mime): Large scale demonstrations data for imitation\", \"Reflexion: Language agents with verbal reinforcement learning\", \"D4: Improving llm pretraining via document de-duplication and diversification\", \"Chatgpt for robotics: Design principles and model abilities\", \"Voyager: An open-ended embodied agent with large language models\", \"Towards a foundation model for generalist robots: Diverse skill learning at scale via automated task and scene generation\", \"Robotic skill acquistion via instruction augmentation with visionlanguage models\", \"React: Synergizing reasoning and acting in language models\", \"More than a million ways to be pushed: A high-fidelity experimental dataset of planar pushing\"], \"Citation\":[]}\n[The End of Assistant's Predicted Answer]\n\n[System]\nWe would like to request your feedback on the performance of the AI assistant in response to the user question displayed above according to the gold answer. Please use the following listed aspects and their descriptions as evaluation criteria:\n    - Accuracy and Hallucinations: The assistant's answer is semantically consistent with the gold answer; The numerical value and order need to be accurate, and there should be no hallucinations.\n    - Completeness: Referring to the reference answers, the assistant's answer should contain all the key points needed to answer the user's question; further elaboration on these key points can be omitted.\nPlease rate whether this answer is suitable for the question. Please note that the gold answer can be considered as a correct answer to the question.\n\nThe assistant receives an overall score on a scale of 1 to 100, where a higher score indicates better overall performance.\nPlease note that if the assistant's answer and the gold answer fully meet the above criteria, its overall rating should be the full marks (100).\nPlease first provide a comprehensive explanation of your evaluation, avoiding any potential bias.\nThen, output a line indicating the score of the Assistant.\n\nPLEASE OUTPUT WITH THE FOLLOWING FORMAT, WHERE THE SCORE IS A SCALE OF 1 TO 100 BY STRICTLY FOLLOWING THIS FORMAT: \"[[score]]\", FOR EXAMPLE \"Rating: [[100]]\":\n<start output>\nEvaluation evidence: your evluation explanation here, no more than 100 words\nRating: [[score]]\n<end output>\n\nNow, start your evaluation:",
        "generate_response": "{\"Reference\":[\"Do as i can and not as i say: Grounding language in robotic affordances\", \"How to prompt your robot: A promptbook for manipulation skills with code as policies\", \"Runaround\", \"Constitutional ai: Harmlessness from ai feedback\", \"Robocat: A self-improving foundation agent for robotic manipulation\", \"RT-1: Robotics transformer for real-world control at scale\", \"RT2: Vision-language-action models transfer web knowledge to robotic control\", \"Pali: A jointly-scaled multilingual language-image model\", \"Leveraging procedural generation to benchmark reinforcement learning\", \"Robonet: Large-scale multi-robot learning\", \"Visual foresight: Model-based deep reinforcement learning for vision-based robotic control\", \"Physically grounded vision-language models for robotic manipulation\", \"Robot learning in homes: Improving generalization and reducing dataset bias\", \"Interactive robot fleet learning with scalable human supervision\", \"BC-z: Zero-shot task generalization with robotic imitation learning\", \"Qt-opt: Scalable deep reinforcement learning for vision-based robotic manipulation\", \"MT-opt: Continuous multi-task robotic reinforcement learning at scale\", \"Hgdagger: Interactive imitation learning with human experts\", \"Learning hand-eye coordination for robotic grasping with deep learning and large-scale data collection\", \"Interactive language: Talking to robots in real time\", \"Scaling robot supervision to hundreds of hours with roboturk: Robotic manipulation dataset through human reasoning and dexterity\", \"Grounding language with visual affordances over unstructured data\", \"Generative agents: Interactive simulacra of human behavior\", \"Supersizing self-supervision: Learning to grasp from 50k tries and 700 robot hours\", \"Grasp learning: Models, methods, and performance\", \"Sayplan: Grounding large language models using 3d scene graphs for scalable task planning\", \"Flexcap: Generating rich, localized, and flexible captions in images\", \"A reduction of imitation learning and structured prediction to no-regret online learning\", \"Multiple interactions made easy (mime): Large scale demonstrations data for imitation\", \"Reflexion: Language agents with verbal reinforcement learning\", \"D4: Improving llm pretraining via document de-duplication and diversification\", \"Chatgpt for robotics: Design principles and model abilities\", \"Voyager: An open-ended embodied agent with large language models\", \"Towards a foundation model for generalist robots: Diverse skill learning at scale via automated task and scene generation\", \"Robotic skill acquistion via instruction augmentation with visionlanguage models\", \"React: Synergizing reasoning and acting in language models\", \"More than a million ways to be pushed: A high-fidelity experimental dataset of planar pushing\"], \"Citation\":[]}",
        "eval_response": "Evaluation evidence: The assistant's answer includes a large number of references that are not mentioned in the gold answer, indicating significant hallucinations. The gold answer specifies two references and two citations, none of which appear in the assistant's response. This discrepancy shows a lack of accuracy and completeness, as the assistant fails to identify any correct references or citations according to the gold answer. The assistant's response does not align with the task requirements, leading to a low score.\n\nRating: [[10]]",
        "score": 10
    },
    {
        "level": 3,
        "set": 2,
        "length": 69631,
        "type": "paper",
        "language": "en",
        "question": "WARM: On the Benefits of Weight Averaged Reward Models",
        "instruction": "We hope you will carefully study the provided papers and determine the citation relationships between them. Please follow the instructions below strictly to complete the task:\n\n#Specific Requirements:\n1. Reference: When a given paper mentions other provided papers, those other papers are considered as \"references\" for the given paper. To summarize in this specific context, references are about what the given paper is using.\n2. Citation: Conversely, when other provided papers mention the given paper in their works, the given paper is being \"cited\" by those other papers. To summarize in this specific context, citations are about who is using the given paper.\n3. Given a paper, you need to determine the citation or reference relationship between this paper and the other papers. Do not consider papers that are not provided.\n3. Please present the paper titles in a json format as follows: {{\"Reference\":[\"Reference Title 1\", \"Reference Title 2\", ..., \"Reference Title n\"], \"Citation\":[\"Citation Title 1\", \"Citation Title 2\", ..., \"Citation Title n\"]}}.\n4. If a paper does not have any references or citations, please leave the corresponding list empty, e.g.{{\"Refernce\":[]}}, {{\"Citation\":[]}}.",
        "prompt_template": "#Papers:\n{docs}\n\n{instruction}\n\n#The paper you need to analyze:\n{question}",
        "doc": [
            "2401.12187.md",
            "2405.10516.md",
            "2405.19107.md"
        ],
        "answer": {
            "Reference": [],
            "Citation": [
                "# Language Models can Evaluate Themselves via Probability Discrepancy ",
                "# Offline Regularised Reinforcement Learning for Large Language Models Alignment "
            ]
        },
        "shuffle_doc": true,
        "id": "e04aff63-6dbe-4542-ab03-a726a47b5c31",
        "prompt": "[Question]\n#Papers:\n{docs}\n\nWe hope you will carefully study the provided papers and determine the citation relationships between them. Please follow the instructions below strictly to complete the task:\n\n#Specific Requirements:\n1. Reference: When a given paper mentions other provided papers, those other papers are considered as \"references\" for the given paper. To summarize in this specific context, references are about what the given paper is using.\n2. Citation: Conversely, when other provided papers mention the given paper in their works, the given paper is being \"cited\" by those other papers. To summarize in this specific context, citations are about who is using the given paper.\n3. Given a paper, you need to determine the citation or reference relationship between this paper and the other papers. Do not consider papers that are not provided.\n3. Please present the paper titles in a json format as follows: {{\"Reference\":[\"Reference Title 1\", \"Reference Title 2\", ..., \"Reference Title n\"], \"Citation\":[\"Citation Title 1\", \"Citation Title 2\", ..., \"Citation Title n\"]}}.\n4. If a paper does not have any references or citations, please leave the corresponding list empty, e.g.{{\"Refernce\":[]}}, {{\"Citation\":[]}}.\n\n#The paper you need to analyze:\nWARM: On the Benefits of Weight Averaged Reward Models\n\n[Gold Answer]\n{'Reference': [], 'Citation': ['# Language Models can Evaluate Themselves via Probability Discrepancy ', '# Offline Regularised Reinforcement Learning for Large Language Models Alignment ']}\n\n[The Start of Assistant's Predicted Answer]\n{\"Reference\":[\"Gemini\", \"GPT-4\", \"Sparks of artificial general intelligence: Early experiments with gpt-4\", \"Improving language understanding by generative pre-training\", \"Bert: Pre-training of deep bidirectional transformers for language understanding\", \"Language models are few-shot learners\", \"Finetuned language models are zero-shot learners\", \"Super-NaturalInstructions: Generalization via declarative instructions on 1600+ NLP tasks\", \"Stanford Alpaca: An instruction-following LLaMA model\", \"Factually consistent summarization via reinforcement learning with textual entailment feedback\", \"On the fragility of learned reward functions\", \"Deep reinforcement learning from human preferences\", \"Fine-tuning language models from human preferences\", \"Learning to summarize with human feedback\", \"Recursively summarizing books with human feedback\", \"LLaMA 2: Open foundation and fine-tuned chat models\", \"Scaling laws for reward model overoptimization\", \"Open problems and fundamental limitations of reinforcement learning from human feedback\", \"Concrete problems in AI safety\", \"Faulty Reward Functions in the Wild\", \"A general language assistant as a laboratory for alignment\", \"Defining and characterizing reward gaming\", \"The effects of reward misspecification: Mapping and mitigating misaligned models\", \"The alignment ceiling: Objective mismatch in reinforcement learning from human feedback\", \"Deal or no deal? end-to-end learning for negotiation dialogues\", \"X-risk analysis for AI research\", \"Natural selection favors AIs over humans\", \"Consequences of misaligned AI\", \"Benchmarks and algorithms for offline preference-based reward learning\", \"Measuring progress on scalable oversight for large language models\", \"Rewarded soups: towards pareto-optimal alignment by interpolating weights fine-tuned on diverse rewards\", \"Training language models to follow instructions with human feedback\", \"Helping or herding? reward model ensembles mitigate but do not eliminate reward hacking\", \"Reward model ensembles help mitigate overoptimization\", \"Simple and scalable predictive uncertainty estimation using deep ensembles\", \"Linear mode connectivity and the lottery ticket hypothesis\", \"What is being transferred in transfer learning?\", \"Model soups: averaging weights of multiple fine-tuned models improves accuracy without increasing inference time\", \"Diverse weight averaging for out-of-distribution generalization\", \"Model Ratatouille: Recycling diverse models for out-of-distribution generalization\", \"SWAD: Domain generalization by seeking flat minima\", \"Domain generalization via invariant feature representation\", \"Invariant risk minimization\", \"Fuse to forget: Bias reduction and selective memorization through model fusion\", \"Spurious feature diversification improves out-of-distribution generalization\", \"Attention is all you need\", \"On the opportunities and risks of foundation models\", \"Learning and transferring mid-level image representations using convolutional neural networks\", \"Constitutional AI: Harmlessness from AI feedback\", \"RLAIF: Scaling reinforcement learning from human feedback with ai feedback\", \"Rank analysis of incomplete block designs: I. the method of paired comparisons\", \"Simple statistical gradient-following algorithms for connectionist reinforcement learning\", \"Proximal policy optimization algorithms\", \"Variable generalization performance of a deep learning model to detect pneumonia in chest radiographs: A cross-sectional study\", \"AI for radiographic COVID-19 detection selects shortcuts over signal\", \"Robust fine-tuning of zero-shot models\", \"Patching open-vocabulary models by interpolating weights\", \"Sequence tutor: Conservative fine-tuning of sequence generation models with kl-control\", \"A theory of regularized markov decision processes\", \"Multi-agent communication meets natural language: Synergies between functional and structural language learning\", \"Countering language drift with seeded iterated learning\", \"Learning human objectives by evaluating hypothetical behavior\", \"Trial without error: Towards safe reinforcement learning via human intervention\", \"Secrets of RLHF in large language models part ii: Reward modeling\", \"Bias plus variance decomposition for zero-one loss functions\", \"Generalization error of ensemble estimators\", \"Specific versus general principles for constitutional ai\", \"Fine-tuning can distort pretrained features and underperform out-of-distribution\", \"Git re-basin: Merging models modulo permutation symmetries\", \"Knowledge is a region in weight space for fine-tuned language models\", \"No one representation to rule them all: Overlapping features of training methods\", \"Averaging weights leads to wider optima and better generalization\", \"Ensemble of averages: Improving model selection and boosting performance in domain generalization\", \"T1; dr: Mining reddit to learn automatic summarization\", \"PaLM 2 technical report\", \"Chain-of-Thought prompting elicits reasoning in large language models\", \"Exploring the limits of transfer learning with a unified text-to-text transformer\", \"Theoretical guarantees on the best-of-n alignment policy\", \"Building Machine Learning Models Like Open Source Software\", \"Branch-Train-Merge: Embarrassingly parallel training of expert language models\", \"Communication-efficient learning of deep networks from decentralized data\", \"Direct preference optimization: Your language model is secretly a reward model\", \"NeuralBeagle14-7B\", \"Last layer re-training is sufficient for robustness to spurious correlations\", \"Benchmarking neural network robustness to common corruptions and perturbations\", \"Variable generalization performance of a deep learning model to detect pneumonia in chest radiographs: A cross-sectional study\", \"AI for radiographic COVID-19 detection selects shortcuts over signal\", \"Robust fine-tuning of zero-shot models\", \"Patching open-vocabulary models by interpolating weights\", \"ColD fusion: Collaborative descent for distributed multitask finetuning\", \"Learning from noisy labels with deep neural networks: A survey\", \"Understanding deep learning requires rethinking generalization\", \"Learning from noisy labels by regularized estimation of annotator confusion\", \"Neftune: Noisy embeddings improve instruction finetuning\", \"Robust loss functions under label noise for deep neural networks\", \"Sample selection with uncertainty of losses for learning with noisy labels\", \"Mentornet: Learning data-driven curriculum for very deep neural networks on corrupted labels\", \"Co-teaching: Robust training of deep neural networks with extremely noisy labels\", \"Ensemble learning in the presence of noise\", \"Algorithms for inverse reinforcement learning\", \"Learning optimal advantage from preferences and mistaking it for reward\", \"Active reward learning from multiple teachers\", \"The impact of preference agreement in reinforcement learning from human feedback: A case study in summarization\", \"Toolaugmented reward modeling\", \"Aligning large multimodal models with factually augmented rlhf\", \"RIME: Robust preference-based reinforcement learning with noisy human preferences\", \"A general theoretical paradigm to understand learning from human preferences\"], \"Citation\":[\"Improving language understanding by generative pre-training\", \"Language models are few-shot learners\", \"Finetuned language models are zero-shot learners\", \"Super-NaturalInstructions: Generalization via declarative instructions on 1600+ NLP tasks\", \"Stanford Alpaca: An instruction-following LLaMA model\", \"Factually consistent summarization via reinforcement learning with textual entailment feedback\", \"On the fragility of learned reward functions\", \"Deep reinforcement learning from human preferences\", \"Fine-tuning language models from human preferences\", \"Learning to summarize with human feedback\", \"Recursively summarizing books with human feedback\", \"LLaMA 2: Open foundation and fine-tuned chat models\", \"Scaling laws for reward model overoptimization\", \"Open problems and fundamental limitations of reinforcement learning from human feedback\", \"Concrete problems in AI safety\", \"Faulty Reward Functions in the Wild\", \"A general language assistant as a laboratory for alignment\", \"Defining and characterizing reward gaming\", \"The effects of reward misspecification: Mapping and mitigating misaligned models\", \"The alignment ceiling: Objective mismatch in reinforcement learning from human feedback\", \"X-risk analysis for AI research\", \"Natural selection favors AIs over humans\", \"Consequences of misaligned AI\", \"Benchmarks and algorithms for offline preference-based reward learning\", \"Measuring progress on scalable oversight for large language models\", \"Rewarded soups: towards pareto-optimal alignment by interpolating weights fine-tuned on diverse rewards\", \"Training language models to follow instructions with human feedback\", \"Helping or herding? reward model ensembles mitigate but do not eliminate reward hacking\", \"Reward model ensembles help mitigate overoptimization\", \"Simple and scalable predictive uncertainty estimation using deep ensembles\", \"Linear mode connectivity and the lottery ticket hypothesis\", \"What is being transferred in transfer learning?\", \"Model soups: averaging weights of multiple fine-tuned models improves accuracy without increasing inference time\", \"Diverse weight averaging for out-of-distribution generalization\", \"Model Ratatouille: Recycling diverse models for out-of-distribution generalization\", \"SWAD: Domain generalization by seeking flat minima\", \"Domain generalization via invariant feature representation\", \"Invariant risk minimization\", \"Fuse to forget: Bias reduction and selective memorization through model fusion\", \"Attention is all you need\", \"On the opportunities and risks of foundation models\", \"Learning and transferring mid-level image representations using convolutional neural networks\", \"Constitutional AI: Harmlessness from AI feedback\", \"RLAIF: Scaling reinforcement learning from human feedback with ai feedback\", \"Last layer re-training is sufficient for robustness to spurious correlations\", \"Benchmarking neural network robustness to common corruptions and perturbations\", \"Variable generalization performance of a deep learning model to detect pneumonia in chest radiographs: A cross-sectional study\", \"AI for radiographic COVID-19 detection selects shortcuts over signal\", \"Robust fine-tuning of zero-shot models\", \"Patching open-vocabulary models by interpolating weights\", \"ColD fusion: Collaborative descent for distributed multitask finetuning\", \"AdaFactor: Adaptive learning rates with sublinear memory cost\", \"Vanishing gradients in reinforcement finetuning of language models\", \"Neftune: Noisy embeddings improve instruction finetuning\"]}\n[The End of Assistant's Predicted Answer]\n\n[System]\nWe would like to request your feedback on the performance of the AI assistant in response to the user question displayed above according to the gold answer. Please use the following listed aspects and their descriptions as evaluation criteria:\n    - Accuracy and Hallucinations: The assistant's answer is semantically consistent with the gold answer; The numerical value and order need to be accurate, and there should be no hallucinations.\n    - Completeness: Referring to the reference answers, the assistant's answer should contain all the key points needed to answer the user's question; further elaboration on these key points can be omitted.\nPlease rate whether this answer is suitable for the question. Please note that the gold answer can be considered as a correct answer to the question.\n\nThe assistant receives an overall score on a scale of 1 to 100, where a higher score indicates better overall performance.\nPlease note that if the assistant's answer and the gold answer fully meet the above criteria, its overall rating should be the full marks (100).\nPlease first provide a comprehensive explanation of your evaluation, avoiding any potential bias.\nThen, output a line indicating the score of the Assistant.\n\nPLEASE OUTPUT WITH THE FOLLOWING FORMAT, WHERE THE SCORE IS A SCALE OF 1 TO 100 BY STRICTLY FOLLOWING THIS FORMAT: \"[[score]]\", FOR EXAMPLE \"Rating: [[100]]\":\n<start output>\nEvaluation evidence: your evluation explanation here, no more than 100 words\nRating: [[score]]\n<end output>\n\nNow, start your evaluation:",
        "generate_response": "{\"Reference\":[\"Gemini\", \"GPT-4\", \"Sparks of artificial general intelligence: Early experiments with gpt-4\", \"Improving language understanding by generative pre-training\", \"Bert: Pre-training of deep bidirectional transformers for language understanding\", \"Language models are few-shot learners\", \"Finetuned language models are zero-shot learners\", \"Super-NaturalInstructions: Generalization via declarative instructions on 1600+ NLP tasks\", \"Stanford Alpaca: An instruction-following LLaMA model\", \"Factually consistent summarization via reinforcement learning with textual entailment feedback\", \"On the fragility of learned reward functions\", \"Deep reinforcement learning from human preferences\", \"Fine-tuning language models from human preferences\", \"Learning to summarize with human feedback\", \"Recursively summarizing books with human feedback\", \"LLaMA 2: Open foundation and fine-tuned chat models\", \"Scaling laws for reward model overoptimization\", \"Open problems and fundamental limitations of reinforcement learning from human feedback\", \"Concrete problems in AI safety\", \"Faulty Reward Functions in the Wild\", \"A general language assistant as a laboratory for alignment\", \"Defining and characterizing reward gaming\", \"The effects of reward misspecification: Mapping and mitigating misaligned models\", \"The alignment ceiling: Objective mismatch in reinforcement learning from human feedback\", \"Deal or no deal? end-to-end learning for negotiation dialogues\", \"X-risk analysis for AI research\", \"Natural selection favors AIs over humans\", \"Consequences of misaligned AI\", \"Benchmarks and algorithms for offline preference-based reward learning\", \"Measuring progress on scalable oversight for large language models\", \"Rewarded soups: towards pareto-optimal alignment by interpolating weights fine-tuned on diverse rewards\", \"Training language models to follow instructions with human feedback\", \"Helping or herding? reward model ensembles mitigate but do not eliminate reward hacking\", \"Reward model ensembles help mitigate overoptimization\", \"Simple and scalable predictive uncertainty estimation using deep ensembles\", \"Linear mode connectivity and the lottery ticket hypothesis\", \"What is being transferred in transfer learning?\", \"Model soups: averaging weights of multiple fine-tuned models improves accuracy without increasing inference time\", \"Diverse weight averaging for out-of-distribution generalization\", \"Model Ratatouille: Recycling diverse models for out-of-distribution generalization\", \"SWAD: Domain generalization by seeking flat minima\", \"Domain generalization via invariant feature representation\", \"Invariant risk minimization\", \"Fuse to forget: Bias reduction and selective memorization through model fusion\", \"Spurious feature diversification improves out-of-distribution generalization\", \"Attention is all you need\", \"On the opportunities and risks of foundation models\", \"Learning and transferring mid-level image representations using convolutional neural networks\", \"Constitutional AI: Harmlessness from AI feedback\", \"RLAIF: Scaling reinforcement learning from human feedback with ai feedback\", \"Rank analysis of incomplete block designs: I. the method of paired comparisons\", \"Simple statistical gradient-following algorithms for connectionist reinforcement learning\", \"Proximal policy optimization algorithms\", \"Variable generalization performance of a deep learning model to detect pneumonia in chest radiographs: A cross-sectional study\", \"AI for radiographic COVID-19 detection selects shortcuts over signal\", \"Robust fine-tuning of zero-shot models\", \"Patching open-vocabulary models by interpolating weights\", \"Sequence tutor: Conservative fine-tuning of sequence generation models with kl-control\", \"A theory of regularized markov decision processes\", \"Multi-agent communication meets natural language: Synergies between functional and structural language learning\", \"Countering language drift with seeded iterated learning\", \"Learning human objectives by evaluating hypothetical behavior\", \"Trial without error: Towards safe reinforcement learning via human intervention\", \"Secrets of RLHF in large language models part ii: Reward modeling\", \"Bias plus variance decomposition for zero-one loss functions\", \"Generalization error of ensemble estimators\", \"Specific versus general principles for constitutional ai\", \"Fine-tuning can distort pretrained features and underperform out-of-distribution\", \"Git re-basin: Merging models modulo permutation symmetries\", \"Knowledge is a region in weight space for fine-tuned language models\", \"No one representation to rule them all: Overlapping features of training methods\", \"Averaging weights leads to wider optima and better generalization\", \"Ensemble of averages: Improving model selection and boosting performance in domain generalization\", \"T1; dr: Mining reddit to learn automatic summarization\", \"PaLM 2 technical report\", \"Chain-of-Thought prompting elicits reasoning in large language models\", \"Exploring the limits of transfer learning with a unified text-to-text transformer\", \"Theoretical guarantees on the best-of-n alignment policy\", \"Building Machine Learning Models Like Open Source Software\", \"Branch-Train-Merge: Embarrassingly parallel training of expert language models\", \"Communication-efficient learning of deep networks from decentralized data\", \"Direct preference optimization: Your language model is secretly a reward model\", \"NeuralBeagle14-7B\", \"Last layer re-training is sufficient for robustness to spurious correlations\", \"Benchmarking neural network robustness to common corruptions and perturbations\", \"Variable generalization performance of a deep learning model to detect pneumonia in chest radiographs: A cross-sectional study\", \"AI for radiographic COVID-19 detection selects shortcuts over signal\", \"Robust fine-tuning of zero-shot models\", \"Patching open-vocabulary models by interpolating weights\", \"ColD fusion: Collaborative descent for distributed multitask finetuning\", \"Learning from noisy labels with deep neural networks: A survey\", \"Understanding deep learning requires rethinking generalization\", \"Learning from noisy labels by regularized estimation of annotator confusion\", \"Neftune: Noisy embeddings improve instruction finetuning\", \"Robust loss functions under label noise for deep neural networks\", \"Sample selection with uncertainty of losses for learning with noisy labels\", \"Mentornet: Learning data-driven curriculum for very deep neural networks on corrupted labels\", \"Co-teaching: Robust training of deep neural networks with extremely noisy labels\", \"Ensemble learning in the presence of noise\", \"Algorithms for inverse reinforcement learning\", \"Learning optimal advantage from preferences and mistaking it for reward\", \"Active reward learning from multiple teachers\", \"The impact of preference agreement in reinforcement learning from human feedback: A case study in summarization\", \"Toolaugmented reward modeling\", \"Aligning large multimodal models with factually augmented rlhf\", \"RIME: Robust preference-based reinforcement learning with noisy human preferences\", \"A general theoretical paradigm to understand learning from human preferences\"], \"Citation\":[\"Improving language understanding by generative pre-training\", \"Language models are few-shot learners\", \"Finetuned language models are zero-shot learners\", \"Super-NaturalInstructions: Generalization via declarative instructions on 1600+ NLP tasks\", \"Stanford Alpaca: An instruction-following LLaMA model\", \"Factually consistent summarization via reinforcement learning with textual entailment feedback\", \"On the fragility of learned reward functions\", \"Deep reinforcement learning from human preferences\", \"Fine-tuning language models from human preferences\", \"Learning to summarize with human feedback\", \"Recursively summarizing books with human feedback\", \"LLaMA 2: Open foundation and fine-tuned chat models\", \"Scaling laws for reward model overoptimization\", \"Open problems and fundamental limitations of reinforcement learning from human feedback\", \"Concrete problems in AI safety\", \"Faulty Reward Functions in the Wild\", \"A general language assistant as a laboratory for alignment\", \"Defining and characterizing reward gaming\", \"The effects of reward misspecification: Mapping and mitigating misaligned models\", \"The alignment ceiling: Objective mismatch in reinforcement learning from human feedback\", \"X-risk analysis for AI research\", \"Natural selection favors AIs over humans\", \"Consequences of misaligned AI\", \"Benchmarks and algorithms for offline preference-based reward learning\", \"Measuring progress on scalable oversight for large language models\", \"Rewarded soups: towards pareto-optimal alignment by interpolating weights fine-tuned on diverse rewards\", \"Training language models to follow instructions with human feedback\", \"Helping or herding? reward model ensembles mitigate but do not eliminate reward hacking\", \"Reward model ensembles help mitigate overoptimization\", \"Simple and scalable predictive uncertainty estimation using deep ensembles\", \"Linear mode connectivity and the lottery ticket hypothesis\", \"What is being transferred in transfer learning?\", \"Model soups: averaging weights of multiple fine-tuned models improves accuracy without increasing inference time\", \"Diverse weight averaging for out-of-distribution generalization\", \"Model Ratatouille: Recycling diverse models for out-of-distribution generalization\", \"SWAD: Domain generalization by seeking flat minima\", \"Domain generalization via invariant feature representation\", \"Invariant risk minimization\", \"Fuse to forget: Bias reduction and selective memorization through model fusion\", \"Attention is all you need\", \"On the opportunities and risks of foundation models\", \"Learning and transferring mid-level image representations using convolutional neural networks\", \"Constitutional AI: Harmlessness from AI feedback\", \"RLAIF: Scaling reinforcement learning from human feedback with ai feedback\", \"Last layer re-training is sufficient for robustness to spurious correlations\", \"Benchmarking neural network robustness to common corruptions and perturbations\", \"Variable generalization performance of a deep learning model to detect pneumonia in chest radiographs: A cross-sectional study\", \"AI for radiographic COVID-19 detection selects shortcuts over signal\", \"Robust fine-tuning of zero-shot models\", \"Patching open-vocabulary models by interpolating weights\", \"ColD fusion: Collaborative descent for distributed multitask finetuning\", \"AdaFactor: Adaptive learning rates with sublinear memory cost\", \"Vanishing gradients in reinforcement finetuning of language models\", \"Neftune: Noisy embeddings improve instruction finetuning\"]}",
        "eval_response": "Evaluation evidence: The assistant's answer does not align with the gold answer in terms of accuracy and hallucinations. The gold answer specifies two citations and no references, while the assistant's answer lists numerous references and citations, many of which are not mentioned in the gold answer. This indicates a significant deviation from the required task, leading to inaccuracies and hallucinations. The completeness is also compromised as the assistant's answer includes unnecessary and incorrect details not present in the gold answer.\n\nRating: [[20]]",
        "score": 20
    },
    {
        "level": 4,
        "set": 2,
        "length": 77850,
        "type": "paper",
        "language": "en",
        "question": "",
        "instruction": "We kindly ask you to thoroughly review the provided papers and construct a citation chain from them. Please adhere to the following instructions strictly while completing the task:\n\n#Task Instructions:\nGiven several papers, you are required to identify and list the longest citation chain, which demonstrates the citation relationship among the provided papers.\n\n#Specific Requirements:\n1.Please present the titles of the papers in the form of a list, as follows: [\"Title of Paper 1\", \"Title of Paper 2\", ..., \"Title of Paper n\"].\n2.Ensure that the citation chain in the list is linear and continuous, meaning that the first paper title in the list (Paper 1) should not cite any other works. Instead, it should be cited by the next paper in the list (Paper 2); subsequently, each paper should then be cited by the next one in the list, continuing up to the last paper (Paper n).\n3.Consider only the citation relationships within the supplied collection of papers, and ensure that the citation chain accurately reflects the sequential citation order among these documents.\n4.Do not take into account any articles not provided, and disregard other non-linear citation relationships.",
        "prompt_template": "{instruction}\n\n#Papers Provided:\n{docs}",
        "doc": [
            "1901.02039.md",
            "2112.13227.md",
            "2305.02536.md",
            "2405.08981.md"
        ],
        "answer": [
            "# SPHERICAL CNNS ON UNSTRUCTURED GRIDS ",
            "# Pseudocylindrical Convolutions for Learned Omnidirectional Image Compression ",
            "# Scanpath Prediction in Panoramic Videos via Expected Code Length Minimization ",
            "# Impact of Design Decisions in Scanpath Modeling "
        ],
        "shuffle_doc": true,
        "id": "4dc55fa6-119c-4745-8b9d-75530c33d6f8",
        "prompt": "[Question]\nWe kindly ask you to thoroughly review the provided papers and construct a citation chain from them. Please adhere to the following instructions strictly while completing the task:\n\n#Task Instructions:\nGiven several papers, you are required to identify and list the longest citation chain, which demonstrates the citation relationship among the provided papers.\n\n#Specific Requirements:\n1.Please present the titles of the papers in the form of a list, as follows: [\"Title of Paper 1\", \"Title of Paper 2\", ..., \"Title of Paper n\"].\n2.Ensure that the citation chain in the list is linear and continuous, meaning that the first paper title in the list (Paper 1) should not cite any other works. Instead, it should be cited by the next paper in the list (Paper 2); subsequently, each paper should then be cited by the next one in the list, continuing up to the last paper (Paper n).\n3.Consider only the citation relationships within the supplied collection of papers, and ensure that the citation chain accurately reflects the sequential citation order among these documents.\n4.Do not take into account any articles not provided, and disregard other non-linear citation relationships.\n\n#Papers Provided:\n{docs}\n\n[Gold Answer]\n['# SPHERICAL CNNS ON UNSTRUCTURED GRIDS ', '# Pseudocylindrical Convolutions for Learned Omnidirectional Image Compression ', '# Scanpath Prediction in Panoramic Videos via Expected Code Length Minimization ', '# Impact of Design Decisions in Scanpath Modeling ']\n\n[The Start of Assistant's Predicted Answer]\n[\"Spherical CNNs on Unstructured Grids\", \"Spherical harmonic basis to achieve similar results\", \"Diffusion-Convolutional Neural Networks (DCNN) for efficient convolution on graph structured data\", \"Generalization of classic CNNs to non-Euclidean domains by using a set of oriented anisotropic diffusion kernels\", \"Linear combination of filter banks to acquire equivariant convolution filters\", \"Reparameterization of convolutional kernels using parabolic and hyperbolic differential basis with regular grid images\", \"Convolution directly on manifolds using cross-correlation based on geodesic distances\", \"Optimal surface parameterization method (seamless toric covers) to parameterize genus-zero shapes into 2D signals for analysis using conventional planar CNNs\", \"Semantic segmentation of equirectangular omnidirectional images\", \"Benchmarks for semantic segmentation of 360 panorama images\", \"3D semantic segmentation on point clouds or voxels\", \"Efficient and lean learning space provided by reparameterizing the learnable convolution kernel as a linear combination of differential operators\", \"Comparison of peak performance achievable by our model with other 3D learning algorithms\", \"Parameter efficiency study by varying model parameters\", \"Joint 2d-3d-semantic data for indoor scene understanding\", \"Polygon mesh processing\", \"Geometric deep learning: going beyond euclidean data\", \"Spectral networks and locally connected networks on graphs\", \"Convolutional neural networks on graphs with fast localized spectral filtering\", \"Learning SO(3) equivariant representations with spherical cnns\", \"Learning spherical representations for detection and classification in omnidirectional images\", \"Discrete differential geometry: An applied introduction\", \"ScanNet: Richly-annotated 3D reconstructions of indoor scenes\", \"ScanComplete: Large-scale scene completion and semantic segmentation for 3D scans\", \"VoxNet: A 3D Convolutional Neural Network for Real-Time Object Recognition\", \"The One Hundred Layers Tiramisu: Fully Convolutional DenseNets for Semantic Segmentation\", \"Semi-supervised classification with graph convolutional networks\", \"Diffusion kernels on graphs and other discrete structures\", \"The MNIST database of handwritten digits\", \"Fully Convolutional Networks for Semantic Segmentation\", \"Convolutional Neural Networks on Surfaces via Seamless Toric Covers\", \"Geodesic Convolutional Neural Networks on Riemannian Manifolds\", \"Segmenting and Tracking Extreme Climate Events Using Neural Networks\", \"Description of the NCAR Community Atmosphere Model (CAM 5.0)\", \"Volumetric and Multi-View CNNs for Object Classification on 3D Data\", \"PointNet: Deep Learning on Point Sets for 3D Classification and Segmentation\", \"PointNet++: Deep Hierarchical Feature Learning on Point Sets in a Metric Space\", \"Deep Neural Networks Motivated by Partial Differential Equations\", \"IM2Pano3D: Extrapolating 360 Structure and Semantics Beyond the Field of View\", \"Learning Spherical Convolution for Fast Features from 360 Imagery\", \"SegCloud: Semantic Segmentation of 3D Point Clouds\", \"Understanding Convolution for Semantic Segmentation\", \"Dynamic Graph CNN for Learning on Point Clouds\", \"3D ShapeNets: A Deep Representation for Volumetric Shapes\", \"SyncSpecCNN: Synchronized Spectral CNN for 3D Shape Segmentation\", \"Distortion-aware CNNS for Spherical Images\"]\n[The End of Assistant's Predicted Answer]\n\n[System]\nWe would like to request your feedback on the performance of the AI assistant in response to the user question displayed above according to the gold answer. Please use the following listed aspects and their descriptions as evaluation criteria:\n    - Accuracy and Hallucinations: The assistant's answer is semantically consistent with the gold answer; The numerical value and order need to be accurate, and there should be no hallucinations.\n    - Completeness: Referring to the reference answers, the assistant's answer should contain all the key points needed to answer the user's question; further elaboration on these key points can be omitted.\nPlease rate whether this answer is suitable for the question. Please note that the gold answer can be considered as a correct answer to the question.\n\nThe assistant receives an overall score on a scale of 1 to 100, where a higher score indicates better overall performance.\nPlease note that if the assistant's answer and the gold answer fully meet the above criteria, its overall rating should be the full marks (100).\nPlease first provide a comprehensive explanation of your evaluation, avoiding any potential bias.\nThen, output a line indicating the score of the Assistant.\n\nPLEASE OUTPUT WITH THE FOLLOWING FORMAT, WHERE THE SCORE IS A SCALE OF 1 TO 100 BY STRICTLY FOLLOWING THIS FORMAT: \"[[score]]\", FOR EXAMPLE \"Rating: [[100]]\":\n<start output>\nEvaluation evidence: your evluation explanation here, no more than 100 words\nRating: [[score]]\n<end output>\n\nNow, start your evaluation:",
        "generate_response": "[\"Spherical CNNs on Unstructured Grids\", \"Spherical harmonic basis to achieve similar results\", \"Diffusion-Convolutional Neural Networks (DCNN) for efficient convolution on graph structured data\", \"Generalization of classic CNNs to non-Euclidean domains by using a set of oriented anisotropic diffusion kernels\", \"Linear combination of filter banks to acquire equivariant convolution filters\", \"Reparameterization of convolutional kernels using parabolic and hyperbolic differential basis with regular grid images\", \"Convolution directly on manifolds using cross-correlation based on geodesic distances\", \"Optimal surface parameterization method (seamless toric covers) to parameterize genus-zero shapes into 2D signals for analysis using conventional planar CNNs\", \"Semantic segmentation of equirectangular omnidirectional images\", \"Benchmarks for semantic segmentation of 360 panorama images\", \"3D semantic segmentation on point clouds or voxels\", \"Efficient and lean learning space provided by reparameterizing the learnable convolution kernel as a linear combination of differential operators\", \"Comparison of peak performance achievable by our model with other 3D learning algorithms\", \"Parameter efficiency study by varying model parameters\", \"Joint 2d-3d-semantic data for indoor scene understanding\", \"Polygon mesh processing\", \"Geometric deep learning: going beyond euclidean data\", \"Spectral networks and locally connected networks on graphs\", \"Convolutional neural networks on graphs with fast localized spectral filtering\", \"Learning SO(3) equivariant representations with spherical cnns\", \"Learning spherical representations for detection and classification in omnidirectional images\", \"Discrete differential geometry: An applied introduction\", \"ScanNet: Richly-annotated 3D reconstructions of indoor scenes\", \"ScanComplete: Large-scale scene completion and semantic segmentation for 3D scans\", \"VoxNet: A 3D Convolutional Neural Network for Real-Time Object Recognition\", \"The One Hundred Layers Tiramisu: Fully Convolutional DenseNets for Semantic Segmentation\", \"Semi-supervised classification with graph convolutional networks\", \"Diffusion kernels on graphs and other discrete structures\", \"The MNIST database of handwritten digits\", \"Fully Convolutional Networks for Semantic Segmentation\", \"Convolutional Neural Networks on Surfaces via Seamless Toric Covers\", \"Geodesic Convolutional Neural Networks on Riemannian Manifolds\", \"Segmenting and Tracking Extreme Climate Events Using Neural Networks\", \"Description of the NCAR Community Atmosphere Model (CAM 5.0)\", \"Volumetric and Multi-View CNNs for Object Classification on 3D Data\", \"PointNet: Deep Learning on Point Sets for 3D Classification and Segmentation\", \"PointNet++: Deep Hierarchical Feature Learning on Point Sets in a Metric Space\", \"Deep Neural Networks Motivated by Partial Differential Equations\", \"IM2Pano3D: Extrapolating 360 Structure and Semantics Beyond the Field of View\", \"Learning Spherical Convolution for Fast Features from 360 Imagery\", \"SegCloud: Semantic Segmentation of 3D Point Clouds\", \"Understanding Convolution for Semantic Segmentation\", \"Dynamic Graph CNN for Learning on Point Clouds\", \"3D ShapeNets: A Deep Representation for Volumetric Shapes\", \"SyncSpecCNN: Synchronized Spectral CNN for 3D Shape Segmentation\", \"Distortion-aware CNNS for Spherical Images\"]",
        "eval_response": "Evaluation evidence: The assistant's response does not align with the task requirements. The provided answer lists a series of paper titles that do not form a citation chain, as required by the task instructions. Additionally, the titles listed in the assistant's response do not match those in the gold answer, indicating a failure to identify the correct papers and their citation relationships. The response includes numerous titles not relevant to constructing a citation chain from the provided papers, leading to inaccuracies and hallucinations.\n\nRating: [[10]]",
        "score": 10
    },
    {
        "level": 3,
        "set": 1,
        "length": 43886,
        "type": "paper",
        "language": "en",
        "question": "Investigating Data Contamination for Pre-training Language Models",
        "instruction": "We hope you will carefully study the provided papers and determine the citation relationships between them. Please follow the instructions below strictly to complete the task:\n\n#Specific Requirements:\n1. Reference: When a given paper mentions other provided papers, those other papers are considered as \"references\" for the given paper. To summarize in this specific context, references are about what the given paper is using.\n2. Citation: Conversely, when other provided papers mention the given paper in their works, the given paper is being \"cited\" by those other papers. To summarize in this specific context, citations are about who is using the given paper.\n3. Given a paper, you need to determine the citation or reference relationship between this paper and the other papers. Do not consider papers that are not provided.\n3. Please present the paper titles in a json format as follows: {{\"Reference\":[\"Reference Title 1\", \"Reference Title 2\", ..., \"Reference Title n\"], \"Citation\":[\"Citation Title 1\", \"Citation Title 2\", ..., \"Citation Title n\"]}}.\n4. If a paper does not have any references or citations, please leave the corresponding list empty, e.g.{{\"Refernce\":[]}}, {{\"Citation\":[]}}.",
        "prompt_template": "#Papers:\n{docs}\n\n{instruction}\n\n#The paper you need to analyze:\n{question}",
        "doc": [
            "2401.06059.md",
            "2405.19313.md",
            "2311.04850.md"
        ],
        "answer": {
            "Reference": [
                "# Rethinking Benchmark and Contamination for Language Models with Rephrased Samples "
            ],
            "Citation": [
                "# Language Models Trained to do Arithmetic Predict Human Risky and Intertemporal Choice "
            ]
        },
        "shuffle_doc": true,
        "id": "59804366-0f40-4da6-838c-bbccec774e42",
        "prompt": "[Question]\n#Papers:\n{docs}\n\nWe hope you will carefully study the provided papers and determine the citation relationships between them. Please follow the instructions below strictly to complete the task:\n\n#Specific Requirements:\n1. Reference: When a given paper mentions other provided papers, those other papers are considered as \"references\" for the given paper. To summarize in this specific context, references are about what the given paper is using.\n2. Citation: Conversely, when other provided papers mention the given paper in their works, the given paper is being \"cited\" by those other papers. To summarize in this specific context, citations are about who is using the given paper.\n3. Given a paper, you need to determine the citation or reference relationship between this paper and the other papers. Do not consider papers that are not provided.\n3. Please present the paper titles in a json format as follows: {{\"Reference\":[\"Reference Title 1\", \"Reference Title 2\", ..., \"Reference Title n\"], \"Citation\":[\"Citation Title 1\", \"Citation Title 2\", ..., \"Citation Title n\"]}}.\n4. If a paper does not have any references or citations, please leave the corresponding list empty, e.g.{{\"Refernce\":[]}}, {{\"Citation\":[]}}.\n\n#The paper you need to analyze:\nInvestigating Data Contamination for Pre-training Language Models\n\n[Gold Answer]\n{'Reference': ['# Rethinking Benchmark and Contamination for Language Models with Rephrased Samples '], 'Citation': ['# Language Models Trained to do Arithmetic Predict Human Risky and Intertemporal Choice ']}\n\n[The Start of Assistant's Predicted Answer]\n{\\\"Citation\\\": [], \\\"Reference\\\": [\\\"A Survey on Evaluation of Large Language Models\\\", \\\"Vicuna: An Open-Source Chatbot Impressing GPT-4 with $90 \\% *$ ChatGPT Quality\\\", \\\"ChatLaw: Open-Source Legal Large Language Model with Integrated External Knowledge Bases\\\", \\\"Compositional Semantic Parsing with Large Language Models\\\", \\\"Open LLM Leaderboard\\\", \\\"How Close is ChatGPT to Human Experts? Comparison Corpus, Evaluation, and Detection\\\", \\\"REALM: Retrieval-Augmented Language Model PreTraining\\\", \\\"Rethinking with Retrieval: Faithful Large Language Model Inference\\\", \\\"Measuring Massive Multitask Language Understanding\\\", \\\"Atlas: Few-shot Learning with Retrieval Augmented Language Models\\\", \\\"Palm 2 technical report\\\", \\\"Language models are few-shot learners\\\", \\\"Membership inference attacks from first principles\\\", \\\"Quantifying memorization across neural language models\\\", \\\"Extracting training data from large language models\\\", \\\"Palm: Scaling language modeling with pathways\\\", \\\"BERT: Pre-training of deep bidirectional transformers for language understanding\\\", \\\"Does learning require memorization? a short tale about a long tail\\\", \\\"The pile: An $800 \\\\mathrm{gb}$ dataset of diverse text for language modeling\\\", \\\"Time travel in llms: Tracing data contamination in large language models\\\", \\\"Textbooks are all you need\\\", \\\"Measuring massive multitask language understanding\\\", \\\"An empirical analysis of compute-optimal large language model training\\\", \\\"Preventing generation of verbatim memorization in language models gives a false sense of privacy\\\", \\\"Membership inference attack susceptibility of clinical language models\\\", \\\"Pretraining language models with human preferences\\\", \\\"Textbooks are all you need ii: phi-1.5 technical report\\\", \\\"Holistic evaluation of language models\\\", \\\"ROUGE: A package for automatic evaluation of summaries\\\", \\\"Data contamination: From memorization to exploitation\\\", \\\"Membership inference on word embedding and beyond\\\", \\\"Membership inference attacks against language models via neighbourhood comparison\\\", \\\"Quantifying privacy risks of masked language models using membership inference attacks\\\", \\\"Abstractive text summarization using sequence-to-sequence RNNs and beyond\\\", \\\"Scalable extraction of training data from (production) language models\\\", \\\"Modern neural networks generalize on small data sets\\\", \\\"Gpt-4 technical report\\\", \\\"Proving test set contamination in black box language models\\\", \\\"Language models are unsupervised multitask learners\\\", \\\"Squad: 100,000+ questions for machine comprehension of text\\\", \\\"Recursive deep models for semantic compositionality over a sentiment treebank\\\", \\\"Llama: Open and efficient foundation language models\\\", \\\"Llama 2: Open foundation and fine-tuned chat models\\\", \\\"How far can camels go? exploring the state of instruction tuning on open resources\\\", \\\"Rethinking benchmark and contamination for language models with rephrased samples\\\", \\\"Towards a unified multi-dimensional evaluator for text generation\\\"]}\"\n[The End of Assistant's Predicted Answer]\n\n[System]\nWe would like to request your feedback on the performance of the AI assistant in response to the user question displayed above according to the gold answer. Please use the following listed aspects and their descriptions as evaluation criteria:\n    - Accuracy and Hallucinations: The assistant's answer is semantically consistent with the gold answer; The numerical value and order need to be accurate, and there should be no hallucinations.\n    - Completeness: Referring to the reference answers, the assistant's answer should contain all the key points needed to answer the user's question; further elaboration on these key points can be omitted.\nPlease rate whether this answer is suitable for the question. Please note that the gold answer can be considered as a correct answer to the question.\n\nThe assistant receives an overall score on a scale of 1 to 100, where a higher score indicates better overall performance.\nPlease note that if the assistant's answer and the gold answer fully meet the above criteria, its overall rating should be the full marks (100).\nPlease first provide a comprehensive explanation of your evaluation, avoiding any potential bias.\nThen, output a line indicating the score of the Assistant.\n\nPLEASE OUTPUT WITH THE FOLLOWING FORMAT, WHERE THE SCORE IS A SCALE OF 1 TO 100 BY STRICTLY FOLLOWING THIS FORMAT: \"[[score]]\", FOR EXAMPLE \"Rating: [[100]]\":\n<start output>\nEvaluation evidence: your evluation explanation here, no more than 100 words\nRating: [[score]]\n<end output>\n\nNow, start your evaluation:",
        "generate_response": "{\\\"Citation\\\": [], \\\"Reference\\\": [\\\"A Survey on Evaluation of Large Language Models\\\", \\\"Vicuna: An Open-Source Chatbot Impressing GPT-4 with $90 \\% *$ ChatGPT Quality\\\", \\\"ChatLaw: Open-Source Legal Large Language Model with Integrated External Knowledge Bases\\\", \\\"Compositional Semantic Parsing with Large Language Models\\\", \\\"Open LLM Leaderboard\\\", \\\"How Close is ChatGPT to Human Experts? Comparison Corpus, Evaluation, and Detection\\\", \\\"REALM: Retrieval-Augmented Language Model PreTraining\\\", \\\"Rethinking with Retrieval: Faithful Large Language Model Inference\\\", \\\"Measuring Massive Multitask Language Understanding\\\", \\\"Atlas: Few-shot Learning with Retrieval Augmented Language Models\\\", \\\"Palm 2 technical report\\\", \\\"Language models are few-shot learners\\\", \\\"Membership inference attacks from first principles\\\", \\\"Quantifying memorization across neural language models\\\", \\\"Extracting training data from large language models\\\", \\\"Palm: Scaling language modeling with pathways\\\", \\\"BERT: Pre-training of deep bidirectional transformers for language understanding\\\", \\\"Does learning require memorization? a short tale about a long tail\\\", \\\"The pile: An $800 \\\\mathrm{gb}$ dataset of diverse text for language modeling\\\", \\\"Time travel in llms: Tracing data contamination in large language models\\\", \\\"Textbooks are all you need\\\", \\\"Measuring massive multitask language understanding\\\", \\\"An empirical analysis of compute-optimal large language model training\\\", \\\"Preventing generation of verbatim memorization in language models gives a false sense of privacy\\\", \\\"Membership inference attack susceptibility of clinical language models\\\", \\\"Pretraining language models with human preferences\\\", \\\"Textbooks are all you need ii: phi-1.5 technical report\\\", \\\"Holistic evaluation of language models\\\", \\\"ROUGE: A package for automatic evaluation of summaries\\\", \\\"Data contamination: From memorization to exploitation\\\", \\\"Membership inference on word embedding and beyond\\\", \\\"Membership inference attacks against language models via neighbourhood comparison\\\", \\\"Quantifying privacy risks of masked language models using membership inference attacks\\\", \\\"Abstractive text summarization using sequence-to-sequence RNNs and beyond\\\", \\\"Scalable extraction of training data from (production) language models\\\", \\\"Modern neural networks generalize on small data sets\\\", \\\"Gpt-4 technical report\\\", \\\"Proving test set contamination in black box language models\\\", \\\"Language models are unsupervised multitask learners\\\", \\\"Squad: 100,000+ questions for machine comprehension of text\\\", \\\"Recursive deep models for semantic compositionality over a sentiment treebank\\\", \\\"Llama: Open and efficient foundation language models\\\", \\\"Llama 2: Open foundation and fine-tuned chat models\\\", \\\"How far can camels go? exploring the state of instruction tuning on open resources\\\", \\\"Rethinking benchmark and contamination for language models with rephrased samples\\\", \\\"Towards a unified multi-dimensional evaluator for text generation\\\"]}\"",
        "eval_response": "Evaluation evidence: The assistant's answer does not match the gold answer. It incorrectly lists multiple references and citations that are not mentioned in the gold answer. The gold answer specifies one reference and one citation, which are both missing in the assistant's response. This indicates issues with accuracy and hallucinations, as the assistant included irrelevant titles and omitted the correct ones. The response is also incomplete as it fails to include the correct reference and citation from the gold answer.\n\nRating: [[10]]",
        "score": 10
    },
    {
        "level": 3,
        "set": 1,
        "length": 37094,
        "type": "paper",
        "language": "en",
        "question": "Soaring from 4K to 400K: Extending LLM's Context with Activation Beacon",
        "instruction": "We hope you will carefully study the provided papers and determine the citation relationships between them. Please follow the instructions below strictly to complete the task:\n\n#Specific Requirements:\n1. Reference: When a given paper mentions other provided papers, those other papers are considered as \"references\" for the given paper. To summarize in this specific context, references are about what the given paper is using.\n2. Citation: Conversely, when other provided papers mention the given paper in their works, the given paper is being \"cited\" by those other papers. To summarize in this specific context, citations are about who is using the given paper.\n3. Given a paper, you need to determine the citation or reference relationship between this paper and the other papers. Do not consider papers that are not provided.\n3. Please present the paper titles in a json format as follows: {{\"Reference\":[\"Reference Title 1\", \"Reference Title 2\", ..., \"Reference Title n\"], \"Citation\":[\"Citation Title 1\", \"Citation Title 2\", ..., \"Citation Title n\"]}}.\n4. If a paper does not have any references or citations, please leave the corresponding list empty, e.g.{{\"Refernce\":[]}}, {{\"Citation\":[]}}.",
        "prompt_template": "#Papers:\n{docs}\n\n{instruction}\n\n#The paper you need to analyze:\n{question}",
        "doc": [
            "2401.03462.md",
            "2404.19553.md",
            "2309.10400.md"
        ],
        "answer": {
            "Reference": [
                "# PoSE: EFFICIENT CONTEXT WINDOW EXTENSION OF LLMS VIA POSITIONAL SKIP-WISE TRAINING "
            ],
            "Citation": [
                "# Extending Llama-3's Context Ten-Fold Overnight "
            ]
        },
        "shuffle_doc": true,
        "id": "f359acf5-39a1-4490-ba4a-95d38de35fa7",
        "prompt": "[Question]\n#Papers:\n{docs}\n\nWe hope you will carefully study the provided papers and determine the citation relationships between them. Please follow the instructions below strictly to complete the task:\n\n#Specific Requirements:\n1. Reference: When a given paper mentions other provided papers, those other papers are considered as \"references\" for the given paper. To summarize in this specific context, references are about what the given paper is using.\n2. Citation: Conversely, when other provided papers mention the given paper in their works, the given paper is being \"cited\" by those other papers. To summarize in this specific context, citations are about who is using the given paper.\n3. Given a paper, you need to determine the citation or reference relationship between this paper and the other papers. Do not consider papers that are not provided.\n3. Please present the paper titles in a json format as follows: {{\"Reference\":[\"Reference Title 1\", \"Reference Title 2\", ..., \"Reference Title n\"], \"Citation\":[\"Citation Title 1\", \"Citation Title 2\", ..., \"Citation Title n\"]}}.\n4. If a paper does not have any references or citations, please leave the corresponding list empty, e.g.{{\"Refernce\":[]}}, {{\"Citation\":[]}}.\n\n#The paper you need to analyze:\nSoaring from 4K to 400K: Extending LLM's Context with Activation Beacon\n\n[Gold Answer]\n{'Reference': ['# PoSE: EFFICIENT CONTEXT WINDOW EXTENSION OF LLMS VIA POSITIONAL SKIP-WISE TRAINING '], 'Citation': [\"# Extending Llama-3's Context Ten-Fold Overnight \"]}\n\n[The Start of Assistant's Predicted Answer]\n{\"Reference\":[\"A Survey on Evaluation of Large Language Models\", \"Vicuna: An Open-Source Chatbot Impressing GPT-4 with $90 \\% *$ ChatGPT Quality\", \"ChatLaw: Open-Source Legal Large Language Model with Integrated External Knowledge Bases\", \"Compositional Semantic Parsing with Large Language Models\", \"Open LLM Leaderboard\", \"How Close is ChatGPT to Human Experts? Comparison Corpus, Evaluation, and Detection\", \"REALM: Retrieval-Augmented Language Model PreTraining\", \"Rethinking with Retrieval: Faithful Large Language Model Inference\", \"Measuring Massive Multitask Language Understanding\", \"Atlas: Few-shot Learning with Retrieval Augmented Language Models\", \"NTK-aware scaled rope\", \"Longbench: A bilingual, multitask benchmark for long context understanding\", \"Longformer: The long-document transformer\", \"Scaling transformer to $1 \\mathrm{~m}$ tokens and beyond with RMT\", \"Extending context window of large language models via positional interpolation\", \"Longlora: Efficient fine-tuning of long-context large language models\", \"Adapting language models to compress contexts\", \"Generating long sequences with sparse transformers\", \"Rethinking attention with performers\", \"Redpajama: An open source recipe to reproduce llama training dataset\", \"Flashattention-2: Faster attention with better parallelism and work partitioning\", \"Landmark attention: Random-access infinite context length for transformers\", \"Learning to compress prompts with gist tokens\", \"Yarn: Efficient context window extension of large language models\", \"Train short, test long: Attention with linear biases enables input length extrapolation\", \"Compressive transformers for long-range sequence modelling\", \"Combiner: Full attention transformer with sparse computation cost\", \"Code llama: Open foundation models for code\", \"Long-range language modeling with self-retrieval\", \"Rectified rotary position embeddings\", \"Roformer: Enhanced transformer with rotary position embedding\", \"A length-extrapolatable transformer\", \"Llama: Open and efficient foundation language models\", \"Llama 2: Open foundation and fine-tuned chat models\", \"Natural language processing with transformers\", \"Focused transformer: Contrastive training for context scaling\", \"Linformer: Self-attention with linear complexity\", \"Augmenting language models with long-term memory\", \"Memorizing transformers\", \"Efficient streaming language models with attention sinks\", \"Retrieval meets long context large language models\", \"Big bird: Transformers for longer sequences\", \"Retrieve anything to augment large language models\", \"Proof-pile\", \"Pose: Efficient context window extension of llms via positional skip-wise training\"], \"Citation\":[]}\n[The End of Assistant's Predicted Answer]\n\n[System]\nWe would like to request your feedback on the performance of the AI assistant in response to the user question displayed above according to the gold answer. Please use the following listed aspects and their descriptions as evaluation criteria:\n    - Accuracy and Hallucinations: The assistant's answer is semantically consistent with the gold answer; The numerical value and order need to be accurate, and there should be no hallucinations.\n    - Completeness: Referring to the reference answers, the assistant's answer should contain all the key points needed to answer the user's question; further elaboration on these key points can be omitted.\nPlease rate whether this answer is suitable for the question. Please note that the gold answer can be considered as a correct answer to the question.\n\nThe assistant receives an overall score on a scale of 1 to 100, where a higher score indicates better overall performance.\nPlease note that if the assistant's answer and the gold answer fully meet the above criteria, its overall rating should be the full marks (100).\nPlease first provide a comprehensive explanation of your evaluation, avoiding any potential bias.\nThen, output a line indicating the score of the Assistant.\n\nPLEASE OUTPUT WITH THE FOLLOWING FORMAT, WHERE THE SCORE IS A SCALE OF 1 TO 100 BY STRICTLY FOLLOWING THIS FORMAT: \"[[score]]\", FOR EXAMPLE \"Rating: [[100]]\":\n<start output>\nEvaluation evidence: your evluation explanation here, no more than 100 words\nRating: [[score]]\n<end output>\n\nNow, start your evaluation:",
        "generate_response": "{\"Reference\":[\"A Survey on Evaluation of Large Language Models\", \"Vicuna: An Open-Source Chatbot Impressing GPT-4 with $90 \\% *$ ChatGPT Quality\", \"ChatLaw: Open-Source Legal Large Language Model with Integrated External Knowledge Bases\", \"Compositional Semantic Parsing with Large Language Models\", \"Open LLM Leaderboard\", \"How Close is ChatGPT to Human Experts? Comparison Corpus, Evaluation, and Detection\", \"REALM: Retrieval-Augmented Language Model PreTraining\", \"Rethinking with Retrieval: Faithful Large Language Model Inference\", \"Measuring Massive Multitask Language Understanding\", \"Atlas: Few-shot Learning with Retrieval Augmented Language Models\", \"NTK-aware scaled rope\", \"Longbench: A bilingual, multitask benchmark for long context understanding\", \"Longformer: The long-document transformer\", \"Scaling transformer to $1 \\mathrm{~m}$ tokens and beyond with RMT\", \"Extending context window of large language models via positional interpolation\", \"Longlora: Efficient fine-tuning of long-context large language models\", \"Adapting language models to compress contexts\", \"Generating long sequences with sparse transformers\", \"Rethinking attention with performers\", \"Redpajama: An open source recipe to reproduce llama training dataset\", \"Flashattention-2: Faster attention with better parallelism and work partitioning\", \"Landmark attention: Random-access infinite context length for transformers\", \"Learning to compress prompts with gist tokens\", \"Yarn: Efficient context window extension of large language models\", \"Train short, test long: Attention with linear biases enables input length extrapolation\", \"Compressive transformers for long-range sequence modelling\", \"Combiner: Full attention transformer with sparse computation cost\", \"Code llama: Open foundation models for code\", \"Long-range language modeling with self-retrieval\", \"Rectified rotary position embeddings\", \"Roformer: Enhanced transformer with rotary position embedding\", \"A length-extrapolatable transformer\", \"Llama: Open and efficient foundation language models\", \"Llama 2: Open foundation and fine-tuned chat models\", \"Natural language processing with transformers\", \"Focused transformer: Contrastive training for context scaling\", \"Linformer: Self-attention with linear complexity\", \"Augmenting language models with long-term memory\", \"Memorizing transformers\", \"Efficient streaming language models with attention sinks\", \"Retrieval meets long context large language models\", \"Big bird: Transformers for longer sequences\", \"Retrieve anything to augment large language models\", \"Proof-pile\", \"Pose: Efficient context window extension of llms via positional skip-wise training\"], \"Citation\":[]}",
        "eval_response": "Evaluation evidence: The assistant's answer inaccurately lists numerous references that are not mentioned in the gold answer, indicating a significant deviation from the correct response. The gold answer specifies only one reference, which the assistant fails to include, leading to a complete mismatch. Additionally, the assistant's citation list is empty, which aligns with the gold answer. However, the error in the reference section is critical, reflecting a lack of accuracy and a presence of hallucinations in the response.\n\nRating: [[20]]",
        "score": 20
    },
    {
        "level": 4,
        "set": 2,
        "length": 86916,
        "type": "paper",
        "language": "en",
        "question": "",
        "instruction": "We kindly ask you to thoroughly review the provided papers and construct a citation chain from them. Please adhere to the following instructions strictly while completing the task:\n\n#Task Instructions:\nGiven several papers, you are required to identify and list the longest citation chain, which demonstrates the citation relationship among the provided papers.\n\n#Specific Requirements:\n1.Please present the titles of the papers in the form of a list, as follows: [\"Title of Paper 1\", \"Title of Paper 2\", ..., \"Title of Paper n\"].\n2.Ensure that the citation chain in the list is linear and continuous, meaning that the first paper title in the list (Paper 1) should not cite any other works. Instead, it should be cited by the next paper in the list (Paper 2); subsequently, each paper should then be cited by the next one in the list, continuing up to the last paper (Paper n).\n3.Consider only the citation relationships within the supplied collection of papers, and ensure that the citation chain accurately reflects the sequential citation order among these documents.\n4.Do not take into account any articles not provided, and disregard other non-linear citation relationships.",
        "prompt_template": "{instruction}\n\n#Papers Provided:\n{docs}",
        "doc": [
            "2303.08891.md",
            "2305.17560.md",
            "2310.12487.md",
            "2405.09285.md"
        ],
        "answer": [
            "# VITO: VISION TRANSFORMER-OPERATOR ",
            "# Scalable Transformer for PDE Surrogate Modeling ",
            "# IMPROVED OPERATOR LEARNING BY ORTHOGONAL ATTENTION ",
            "# Positional Knowledge is All You Need: Position-induced Transformer (PiT) for Operator Learning "
        ],
        "shuffle_doc": true,
        "id": "7be41e02-4a50-4983-831f-65fa4d06331b",
        "prompt": "[Question]\nWe kindly ask you to thoroughly review the provided papers and construct a citation chain from them. Please adhere to the following instructions strictly while completing the task:\n\n#Task Instructions:\nGiven several papers, you are required to identify and list the longest citation chain, which demonstrates the citation relationship among the provided papers.\n\n#Specific Requirements:\n1.Please present the titles of the papers in the form of a list, as follows: [\"Title of Paper 1\", \"Title of Paper 2\", ..., \"Title of Paper n\"].\n2.Ensure that the citation chain in the list is linear and continuous, meaning that the first paper title in the list (Paper 1) should not cite any other works. Instead, it should be cited by the next paper in the list (Paper 2); subsequently, each paper should then be cited by the next one in the list, continuing up to the last paper (Paper n).\n3.Consider only the citation relationships within the supplied collection of papers, and ensure that the citation chain accurately reflects the sequential citation order among these documents.\n4.Do not take into account any articles not provided, and disregard other non-linear citation relationships.\n\n#Papers Provided:\n{docs}\n\n[Gold Answer]\n['# VITO: VISION TRANSFORMER-OPERATOR ', '# Scalable Transformer for PDE Surrogate Modeling ', '# IMPROVED OPERATOR LEARNING BY ORTHOGONAL ATTENTION ', '# Positional Knowledge is All You Need: Position-induced Transformer (PiT) for Operator Learning ']\n\n[The Start of Assistant's Predicted Answer]\n[\"IMPROVED OPERATOR LEARNING BY ORTHOGONAL ATTENTION\", \"Scalable Transformer for PDE Surrogate Modeling\", \"Neural machine translation by jointly learning to align and translate\", \"Attention is all you need\", \"An image is worth $16 \\\\times 16$ words: Transformers for image recognition at scale\", \"Deep Operator Network (DeepONet)\", \"Deep Residual Learning for Image Recognition\", \"Batch Normalization: Accelerating Deep Network Training by Reducing Internal Covariate Shift\", \"Gaussian error linear units (gelus)\", \"Self-Attention with Relative Position Representations\", \"Rethinking and improving relative position encoding for vision transformer\", \"Xception: Deep learning with depthwise separable convolutions\", \"Fully convolutional networks for semantic segmentation\"]\n[The End of Assistant's Predicted Answer]\n\n[System]\nWe would like to request your feedback on the performance of the AI assistant in response to the user question displayed above according to the gold answer. Please use the following listed aspects and their descriptions as evaluation criteria:\n    - Accuracy and Hallucinations: The assistant's answer is semantically consistent with the gold answer; The numerical value and order need to be accurate, and there should be no hallucinations.\n    - Completeness: Referring to the reference answers, the assistant's answer should contain all the key points needed to answer the user's question; further elaboration on these key points can be omitted.\nPlease rate whether this answer is suitable for the question. Please note that the gold answer can be considered as a correct answer to the question.\n\nThe assistant receives an overall score on a scale of 1 to 100, where a higher score indicates better overall performance.\nPlease note that if the assistant's answer and the gold answer fully meet the above criteria, its overall rating should be the full marks (100).\nPlease first provide a comprehensive explanation of your evaluation, avoiding any potential bias.\nThen, output a line indicating the score of the Assistant.\n\nPLEASE OUTPUT WITH THE FOLLOWING FORMAT, WHERE THE SCORE IS A SCALE OF 1 TO 100 BY STRICTLY FOLLOWING THIS FORMAT: \"[[score]]\", FOR EXAMPLE \"Rating: [[100]]\":\n<start output>\nEvaluation evidence: your evluation explanation here, no more than 100 words\nRating: [[score]]\n<end output>\n\nNow, start your evaluation:",
        "generate_response": "[\"IMPROVED OPERATOR LEARNING BY ORTHOGONAL ATTENTION\", \"Scalable Transformer for PDE Surrogate Modeling\", \"Neural machine translation by jointly learning to align and translate\", \"Attention is all you need\", \"An image is worth $16 \\\\times 16$ words: Transformers for image recognition at scale\", \"Deep Operator Network (DeepONet)\", \"Deep Residual Learning for Image Recognition\", \"Batch Normalization: Accelerating Deep Network Training by Reducing Internal Covariate Shift\", \"Gaussian error linear units (gelus)\", \"Self-Attention with Relative Position Representations\", \"Rethinking and improving relative position encoding for vision transformer\", \"Xception: Deep learning with depthwise separable convolutions\", \"Fully convolutional networks for semantic segmentation\"]",
        "eval_response": "Evaluation evidence: The assistant's response does not align with the gold answer, indicating a failure in task comprehension or execution. The gold answer lists a specific citation chain among four papers, while the assistant's response includes a completely different and much longer list of papers, none of which match the titles in the gold answer. This discrepancy shows a lack of accuracy and introduces multiple hallucinations (incorrect or irrelevant paper titles), failing to meet the task's requirements of identifying and listing a citation chain from the provided papers.\n\nRating: [[10]]",
        "score": 10
    },
    {
        "level": 4,
        "set": 3,
        "length": 108441,
        "type": "paper",
        "language": "en",
        "question": "",
        "instruction": "We kindly ask you to thoroughly review the provided papers and construct a citation chain from them. Please adhere to the following instructions strictly while completing the task:\n\n#Task Instructions:\nGiven several papers, you are required to identify and list the longest citation chain, which demonstrates the citation relationship among the provided papers.\n\n#Specific Requirements:\n1.Please present the titles of the papers in the form of a list, as follows: [\"Title of Paper 1\", \"Title of Paper 2\", ..., \"Title of Paper n\"].\n2.Ensure that the citation chain in the list is linear and continuous, meaning that the first paper title in the list (Paper 1) should not cite any other works. Instead, it should be cited by the next paper in the list (Paper 2); subsequently, each paper should then be cited by the next one in the list, continuing up to the last paper (Paper n).\n3.Consider only the citation relationships within the supplied collection of papers, and ensure that the citation chain accurately reflects the sequential citation order among these documents.\n4.Do not take into account any articles not provided, and disregard other non-linear citation relationships.",
        "prompt_template": "{instruction}\n\n#Papers Provided:\n{docs}",
        "doc": [
            "2209.02128.md",
            "2307.10719.md",
            "2308.12950.md",
            "2310.06825.md",
            "2405.07518.md"
        ],
        "answer": [
            "# EVALUATING THE SUSCEPTIBILITY OF PRE-TRAINED LANGUAGE MODELS VIA HANDCRAFTED ADVERSARIAL EXAMPLES ",
            "# LLM CensorSHiP: A MACHINE LEARNing CHALLENGE OR A COMPUTER SECURITY PROBLEM? ",
            "# Code Llama: Open Foundation Models for Code ",
            "# Mistral 7B ",
            "# SambaNova SN40L: Scaling the AI Memory Wall with Dataflow and Composition of Experts "
        ],
        "shuffle_doc": true,
        "id": "8b9dde48-3ee3-4c72-ac9e-7dabadbbcfc3",
        "prompt": "[Question]\nWe kindly ask you to thoroughly review the provided papers and construct a citation chain from them. Please adhere to the following instructions strictly while completing the task:\n\n#Task Instructions:\nGiven several papers, you are required to identify and list the longest citation chain, which demonstrates the citation relationship among the provided papers.\n\n#Specific Requirements:\n1.Please present the titles of the papers in the form of a list, as follows: [\"Title of Paper 1\", \"Title of Paper 2\", ..., \"Title of Paper n\"].\n2.Ensure that the citation chain in the list is linear and continuous, meaning that the first paper title in the list (Paper 1) should not cite any other works. Instead, it should be cited by the next paper in the list (Paper 2); subsequently, each paper should then be cited by the next one in the list, continuing up to the last paper (Paper n).\n3.Consider only the citation relationships within the supplied collection of papers, and ensure that the citation chain accurately reflects the sequential citation order among these documents.\n4.Do not take into account any articles not provided, and disregard other non-linear citation relationships.\n\n#Papers Provided:\n{docs}\n\n[Gold Answer]\n['# EVALUATING THE SUSCEPTIBILITY OF PRE-TRAINED LANGUAGE MODELS VIA HANDCRAFTED ADVERSARIAL EXAMPLES ', '# LLM CensorSHiP: A MACHINE LEARNing CHALLENGE OR A COMPUTER SECURITY PROBLEM? ', '# Code Llama: Open Foundation Models for Code ', '# Mistral 7B ', '# SambaNova SN40L: Scaling the AI Memory Wall with Dataflow and Composition of Experts ']\n\n[The Start of Assistant's Predicted Answer]\n[\"Code Llama: Open Foundation Models for Code\", \"Llama 2: Open foundation and fine-tuned chat models\"]\n[The End of Assistant's Predicted Answer]\n\n[System]\nWe would like to request your feedback on the performance of the AI assistant in response to the user question displayed above according to the gold answer. Please use the following listed aspects and their descriptions as evaluation criteria:\n    - Accuracy and Hallucinations: The assistant's answer is semantically consistent with the gold answer; The numerical value and order need to be accurate, and there should be no hallucinations.\n    - Completeness: Referring to the reference answers, the assistant's answer should contain all the key points needed to answer the user's question; further elaboration on these key points can be omitted.\nPlease rate whether this answer is suitable for the question. Please note that the gold answer can be considered as a correct answer to the question.\n\nThe assistant receives an overall score on a scale of 1 to 100, where a higher score indicates better overall performance.\nPlease note that if the assistant's answer and the gold answer fully meet the above criteria, its overall rating should be the full marks (100).\nPlease first provide a comprehensive explanation of your evaluation, avoiding any potential bias.\nThen, output a line indicating the score of the Assistant.\n\nPLEASE OUTPUT WITH THE FOLLOWING FORMAT, WHERE THE SCORE IS A SCALE OF 1 TO 100 BY STRICTLY FOLLOWING THIS FORMAT: \"[[score]]\", FOR EXAMPLE \"Rating: [[100]]\":\n<start output>\nEvaluation evidence: your evluation explanation here, no more than 100 words\nRating: [[score]]\n<end output>\n\nNow, start your evaluation:",
        "generate_response": "[\"Code Llama: Open Foundation Models for Code\", \"Llama 2: Open foundation and fine-tuned chat models\"]",
        "eval_response": "Evaluation evidence: The assistant's answer does not align with the gold answer, indicating a failure in identifying the correct citation chain among the provided papers. The titles listed by the assistant do not match those in the gold answer, and the chain is incomplete and incorrect. This shows issues with both accuracy and completeness, as the assistant failed to identify the correct papers and their citation relationships, and also included a title not present in the gold answer, which is a hallucination.\n\nRating: [[20]]",
        "score": 20
    },
    {
        "level": 3,
        "set": 2,
        "length": 89429,
        "type": "paper",
        "language": "en",
        "question": "WARM: On the Benefits of Weight Averaged Reward Models",
        "instruction": "We hope you will carefully study the provided papers and determine the citation relationships between them. Please follow the instructions below strictly to complete the task:\n\n#Specific Requirements:\n1. Reference: When a given paper mentions other provided papers, those other papers are considered as \"references\" for the given paper. To summarize in this specific context, references are about what the given paper is using.\n2. Citation: Conversely, when other provided papers mention the given paper in their works, the given paper is being \"cited\" by those other papers. To summarize in this specific context, citations are about who is using the given paper.\n3. Given a paper, you need to determine the citation or reference relationship between this paper and the other papers. Do not consider papers that are not provided.\n3. Please present the paper titles in a json format as follows: {{\"Reference\":[\"Reference Title 1\", \"Reference Title 2\", ..., \"Reference Title n\"], \"Citation\":[\"Citation Title 1\", \"Citation Title 2\", ..., \"Citation Title n\"]}}.\n4. If a paper does not have any references or citations, please leave the corresponding list empty, e.g.{{\"Refernce\":[]}}, {{\"Citation\":[]}}.",
        "prompt_template": "#Papers:\n{docs}\n\n{instruction}\n\n#The paper you need to analyze:\n{question}",
        "doc": [
            "2401.12187.md",
            "2405.04781.md",
            "2405.10516.md",
            "2401.00595.md",
            "2401.01879.md"
        ],
        "answer": {
            "Reference": [
                "# State of What Art? <br> A Call for Multi-Prompt LLM Evaluation ",
                "# Theoretical guarantees on the best-of-n alignment policy "
            ],
            "Citation": [
                "# COURSEGPT-ZH: AN EDUCATIONAL LARGE LANGUAGE MODEL BASED ON KNOWLEDGE DISTILLATION INCORPORATING PROMPT OPTIMIZATION * ",
                "# Language Models can Evaluate Themselves via Probability Discrepancy "
            ]
        },
        "shuffle_doc": true,
        "id": "8f752dd8-8baa-4a9b-b0bf-bca6c78aeb95",
        "prompt": "[Question]\n#Papers:\n{docs}\n\nWe hope you will carefully study the provided papers and determine the citation relationships between them. Please follow the instructions below strictly to complete the task:\n\n#Specific Requirements:\n1. Reference: When a given paper mentions other provided papers, those other papers are considered as \"references\" for the given paper. To summarize in this specific context, references are about what the given paper is using.\n2. Citation: Conversely, when other provided papers mention the given paper in their works, the given paper is being \"cited\" by those other papers. To summarize in this specific context, citations are about who is using the given paper.\n3. Given a paper, you need to determine the citation or reference relationship between this paper and the other papers. Do not consider papers that are not provided.\n3. Please present the paper titles in a json format as follows: {{\"Reference\":[\"Reference Title 1\", \"Reference Title 2\", ..., \"Reference Title n\"], \"Citation\":[\"Citation Title 1\", \"Citation Title 2\", ..., \"Citation Title n\"]}}.\n4. If a paper does not have any references or citations, please leave the corresponding list empty, e.g.{{\"Refernce\":[]}}, {{\"Citation\":[]}}.\n\n#The paper you need to analyze:\nWARM: On the Benefits of Weight Averaged Reward Models\n\n[Gold Answer]\n{'Reference': ['# State of What Art? <br> A Call for Multi-Prompt LLM Evaluation ', '# Theoretical guarantees on the best-of-n alignment policy '], 'Citation': ['# COURSEGPT-ZH: AN EDUCATIONAL LARGE LANGUAGE MODEL BASED ON KNOWLEDGE DISTILLATION INCORPORATING PROMPT OPTIMIZATION * ', '# Language Models can Evaluate Themselves via Probability Discrepancy ']}\n\n[The Start of Assistant's Predicted Answer]\n{\"Reference\":[\"Gemini\", \"GPT-4\", \"Sparks of artificial general intelligence: Early experiments with gpt-4\", \"Improving language understanding by generative pre-training\", \"BERT: Pre-training of deep bidirectional transformers for language understanding\", \"Language models are few-shot learners\", \"Finetuned language models are zero-shot learners\", \"Super-NaturalInstructions: Generalization via declarative instructions on 1600+ NLP tasks\", \"Stanford Alpaca: An instruction-following LLaMA model\", \"Factually consistent summarization via reinforcement learning with textual entailment feedback\", \"On the fragility of learned reward functions\", \"Deep reinforcement learning from human preferences\", \"Fine-tuning language models from human preferences\", \"Learning to summarize with human feedback\", \"Recursively summarizing books with human feedback\", \"LLaMA 2: Open foundation and fine-tuned chat models\", \"Scaling laws for reward model overoptimization\", \"Open problems and fundamental limitations of reinforcement learning from human feedback\", \"Concrete problems in AI safety\", \"Faulty Reward Functions in the Wild\", \"A general language assistant as a laboratory for alignment\", \"Defining and characterizing reward gaming\", \"The effects of reward misspecification: Mapping and mitigating misaligned models\", \"The alignment ceiling: Objective mismatch in reinforcement learning from human feedback\", \"Deal or no deal? end-to-end learning for negotiation dialogues\", \"A long way to go: Investigating length correlations in rlhf\", \"X-risk analysis for AI research\", \"Natural selection favors AIs over humans\", \"Consequences of misaligned AI\", \"Benchmarks and algorithms for offline preference-based reward learning\", \"Measuring progress on scalable oversight for large language models\", \"Rewarded soups: towards pareto-optimal alignment by interpolating weights fine-tuned on diverse rewards\", \"Training language models to follow instructions with human feedback\", \"Helping or herding? reward model ensembles mitigate but do not eliminate reward hacking\", \"Reward model ensembles help mitigate overoptimization\", \"Simple and scalable predictive uncertainty estimation using deep ensembles\", \"Linear mode connectivity and the lottery ticket hypothesis\", \"What is being transferred in transfer learning?\", \"Model soups: averaging weights of multiple fine-tuned models improves accuracy without increasing inference time\", \"Diverse weight averaging for out-of-distribution generalization\", \"Model Ratatouille: Recycling diverse models for out-of-distribution generalization\", \"SWAD: Domain generalization by seeking flat minima\", \"Domain generalization via invariant feature representation\", \"Invariant risk minimization\", \"Fuse to forget: Bias reduction and selective memorization through model fusion\", \"Spurious feature diversification improves out-of-distribution generalization\", \"Attention is all you need\", \"On the opportunities and risks of foundation models\", \"Learning and transferring mid-level image representations using convolutional neural networks\", \"Constitutional AI: Harmlessness from AI feedback\", \"RLAIF: Scaling reinforcement learning from human feedback with ai feedback\", \"Rank analysis of incomplete block designs: I. the method of paired comparisons\", \"Simple statistical gradient-following algorithms for connectionist reinforcement learning\", \"Proximal policy optimization algorithms\", \"Underspecification presents challenges for credibility in modern machine learning\", \"ID and OOD performance are sometimes inversely correlated on real-world datasets\", \"On calibration of modern neural networks\", \"Can you trust your model's uncertainty? evaluating predictive uncertainty under dataset shift\", \"On calibration and out-of-domain generalization\", \"Bounded rationality\", \"On the feasibility of learning, rather than assuming, human biases for reward inference\", \"On the challenges and practices of reinforcement learning from real human feedback\", \"Red teaming language models to reduce harms: Methods, scaling behaviors, and lessons learned\", \"Rewarding chatbots for real-world engagement with millions of users\", \"Sequence tutor: Conservative fine-tuning of sequence generation models with kl-control\", \"A theory of regularized markov decision processes\", \"Multi-agent communication meets natural language: Synergies between functional and structural language learning\", \"Countering language drift with seeded iterated learning\", \"Learning human objectives by evaluating hypothetical behavior\", \"Trial without error: Towards safe reinforcement learning via human intervention\", \"Secrets of rlhf in large language models part ii: Reward modeling\", \"Bias plus variance decomposition for zero-one loss functions\", \"Generalization error of ensemble estimators\", \"Specific versus general principles for constitutional ai\", \"Fine-tuning can distort pretrained features and underperform out-of-distribution\", \"Git re-basin: Merging models modulo permutation symmetries\", \"Knowledge is a region in weight space for fine-tuned language models\", \"No one representation to rule them all: Overlapping features of training methods\", \"Averaging weights leads to wider optima and better generalization\", \"Ensemble of averages: Improving model selection and boosting performance in domain generalization\", \"T1; dr: Mining reddit to learn automatic summarization\", \"PaLM 2 technical report\", \"Chain-of-Thought prompting elicits reasoning in large language models\", \"Fishr: Invariant gradient variances for out-of-distribution generalization\", \"Exploring the limits of transfer learning with a unified text-to-text transformer\", \"Building Machine Learning Models Like Open Source Software\", \"Branch-Train-Merge: Embarrassingly parallel training of expert language models\", \"Communication-efficient learning of deep networks from decentralized data\", \"Fine-grained human feedback gives better rewards for language model training\", \"Momentum-based weight interpolation of strong zero-shot models for continual learning\", \"Weight averaging: A simple yet effective method to overcome catastrophic forgetting in automatic speech recognition\", \"Direct preference optimization: Your language model is secretly a reward model\", \"NeuralBeagle14-7B\", \"Benchmarking neural network robustness to common corruptions and perturbations\", \"Variable generalization performance of a deep learning model to detect pneumonia in chest radiographs: A cross-sectional study\", \"AI for radiographic COVID-19 detection selects shortcuts over signal\", \"Robust fine-tuning of zero-shot models\", \"Patching open-vocabulary models by interpolating weights\", \"ColD fusion: Collaborative descent for distributed multitask finetuning\", \"Pareto manifold learning: Tackling multiple tasks via ensembles of single-task models\", \"Unival: Unified model for image, video, audio and language\", \"Seasoning model soups for robustness to adversarial and natural distribution shifts\", \"Linear connectivity reveals generalization strategies\", \"Improving stability in deep reinforcement learning with weight averaging\", \"Learning a subspace of policies for online adaptation in reinforcement learning\", \"Merging decision transformers: Weight averaging for forming multi-task policies\", \"Language model alignment with elastic reset\", \"Editing models with task arithmetic\", \"Elastic weight removal for faithful and abstractive dialogue generation\", \"Learning from noisy labels with deep neural networks: A survey\", \"Understanding deep learning requires rethinking generalization\", \"Learning from noisy labels by regularized estimation of annotator confusion\", \"Neftune: Noisy embeddings improve instruction finetuning\", \"Robust loss functions under label noise for deep neural networks\", \"Sample selection with uncertainty of losses for learning with noisy labels\", \"Mentornet: Learning data-driven curriculum for very deep neural networks on corrupted labels\", \"Co-teaching: Robust training of deep neural networks with extremely noisy labels\", \"Ensemble learning in the presence of noise\", \"Algorithms for inverse reinforcement learning\", \"Learning optimal advantage from preferences and mistaking it for reward\", \"Active reward learning from multiple teachers\", \"The impact of preference agreement in reinforcement learning from human feedback: A case study in summarization\", \"Toolaugmented reward modeling\", \"Aligning large multimodal models with factually augmented rlhf\", \"RIME: Robust preference-based reinforcement learning with noisy human preferences\", \"A general theoretical paradigm to understand learning from human preferences\", \"Adafactor: Adaptive learning rates with sublinear memory cost\", \"Vanishing gradients in reinforcement finetuning of language models\"], \"Citation\":[\"Theoretical guarantees on the best-of-n alignment policy\"]}\n[The End of Assistant's Predicted Answer]\n\n[System]\nWe would like to request your feedback on the performance of the AI assistant in response to the user question displayed above according to the gold answer. Please use the following listed aspects and their descriptions as evaluation criteria:\n    - Accuracy and Hallucinations: The assistant's answer is semantically consistent with the gold answer; The numerical value and order need to be accurate, and there should be no hallucinations.\n    - Completeness: Referring to the reference answers, the assistant's answer should contain all the key points needed to answer the user's question; further elaboration on these key points can be omitted.\nPlease rate whether this answer is suitable for the question. Please note that the gold answer can be considered as a correct answer to the question.\n\nThe assistant receives an overall score on a scale of 1 to 100, where a higher score indicates better overall performance.\nPlease note that if the assistant's answer and the gold answer fully meet the above criteria, its overall rating should be the full marks (100).\nPlease first provide a comprehensive explanation of your evaluation, avoiding any potential bias.\nThen, output a line indicating the score of the Assistant.\n\nPLEASE OUTPUT WITH THE FOLLOWING FORMAT, WHERE THE SCORE IS A SCALE OF 1 TO 100 BY STRICTLY FOLLOWING THIS FORMAT: \"[[score]]\", FOR EXAMPLE \"Rating: [[100]]\":\n<start output>\nEvaluation evidence: your evluation explanation here, no more than 100 words\nRating: [[score]]\n<end output>\n\nNow, start your evaluation:",
        "generate_response": "{\"Reference\":[\"Gemini\", \"GPT-4\", \"Sparks of artificial general intelligence: Early experiments with gpt-4\", \"Improving language understanding by generative pre-training\", \"BERT: Pre-training of deep bidirectional transformers for language understanding\", \"Language models are few-shot learners\", \"Finetuned language models are zero-shot learners\", \"Super-NaturalInstructions: Generalization via declarative instructions on 1600+ NLP tasks\", \"Stanford Alpaca: An instruction-following LLaMA model\", \"Factually consistent summarization via reinforcement learning with textual entailment feedback\", \"On the fragility of learned reward functions\", \"Deep reinforcement learning from human preferences\", \"Fine-tuning language models from human preferences\", \"Learning to summarize with human feedback\", \"Recursively summarizing books with human feedback\", \"LLaMA 2: Open foundation and fine-tuned chat models\", \"Scaling laws for reward model overoptimization\", \"Open problems and fundamental limitations of reinforcement learning from human feedback\", \"Concrete problems in AI safety\", \"Faulty Reward Functions in the Wild\", \"A general language assistant as a laboratory for alignment\", \"Defining and characterizing reward gaming\", \"The effects of reward misspecification: Mapping and mitigating misaligned models\", \"The alignment ceiling: Objective mismatch in reinforcement learning from human feedback\", \"Deal or no deal? end-to-end learning for negotiation dialogues\", \"A long way to go: Investigating length correlations in rlhf\", \"X-risk analysis for AI research\", \"Natural selection favors AIs over humans\", \"Consequences of misaligned AI\", \"Benchmarks and algorithms for offline preference-based reward learning\", \"Measuring progress on scalable oversight for large language models\", \"Rewarded soups: towards pareto-optimal alignment by interpolating weights fine-tuned on diverse rewards\", \"Training language models to follow instructions with human feedback\", \"Helping or herding? reward model ensembles mitigate but do not eliminate reward hacking\", \"Reward model ensembles help mitigate overoptimization\", \"Simple and scalable predictive uncertainty estimation using deep ensembles\", \"Linear mode connectivity and the lottery ticket hypothesis\", \"What is being transferred in transfer learning?\", \"Model soups: averaging weights of multiple fine-tuned models improves accuracy without increasing inference time\", \"Diverse weight averaging for out-of-distribution generalization\", \"Model Ratatouille: Recycling diverse models for out-of-distribution generalization\", \"SWAD: Domain generalization by seeking flat minima\", \"Domain generalization via invariant feature representation\", \"Invariant risk minimization\", \"Fuse to forget: Bias reduction and selective memorization through model fusion\", \"Spurious feature diversification improves out-of-distribution generalization\", \"Attention is all you need\", \"On the opportunities and risks of foundation models\", \"Learning and transferring mid-level image representations using convolutional neural networks\", \"Constitutional AI: Harmlessness from AI feedback\", \"RLAIF: Scaling reinforcement learning from human feedback with ai feedback\", \"Rank analysis of incomplete block designs: I. the method of paired comparisons\", \"Simple statistical gradient-following algorithms for connectionist reinforcement learning\", \"Proximal policy optimization algorithms\", \"Underspecification presents challenges for credibility in modern machine learning\", \"ID and OOD performance are sometimes inversely correlated on real-world datasets\", \"On calibration of modern neural networks\", \"Can you trust your model's uncertainty? evaluating predictive uncertainty under dataset shift\", \"On calibration and out-of-domain generalization\", \"Bounded rationality\", \"On the feasibility of learning, rather than assuming, human biases for reward inference\", \"On the challenges and practices of reinforcement learning from real human feedback\", \"Red teaming language models to reduce harms: Methods, scaling behaviors, and lessons learned\", \"Rewarding chatbots for real-world engagement with millions of users\", \"Sequence tutor: Conservative fine-tuning of sequence generation models with kl-control\", \"A theory of regularized markov decision processes\", \"Multi-agent communication meets natural language: Synergies between functional and structural language learning\", \"Countering language drift with seeded iterated learning\", \"Learning human objectives by evaluating hypothetical behavior\", \"Trial without error: Towards safe reinforcement learning via human intervention\", \"Secrets of rlhf in large language models part ii: Reward modeling\", \"Bias plus variance decomposition for zero-one loss functions\", \"Generalization error of ensemble estimators\", \"Specific versus general principles for constitutional ai\", \"Fine-tuning can distort pretrained features and underperform out-of-distribution\", \"Git re-basin: Merging models modulo permutation symmetries\", \"Knowledge is a region in weight space for fine-tuned language models\", \"No one representation to rule them all: Overlapping features of training methods\", \"Averaging weights leads to wider optima and better generalization\", \"Ensemble of averages: Improving model selection and boosting performance in domain generalization\", \"T1; dr: Mining reddit to learn automatic summarization\", \"PaLM 2 technical report\", \"Chain-of-Thought prompting elicits reasoning in large language models\", \"Fishr: Invariant gradient variances for out-of-distribution generalization\", \"Exploring the limits of transfer learning with a unified text-to-text transformer\", \"Building Machine Learning Models Like Open Source Software\", \"Branch-Train-Merge: Embarrassingly parallel training of expert language models\", \"Communication-efficient learning of deep networks from decentralized data\", \"Fine-grained human feedback gives better rewards for language model training\", \"Momentum-based weight interpolation of strong zero-shot models for continual learning\", \"Weight averaging: A simple yet effective method to overcome catastrophic forgetting in automatic speech recognition\", \"Direct preference optimization: Your language model is secretly a reward model\", \"NeuralBeagle14-7B\", \"Benchmarking neural network robustness to common corruptions and perturbations\", \"Variable generalization performance of a deep learning model to detect pneumonia in chest radiographs: A cross-sectional study\", \"AI for radiographic COVID-19 detection selects shortcuts over signal\", \"Robust fine-tuning of zero-shot models\", \"Patching open-vocabulary models by interpolating weights\", \"ColD fusion: Collaborative descent for distributed multitask finetuning\", \"Pareto manifold learning: Tackling multiple tasks via ensembles of single-task models\", \"Unival: Unified model for image, video, audio and language\", \"Seasoning model soups for robustness to adversarial and natural distribution shifts\", \"Linear connectivity reveals generalization strategies\", \"Improving stability in deep reinforcement learning with weight averaging\", \"Learning a subspace of policies for online adaptation in reinforcement learning\", \"Merging decision transformers: Weight averaging for forming multi-task policies\", \"Language model alignment with elastic reset\", \"Editing models with task arithmetic\", \"Elastic weight removal for faithful and abstractive dialogue generation\", \"Learning from noisy labels with deep neural networks: A survey\", \"Understanding deep learning requires rethinking generalization\", \"Learning from noisy labels by regularized estimation of annotator confusion\", \"Neftune: Noisy embeddings improve instruction finetuning\", \"Robust loss functions under label noise for deep neural networks\", \"Sample selection with uncertainty of losses for learning with noisy labels\", \"Mentornet: Learning data-driven curriculum for very deep neural networks on corrupted labels\", \"Co-teaching: Robust training of deep neural networks with extremely noisy labels\", \"Ensemble learning in the presence of noise\", \"Algorithms for inverse reinforcement learning\", \"Learning optimal advantage from preferences and mistaking it for reward\", \"Active reward learning from multiple teachers\", \"The impact of preference agreement in reinforcement learning from human feedback: A case study in summarization\", \"Toolaugmented reward modeling\", \"Aligning large multimodal models with factually augmented rlhf\", \"RIME: Robust preference-based reinforcement learning with noisy human preferences\", \"A general theoretical paradigm to understand learning from human preferences\", \"Adafactor: Adaptive learning rates with sublinear memory cost\", \"Vanishing gradients in reinforcement finetuning of language models\"], \"Citation\":[\"Theoretical guarantees on the best-of-n alignment policy\"]}",
        "eval_response": "Evaluation evidence: The assistant's answer does not match the gold answer in terms of accuracy and completeness. The references and citations listed in the assistant's response are entirely different from those in the gold answer, indicating a significant deviation and hallucination. The assistant's response includes a vast list of unrelated papers, which are not mentioned in the gold answer, thus failing to meet the required task specifications.\n\nRating: [[10]]",
        "score": 10
    },
    {
        "level": 3,
        "set": 3,
        "length": 113903,
        "type": "paper",
        "language": "en",
        "question": "TOFU: A Task of Fictitious Unlearning for LLMs",
        "instruction": "We hope you will carefully study the provided papers and determine the citation relationships between them. Please follow the instructions below strictly to complete the task:\n\n#Specific Requirements:\n1. Reference: When a given paper mentions other provided papers, those other papers are considered as \"references\" for the given paper. To summarize in this specific context, references are about what the given paper is using.\n2. Citation: Conversely, when other provided papers mention the given paper in their works, the given paper is being \"cited\" by those other papers. To summarize in this specific context, citations are about who is using the given paper.\n3. Given a paper, you need to determine the citation or reference relationship between this paper and the other papers. Do not consider papers that are not provided.\n3. Please present the paper titles in a json format as follows: {{\"Reference\":[\"Reference Title 1\", \"Reference Title 2\", ..., \"Reference Title n\"], \"Citation\":[\"Citation Title 1\", \"Citation Title 2\", ..., \"Citation Title n\"]}}.\n4. If a paper does not have any references or citations, please leave the corresponding list empty, e.g.{{\"Refernce\":[]}}, {{\"Citation\":[]}}.",
        "prompt_template": "#Papers:\n{docs}\n\n{instruction}\n\n#The paper you need to analyze:\n{question}",
        "doc": [
            "2401.06121.md",
            "2404.18239.md",
            "2405.12523.md",
            "2310.02238.md",
            "2310.16789.md"
        ],
        "answer": {
            "Reference": [
                "# Who's Harry Potter? Approximate Unlearning in LLMs ",
                "# DETECTING PRETRAINING DATA FROM LARGE LANGUAGE MODELS "
            ],
            "Citation": [
                "# SOUL: Unlocking the Power of Second-Order Optimization for LLM Unlearning ",
                "# Single Image Unlearning: Efficient Machine Unlearning in Multimodal Large Language Models "
            ]
        },
        "shuffle_doc": true,
        "id": "109265cb-8aee-4ee4-b7c0-376350b39e3e",
        "prompt": "[Question]\n#Papers:\n{docs}\n\nWe hope you will carefully study the provided papers and determine the citation relationships between them. Please follow the instructions below strictly to complete the task:\n\n#Specific Requirements:\n1. Reference: When a given paper mentions other provided papers, those other papers are considered as \"references\" for the given paper. To summarize in this specific context, references are about what the given paper is using.\n2. Citation: Conversely, when other provided papers mention the given paper in their works, the given paper is being \"cited\" by those other papers. To summarize in this specific context, citations are about who is using the given paper.\n3. Given a paper, you need to determine the citation or reference relationship between this paper and the other papers. Do not consider papers that are not provided.\n3. Please present the paper titles in a json format as follows: {{\"Reference\":[\"Reference Title 1\", \"Reference Title 2\", ..., \"Reference Title n\"], \"Citation\":[\"Citation Title 1\", \"Citation Title 2\", ..., \"Citation Title n\"]}}.\n4. If a paper does not have any references or citations, please leave the corresponding list empty, e.g.{{\"Refernce\":[]}}, {{\"Citation\":[]}}.\n\n#The paper you need to analyze:\nTOFU: A Task of Fictitious Unlearning for LLMs\n\n[Gold Answer]\n{'Reference': [\"# Who's Harry Potter? Approximate Unlearning in LLMs \", '# DETECTING PRETRAINING DATA FROM LARGE LANGUAGE MODELS '], 'Citation': ['# SOUL: Unlocking the Power of Second-Order Optimization for LLM Unlearning ', '# Single Image Unlearning: Efficient Machine Unlearning in Multimodal Large Language Models ']}\n\n[The Start of Assistant's Predicted Answer]\n{\"Reference\": [\"Unlearn what you want to forget: Efficient unlearning for llms\", \"Who's harry potter? approximate unlearning for llms\", \"Towards adversarial evaluations for inexact machine unlearning\", \"Extracting training data from large language models\", \"Membership inference attacks from first principles\", \"A Survey on Evaluation of Large Language Models\", \"Vicuna: An Open-Source Chatbot Impressing GPT-4 with $90 \\% *$ ChatGPT Quality\", \"ChatLaw: Open-Source Legal Large Language Model with Integrated External Knowledge Bases\", \"Compositional Semantic Parsing with Large Language Models\", \"Open LLM Leaderboard\", \"How Close is ChatGPT to Human Experts? Comparison Corpus, Evaluation, and Detection\", \"REALM: Retrieval-Augmented Language Model PreTraining\", \"Rethinking with Retrieval: Faithful Large Language Model Inference\", \"Measuring Massive Multitask Language Understanding\", \"Atlas: Few-shot Learning with Retrieval Augmented Language Models\", \"Machine unlearning\", \"Eternal sunshine of the spotless net: Selective forgetting in deep networks\", \"Certified data removal from machine learning models\", \"Separate the wheat from the chaff: Model deficiency unlearning via parameter-efficient module operation\", \"Are large pre-trained language models leaking your personal information?\", \"Knowledge unlearning for mitigating privacy risks in language models\", \"Propile: Probing privacy leakage in large language models\", \"The brainy student: Scalable unlearning by selectively disobeying the teacher\", \"Towards unbounded machine unlearning\", \"Textbooks are all you need ii: phi-1.5 technical report\", \"Rouge: A package for automatic evaluation of summaries\", \"Quark: Controllable text generation with reinforced unlearning\", \"Dataset inference: Ownership resolution in machine learning\", \"Catastrophic interference in connectionist networks: The sequential learning problem\", \"Locating and editing factual associations in gpt\", \"Adversary instantiation: Lower bounds for differentially private machine learning\", \"Ccpa regulations: Final regulation text\", \"Can sensitive information be deleted from llms? objectives for defending against extraction attacks\", \"In-context unlearning: Language models as few shot unlearners\", \"Direct preference optimization: Your language model is secretly a reward model\", \"Remember what you want to forget: Algorithms for machine unlearning\", \"Detecting pretraining data from large language models\", \"Membership inference attacks against machine learning models\", \"Privacy auditing with one (1\", \"On the necessity of auditable algorithmic definitions for machine unlearning\", \"Universal and transferable adversarial attacks on aligned language models\", \"A comprehensive study of knowledge editing for large language models\", \"Llama 2: Open foundation and fine-tuned chat models\", \"Regulation (eu) 2016/679 of the european parliament and of the council\", \"The eu general data protection regulation (gdpr). A Practical Guide\", \"Jailbroken: How does llm safety training fail?\", \"Large language model unlearning\"], \"Citation\": []}\n[The End of Assistant's Predicted Answer]\n\n[System]\nWe would like to request your feedback on the performance of the AI assistant in response to the user question displayed above according to the gold answer. Please use the following listed aspects and their descriptions as evaluation criteria:\n    - Accuracy and Hallucinations: The assistant's answer is semantically consistent with the gold answer; The numerical value and order need to be accurate, and there should be no hallucinations.\n    - Completeness: Referring to the reference answers, the assistant's answer should contain all the key points needed to answer the user's question; further elaboration on these key points can be omitted.\nPlease rate whether this answer is suitable for the question. Please note that the gold answer can be considered as a correct answer to the question.\n\nThe assistant receives an overall score on a scale of 1 to 100, where a higher score indicates better overall performance.\nPlease note that if the assistant's answer and the gold answer fully meet the above criteria, its overall rating should be the full marks (100).\nPlease first provide a comprehensive explanation of your evaluation, avoiding any potential bias.\nThen, output a line indicating the score of the Assistant.\n\nPLEASE OUTPUT WITH THE FOLLOWING FORMAT, WHERE THE SCORE IS A SCALE OF 1 TO 100 BY STRICTLY FOLLOWING THIS FORMAT: \"[[score]]\", FOR EXAMPLE \"Rating: [[100]]\":\n<start output>\nEvaluation evidence: your evluation explanation here, no more than 100 words\nRating: [[score]]\n<end output>\n\nNow, start your evaluation:",
        "generate_response": "{\"Reference\": [\"Unlearn what you want to forget: Efficient unlearning for llms\", \"Who's harry potter? approximate unlearning for llms\", \"Towards adversarial evaluations for inexact machine unlearning\", \"Extracting training data from large language models\", \"Membership inference attacks from first principles\", \"A Survey on Evaluation of Large Language Models\", \"Vicuna: An Open-Source Chatbot Impressing GPT-4 with $90 \\% *$ ChatGPT Quality\", \"ChatLaw: Open-Source Legal Large Language Model with Integrated External Knowledge Bases\", \"Compositional Semantic Parsing with Large Language Models\", \"Open LLM Leaderboard\", \"How Close is ChatGPT to Human Experts? Comparison Corpus, Evaluation, and Detection\", \"REALM: Retrieval-Augmented Language Model PreTraining\", \"Rethinking with Retrieval: Faithful Large Language Model Inference\", \"Measuring Massive Multitask Language Understanding\", \"Atlas: Few-shot Learning with Retrieval Augmented Language Models\", \"Machine unlearning\", \"Eternal sunshine of the spotless net: Selective forgetting in deep networks\", \"Certified data removal from machine learning models\", \"Separate the wheat from the chaff: Model deficiency unlearning via parameter-efficient module operation\", \"Are large pre-trained language models leaking your personal information?\", \"Knowledge unlearning for mitigating privacy risks in language models\", \"Propile: Probing privacy leakage in large language models\", \"The brainy student: Scalable unlearning by selectively disobeying the teacher\", \"Towards unbounded machine unlearning\", \"Textbooks are all you need ii: phi-1.5 technical report\", \"Rouge: A package for automatic evaluation of summaries\", \"Quark: Controllable text generation with reinforced unlearning\", \"Dataset inference: Ownership resolution in machine learning\", \"Catastrophic interference in connectionist networks: The sequential learning problem\", \"Locating and editing factual associations in gpt\", \"Adversary instantiation: Lower bounds for differentially private machine learning\", \"Ccpa regulations: Final regulation text\", \"Can sensitive information be deleted from llms? objectives for defending against extraction attacks\", \"In-context unlearning: Language models as few shot unlearners\", \"Direct preference optimization: Your language model is secretly a reward model\", \"Remember what you want to forget: Algorithms for machine unlearning\", \"Detecting pretraining data from large language models\", \"Membership inference attacks against machine learning models\", \"Privacy auditing with one (1\", \"On the necessity of auditable algorithmic definitions for machine unlearning\", \"Universal and transferable adversarial attacks on aligned language models\", \"A comprehensive study of knowledge editing for large language models\", \"Llama 2: Open foundation and fine-tuned chat models\", \"Regulation (eu) 2016/679 of the european parliament and of the council\", \"The eu general data protection regulation (gdpr). A Practical Guide\", \"Jailbroken: How does llm safety training fail?\", \"Large language model unlearning\"], \"Citation\": []}",
        "eval_response": "Evaluation evidence: The assistant's answer shows a significant deviation from the gold answer in terms of accuracy and completeness. The reference titles listed do not match those in the gold answer, indicating a misunderstanding or misinterpretation of the source material. Additionally, the assistant's answer includes a large number of references that are not relevant or mentioned in the gold answer, suggesting hallucinations. The citation section is also incorrect as it is left empty, whereas the gold answer lists specific citations. This results in a failure to meet the required task specifications.\n\nRating: [[20]]",
        "score": 20
    },
    {
        "level": 3,
        "set": 2,
        "length": 81991,
        "type": "paper",
        "language": "en",
        "question": "AUTORT: EMBODIED FOUNDATION MODELS FOR LARGE SCALE ORCHESTRATION OF ROBOTIC AGENTS",
        "instruction": "We hope you will carefully study the provided papers and determine the citation relationships between them. Please follow the instructions below strictly to complete the task:\n\n#Specific Requirements:\n1. Reference: When a given paper mentions other provided papers, those other papers are considered as \"references\" for the given paper. To summarize in this specific context, references are about what the given paper is using.\n2. Citation: Conversely, when other provided papers mention the given paper in their works, the given paper is being \"cited\" by those other papers. To summarize in this specific context, citations are about who is using the given paper.\n3. Given a paper, you need to determine the citation or reference relationship between this paper and the other papers. Do not consider papers that are not provided.\n3. Please present the paper titles in a json format as follows: {{\"Reference\":[\"Reference Title 1\", \"Reference Title 2\", ..., \"Reference Title n\"], \"Citation\":[\"Citation Title 1\", \"Citation Title 2\", ..., \"Citation Title n\"]}}.\n4. If a paper does not have any references or citations, please leave the corresponding list empty, e.g.{{\"Refernce\":[]}}, {{\"Citation\":[]}}.",
        "prompt_template": "#Papers:\n{docs}\n\n{instruction}\n\n#The paper you need to analyze:\n{question}",
        "doc": [
            "2401.12963.md",
            "2405.05824.md",
            "2405.11537.md",
            "2307.15818.md",
            "2308.12284.md"
        ],
        "answer": {
            "Reference": [
                "# RT-2: Vision-Language-Action Models Transfer Web Knowledge to Robotic Control ",
                "# D4: Improving LLM Pretraining via Document De-Duplication and Diversification "
            ],
            "Citation": [
                "# Robots Can Feel: LLM-based Framework for Robot Ethical Reasoning ",
                "# VR-GPT: Visual Language Model for Intelligent Virtual Reality Applications "
            ]
        },
        "shuffle_doc": true,
        "id": "090ded61-06ca-43ba-b486-41cb53ace06c",
        "prompt": "[Question]\n#Papers:\n{docs}\n\nWe hope you will carefully study the provided papers and determine the citation relationships between them. Please follow the instructions below strictly to complete the task:\n\n#Specific Requirements:\n1. Reference: When a given paper mentions other provided papers, those other papers are considered as \"references\" for the given paper. To summarize in this specific context, references are about what the given paper is using.\n2. Citation: Conversely, when other provided papers mention the given paper in their works, the given paper is being \"cited\" by those other papers. To summarize in this specific context, citations are about who is using the given paper.\n3. Given a paper, you need to determine the citation or reference relationship between this paper and the other papers. Do not consider papers that are not provided.\n3. Please present the paper titles in a json format as follows: {{\"Reference\":[\"Reference Title 1\", \"Reference Title 2\", ..., \"Reference Title n\"], \"Citation\":[\"Citation Title 1\", \"Citation Title 2\", ..., \"Citation Title n\"]}}.\n4. If a paper does not have any references or citations, please leave the corresponding list empty, e.g.{{\"Refernce\":[]}}, {{\"Citation\":[]}}.\n\n#The paper you need to analyze:\nAUTORT: EMBODIED FOUNDATION MODELS FOR LARGE SCALE ORCHESTRATION OF ROBOTIC AGENTS\n\n[Gold Answer]\n{'Reference': ['# RT-2: Vision-Language-Action Models Transfer Web Knowledge to Robotic Control ', '# D4: Improving LLM Pretraining via Document De-Duplication and Diversification '], 'Citation': ['# Robots Can Feel: LLM-based Framework for Robot Ethical Reasoning ', '# VR-GPT: Visual Language Model for Intelligent Virtual Reality Applications ']}\n\n[The Start of Assistant's Predicted Answer]\n{\"Reference\": [\"Do as i can and not as i say: Grounding language in robotic affordances\", \"How to prompt your robot: A promptbook for manipulation skills with code as policies\", \"Runaround\", \"Constitutional ai: Harmlessness from ai feedback\", \"Robocat: A self-improving foundation agent for robotic manipulation\", \"RT-1: Robotics transformer for real-world control at scale\", \"RT2: Vision-language-action models transfer web knowledge to robotic control\", \"Universal sentence encoder\", \"Open-vocabulary queryable scene representations for real world planning\", \"Pali: A jointly-scaled multilingual language-image model\", \"Leveraging procedural generation to benchmark reinforcement learning\", \"Robonet: Large-scale multi-robot learning\", \"Palm-e: An embodied multimodal language model\", \"Visual foresight: Model-based deep reinforcement learning for vision-based robotic control\", \"Physically grounded vision-language models for robotic manipulation\", \"Robot learning in homes: Improving generalization and reducing dataset bias\", \"Fleet-dagger: Interactive robot fleet learning with scalable human supervision\", \"BC-z: Zero-shot task generalization with robotic imitation learning\", \"Qt-opt: Scalable deep reinforcement learning for vision-based robotic manipulation\", \"MT-OPT: Continuous multi-task robotic reinforcement learning at scale\", \"Hgdagger: Interactive imitation learning with human experts\", \"Learning hand-eye coordination for robotic grasping with deep learning and large-scale data collection\", \"Code as policies: Language model programs for embodied control\", \"Interactive language: Talking to robots in real time\", \"Scaling robot supervision to hundreds of hours with roboturk: Robotic manipulation dataset through human reasoning and dexterity\", \"Grounding language with visual affordances over unstructured data\", \"Generative agents: Interactive simulacra of human behavior\", \"Supersizing self-supervision: Learning to grasp from 50k tries and 700 robot hours\", \"Grasp learning: Models, methods, and performance\", \"Sayplan: Grounding large language models using 3d scene graphs for scalable task planning\", \"Flexcap: Generating rich, localized, and flexible captions in images\", \"A reduction of imitation learning and structured prediction to no-regret online learning\", \"Multiple interactions made easy (mime): Large scale demonstrations data for imitation\", \"Reflexion: Language agents with verbal reinforcement learning\", \"D4: Improving llm pretraining via document de-duplication and diversification\", \"Chatgpt for robotics: Design principles and model abilities\", \"Voyager: An open-ended embodied agent with large language models\", \"Towards a foundation model for generalist robots: Diverse skill learning at scale via automated task and scene generation\", \"Robotic skill acquistion via instruction augmentation with visionlanguage models\", \"React: Synergizing reasoning and acting in language models\", \"More than a million ways to be pushed: A high-fidelity experimental dataset of planar pushing\"], \"Citation\": [\"Robots Can Feel: LLM-based Framework for Robot Ethical Reasoning\"]}\n[The End of Assistant's Predicted Answer]\n\n[System]\nWe would like to request your feedback on the performance of the AI assistant in response to the user question displayed above according to the gold answer. Please use the following listed aspects and their descriptions as evaluation criteria:\n    - Accuracy and Hallucinations: The assistant's answer is semantically consistent with the gold answer; The numerical value and order need to be accurate, and there should be no hallucinations.\n    - Completeness: Referring to the reference answers, the assistant's answer should contain all the key points needed to answer the user's question; further elaboration on these key points can be omitted.\nPlease rate whether this answer is suitable for the question. Please note that the gold answer can be considered as a correct answer to the question.\n\nThe assistant receives an overall score on a scale of 1 to 100, where a higher score indicates better overall performance.\nPlease note that if the assistant's answer and the gold answer fully meet the above criteria, its overall rating should be the full marks (100).\nPlease first provide a comprehensive explanation of your evaluation, avoiding any potential bias.\nThen, output a line indicating the score of the Assistant.\n\nPLEASE OUTPUT WITH THE FOLLOWING FORMAT, WHERE THE SCORE IS A SCALE OF 1 TO 100 BY STRICTLY FOLLOWING THIS FORMAT: \"[[score]]\", FOR EXAMPLE \"Rating: [[100]]\":\n<start output>\nEvaluation evidence: your evluation explanation here, no more than 100 words\nRating: [[score]]\n<end output>\n\nNow, start your evaluation:",
        "generate_response": "{\"Reference\": [\"Do as i can and not as i say: Grounding language in robotic affordances\", \"How to prompt your robot: A promptbook for manipulation skills with code as policies\", \"Runaround\", \"Constitutional ai: Harmlessness from ai feedback\", \"Robocat: A self-improving foundation agent for robotic manipulation\", \"RT-1: Robotics transformer for real-world control at scale\", \"RT2: Vision-language-action models transfer web knowledge to robotic control\", \"Universal sentence encoder\", \"Open-vocabulary queryable scene representations for real world planning\", \"Pali: A jointly-scaled multilingual language-image model\", \"Leveraging procedural generation to benchmark reinforcement learning\", \"Robonet: Large-scale multi-robot learning\", \"Palm-e: An embodied multimodal language model\", \"Visual foresight: Model-based deep reinforcement learning for vision-based robotic control\", \"Physically grounded vision-language models for robotic manipulation\", \"Robot learning in homes: Improving generalization and reducing dataset bias\", \"Fleet-dagger: Interactive robot fleet learning with scalable human supervision\", \"BC-z: Zero-shot task generalization with robotic imitation learning\", \"Qt-opt: Scalable deep reinforcement learning for vision-based robotic manipulation\", \"MT-OPT: Continuous multi-task robotic reinforcement learning at scale\", \"Hgdagger: Interactive imitation learning with human experts\", \"Learning hand-eye coordination for robotic grasping with deep learning and large-scale data collection\", \"Code as policies: Language model programs for embodied control\", \"Interactive language: Talking to robots in real time\", \"Scaling robot supervision to hundreds of hours with roboturk: Robotic manipulation dataset through human reasoning and dexterity\", \"Grounding language with visual affordances over unstructured data\", \"Generative agents: Interactive simulacra of human behavior\", \"Supersizing self-supervision: Learning to grasp from 50k tries and 700 robot hours\", \"Grasp learning: Models, methods, and performance\", \"Sayplan: Grounding large language models using 3d scene graphs for scalable task planning\", \"Flexcap: Generating rich, localized, and flexible captions in images\", \"A reduction of imitation learning and structured prediction to no-regret online learning\", \"Multiple interactions made easy (mime): Large scale demonstrations data for imitation\", \"Reflexion: Language agents with verbal reinforcement learning\", \"D4: Improving llm pretraining via document de-duplication and diversification\", \"Chatgpt for robotics: Design principles and model abilities\", \"Voyager: An open-ended embodied agent with large language models\", \"Towards a foundation model for generalist robots: Diverse skill learning at scale via automated task and scene generation\", \"Robotic skill acquistion via instruction augmentation with visionlanguage models\", \"React: Synergizing reasoning and acting in language models\", \"More than a million ways to be pushed: A high-fidelity experimental dataset of planar pushing\"], \"Citation\": [\"Robots Can Feel: LLM-based Framework for Robot Ethical Reasoning\"]}",
        "eval_response": "Evaluation evidence: The assistant's answer includes a large number of references that are not mentioned in the gold answer, indicating significant hallucinations. The gold answer specifies only two references and two citations, while the assistant lists an extensive array of unrelated titles. This discrepancy shows a lack of accuracy and introduces irrelevant information, failing to meet the completeness criterion by not focusing on the key points specified in the gold answer.\n\nRating: [[20]]",
        "score": 20
    },
    {
        "level": 3,
        "set": 2,
        "length": 71878,
        "type": "paper",
        "language": "en",
        "question": "Self-Discover: Large Language Models Self-Compose Reasoning Structures",
        "instruction": "We hope you will carefully study the provided papers and determine the citation relationships between them. Please follow the instructions below strictly to complete the task:\n\n#Specific Requirements:\n1. Reference: When a given paper mentions other provided papers, those other papers are considered as \"references\" for the given paper. To summarize in this specific context, references are about what the given paper is using.\n2. Citation: Conversely, when other provided papers mention the given paper in their works, the given paper is being \"cited\" by those other papers. To summarize in this specific context, citations are about who is using the given paper.\n3. Given a paper, you need to determine the citation or reference relationship between this paper and the other papers. Do not consider papers that are not provided.\n3. Please present the paper titles in a json format as follows: {{\"Reference\":[\"Reference Title 1\", \"Reference Title 2\", ..., \"Reference Title n\"], \"Citation\":[\"Citation Title 1\", \"Citation Title 2\", ..., \"Citation Title n\"]}}.\n4. If a paper does not have any references or citations, please leave the corresponding list empty, e.g.{{\"Refernce\":[]}}, {{\"Citation\":[]}}.",
        "prompt_template": "#Papers:\n{docs}\n\n{instruction}\n\n#The paper you need to analyze:\n{question}",
        "doc": [
            "2402.03620.md",
            "2404.02575.md",
            "2405.14804.md"
        ],
        "answer": {
            "Reference": [],
            "Citation": [
                "# Language Models as Compilers: Simulating Pseudocode Execution Improves Algorithmic Reasoning in Language Models ",
                "# Can LLMs Solve Longer Math Word Problems Better? "
            ]
        },
        "shuffle_doc": true,
        "id": "a5564b36-c608-4ff4-8a75-7c1b061a4b13",
        "prompt": "[Question]\n#Papers:\n{docs}\n\nWe hope you will carefully study the provided papers and determine the citation relationships between them. Please follow the instructions below strictly to complete the task:\n\n#Specific Requirements:\n1. Reference: When a given paper mentions other provided papers, those other papers are considered as \"references\" for the given paper. To summarize in this specific context, references are about what the given paper is using.\n2. Citation: Conversely, when other provided papers mention the given paper in their works, the given paper is being \"cited\" by those other papers. To summarize in this specific context, citations are about who is using the given paper.\n3. Given a paper, you need to determine the citation or reference relationship between this paper and the other papers. Do not consider papers that are not provided.\n3. Please present the paper titles in a json format as follows: {{\"Reference\":[\"Reference Title 1\", \"Reference Title 2\", ..., \"Reference Title n\"], \"Citation\":[\"Citation Title 1\", \"Citation Title 2\", ..., \"Citation Title n\"]}}.\n4. If a paper does not have any references or citations, please leave the corresponding list empty, e.g.{{\"Refernce\":[]}}, {{\"Citation\":[]}}.\n\n#The paper you need to analyze:\nSelf-Discover: Large Language Models Self-Compose Reasoning Structures\n\n[Gold Answer]\n{'Reference': [], 'Citation': ['# Language Models as Compilers: Simulating Pseudocode Execution Improves Algorithmic Reasoning in Language Models ', '# Can LLMs Solve Longer Math Word Problems Better? ']}\n\n[The Start of Assistant's Predicted Answer]\n{{\"Reference\":[\"A Survey on Evaluation of Large Language Models\", \"Vicuna: An Open-Source Chatbot Impressing GPT-4 with $90 \\% *$ ChatGPT Quality\", \"ChatLaw: Open-Source Legal Large Language Model with Integrated External Knowledge Bases\", \"Compositional Semantic Parsing with Large Language Models\", \"Open LLM Leaderboard\", \"How Close is ChatGPT to Human Experts? Comparison Corpus, Evaluation, and Detection\", \"Rethinking with Retrieval: Faithful Large Language Model Inference\", \"Measuring Massive Multitask Language Understanding\", \"Thinking for Doing (T4D)\", \"MATH\", \"Chain of Thought (Kojima et al., 2022)\", \"Plan-and-Solve (Wang et al., 2023)\", \"LLMs as optimizers (OPRO) (Yang et al., 2023)\", \"Palm 2 technical report (Anil et al., 2023)\", \"Graph of thoughts: Solving elaborate problems with large language models (Besta et al., 2023)\", \"Language models are few-shot learners (Brown et al., 2020)\", \"Skills-in-context prompting: Unlocking compositionality in large language models (Chen et al., 2023)\", \"Program of thoughts prompting: Disentangling computation from reasoning for numerical reasoning tasks (Chen et al., 2022)\", \"Palm: Scaling language modeling with pathways (Chowdhery et al., 2022)\", \"Scaling instruction-finetuned language models (Chung et al., 2022)\", \"Training verifiers to solve math word problems (Cobbe et al., 2021)\", \"Compositional semantic parsing with large language models (Drozdov et al., 2022)\", \"Self-referential self-improvement via prompt evolution (Fernando et al., 2023)\", \"Strategyllm: Large language models as strategy generators, executors, optimizers, and evaluators for problem solving (Gao et al., 2023a)\", \"Pal: Program-aided language models (Gao et al., 2023b)\", \"Reasoning with language model is planning with world model (Hao et al., 2023)\", \"Measuring mathematical problem solving with the math dataset (Hendrycks et al., 2021)\", \"Decomposed prompting: A modular approach for solving complex tasks (Khot et al., 2022)\", \"Large language models are zero-shot reasoners (Kojima et al., 2022)\", \"Summary of long instructions is better for program synthesis (Kuznia et al., 2022)\", \"Plan, verify and switch: Integrated reasoning with diverse x-of-thoughts (Liu et al., 2023)\", \"HELP ME THINK: A simple prompting strategy for non-experts to create customized content with models (Mishra and Nouri, 2023)\", \"Lila: A unified benchmark for mathematical reasoning (Mishra et al., 2022a)\", \"Reframing instructional prompts to gptk's language (Mishra et al., 2022b)\", \"Crosstask generalization via natural language crowdsourcing instructions (Mishra et al., 2022c)\", \"Elements of a theory of human problem solving (Newell et al., 1958)\", \"Show your work: Scratchpads for intermediate computation with language models (Nye et al., 2021)\", \"Chatgpt: Optimizing language models for dialogue (OpenAI, 2022)\", \"Json generation mode (OpenAI, 2023a)\", \"Gpt-4 technical report (OpenAI, 2023b)\", \"Training language models to follow instructions with human feedback (Ouyang et al., 2022)\", \"Is a question decomposition unit all we need? (Patel et al., 2022)\", \"How to solve it: A new aspect of mathematical method (Polya, 2004)\", \"Skills, rules, and knowledge; signals, signs, and symbols, and other distinctions in human performance models (Rasmussen, 1983)\", \"Branch-solve-merge improves large language model evaluation and generation (Saha et al., 2023)\", \"Beyond the imitation game: Quantifying and extrapolating the capabilities of language models (Srivastava et al., 2023)\", \"Challenging big-bench tasks and whether chain-of-thought can solve them (Suzgun et al., 2022)\", \"Llama 2: Open foundation and finetuned chat models (Touvron et al., 2023)\", \"Attention is all you need (Vaswani et al., 2017)\", \"Plan-and-solve prompting: Improving zero-shot chain-of-thought reasoning by large language models (Wang et al., 2023)\", \"Selfconsistency improves chain of thought reasoning in language models (Wang et al., 2022)\", \"Finetuned language models are zero-shot learners (Wei et al., 2021)\", \"Chain-of-thought prompting elicits reasoning in large language models (Wei et al., 2022)\", \"Large language models as optimizers (Yang et al., 2023)\", \"Tree of thoughts: Deliberate problem solving with large language models (Yao et al., 2023a)\", \"Beyond chain-of-thought, effective graph-of-thought reasoning in large language models (Yao et al., 2023b)\", \"Large language models as analogical reasoners (Yasunaga et al., 2023)\", \"Take a step back: Evoking reasoning via abstraction in large language models (Zheng et al., 2023)\", \"Adapting language models for zero-shot learning by metatuning on dataset and prompt collections (Zhong et al., 2021)\", \"Least-to-most prompting enables complex reasoning in large language models (Zhou et al., 2022a)\", \"How far are large language models from agents with theory-of-mind? (Zhou et al., 2023)\", \"Large language models are humanlevel prompt engineers (Zhou et al., 2022b)\"], \"Citation\":[]}}\"\n[The End of Assistant's Predicted Answer]\n\n[System]\nWe would like to request your feedback on the performance of the AI assistant in response to the user question displayed above according to the gold answer. Please use the following listed aspects and their descriptions as evaluation criteria:\n    - Accuracy and Hallucinations: The assistant's answer is semantically consistent with the gold answer; The numerical value and order need to be accurate, and there should be no hallucinations.\n    - Completeness: Referring to the reference answers, the assistant's answer should contain all the key points needed to answer the user's question; further elaboration on these key points can be omitted.\nPlease rate whether this answer is suitable for the question. Please note that the gold answer can be considered as a correct answer to the question.\n\nThe assistant receives an overall score on a scale of 1 to 100, where a higher score indicates better overall performance.\nPlease note that if the assistant's answer and the gold answer fully meet the above criteria, its overall rating should be the full marks (100).\nPlease first provide a comprehensive explanation of your evaluation, avoiding any potential bias.\nThen, output a line indicating the score of the Assistant.\n\nPLEASE OUTPUT WITH THE FOLLOWING FORMAT, WHERE THE SCORE IS A SCALE OF 1 TO 100 BY STRICTLY FOLLOWING THIS FORMAT: \"[[score]]\", FOR EXAMPLE \"Rating: [[100]]\":\n<start output>\nEvaluation evidence: your evluation explanation here, no more than 100 words\nRating: [[score]]\n<end output>\n\nNow, start your evaluation:",
        "generate_response": "{{\"Reference\":[\"A Survey on Evaluation of Large Language Models\", \"Vicuna: An Open-Source Chatbot Impressing GPT-4 with $90 \\% *$ ChatGPT Quality\", \"ChatLaw: Open-Source Legal Large Language Model with Integrated External Knowledge Bases\", \"Compositional Semantic Parsing with Large Language Models\", \"Open LLM Leaderboard\", \"How Close is ChatGPT to Human Experts? Comparison Corpus, Evaluation, and Detection\", \"Rethinking with Retrieval: Faithful Large Language Model Inference\", \"Measuring Massive Multitask Language Understanding\", \"Thinking for Doing (T4D)\", \"MATH\", \"Chain of Thought (Kojima et al., 2022)\", \"Plan-and-Solve (Wang et al., 2023)\", \"LLMs as optimizers (OPRO) (Yang et al., 2023)\", \"Palm 2 technical report (Anil et al., 2023)\", \"Graph of thoughts: Solving elaborate problems with large language models (Besta et al., 2023)\", \"Language models are few-shot learners (Brown et al., 2020)\", \"Skills-in-context prompting: Unlocking compositionality in large language models (Chen et al., 2023)\", \"Program of thoughts prompting: Disentangling computation from reasoning for numerical reasoning tasks (Chen et al., 2022)\", \"Palm: Scaling language modeling with pathways (Chowdhery et al., 2022)\", \"Scaling instruction-finetuned language models (Chung et al., 2022)\", \"Training verifiers to solve math word problems (Cobbe et al., 2021)\", \"Compositional semantic parsing with large language models (Drozdov et al., 2022)\", \"Self-referential self-improvement via prompt evolution (Fernando et al., 2023)\", \"Strategyllm: Large language models as strategy generators, executors, optimizers, and evaluators for problem solving (Gao et al., 2023a)\", \"Pal: Program-aided language models (Gao et al., 2023b)\", \"Reasoning with language model is planning with world model (Hao et al., 2023)\", \"Measuring mathematical problem solving with the math dataset (Hendrycks et al., 2021)\", \"Decomposed prompting: A modular approach for solving complex tasks (Khot et al., 2022)\", \"Large language models are zero-shot reasoners (Kojima et al., 2022)\", \"Summary of long instructions is better for program synthesis (Kuznia et al., 2022)\", \"Plan, verify and switch: Integrated reasoning with diverse x-of-thoughts (Liu et al., 2023)\", \"HELP ME THINK: A simple prompting strategy for non-experts to create customized content with models (Mishra and Nouri, 2023)\", \"Lila: A unified benchmark for mathematical reasoning (Mishra et al., 2022a)\", \"Reframing instructional prompts to gptk's language (Mishra et al., 2022b)\", \"Crosstask generalization via natural language crowdsourcing instructions (Mishra et al., 2022c)\", \"Elements of a theory of human problem solving (Newell et al., 1958)\", \"Show your work: Scratchpads for intermediate computation with language models (Nye et al., 2021)\", \"Chatgpt: Optimizing language models for dialogue (OpenAI, 2022)\", \"Json generation mode (OpenAI, 2023a)\", \"Gpt-4 technical report (OpenAI, 2023b)\", \"Training language models to follow instructions with human feedback (Ouyang et al., 2022)\", \"Is a question decomposition unit all we need? (Patel et al., 2022)\", \"How to solve it: A new aspect of mathematical method (Polya, 2004)\", \"Skills, rules, and knowledge; signals, signs, and symbols, and other distinctions in human performance models (Rasmussen, 1983)\", \"Branch-solve-merge improves large language model evaluation and generation (Saha et al., 2023)\", \"Beyond the imitation game: Quantifying and extrapolating the capabilities of language models (Srivastava et al., 2023)\", \"Challenging big-bench tasks and whether chain-of-thought can solve them (Suzgun et al., 2022)\", \"Llama 2: Open foundation and finetuned chat models (Touvron et al., 2023)\", \"Attention is all you need (Vaswani et al., 2017)\", \"Plan-and-solve prompting: Improving zero-shot chain-of-thought reasoning by large language models (Wang et al., 2023)\", \"Selfconsistency improves chain of thought reasoning in language models (Wang et al., 2022)\", \"Finetuned language models are zero-shot learners (Wei et al., 2021)\", \"Chain-of-thought prompting elicits reasoning in large language models (Wei et al., 2022)\", \"Large language models as optimizers (Yang et al., 2023)\", \"Tree of thoughts: Deliberate problem solving with large language models (Yao et al., 2023a)\", \"Beyond chain-of-thought, effective graph-of-thought reasoning in large language models (Yao et al., 2023b)\", \"Large language models as analogical reasoners (Yasunaga et al., 2023)\", \"Take a step back: Evoking reasoning via abstraction in large language models (Zheng et al., 2023)\", \"Adapting language models for zero-shot learning by metatuning on dataset and prompt collections (Zhong et al., 2021)\", \"Least-to-most prompting enables complex reasoning in large language models (Zhou et al., 2022a)\", \"How far are large language models from agents with theory-of-mind? (Zhou et al., 2023)\", \"Large language models are humanlevel prompt engineers (Zhou et al., 2022b)\"], \"Citation\":[]}}\"",
        "eval_response": "Evaluation evidence: The assistant's response does not align with the gold answer. The gold answer specifies no references and two citations for the paper \"Self-Discover: Large Language Models Self-Compose Reasoning Structures.\" However, the assistant's response lists numerous references incorrectly and no citations, directly contradicting the gold answer. This indicates issues with accuracy and hallucinations, as the assistant provided incorrect and extraneous information not supported by the gold answer.\n\nRating: [[10]]",
        "score": 10
    },
    {
        "level": 3,
        "set": 2,
        "length": 84374,
        "type": "paper",
        "language": "en",
        "question": "Mastering Text-to-Image Diffusion: Recaptioning, Planning, and Generating with Multimodal LLMs",
        "instruction": "We hope you will carefully study the provided papers and determine the citation relationships between them. Please follow the instructions below strictly to complete the task:\n\n#Specific Requirements:\n1. Reference: When a given paper mentions other provided papers, those other papers are considered as \"references\" for the given paper. To summarize in this specific context, references are about what the given paper is using.\n2. Citation: Conversely, when other provided papers mention the given paper in their works, the given paper is being \"cited\" by those other papers. To summarize in this specific context, citations are about who is using the given paper.\n3. Given a paper, you need to determine the citation or reference relationship between this paper and the other papers. Do not consider papers that are not provided.\n3. Please present the paper titles in a json format as follows: {{\"Reference\":[\"Reference Title 1\", \"Reference Title 2\", ..., \"Reference Title n\"], \"Citation\":[\"Citation Title 1\", \"Citation Title 2\", ..., \"Citation Title n\"]}}.\n4. If a paper does not have any references or citations, please leave the corresponding list empty, e.g.{{\"Refernce\":[]}}, {{\"Citation\":[]}}.",
        "prompt_template": "#Papers:\n{docs}\n\n{instruction}\n\n#The paper you need to analyze:\n{question}",
        "doc": [
            "2401.11708.md",
            "2405.05945.md",
            "2405.08748.md",
            "2405.15287.md"
        ],
        "answer": {
            "Reference": [],
            "Citation": [
                "# Lumina-T2X: Transforming Text into ",
                "# Hunyuan-DiT : A Powerful Multi-Resolution Diffusion Transformer with Fine-Grained Chinese Understanding ",
                "# StyleMaster: Towards Flexible Stylized Image Generation with Diffusion Models "
            ]
        },
        "shuffle_doc": true,
        "id": "0a330b4b-d5b9-471c-a615-bce7d086912f",
        "prompt": "[Question]\n#Papers:\n{docs}\n\nWe hope you will carefully study the provided papers and determine the citation relationships between them. Please follow the instructions below strictly to complete the task:\n\n#Specific Requirements:\n1. Reference: When a given paper mentions other provided papers, those other papers are considered as \"references\" for the given paper. To summarize in this specific context, references are about what the given paper is using.\n2. Citation: Conversely, when other provided papers mention the given paper in their works, the given paper is being \"cited\" by those other papers. To summarize in this specific context, citations are about who is using the given paper.\n3. Given a paper, you need to determine the citation or reference relationship between this paper and the other papers. Do not consider papers that are not provided.\n3. Please present the paper titles in a json format as follows: {{\"Reference\":[\"Reference Title 1\", \"Reference Title 2\", ..., \"Reference Title n\"], \"Citation\":[\"Citation Title 1\", \"Citation Title 2\", ..., \"Citation Title n\"]}}.\n4. If a paper does not have any references or citations, please leave the corresponding list empty, e.g.{{\"Refernce\":[]}}, {{\"Citation\":[]}}.\n\n#The paper you need to analyze:\nMastering Text-to-Image Diffusion: Recaptioning, Planning, and Generating with Multimodal LLMs\n\n[Gold Answer]\n{'Reference': [], 'Citation': ['# Lumina-T2X: Transforming Text into ', '# Hunyuan-DiT : A Powerful Multi-Resolution Diffusion Transformer with Fine-Grained Chinese Understanding ', '# StyleMaster: Towards Flexible Stylized Image Generation with Diffusion Models ']}\n\n[The Start of Assistant's Predicted Answer]\n{\"Reference\":[\"Spatext: Spatio-textual representation for controllable image generation\", \"Multidiffusion: Fusing diffusion paths for controlled image generation\", \"Improving image generation with better captions\", \"Instructpix2pix: Learning to follow image editing instructions\", \"Masactrl: Tuning-free mutual self-attention control for consistent image synthesis and editing\", \"Introducing chatgpt\", \"Attend-and-excite: Attention-based semantic guidance for text-to-image diffusion models\", \"Pixart-alpha: Fast training of diffusion transformer for photorealistic text-to-image synthesis\", \"Training-free layout control with cross-attention guidance\", \"Layoutllmt2i: Eliciting layout guidance from llm for text-to-image generation\", \"Score-based generative modeling through stochastic differential equations\", \"Learning to summarize with human feedback\", \"Dreamsync: Aligning text-to-image generation with image understanding feedback\", \"Compositional text-to-image synthesis with attention map control of diffusion models\", \"Self-correcting llm-controlled diffusion models\", \"Boxdiff: Text-to-image synthesis with training-free box-constrained diffusion\", \"Imagereward: Learning and evaluating human preferences for text-to-image generation\", \"Diffusion models: A comprehensive survey of methods and applications\", \"Cross-modal contextualized diffusion models for text-guided visual generation and editing\", \"Regioncontrolled text-to-image generation\", \"Enhanced visual instruction tuning for textrich image understanding\", \"Multimodal chain-of-thought reasoning in language models\", \"Generalized decoding for pixel, image, and language\"], \"Citation\":[\"StyleMaster: Towards Flexible Stylized Image Generation with Diffusion Models\"]}\n[The End of Assistant's Predicted Answer]\n\n[System]\nWe would like to request your feedback on the performance of the AI assistant in response to the user question displayed above according to the gold answer. Please use the following listed aspects and their descriptions as evaluation criteria:\n    - Accuracy and Hallucinations: The assistant's answer is semantically consistent with the gold answer; The numerical value and order need to be accurate, and there should be no hallucinations.\n    - Completeness: Referring to the reference answers, the assistant's answer should contain all the key points needed to answer the user's question; further elaboration on these key points can be omitted.\nPlease rate whether this answer is suitable for the question. Please note that the gold answer can be considered as a correct answer to the question.\n\nThe assistant receives an overall score on a scale of 1 to 100, where a higher score indicates better overall performance.\nPlease note that if the assistant's answer and the gold answer fully meet the above criteria, its overall rating should be the full marks (100).\nPlease first provide a comprehensive explanation of your evaluation, avoiding any potential bias.\nThen, output a line indicating the score of the Assistant.\n\nPLEASE OUTPUT WITH THE FOLLOWING FORMAT, WHERE THE SCORE IS A SCALE OF 1 TO 100 BY STRICTLY FOLLOWING THIS FORMAT: \"[[score]]\", FOR EXAMPLE \"Rating: [[100]]\":\n<start output>\nEvaluation evidence: your evluation explanation here, no more than 100 words\nRating: [[score]]\n<end output>\n\nNow, start your evaluation:",
        "generate_response": "{\"Reference\":[\"Spatext: Spatio-textual representation for controllable image generation\", \"Multidiffusion: Fusing diffusion paths for controlled image generation\", \"Improving image generation with better captions\", \"Instructpix2pix: Learning to follow image editing instructions\", \"Masactrl: Tuning-free mutual self-attention control for consistent image synthesis and editing\", \"Introducing chatgpt\", \"Attend-and-excite: Attention-based semantic guidance for text-to-image diffusion models\", \"Pixart-alpha: Fast training of diffusion transformer for photorealistic text-to-image synthesis\", \"Training-free layout control with cross-attention guidance\", \"Layoutllmt2i: Eliciting layout guidance from llm for text-to-image generation\", \"Score-based generative modeling through stochastic differential equations\", \"Learning to summarize with human feedback\", \"Dreamsync: Aligning text-to-image generation with image understanding feedback\", \"Compositional text-to-image synthesis with attention map control of diffusion models\", \"Self-correcting llm-controlled diffusion models\", \"Boxdiff: Text-to-image synthesis with training-free box-constrained diffusion\", \"Imagereward: Learning and evaluating human preferences for text-to-image generation\", \"Diffusion models: A comprehensive survey of methods and applications\", \"Cross-modal contextualized diffusion models for text-guided visual generation and editing\", \"Regioncontrolled text-to-image generation\", \"Enhanced visual instruction tuning for textrich image understanding\", \"Multimodal chain-of-thought reasoning in language models\", \"Generalized decoding for pixel, image, and language\"], \"Citation\":[\"StyleMaster: Towards Flexible Stylized Image Generation with Diffusion Models\"]}",
        "eval_response": "Evaluation evidence: The assistant's answer includes a long list of references that are not mentioned in the gold answer, indicating significant hallucinations. The citation part of the answer only matches one title from the gold answer, missing two other citations and adding extra, incorrect references. This shows issues with both accuracy and hallucinations. The answer is not complete as it fails to correctly identify all citations and incorrectly lists numerous references.\n\nRating: [[20]]",
        "score": 20
    },
    {
        "level": 3,
        "set": 3,
        "length": 181719,
        "type": "paper",
        "language": "en",
        "question": "TravelPlanner: A Benchmark for Real-World Planning with Language Agents",
        "instruction": "We hope you will carefully study the provided papers and determine the citation relationships between them. Please follow the instructions below strictly to complete the task:\n\n#Specific Requirements:\n1. Reference: When a given paper mentions other provided papers, those other papers are considered as \"references\" for the given paper. To summarize in this specific context, references are about what the given paper is using.\n2. Citation: Conversely, when other provided papers mention the given paper in their works, the given paper is being \"cited\" by those other papers. To summarize in this specific context, citations are about who is using the given paper.\n3. Given a paper, you need to determine the citation or reference relationship between this paper and the other papers. Do not consider papers that are not provided.\n3. Please present the paper titles in a json format as follows: {{\"Reference\":[\"Reference Title 1\", \"Reference Title 2\", ..., \"Reference Title n\"], \"Citation\":[\"Citation Title 1\", \"Citation Title 2\", ..., \"Citation Title n\"]}}.\n4. If a paper does not have any references or citations, please leave the corresponding list empty, e.g.{{\"Refernce\":[]}}, {{\"Citation\":[]}}.",
        "prompt_template": "#Papers:\n{docs}\n\n{instruction}\n\n#The paper you need to analyze:\n{question}",
        "doc": [
            "2402.01622.md",
            "2404.18231.md",
            "2405.02714.md",
            "2405.16510.md",
            "2405.18208.md",
            "2405.20625.md",
            "2310.06825.md",
            "2310.10634.md",
            "2310.13227.md",
            "2401.01614.md",
            "2401.04088.md"
        ],
        "answer": {
            "Reference": [
                "# Mistral 7B ",
                "# OpenAgents: AN OPEN PlATFORM FOR LANGUAGE AGENTS IN THE WILD ",
                "# ToolChaIN*: EFFiCIENT ACtion SpaCe NAVIGATION in Large Language ModelS With A* Search ",
                "# GPT-4V(ision) is a Generalist Web Agent, if Grounded ",
                "# Mixtral of Experts "
            ],
            "Citation": [
                "# Beyond Relevance: Evaluate and Improve Retrievers on Perspective Awareness ",
                "# Meta-Task Planning for Language Agents ",
                "# A Human-Like Reasoning Framework for Multi-Phases Planning Task with Large Language Models ",
                "# Robust Planning with LLM-Modulo Framework: Case Study in Travel Planning "
            ]
        },
        "shuffle_doc": true,
        "id": "6622b8f8-aec4-4daa-8fb5-4a7b224ac093",
        "prompt": "[Question]\n#Papers:\n{docs}\n\nWe hope you will carefully study the provided papers and determine the citation relationships between them. Please follow the instructions below strictly to complete the task:\n\n#Specific Requirements:\n1. Reference: When a given paper mentions other provided papers, those other papers are considered as \"references\" for the given paper. To summarize in this specific context, references are about what the given paper is using.\n2. Citation: Conversely, when other provided papers mention the given paper in their works, the given paper is being \"cited\" by those other papers. To summarize in this specific context, citations are about who is using the given paper.\n3. Given a paper, you need to determine the citation or reference relationship between this paper and the other papers. Do not consider papers that are not provided.\n3. Please present the paper titles in a json format as follows: {{\"Reference\":[\"Reference Title 1\", \"Reference Title 2\", ..., \"Reference Title n\"], \"Citation\":[\"Citation Title 1\", \"Citation Title 2\", ..., \"Citation Title n\"]}}.\n4. If a paper does not have any references or citations, please leave the corresponding list empty, e.g.{{\"Refernce\":[]}}, {{\"Citation\":[]}}.\n\n#The paper you need to analyze:\nTravelPlanner: A Benchmark for Real-World Planning with Language Agents\n\n[Gold Answer]\n{'Reference': ['# Mistral 7B ', '# OpenAgents: AN OPEN PlATFORM FOR LANGUAGE AGENTS IN THE WILD ', '# ToolChaIN*: EFFiCIENT ACtion SpaCe NAVIGATION in Large Language ModelS With A* Search ', '# GPT-4V(ision) is a Generalist Web Agent, if Grounded ', '# Mixtral of Experts '], 'Citation': ['# Beyond Relevance: Evaluate and Improve Retrievers on Perspective Awareness ', '# Meta-Task Planning for Language Agents ', '# A Human-Like Reasoning Framework for Multi-Phases Planning Task with Large Language Models ', '# Robust Planning with LLM-Modulo Framework: Case Study in Travel Planning ']}\n\n[The Start of Assistant's Predicted Answer]\n{\"Reference\":[\"Robot planning in the real world: Research challenges and opportunities\", \"Language models as agent models\", \"Autogpt\", \"Graph of thoughts: Solving elaborate problems with large language models\", \"Language models are few-shot learners\", \"Deep blue\", \"Walking down the memory maze: Beyond context limit through interactive reading\", \"Training verifiers to solve math word problems\", \"Dart: an example of accelerated evolutionary development\", \"Mind2web: Towards a generalist agent for the web\", \"Gemini: a family of highly capable multimodal models\", \"Openagi: When llm meets domain experts\", \"HTN planning: Overview, comparison, and beyond\", \"Planning and the brain\", \"A cognitive model of planning\", \"Planning with theory of mind\", \"Automated planning for robotics\", \"Solving quantitative reasoning problems with language models\", \"API-Bank: A comprehensive benchmark for tool-augmented llms\", \"Unleashing infinite-length input capacity for large-scale language models with self-controlled memory system\", \"LLM+P: Empowering large language models with optimal planning proficiency\", \"Lost in the Middle: How language models use long contexts\", \"AgentBench: Evaluating llms as agents\", \"Chameleon: Plug-and-play compositional reasoning with large language models\", \"Planning in the brain\", \"Artificial intelligence a modern approach\", \"Toolformer: Language models can teach themselves to use tools\", \"HuggingGPT: Solving AI tasks with chatGPT and its friends in hugging face\", \"Reflexion: Language agents with verbal reinforcement learning\", \"Mastering the game of go with deep neural networks and tree search\", \"The predictron: End-to-end learning and planning\", \"LLM-Planner: Few-shot grounded planning for embodied agents with large language models\", \"Language agents: a critical evolutionary step of artificial intelligence\", \"Cognitive architectures for language agents\", \"Llama: Open and efficient foundation language models\", \"Llama 2: Open foundation and finetuned chat models\", \"PlanBench: An extensible benchmark for evaluating large language models on planning and reasoning about change\", \"Voyager: An open-ended embodied agent with large language models\", \"Chain-of-thought prompting elicits reasoning in large language models\", \"LLM-powered autonomous agents\", \"On the tool manipulation capability of open-source large language models\", \"React: Synergizing reasoning and acting in language models\", \"Tree of thoughts: Deliberate problem solving with large language models\", \"Distilling script knowledge from large language models for constrained language planning\", \"GPT-4V(ision) is a generalist web agent, if grounded\", \"Memorybank: Enhancing large language models with long-term memory\", \"WebArena: A realistic web environment for building autonomous agents\", \"RecurrentGPT: Interactive generation of (arbitrarily) long text\", \"ToolQA: A dataset for LLM question answering with external tools\", \"Toolchain*: Efficient action space navigation in large language models with $A^{*}$ search\"], \"Citation\":[\"Robust Planning with LLM-Modulo Framework: Case Study in Travel Planning\", \"React: Synergizing reasoning and acting in language models\"]}\n[The End of Assistant's Predicted Answer]\n\n[System]\nWe would like to request your feedback on the performance of the AI assistant in response to the user question displayed above according to the gold answer. Please use the following listed aspects and their descriptions as evaluation criteria:\n    - Accuracy and Hallucinations: The assistant's answer is semantically consistent with the gold answer; The numerical value and order need to be accurate, and there should be no hallucinations.\n    - Completeness: Referring to the reference answers, the assistant's answer should contain all the key points needed to answer the user's question; further elaboration on these key points can be omitted.\nPlease rate whether this answer is suitable for the question. Please note that the gold answer can be considered as a correct answer to the question.\n\nThe assistant receives an overall score on a scale of 1 to 100, where a higher score indicates better overall performance.\nPlease note that if the assistant's answer and the gold answer fully meet the above criteria, its overall rating should be the full marks (100).\nPlease first provide a comprehensive explanation of your evaluation, avoiding any potential bias.\nThen, output a line indicating the score of the Assistant.\n\nPLEASE OUTPUT WITH THE FOLLOWING FORMAT, WHERE THE SCORE IS A SCALE OF 1 TO 100 BY STRICTLY FOLLOWING THIS FORMAT: \"[[score]]\", FOR EXAMPLE \"Rating: [[100]]\":\n<start output>\nEvaluation evidence: your evluation explanation here, no more than 100 words\nRating: [[score]]\n<end output>\n\nNow, start your evaluation:",
        "generate_response": "{\"Reference\":[\"Robot planning in the real world: Research challenges and opportunities\", \"Language models as agent models\", \"Autogpt\", \"Graph of thoughts: Solving elaborate problems with large language models\", \"Language models are few-shot learners\", \"Deep blue\", \"Walking down the memory maze: Beyond context limit through interactive reading\", \"Training verifiers to solve math word problems\", \"Dart: an example of accelerated evolutionary development\", \"Mind2web: Towards a generalist agent for the web\", \"Gemini: a family of highly capable multimodal models\", \"Openagi: When llm meets domain experts\", \"HTN planning: Overview, comparison, and beyond\", \"Planning and the brain\", \"A cognitive model of planning\", \"Planning with theory of mind\", \"Automated planning for robotics\", \"Solving quantitative reasoning problems with language models\", \"API-Bank: A comprehensive benchmark for tool-augmented llms\", \"Unleashing infinite-length input capacity for large-scale language models with self-controlled memory system\", \"LLM+P: Empowering large language models with optimal planning proficiency\", \"Lost in the Middle: How language models use long contexts\", \"AgentBench: Evaluating llms as agents\", \"Chameleon: Plug-and-play compositional reasoning with large language models\", \"Planning in the brain\", \"Artificial intelligence a modern approach\", \"Toolformer: Language models can teach themselves to use tools\", \"HuggingGPT: Solving AI tasks with chatGPT and its friends in hugging face\", \"Reflexion: Language agents with verbal reinforcement learning\", \"Mastering the game of go with deep neural networks and tree search\", \"The predictron: End-to-end learning and planning\", \"LLM-Planner: Few-shot grounded planning for embodied agents with large language models\", \"Language agents: a critical evolutionary step of artificial intelligence\", \"Cognitive architectures for language agents\", \"Llama: Open and efficient foundation language models\", \"Llama 2: Open foundation and finetuned chat models\", \"PlanBench: An extensible benchmark for evaluating large language models on planning and reasoning about change\", \"Voyager: An open-ended embodied agent with large language models\", \"Chain-of-thought prompting elicits reasoning in large language models\", \"LLM-powered autonomous agents\", \"On the tool manipulation capability of open-source large language models\", \"React: Synergizing reasoning and acting in language models\", \"Tree of thoughts: Deliberate problem solving with large language models\", \"Distilling script knowledge from large language models for constrained language planning\", \"GPT-4V(ision) is a generalist web agent, if grounded\", \"Memorybank: Enhancing large language models with long-term memory\", \"WebArena: A realistic web environment for building autonomous agents\", \"RecurrentGPT: Interactive generation of (arbitrarily) long text\", \"ToolQA: A dataset for LLM question answering with external tools\", \"Toolchain*: Efficient action space navigation in large language models with $A^{*}$ search\"], \"Citation\":[\"Robust Planning with LLM-Modulo Framework: Case Study in Travel Planning\", \"React: Synergizing reasoning and acting in language models\"]}",
        "eval_response": "Evaluation evidence: The assistant's answer does not match the gold answer in terms of accuracy and completeness. The references and citations listed in the assistant's answer are significantly different from those in the gold answer, indicating a high level of hallucination and a lack of completeness. The assistant failed to accurately identify the correct references and citations for the given paper, which is crucial for the task.\n\nRating: [[10]]",
        "score": 10
    },
    {
        "level": 3,
        "set": 3,
        "length": 103533,
        "type": "paper",
        "language": "en",
        "question": "Risk Taxonomy, Mitigation, and Assessment Benchmarks of Large Language Model Systems",
        "instruction": "We hope you will carefully study the provided papers and determine the citation relationships between them. Please follow the instructions below strictly to complete the task:\n\n#Specific Requirements:\n1. Reference: When a given paper mentions other provided papers, those other papers are considered as \"references\" for the given paper. To summarize in this specific context, references are about what the given paper is using.\n2. Citation: Conversely, when other provided papers mention the given paper in their works, the given paper is being \"cited\" by those other papers. To summarize in this specific context, citations are about who is using the given paper.\n3. Given a paper, you need to determine the citation or reference relationship between this paper and the other papers. Do not consider papers that are not provided.\n3. Please present the paper titles in a json format as follows: {{\"Reference\":[\"Reference Title 1\", \"Reference Title 2\", ..., \"Reference Title n\"], \"Citation\":[\"Citation Title 1\", \"Citation Title 2\", ..., \"Citation Title n\"]}}.\n4. If a paper does not have any references or citations, please leave the corresponding list empty, e.g.{{\"Refernce\":[]}}, {{\"Citation\":[]}}.",
        "prompt_template": "#Papers:\n{docs}\n\n{instruction}\n\n#The paper you need to analyze:\n{question}",
        "doc": [
            "2401.05778.md",
            "2404.05264.md",
            "2311.11855.md",
            "2312.07930.md"
        ],
        "answer": {
            "Reference": [
                "# Evil Geniuses: Delving into the Safety of LLM-based Agents ",
                "# Towards Optimal Statistical Watermarking "
            ],
            "Citation": [
                "# Unbridled Icarus: A Survey of the Potential Perils of Image Inputs in Multimodal Large Language Model Security "
            ]
        },
        "shuffle_doc": true,
        "id": "c9490921-8058-4743-9819-2c2ce23d19b3",
        "prompt": "[Question]\n#Papers:\n{docs}\n\nWe hope you will carefully study the provided papers and determine the citation relationships between them. Please follow the instructions below strictly to complete the task:\n\n#Specific Requirements:\n1. Reference: When a given paper mentions other provided papers, those other papers are considered as \"references\" for the given paper. To summarize in this specific context, references are about what the given paper is using.\n2. Citation: Conversely, when other provided papers mention the given paper in their works, the given paper is being \"cited\" by those other papers. To summarize in this specific context, citations are about who is using the given paper.\n3. Given a paper, you need to determine the citation or reference relationship between this paper and the other papers. Do not consider papers that are not provided.\n3. Please present the paper titles in a json format as follows: {{\"Reference\":[\"Reference Title 1\", \"Reference Title 2\", ..., \"Reference Title n\"], \"Citation\":[\"Citation Title 1\", \"Citation Title 2\", ..., \"Citation Title n\"]}}.\n4. If a paper does not have any references or citations, please leave the corresponding list empty, e.g.{{\"Refernce\":[]}}, {{\"Citation\":[]}}.\n\n#The paper you need to analyze:\nRisk Taxonomy, Mitigation, and Assessment Benchmarks of Large Language Model Systems\n\n[Gold Answer]\n{'Reference': ['# Evil Geniuses: Delving into the Safety of LLM-based Agents ', '# Towards Optimal Statistical Watermarking '], 'Citation': ['# Unbridled Icarus: A Survey of the Potential Perils of Image Inputs in Multimodal Large Language Model Security ']}\n\n[The Start of Assistant's Predicted Answer]\n{\"Reference\": [\"Language models are few-shot learners\", \"GPT-4 technical report\", \"Llama: Open and efficient foundation language models\", \"Llama 2: Open foundation and fine-tuned chat models\", \"GLM-130B: an open bilingual pretrained model\", \"Codet5+: Open code large language models for code understanding and generation\", \"In-context instruction learning\", \"Chain-of-thought prompting elicits reasoning in large language models\", \"Tree of thoughts: Deliberate problem solving with large language models\", \"Graph of thoughts: Solving elaborate problems with large language models\", \"Training language models to follow instructions with human feedback\", \"Introducing chatgpt\", \"March 20 chatgpt outage: Here's what happened\", \"do anything now: Characterizing and evaluating in-the-wild jailbreak prompts on large language models\", \"A survey on chatgpt: Ai-generated contents, challenges, and solutions\", \"Decodingtrust: A comprehensive assessment of trustworthiness in GPT models\", \"Trustworthy llms: a survey and guideline for evaluating large language models' alignment\", \"From chatgpt to threatgpt: Impact of generative AI in cybersecurity and privacy\", \"A survey of safety and trustworthiness of large language models through the lens of verification and validation\"], \"Citation\": []}\n[The End of Assistant's Predicted Answer]\n\n[System]\nWe would like to request your feedback on the performance of the AI assistant in response to the user question displayed above according to the gold answer. Please use the following listed aspects and their descriptions as evaluation criteria:\n    - Accuracy and Hallucinations: The assistant's answer is semantically consistent with the gold answer; The numerical value and order need to be accurate, and there should be no hallucinations.\n    - Completeness: Referring to the reference answers, the assistant's answer should contain all the key points needed to answer the user's question; further elaboration on these key points can be omitted.\nPlease rate whether this answer is suitable for the question. Please note that the gold answer can be considered as a correct answer to the question.\n\nThe assistant receives an overall score on a scale of 1 to 100, where a higher score indicates better overall performance.\nPlease note that if the assistant's answer and the gold answer fully meet the above criteria, its overall rating should be the full marks (100).\nPlease first provide a comprehensive explanation of your evaluation, avoiding any potential bias.\nThen, output a line indicating the score of the Assistant.\n\nPLEASE OUTPUT WITH THE FOLLOWING FORMAT, WHERE THE SCORE IS A SCALE OF 1 TO 100 BY STRICTLY FOLLOWING THIS FORMAT: \"[[score]]\", FOR EXAMPLE \"Rating: [[100]]\":\n<start output>\nEvaluation evidence: your evluation explanation here, no more than 100 words\nRating: [[score]]\n<end output>\n\nNow, start your evaluation:",
        "generate_response": "{\"Reference\": [\"Language models are few-shot learners\", \"GPT-4 technical report\", \"Llama: Open and efficient foundation language models\", \"Llama 2: Open foundation and fine-tuned chat models\", \"GLM-130B: an open bilingual pretrained model\", \"Codet5+: Open code large language models for code understanding and generation\", \"In-context instruction learning\", \"Chain-of-thought prompting elicits reasoning in large language models\", \"Tree of thoughts: Deliberate problem solving with large language models\", \"Graph of thoughts: Solving elaborate problems with large language models\", \"Training language models to follow instructions with human feedback\", \"Introducing chatgpt\", \"March 20 chatgpt outage: Here's what happened\", \"do anything now: Characterizing and evaluating in-the-wild jailbreak prompts on large language models\", \"A survey on chatgpt: Ai-generated contents, challenges, and solutions\", \"Decodingtrust: A comprehensive assessment of trustworthiness in GPT models\", \"Trustworthy llms: a survey and guideline for evaluating large language models' alignment\", \"From chatgpt to threatgpt: Impact of generative AI in cybersecurity and privacy\", \"A survey of safety and trustworthiness of large language models through the lens of verification and validation\"], \"Citation\": []}",
        "eval_response": "Evaluation evidence: The assistant's answer does not match the gold answer in terms of accuracy and completeness. The references listed in the assistant's answer do not align with those in the gold answer, and it incorrectly omits the citation provided in the gold answer. The assistant's response includes multiple titles that are not mentioned in the gold answer, indicating hallucinations. Therefore, the assistant's performance is poor in both accuracy and completeness.\n\nRating: [[10]]",
        "score": 10
    },
    {
        "level": 4,
        "set": 3,
        "length": 127798,
        "type": "paper",
        "language": "en",
        "question": "",
        "instruction": "We kindly ask you to thoroughly review the provided papers and construct a citation chain from them. Please adhere to the following instructions strictly while completing the task:\n\n#Task Instructions:\nGiven several papers, you are required to identify and list the longest citation chain, which demonstrates the citation relationship among the provided papers.\n\n#Specific Requirements:\n1.Please present the titles of the papers in the form of a list, as follows: [\"Title of Paper 1\", \"Title of Paper 2\", ..., \"Title of Paper n\"].\n2.Ensure that the citation chain in the list is linear and continuous, meaning that the first paper title in the list (Paper 1) should not cite any other works. Instead, it should be cited by the next paper in the list (Paper 2); subsequently, each paper should then be cited by the next one in the list, continuing up to the last paper (Paper n).\n3.Consider only the citation relationships within the supplied collection of papers, and ensure that the citation chain accurately reflects the sequential citation order among these documents.\n4.Do not take into account any articles not provided, and disregard other non-linear citation relationships.",
        "prompt_template": "{instruction}\n\n#Papers Provided:\n{docs}",
        "doc": [
            "2211.05244.md",
            "2304.04661.md",
            "2306.11417.md",
            "2405.09330.md"
        ],
        "answer": [
            "# Deep Learning for Time Series Anomaly Detection: A Survey ",
            "# AI for IT Operations (AIOps) on Cloud Platforms: Reviews, Opportunities and Challenges ",
            "# PyRCA: A Library for Metric-based Root Cause Analysis ",
            "# BARO: Robust Root Cause Analysis for Microservices via Multivariate Bayesian Online Change Point Detection "
        ],
        "shuffle_doc": true,
        "id": "5841087c-d2ab-4bdf-97bf-d276c40bde5c",
        "prompt": "[Question]\nWe kindly ask you to thoroughly review the provided papers and construct a citation chain from them. Please adhere to the following instructions strictly while completing the task:\n\n#Task Instructions:\nGiven several papers, you are required to identify and list the longest citation chain, which demonstrates the citation relationship among the provided papers.\n\n#Specific Requirements:\n1.Please present the titles of the papers in the form of a list, as follows: [\"Title of Paper 1\", \"Title of Paper 2\", ..., \"Title of Paper n\"].\n2.Ensure that the citation chain in the list is linear and continuous, meaning that the first paper title in the list (Paper 1) should not cite any other works. Instead, it should be cited by the next paper in the list (Paper 2); subsequently, each paper should then be cited by the next one in the list, continuing up to the last paper (Paper n).\n3.Consider only the citation relationships within the supplied collection of papers, and ensure that the citation chain accurately reflects the sequential citation order among these documents.\n4.Do not take into account any articles not provided, and disregard other non-linear citation relationships.\n\n#Papers Provided:\n{docs}\n\n[Gold Answer]\n['# Deep Learning for Time Series Anomaly Detection: A Survey ', '# AI for IT Operations (AIOps) on Cloud Platforms: Reviews, Opportunities and Challenges ', '# PyRCA: A Library for Metric-based Root Cause Analysis ', '# BARO: Robust Root Cause Analysis for Microservices via Multivariate Bayesian Online Change Point Detection ']\n\n[The Start of Assistant's Predicted Answer]\n[]\n[The End of Assistant's Predicted Answer]\n\n[System]\nWe would like to request your feedback on the performance of the AI assistant in response to the user question displayed above according to the gold answer. Please use the following listed aspects and their descriptions as evaluation criteria:\n    - Accuracy and Hallucinations: The assistant's answer is semantically consistent with the gold answer; The numerical value and order need to be accurate, and there should be no hallucinations.\n    - Completeness: Referring to the reference answers, the assistant's answer should contain all the key points needed to answer the user's question; further elaboration on these key points can be omitted.\nPlease rate whether this answer is suitable for the question. Please note that the gold answer can be considered as a correct answer to the question.\n\nThe assistant receives an overall score on a scale of 1 to 100, where a higher score indicates better overall performance.\nPlease note that if the assistant's answer and the gold answer fully meet the above criteria, its overall rating should be the full marks (100).\nPlease first provide a comprehensive explanation of your evaluation, avoiding any potential bias.\nThen, output a line indicating the score of the Assistant.\n\nPLEASE OUTPUT WITH THE FOLLOWING FORMAT, WHERE THE SCORE IS A SCALE OF 1 TO 100 BY STRICTLY FOLLOWING THIS FORMAT: \"[[score]]\", FOR EXAMPLE \"Rating: [[100]]\":\n<start output>\nEvaluation evidence: your evluation explanation here, no more than 100 words\nRating: [[score]]\n<end output>\n\nNow, start your evaluation:",
        "generate_response": "[]",
        "eval_response": "Evaluation evidence: The assistant's response is empty, which indicates a failure to provide any answer to the task of listing the longest citation chain among the provided papers. This results in a complete lack of accuracy and completeness as it does not address the user's question at all. There are no hallucinations since there is no content in the response. However, the absence of any attempt to answer the question leads to the lowest possible score.\n\nRating: [[1]]",
        "score": 1
    },
    {
        "level": 3,
        "set": 1,
        "length": 46142,
        "type": "paper",
        "language": "en",
        "question": "GPT-4V(ision) is a Generalist Web Agent, if Grounded",
        "instruction": "We hope you will carefully study the provided papers and determine the citation relationships between them. Please follow the instructions below strictly to complete the task:\n\n#Specific Requirements:\n1. Reference: When a given paper mentions other provided papers, those other papers are considered as \"references\" for the given paper. To summarize in this specific context, references are about what the given paper is using.\n2. Citation: Conversely, when other provided papers mention the given paper in their works, the given paper is being \"cited\" by those other papers. To summarize in this specific context, citations are about who is using the given paper.\n3. Given a paper, you need to determine the citation or reference relationship between this paper and the other papers. Do not consider papers that are not provided.\n3. Please present the paper titles in a json format as follows: {{\"Reference\":[\"Reference Title 1\", \"Reference Title 2\", ..., \"Reference Title n\"], \"Citation\":[\"Citation Title 1\", \"Citation Title 2\", ..., \"Citation Title n\"]}}.\n4. If a paper does not have any references or citations, please leave the corresponding list empty, e.g.{{\"Refernce\":[]}}, {{\"Citation\":[]}}.",
        "prompt_template": "#Papers:\n{docs}\n\n{instruction}\n\n#The paper you need to analyze:\n{question}",
        "doc": [
            "2401.01614.md",
            "2405.04497.md",
            "2401.10935.md"
        ],
        "answer": {
            "Reference": [
                "# SeeClick: Harnessing GUI Grounding for Advanced Visual GUI Agents "
            ],
            "Citation": [
                "# Unveiling Disparities in Web Task Handling Between Human and Web Agent "
            ]
        },
        "shuffle_doc": true,
        "id": "a5075845-053d-4570-a282-cc74baf7e284",
        "prompt": "[Question]\n#Papers:\n{docs}\n\nWe hope you will carefully study the provided papers and determine the citation relationships between them. Please follow the instructions below strictly to complete the task:\n\n#Specific Requirements:\n1. Reference: When a given paper mentions other provided papers, those other papers are considered as \"references\" for the given paper. To summarize in this specific context, references are about what the given paper is using.\n2. Citation: Conversely, when other provided papers mention the given paper in their works, the given paper is being \"cited\" by those other papers. To summarize in this specific context, citations are about who is using the given paper.\n3. Given a paper, you need to determine the citation or reference relationship between this paper and the other papers. Do not consider papers that are not provided.\n3. Please present the paper titles in a json format as follows: {{\"Reference\":[\"Reference Title 1\", \"Reference Title 2\", ..., \"Reference Title n\"], \"Citation\":[\"Citation Title 1\", \"Citation Title 2\", ..., \"Citation Title n\"]}}.\n4. If a paper does not have any references or citations, please leave the corresponding list empty, e.g.{{\"Refernce\":[]}}, {{\"Citation\":[]}}.\n\n#The paper you need to analyze:\nGPT-4V(ision) is a Generalist Web Agent, if Grounded\n\n[Gold Answer]\n{'Reference': ['# SeeClick: Harnessing GUI Grounding for Advanced Visual GUI Agents '], 'Citation': ['# Unveiling Disparities in Web Task Handling Between Human and Web Agent ']}\n\n[The Start of Assistant's Predicted Answer]\n{\"Reference\":[\"A Survey on Evaluation of Large Language Models\", \"Vicuna: An Open-Source Chatbot Impressing GPT-4 with $90 \\% *$ ChatGPT Quality\", \"Compositional Semantic Parsing with Large Language Models\", \"Open LLM Leaderboard\", \"How Close is ChatGPT to Human Experts? Comparison Corpus, Evaluation, and Detection\", \"Mind2web: Towards a generalist agent for the web\", \"Multimodal web navigation with instruction-finetuned foundation models\", \"Making the $\\mathrm{v}$ in vqa matter: Elevating the role of image understanding in visual question answering\", \"Don't generate, discriminate: A proposal for grounding language models to real-world environments\", \"Hallusionbench: An advanced diagnostic suite for entangled language hallucination\\&visual illusion in large vision-language models\", \"Understanding html with large language models\", \"Pix2struct: Screenshot parsing as pretraining for visual language understanding\", \"Blip2: Bootstrapping language-image pre-training with frozen image encoders and large language models\", \"Improved baselines with visual instruction tuning\", \"Visual instruction tuning\", \"Visual instruction tuning\", \"Agentbench: Evaluating llms as agents\", \"Learn to explain: Multimodal reasoning via thought chains for science question answering\", \"Flin: A flexible natural language interface for web navigation\", \"Gpt-4 technical report\", \"Kosmos-2: Grounding multimodal large language models to the world\", \"Flickr30k entities: Collecting region-to-phrase correspondences for richer image-to-sentence models\", \"Learning transferable visual models from natural language supervision\", \"Scienceqa: a novel resource for question answering on scholarly articles\", \"From pixels to ui actions: Learning to follow instructions via graphical user interfaces\", \"How much can clip benefit vision-and-language tasks?\", \"World of bits: An open-domain platform for web-based agents\", \"What does clip know about a red circle? visual prompt engineering for vlms\", \"Hierarchical prompting assists large language model on web navigation\", \"Gpt-4v in wonderland: Large multimodal models for zero-shot smartphone gui navigation\", \"Set-of-mark prompting unleashes extraordinary visual grounding in gpt-4v\", \"Fine-grained visual prompting\", \"The dawn of lmms: Preliminary explorations with gpt-4v(ision)\", \"Webshop: Towards scalable real-world web interaction with grounded language agents\", \"Ferret: Refer and ground anything anywhere at any granularity\", \"Mmmu: A massive multi-discipline multimodal understanding and reasoning benchmark for expert agi\", \"Gpt-4v(ision) as a generalist evaluator for vision-language tasks\", \"Bubogpt: Enabling visual grounding in multi-modal llms\", \"Agieval: A human-centric benchmark for evaluating foundation models\", \"Scene parsing through ade20k dataset\"], \"Citation\":[]}\n[The End of Assistant's Predicted Answer]\n\n[System]\nWe would like to request your feedback on the performance of the AI assistant in response to the user question displayed above according to the gold answer. Please use the following listed aspects and their descriptions as evaluation criteria:\n    - Accuracy and Hallucinations: The assistant's answer is semantically consistent with the gold answer; The numerical value and order need to be accurate, and there should be no hallucinations.\n    - Completeness: Referring to the reference answers, the assistant's answer should contain all the key points needed to answer the user's question; further elaboration on these key points can be omitted.\nPlease rate whether this answer is suitable for the question. Please note that the gold answer can be considered as a correct answer to the question.\n\nThe assistant receives an overall score on a scale of 1 to 100, where a higher score indicates better overall performance.\nPlease note that if the assistant's answer and the gold answer fully meet the above criteria, its overall rating should be the full marks (100).\nPlease first provide a comprehensive explanation of your evaluation, avoiding any potential bias.\nThen, output a line indicating the score of the Assistant.\n\nPLEASE OUTPUT WITH THE FOLLOWING FORMAT, WHERE THE SCORE IS A SCALE OF 1 TO 100 BY STRICTLY FOLLOWING THIS FORMAT: \"[[score]]\", FOR EXAMPLE \"Rating: [[100]]\":\n<start output>\nEvaluation evidence: your evluation explanation here, no more than 100 words\nRating: [[score]]\n<end output>\n\nNow, start your evaluation:",
        "generate_response": "{\"Reference\":[\"A Survey on Evaluation of Large Language Models\", \"Vicuna: An Open-Source Chatbot Impressing GPT-4 with $90 \\% *$ ChatGPT Quality\", \"Compositional Semantic Parsing with Large Language Models\", \"Open LLM Leaderboard\", \"How Close is ChatGPT to Human Experts? Comparison Corpus, Evaluation, and Detection\", \"Mind2web: Towards a generalist agent for the web\", \"Multimodal web navigation with instruction-finetuned foundation models\", \"Making the $\\mathrm{v}$ in vqa matter: Elevating the role of image understanding in visual question answering\", \"Don't generate, discriminate: A proposal for grounding language models to real-world environments\", \"Hallusionbench: An advanced diagnostic suite for entangled language hallucination\\&visual illusion in large vision-language models\", \"Understanding html with large language models\", \"Pix2struct: Screenshot parsing as pretraining for visual language understanding\", \"Blip2: Bootstrapping language-image pre-training with frozen image encoders and large language models\", \"Improved baselines with visual instruction tuning\", \"Visual instruction tuning\", \"Visual instruction tuning\", \"Agentbench: Evaluating llms as agents\", \"Learn to explain: Multimodal reasoning via thought chains for science question answering\", \"Flin: A flexible natural language interface for web navigation\", \"Gpt-4 technical report\", \"Kosmos-2: Grounding multimodal large language models to the world\", \"Flickr30k entities: Collecting region-to-phrase correspondences for richer image-to-sentence models\", \"Learning transferable visual models from natural language supervision\", \"Scienceqa: a novel resource for question answering on scholarly articles\", \"From pixels to ui actions: Learning to follow instructions via graphical user interfaces\", \"How much can clip benefit vision-and-language tasks?\", \"World of bits: An open-domain platform for web-based agents\", \"What does clip know about a red circle? visual prompt engineering for vlms\", \"Hierarchical prompting assists large language model on web navigation\", \"Gpt-4v in wonderland: Large multimodal models for zero-shot smartphone gui navigation\", \"Set-of-mark prompting unleashes extraordinary visual grounding in gpt-4v\", \"Fine-grained visual prompting\", \"The dawn of lmms: Preliminary explorations with gpt-4v(ision)\", \"Webshop: Towards scalable real-world web interaction with grounded language agents\", \"Ferret: Refer and ground anything anywhere at any granularity\", \"Mmmu: A massive multi-discipline multimodal understanding and reasoning benchmark for expert agi\", \"Gpt-4v(ision) as a generalist evaluator for vision-language tasks\", \"Bubogpt: Enabling visual grounding in multi-modal llms\", \"Agieval: A human-centric benchmark for evaluating foundation models\", \"Scene parsing through ade20k dataset\"], \"Citation\":[]}",
        "eval_response": "Evaluation evidence: The assistant's answer does not match the gold answer in terms of accuracy. The references listed in the assistant's response do not include the one mentioned in the gold answer, and it incorrectly lists numerous other papers not cited by the given paper. Additionally, the assistant's response incorrectly states there are no citations, whereas the gold answer lists one. This indicates issues with both accuracy and completeness, as the assistant failed to identify the correct references and citations.\n\nRating: [[20]]",
        "score": 20
    },
    {
        "level": 3,
        "set": 2,
        "length": 62207,
        "type": "paper",
        "language": "en",
        "question": "Seven Failure Points When Engineering a Retrieval Augmented Generation System",
        "instruction": "We hope you will carefully study the provided papers and determine the citation relationships between them. Please follow the instructions below strictly to complete the task:\n\n#Specific Requirements:\n1. Reference: When a given paper mentions other provided papers, those other papers are considered as \"references\" for the given paper. To summarize in this specific context, references are about what the given paper is using.\n2. Citation: Conversely, when other provided papers mention the given paper in their works, the given paper is being \"cited\" by those other papers. To summarize in this specific context, citations are about who is using the given paper.\n3. Given a paper, you need to determine the citation or reference relationship between this paper and the other papers. Do not consider papers that are not provided.\n3. Please present the paper titles in a json format as follows: {{\"Reference\":[\"Reference Title 1\", \"Reference Title 2\", ..., \"Reference Title n\"], \"Citation\":[\"Citation Title 1\", \"Citation Title 2\", ..., \"Citation Title n\"]}}.\n4. If a paper does not have any references or citations, please leave the corresponding list empty, e.g.{{\"Refernce\":[]}}, {{\"Citation\":[]}}.",
        "prompt_template": "#Papers:\n{docs}\n\n{instruction}\n\n#The paper you need to analyze:\n{question}",
        "doc": [
            "2401.05856.md",
            "2404.19705.md",
            "2405.07437.md",
            "2307.03172.md",
            "2309.01431.md"
        ],
        "answer": {
            "Reference": [
                "# Lost in the Middle: How Language Models Use Long Contexts ",
                "# Benchmarking Large Language Models in Retrieval-Augmented Generation "
            ],
            "Citation": [
                "# When to Retrieve: Teaching LLMs to Utilize Information Retrieval Effectively ",
                "# Evaluation of Retrieval-Augmented Generation: A Survey "
            ]
        },
        "shuffle_doc": true,
        "id": "81dcc135-168d-4dbd-906c-415ac9e22806",
        "prompt": "[Question]\n#Papers:\n{docs}\n\nWe hope you will carefully study the provided papers and determine the citation relationships between them. Please follow the instructions below strictly to complete the task:\n\n#Specific Requirements:\n1. Reference: When a given paper mentions other provided papers, those other papers are considered as \"references\" for the given paper. To summarize in this specific context, references are about what the given paper is using.\n2. Citation: Conversely, when other provided papers mention the given paper in their works, the given paper is being \"cited\" by those other papers. To summarize in this specific context, citations are about who is using the given paper.\n3. Given a paper, you need to determine the citation or reference relationship between this paper and the other papers. Do not consider papers that are not provided.\n3. Please present the paper titles in a json format as follows: {{\"Reference\":[\"Reference Title 1\", \"Reference Title 2\", ..., \"Reference Title n\"], \"Citation\":[\"Citation Title 1\", \"Citation Title 2\", ..., \"Citation Title n\"]}}.\n4. If a paper does not have any references or citations, please leave the corresponding list empty, e.g.{{\"Refernce\":[]}}, {{\"Citation\":[]}}.\n\n#The paper you need to analyze:\nSeven Failure Points When Engineering a Retrieval Augmented Generation System\n\n[Gold Answer]\n{'Reference': ['# Lost in the Middle: How Language Models Use Long Contexts ', '# Benchmarking Large Language Models in Retrieval-Augmented Generation '], 'Citation': ['# When to Retrieve: Teaching LLMs to Utilize Information Retrieval Effectively ', '# Evaluation of Retrieval-Augmented Generation: A Survey ']}\n\n[The Start of Assistant's Predicted Answer]\n{\"Reference\":[\"GPTCache: An Open-Source Semantic Cache for LLM Applications Enabling Faster Answers and Cost Savings\", \"Self-adaptive Machine Learning Systems: Research Challenges and Opportunities\", \"Benchmarking Large Language Models in Retrieval-Augmented Generation\", \"Efficient Open Domain Multi-Hop Question Answering with Few-Shot Data Synthesis\", \"Threshy: Supporting safe usage of intelligent web services\", \"Beware the evolving 'intelligent'web service! An integration architecture tactic to guard AI-first components\", \"Retrieval augmented language model pre-training\", \"Fidlight: Efficient and effective retrieval-augmented text generation\", \"Leveraging passage retrieval with generative models for open domain question answering\", \"BioASQ-QA: A manually curated corpus for biomedical question answering\", \"LoRA Fine-tuning Efficiently Undoes Safety Training in Llama 2-Chat 70B\", \"Retrieval-augmented generation for knowledge-intensive nlp tasks\", \"Lost in the middle: How language models use long contexts\", \"G-eval: Nlg evaluation using gpt-4 with better human alignment, may 2023\", \"Retrieval-based prompt selection for code-related few-shot learning\", \"GPT-4 Technical Report\", \"Robust speech recognition via large-scale weak supervision\", \"Improving the domain adaptation of retrieval augmented generation (RAG) models for open domain question answering\", \"Large language models for information retrieval: A survey\"], \"Citation\":[]}\n[The End of Assistant's Predicted Answer]\n\n[System]\nWe would like to request your feedback on the performance of the AI assistant in response to the user question displayed above according to the gold answer. Please use the following listed aspects and their descriptions as evaluation criteria:\n    - Accuracy and Hallucinations: The assistant's answer is semantically consistent with the gold answer; The numerical value and order need to be accurate, and there should be no hallucinations.\n    - Completeness: Referring to the reference answers, the assistant's answer should contain all the key points needed to answer the user's question; further elaboration on these key points can be omitted.\nPlease rate whether this answer is suitable for the question. Please note that the gold answer can be considered as a correct answer to the question.\n\nThe assistant receives an overall score on a scale of 1 to 100, where a higher score indicates better overall performance.\nPlease note that if the assistant's answer and the gold answer fully meet the above criteria, its overall rating should be the full marks (100).\nPlease first provide a comprehensive explanation of your evaluation, avoiding any potential bias.\nThen, output a line indicating the score of the Assistant.\n\nPLEASE OUTPUT WITH THE FOLLOWING FORMAT, WHERE THE SCORE IS A SCALE OF 1 TO 100 BY STRICTLY FOLLOWING THIS FORMAT: \"[[score]]\", FOR EXAMPLE \"Rating: [[100]]\":\n<start output>\nEvaluation evidence: your evluation explanation here, no more than 100 words\nRating: [[score]]\n<end output>\n\nNow, start your evaluation:",
        "generate_response": "{\"Reference\":[\"GPTCache: An Open-Source Semantic Cache for LLM Applications Enabling Faster Answers and Cost Savings\", \"Self-adaptive Machine Learning Systems: Research Challenges and Opportunities\", \"Benchmarking Large Language Models in Retrieval-Augmented Generation\", \"Efficient Open Domain Multi-Hop Question Answering with Few-Shot Data Synthesis\", \"Threshy: Supporting safe usage of intelligent web services\", \"Beware the evolving 'intelligent'web service! An integration architecture tactic to guard AI-first components\", \"Retrieval augmented language model pre-training\", \"Fidlight: Efficient and effective retrieval-augmented text generation\", \"Leveraging passage retrieval with generative models for open domain question answering\", \"BioASQ-QA: A manually curated corpus for biomedical question answering\", \"LoRA Fine-tuning Efficiently Undoes Safety Training in Llama 2-Chat 70B\", \"Retrieval-augmented generation for knowledge-intensive nlp tasks\", \"Lost in the middle: How language models use long contexts\", \"G-eval: Nlg evaluation using gpt-4 with better human alignment, may 2023\", \"Retrieval-based prompt selection for code-related few-shot learning\", \"GPT-4 Technical Report\", \"Robust speech recognition via large-scale weak supervision\", \"Improving the domain adaptation of retrieval augmented generation (RAG) models for open domain question answering\", \"Large language models for information retrieval: A survey\"], \"Citation\":[]}",
        "eval_response": "Evaluation evidence: The assistant's answer inaccurately lists references that are not mentioned in the gold answer, indicating hallucinations. It also fails to list any citations, whereas the gold answer specifies two papers that cite the analyzed paper. This shows issues with both accuracy and completeness. The assistant's answer does not align well with the gold answer, missing key points and adding incorrect information.\n\nRating: [[20]]",
        "score": 20
    },
    {
        "level": 4,
        "set": 2,
        "length": 72270,
        "type": "paper",
        "language": "en",
        "question": "",
        "instruction": "We kindly ask you to thoroughly review the provided papers and construct a citation chain from them. Please adhere to the following instructions strictly while completing the task:\n\n#Task Instructions:\nGiven several papers, you are required to identify and list the longest citation chain, which demonstrates the citation relationship among the provided papers.\n\n#Specific Requirements:\n1.Please present the titles of the papers in the form of a list, as follows: [\"Title of Paper 1\", \"Title of Paper 2\", ..., \"Title of Paper n\"].\n2.Ensure that the citation chain in the list is linear and continuous, meaning that the first paper title in the list (Paper 1) should not cite any other works. Instead, it should be cited by the next paper in the list (Paper 2); subsequently, each paper should then be cited by the next one in the list, continuing up to the last paper (Paper n).\n3.Consider only the citation relationships within the supplied collection of papers, and ensure that the citation chain accurately reflects the sequential citation order among these documents.\n4.Do not take into account any articles not provided, and disregard other non-linear citation relationships.",
        "prompt_template": "{instruction}\n\n#Papers Provided:\n{docs}",
        "doc": [
            "1612.04662.md",
            "2001.09186.md",
            "2107.01384.md",
            "2308.10960.md",
            "2405.03456.md"
        ],
        "answer": [
            "# Lightweight compression with encryption based on Asymmetric Numeral Systems ",
            "# A tutorial on the range variant of asymmetric numeral systems ",
            "# ATC: an Advanced Tucker Compression library for multidimensional data ",
            "# Hierarchical Lowrank Arithmetic with Binary Compression ",
            "# Performance of $\\mathcal{H}$-Matrix-Vector Multiplication with Floating Point Compression "
        ],
        "shuffle_doc": true,
        "id": "fbcd00e4-96c9-48b0-bd14-6284173de021",
        "prompt": "[Question]\nWe kindly ask you to thoroughly review the provided papers and construct a citation chain from them. Please adhere to the following instructions strictly while completing the task:\n\n#Task Instructions:\nGiven several papers, you are required to identify and list the longest citation chain, which demonstrates the citation relationship among the provided papers.\n\n#Specific Requirements:\n1.Please present the titles of the papers in the form of a list, as follows: [\"Title of Paper 1\", \"Title of Paper 2\", ..., \"Title of Paper n\"].\n2.Ensure that the citation chain in the list is linear and continuous, meaning that the first paper title in the list (Paper 1) should not cite any other works. Instead, it should be cited by the next paper in the list (Paper 2); subsequently, each paper should then be cited by the next one in the list, continuing up to the last paper (Paper n).\n3.Consider only the citation relationships within the supplied collection of papers, and ensure that the citation chain accurately reflects the sequential citation order among these documents.\n4.Do not take into account any articles not provided, and disregard other non-linear citation relationships.\n\n#Papers Provided:\n{docs}\n\n[Gold Answer]\n['# Lightweight compression with encryption based on Asymmetric Numeral Systems ', '# A tutorial on the range variant of asymmetric numeral systems ', '# ATC: an Advanced Tucker Compression library for multidimensional data ', '# Hierarchical Lowrank Arithmetic with Binary Compression ', '# Performance of $\\\\mathcal{H}$-Matrix-Vector Multiplication with Floating Point Compression ']\n\n[The Start of Assistant's Predicted Answer]\n[\"The Use of Asymmetric Numeral Systems as an Accurate Replacement for Huffman Coding\", \"Lightweight compression with encryption based on Asymmetric Numeral Systems\"]\n[The End of Assistant's Predicted Answer]\n\n[System]\nWe would like to request your feedback on the performance of the AI assistant in response to the user question displayed above according to the gold answer. Please use the following listed aspects and their descriptions as evaluation criteria:\n    - Accuracy and Hallucinations: The assistant's answer is semantically consistent with the gold answer; The numerical value and order need to be accurate, and there should be no hallucinations.\n    - Completeness: Referring to the reference answers, the assistant's answer should contain all the key points needed to answer the user's question; further elaboration on these key points can be omitted.\nPlease rate whether this answer is suitable for the question. Please note that the gold answer can be considered as a correct answer to the question.\n\nThe assistant receives an overall score on a scale of 1 to 100, where a higher score indicates better overall performance.\nPlease note that if the assistant's answer and the gold answer fully meet the above criteria, its overall rating should be the full marks (100).\nPlease first provide a comprehensive explanation of your evaluation, avoiding any potential bias.\nThen, output a line indicating the score of the Assistant.\n\nPLEASE OUTPUT WITH THE FOLLOWING FORMAT, WHERE THE SCORE IS A SCALE OF 1 TO 100 BY STRICTLY FOLLOWING THIS FORMAT: \"[[score]]\", FOR EXAMPLE \"Rating: [[100]]\":\n<start output>\nEvaluation evidence: your evluation explanation here, no more than 100 words\nRating: [[score]]\n<end output>\n\nNow, start your evaluation:",
        "generate_response": "[\"The Use of Asymmetric Numeral Systems as an Accurate Replacement for Huffman Coding\", \"Lightweight compression with encryption based on Asymmetric Numeral Systems\"]",
        "eval_response": "Evaluation evidence: The assistant's answer does not match the gold answer in terms of the titles of the papers and the length of the citation chain. The assistant's response includes only two titles, whereas the gold answer contains five. Additionally, the titles in the assistant's answer do not appear in the gold answer, indicating a mismatch in content and a failure to identify the correct citation chain. This results in inaccuracies and incompleteness in the response.\n\nRating: [[20]]",
        "score": 20
    },
    {
        "level": 3,
        "set": 2,
        "length": 91115,
        "type": "paper",
        "language": "en",
        "question": "Towards Conversational Diagnostic AI",
        "instruction": "We hope you will carefully study the provided papers and determine the citation relationships between them. Please follow the instructions below strictly to complete the task:\n\n#Specific Requirements:\n1. Reference: When a given paper mentions other provided papers, those other papers are considered as \"references\" for the given paper. To summarize in this specific context, references are about what the given paper is using.\n2. Citation: Conversely, when other provided papers mention the given paper in their works, the given paper is being \"cited\" by those other papers. To summarize in this specific context, citations are about who is using the given paper.\n3. Given a paper, you need to determine the citation or reference relationship between this paper and the other papers. Do not consider papers that are not provided.\n3. Please present the paper titles in a json format as follows: {{\"Reference\":[\"Reference Title 1\", \"Reference Title 2\", ..., \"Reference Title n\"], \"Citation\":[\"Citation Title 1\", \"Citation Title 2\", ..., \"Citation Title n\"]}}.\n4. If a paper does not have any references or citations, please leave the corresponding list empty, e.g.{{\"Refernce\":[]}}, {{\"Citation\":[]}}.",
        "prompt_template": "#Papers:\n{docs}\n\n{instruction}\n\n#The paper you need to analyze:\n{question}",
        "doc": [
            "2401.05654.md",
            "2404.13066.md",
            "2405.06093.md",
            "2311.16452.md",
            "2312.00164.md"
        ],
        "answer": {
            "Reference": [
                "# Can Generalist Foundation Models Outcompete Special-Purpose Tuning? Case Study in Medicine ",
                "# Towards Accurate Differential "
            ],
            "Citation": [
                "# LEVERAGING LARGE LANGUAGE ModeL AS SiMULATED PATIENTS FOR CLINICAL EDUCATION ",
                "# Selective Fine-tuning on LLM-labeled Data May Reduce Reliance on Human Annotation: A Case Study Using Schedule-of-Event Table Detection "
            ]
        },
        "shuffle_doc": true,
        "id": "bd75937e-2fdd-4210-8016-38f90c04591a",
        "prompt": "[Question]\n#Papers:\n{docs}\n\nWe hope you will carefully study the provided papers and determine the citation relationships between them. Please follow the instructions below strictly to complete the task:\n\n#Specific Requirements:\n1. Reference: When a given paper mentions other provided papers, those other papers are considered as \"references\" for the given paper. To summarize in this specific context, references are about what the given paper is using.\n2. Citation: Conversely, when other provided papers mention the given paper in their works, the given paper is being \"cited\" by those other papers. To summarize in this specific context, citations are about who is using the given paper.\n3. Given a paper, you need to determine the citation or reference relationship between this paper and the other papers. Do not consider papers that are not provided.\n3. Please present the paper titles in a json format as follows: {{\"Reference\":[\"Reference Title 1\", \"Reference Title 2\", ..., \"Reference Title n\"], \"Citation\":[\"Citation Title 1\", \"Citation Title 2\", ..., \"Citation Title n\"]}}.\n4. If a paper does not have any references or citations, please leave the corresponding list empty, e.g.{{\"Refernce\":[]}}, {{\"Citation\":[]}}.\n\n#The paper you need to analyze:\nTowards Conversational Diagnostic AI\n\n[Gold Answer]\n{'Reference': ['# Can Generalist Foundation Models Outcompete Special-Purpose Tuning? Case Study in Medicine ', '# Towards Accurate Differential '], 'Citation': ['# LEVERAGING LARGE LANGUAGE ModeL AS SiMULATED PATIENTS FOR CLINICAL EDUCATION ', '# Selective Fine-tuning on LLM-labeled Data May Reduce Reliance on Human Annotation: A Case Study Using Schedule-of-Event Table Detection ']}\n\n[The Start of Assistant's Predicted Answer]\n{\"Reference\":[\"Interviewing the patient\", \"Contributions of the history, physical examination, and laboratory investigation in making medical diagnoses\", \"Relative contributions of history-taking, physical examination, and investigations in making medical diagnosis\", \"Teaching clinical medicine by iterative hypothesis testing: let's preach what we practice\", \"A study on relative contributions of the history, physical examination and investigations in making medical diagnosis\", \"The importance of the history in the medical clinic and the cost of unnecessary tests\", \"Skills for communicating with patients\", \"Global supply of health professionals\", \"GPT-4 Technical Report\", \"PaLM 2 Technical Report\", \"Gemini: A Family of Highly Capable Multimodal Models\", \"Large Language Models Encode Clinical Knowledge\", \"Towards expert-level medical question answering with large language models\", \"Can Generalist Foundation Models Outcompete Special-Purpose Tuning? Case Study in Medicine\", \"LaMDA: Language models for dialog applications\", \"Introducing ChatGPT\", \"Clinical Camel: An Open-Source Expert-Level Medical Language Model with Dialogue-Based Knowledge Encoding\", \"MEDITRON-70B: Scaling Medical Pretraining for Large Language Models\", \"History taking is a complex skill\", \"Best practice\" for patient-centered communication: a narrative review\", \"What disease does this patient have? a large-scale open domain question answering dataset from medical exams\", \"MIMIC-III, a freely accessible critical care database\", \"Speech recognition for medical conversations\", \"A computational approach to understanding empathy expressed in text-based mental health support\", \"Improving language model negotiation with self-play and in-context learning from ai feedback\", \"Overview of the mediqa-chat 2023 shared tasks on the summarization \\\\& generation of doctor-patient conversations\", \"Overview of the ImageCLLEF 2023: Multimedia Retrieval in Medical, Social Media and Internet Applications\", \"DialMed: A Dataset for Dialogue-based Medication Recommendation\", \"Incorporating Medical Knowledge to Transformer-based Language Models for Medical Dialogue Generation\", \"MRCP (UK) PART 2 Clinical Examination (PACES): a review of the first four examination sessions\", \"The Objective Structured Clinical Examination. The new gold standard for evaluating postgraduate clinical performance\", \"The objective structured clinical examination: a step in the direction of competency-based evaluation\", \"Defining and assessing professional competence\", \"Handbook of econometrics\", \"Controlling the false discovery rate: a practical and powerful approach to multiple testing\", \"Wilcoxon signed-rank test\", \"Teaching history taking to medical students: a systematic review\", \"Effect of communications training on medical student performance\", \"Communication skills education in medical school and beyond\", \"Teaching and assessing communication skills in the postgraduate medical setting: a systematic scoping review\", \"Improving communication skills: a course for academic medical center surgery residents and faculty\", \"UK consensus statement on the content of communication curricula in undergraduate medical education\", \"Endpoints in medical communication research, proposing a framework of functions and outcomes\", \"Patient-centered communication in cancer care: promoting healing and reducing suffering\", \"Assessing communication competence: a review of current tools\", \"Medical history\", \"What are consultation models for?\", \"Implementation of virtual OSCE in health professions education: A systematic review\", \"Multiwoz-a large-scale multi-domain wizard-of-oz dataset for task-oriented dialogue modelling\", \"Airdialogue: An environment for goal-oriented dialogue research\", \"Decision-Oriented Dialogue for Human-AI Collaboration\", \"Attention is all you need\", \"Training language models to follow instructions with human feedback\", \"Ethical-advice taker: Do language models understand natural language interventions?\", \"Self-critiquing models for assisting human evaluators\", \"Training language models with language feedback at scale\", \"Improving alignment of dialogue agents via targeted human judgements\", \"Constitutional AI: Harmlessness from AI feedback\", \"A general language assistant as a laboratory for alignment\", \"Clinical BERTScore: An Improved Measure of Automatic Speech Recognition Performance in Clinical Settings\", \"Overview of the medical question answering task at TREC 2017 LiveQA\", \"The diagnostic and triage accuracy of digital and online symptom checker tools: a systematic review\", \"Accuracy of a Generative Artificial Intelligence Model in a Complex Diagnostic Challenge\", \"Enhancing the reliability and accuracy of AI-enabled diagnosis via complementarity-driven deferral to clinicians\", \"Methods in teaching consultation-liaison psychiatry\", \"What is missing from patient histories?\", \"Lessons for Clinicians From Physician-Patient\", \"Patient-centered interviewing: an evidence-based method\", \"Care, health, and cost\", \"From triple to quadruple aim: care of the patient requires care of the provider\", \"Patient race/ethnicity and quality of patient-physician communication during medical visits\", \"Physician gender effects in medical communication: a meta-analytic review\", \"Red teaming language models with language models\", \"Red teaming language models to reduce harms: Methods, scaling behaviors, and lessons learned\", \"Gptfuzzer: Red teaming large language models with auto-generated jailbreak prompts\", \"MART: Improving LLM Safety with Multi-round Automatic Red-Teaming\", \"Model cards for model reporting\", \"Interactive model cards: A human-centered approach to model documentation\", \"Data cards: Purposeful and transparent dataset documentation for responsible ai\", \"How Linguistically Fair Are Multilingual Pre-Trained Language Models?\", \"You reap what you sow: On the challenges of bias evaluation under multilingual settings\", \"MegaVerse: Benchmarking Large Language Models Across Languages, Modalities, Models and Tasks\", \"Glot500: Scaling Multilingual Corpora and Language Models to 500 Languages\", \"Democratizing LLMs for Low-Resource Languages by Leveraging their English Dominant Abilities with Linguistically-Diverse Prompts\", \"Having Beer after Prayer? Measuring Cultural Bias in Large Language Models\", \"Fairness in Language Models Beyond English: Gaps and Challenges\", \"Are Large Language Model-based Evaluators the Solution to Scaling Up Multilingual Evaluation?\", \"Conformal Language Modeling\", \"Quantifying Uncertainty in Answers from any Language Model and Enhancing their Trustworthiness\", \"Look Before You Leap: An Exploratory Study of Uncertainty Measurement for Large Language Models\", \"Uncertainty-aware Language Modeling for Selective Question Answering\", \"Mind the gap: Assessing temporal generalization in neural language models\"], \"Citation\":[]}\n[The End of Assistant's Predicted Answer]\n\n[System]\nWe would like to request your feedback on the performance of the AI assistant in response to the user question displayed above according to the gold answer. Please use the following listed aspects and their descriptions as evaluation criteria:\n    - Accuracy and Hallucinations: The assistant's answer is semantically consistent with the gold answer; The numerical value and order need to be accurate, and there should be no hallucinations.\n    - Completeness: Referring to the reference answers, the assistant's answer should contain all the key points needed to answer the user's question; further elaboration on these key points can be omitted.\nPlease rate whether this answer is suitable for the question. Please note that the gold answer can be considered as a correct answer to the question.\n\nThe assistant receives an overall score on a scale of 1 to 100, where a higher score indicates better overall performance.\nPlease note that if the assistant's answer and the gold answer fully meet the above criteria, its overall rating should be the full marks (100).\nPlease first provide a comprehensive explanation of your evaluation, avoiding any potential bias.\nThen, output a line indicating the score of the Assistant.\n\nPLEASE OUTPUT WITH THE FOLLOWING FORMAT, WHERE THE SCORE IS A SCALE OF 1 TO 100 BY STRICTLY FOLLOWING THIS FORMAT: \"[[score]]\", FOR EXAMPLE \"Rating: [[100]]\":\n<start output>\nEvaluation evidence: your evluation explanation here, no more than 100 words\nRating: [[score]]\n<end output>\n\nNow, start your evaluation:",
        "generate_response": "{\"Reference\":[\"Interviewing the patient\", \"Contributions of the history, physical examination, and laboratory investigation in making medical diagnoses\", \"Relative contributions of history-taking, physical examination, and investigations in making medical diagnosis\", \"Teaching clinical medicine by iterative hypothesis testing: let's preach what we practice\", \"A study on relative contributions of the history, physical examination and investigations in making medical diagnosis\", \"The importance of the history in the medical clinic and the cost of unnecessary tests\", \"Skills for communicating with patients\", \"Global supply of health professionals\", \"GPT-4 Technical Report\", \"PaLM 2 Technical Report\", \"Gemini: A Family of Highly Capable Multimodal Models\", \"Large Language Models Encode Clinical Knowledge\", \"Towards expert-level medical question answering with large language models\", \"Can Generalist Foundation Models Outcompete Special-Purpose Tuning? Case Study in Medicine\", \"LaMDA: Language models for dialog applications\", \"Introducing ChatGPT\", \"Clinical Camel: An Open-Source Expert-Level Medical Language Model with Dialogue-Based Knowledge Encoding\", \"MEDITRON-70B: Scaling Medical Pretraining for Large Language Models\", \"History taking is a complex skill\", \"Best practice\" for patient-centered communication: a narrative review\", \"What disease does this patient have? a large-scale open domain question answering dataset from medical exams\", \"MIMIC-III, a freely accessible critical care database\", \"Speech recognition for medical conversations\", \"A computational approach to understanding empathy expressed in text-based mental health support\", \"Improving language model negotiation with self-play and in-context learning from ai feedback\", \"Overview of the mediqa-chat 2023 shared tasks on the summarization \\\\& generation of doctor-patient conversations\", \"Overview of the ImageCLLEF 2023: Multimedia Retrieval in Medical, Social Media and Internet Applications\", \"DialMed: A Dataset for Dialogue-based Medication Recommendation\", \"Incorporating Medical Knowledge to Transformer-based Language Models for Medical Dialogue Generation\", \"MRCP (UK) PART 2 Clinical Examination (PACES): a review of the first four examination sessions\", \"The Objective Structured Clinical Examination. The new gold standard for evaluating postgraduate clinical performance\", \"The objective structured clinical examination: a step in the direction of competency-based evaluation\", \"Defining and assessing professional competence\", \"Handbook of econometrics\", \"Controlling the false discovery rate: a practical and powerful approach to multiple testing\", \"Wilcoxon signed-rank test\", \"Teaching history taking to medical students: a systematic review\", \"Effect of communications training on medical student performance\", \"Communication skills education in medical school and beyond\", \"Teaching and assessing communication skills in the postgraduate medical setting: a systematic scoping review\", \"Improving communication skills: a course for academic medical center surgery residents and faculty\", \"UK consensus statement on the content of communication curricula in undergraduate medical education\", \"Endpoints in medical communication research, proposing a framework of functions and outcomes\", \"Patient-centered communication in cancer care: promoting healing and reducing suffering\", \"Assessing communication competence: a review of current tools\", \"Medical history\", \"What are consultation models for?\", \"Implementation of virtual OSCE in health professions education: A systematic review\", \"Multiwoz-a large-scale multi-domain wizard-of-oz dataset for task-oriented dialogue modelling\", \"Airdialogue: An environment for goal-oriented dialogue research\", \"Decision-Oriented Dialogue for Human-AI Collaboration\", \"Attention is all you need\", \"Training language models to follow instructions with human feedback\", \"Ethical-advice taker: Do language models understand natural language interventions?\", \"Self-critiquing models for assisting human evaluators\", \"Training language models with language feedback at scale\", \"Improving alignment of dialogue agents via targeted human judgements\", \"Constitutional AI: Harmlessness from AI feedback\", \"A general language assistant as a laboratory for alignment\", \"Clinical BERTScore: An Improved Measure of Automatic Speech Recognition Performance in Clinical Settings\", \"Overview of the medical question answering task at TREC 2017 LiveQA\", \"The diagnostic and triage accuracy of digital and online symptom checker tools: a systematic review\", \"Accuracy of a Generative Artificial Intelligence Model in a Complex Diagnostic Challenge\", \"Enhancing the reliability and accuracy of AI-enabled diagnosis via complementarity-driven deferral to clinicians\", \"Methods in teaching consultation-liaison psychiatry\", \"What is missing from patient histories?\", \"Lessons for Clinicians From Physician-Patient\", \"Patient-centered interviewing: an evidence-based method\", \"Care, health, and cost\", \"From triple to quadruple aim: care of the patient requires care of the provider\", \"Patient race/ethnicity and quality of patient-physician communication during medical visits\", \"Physician gender effects in medical communication: a meta-analytic review\", \"Red teaming language models with language models\", \"Red teaming language models to reduce harms: Methods, scaling behaviors, and lessons learned\", \"Gptfuzzer: Red teaming large language models with auto-generated jailbreak prompts\", \"MART: Improving LLM Safety with Multi-round Automatic Red-Teaming\", \"Model cards for model reporting\", \"Interactive model cards: A human-centered approach to model documentation\", \"Data cards: Purposeful and transparent dataset documentation for responsible ai\", \"How Linguistically Fair Are Multilingual Pre-Trained Language Models?\", \"You reap what you sow: On the challenges of bias evaluation under multilingual settings\", \"MegaVerse: Benchmarking Large Language Models Across Languages, Modalities, Models and Tasks\", \"Glot500: Scaling Multilingual Corpora and Language Models to 500 Languages\", \"Democratizing LLMs for Low-Resource Languages by Leveraging their English Dominant Abilities with Linguistically-Diverse Prompts\", \"Having Beer after Prayer? Measuring Cultural Bias in Large Language Models\", \"Fairness in Language Models Beyond English: Gaps and Challenges\", \"Are Large Language Model-based Evaluators the Solution to Scaling Up Multilingual Evaluation?\", \"Conformal Language Modeling\", \"Quantifying Uncertainty in Answers from any Language Model and Enhancing their Trustworthiness\", \"Look Before You Leap: An Exploratory Study of Uncertainty Measurement for Large Language Models\", \"Uncertainty-aware Language Modeling for Selective Question Answering\", \"Mind the gap: Assessing temporal generalization in neural language models\"], \"Citation\":[]}",
        "eval_response": "Evaluation evidence: The assistant's answer does not align with the gold answer in terms of accuracy and completeness. The references listed in the assistant's answer do not match those in the gold answer, indicating a significant deviation and hallucination of content. Additionally, the assistant's answer includes an extensive list of references that are not relevant to the given task, further demonstrating inaccuracies and a lack of focus on the required task details. The citation section is correctly empty, aligning with the gold answer, but this does not compensate for the inaccuracies in the references section.\n\nRating: [[20]]",
        "score": 20
    },
    {
        "level": 3,
        "set": 2,
        "length": 72725,
        "type": "paper",
        "language": "en",
        "question": "Scalable Pre-training of Large Autoregressive Image Models",
        "instruction": "We hope you will carefully study the provided papers and determine the citation relationships between them. Please follow the instructions below strictly to complete the task:\n\n#Specific Requirements:\n1. Reference: When a given paper mentions other provided papers, those other papers are considered as \"references\" for the given paper. To summarize in this specific context, references are about what the given paper is using.\n2. Citation: Conversely, when other provided papers mention the given paper in their works, the given paper is being \"cited\" by those other papers. To summarize in this specific context, citations are about who is using the given paper.\n3. Given a paper, you need to determine the citation or reference relationship between this paper and the other papers. Do not consider papers that are not provided.\n3. Please present the paper titles in a json format as follows: {{\"Reference\":[\"Reference Title 1\", \"Reference Title 2\", ..., \"Reference Title n\"], \"Citation\":[\"Citation Title 1\", \"Citation Title 2\", ..., \"Citation Title n\"]}}.\n4. If a paper does not have any references or citations, please leave the corresponding list empty, e.g.{{\"Refernce\":[]}}, {{\"Citation\":[]}}.",
        "prompt_template": "#Papers:\n{docs}\n\n{instruction}\n\n#The paper you need to analyze:\n{question}",
        "doc": [
            "2401.08541.md",
            "2302.13971.md",
            "2303.13496.md"
        ],
        "answer": {
            "Reference": [
                "# LLaMA: Open and Efficient Foundation Language Models ",
                "# The effectiveness of MAE pre-pretraining for billion-scale pretraining "
            ],
            "Citation": []
        },
        "shuffle_doc": true,
        "id": "36f1a89b-ba33-449a-ad66-95ac6e429156",
        "prompt": "[Question]\n#Papers:\n{docs}\n\nWe hope you will carefully study the provided papers and determine the citation relationships between them. Please follow the instructions below strictly to complete the task:\n\n#Specific Requirements:\n1. Reference: When a given paper mentions other provided papers, those other papers are considered as \"references\" for the given paper. To summarize in this specific context, references are about what the given paper is using.\n2. Citation: Conversely, when other provided papers mention the given paper in their works, the given paper is being \"cited\" by those other papers. To summarize in this specific context, citations are about who is using the given paper.\n3. Given a paper, you need to determine the citation or reference relationship between this paper and the other papers. Do not consider papers that are not provided.\n3. Please present the paper titles in a json format as follows: {{\"Reference\":[\"Reference Title 1\", \"Reference Title 2\", ..., \"Reference Title n\"], \"Citation\":[\"Citation Title 1\", \"Citation Title 2\", ..., \"Citation Title n\"]}}.\n4. If a paper does not have any references or citations, please leave the corresponding list empty, e.g.{{\"Refernce\":[]}}, {{\"Citation\":[]}}.\n\n#The paper you need to analyze:\nScalable Pre-training of Large Autoregressive Image Models\n\n[Gold Answer]\n{'Reference': ['# LLaMA: Open and Efficient Foundation Language Models ', '# The effectiveness of MAE pre-pretraining for billion-scale pretraining '], 'Citation': []}\n\n[The Start of Assistant's Predicted Answer]\n{\"Reference\":[\"V-JEPA: Latent video prediction for visual representation learning\", \"Self-supervised learning from images with a joint-embedding predictive architecture\", \"Sequential modeling enables scalable learning for large vision models\", \"From detection of individual metastases to classification of lymph node status at the patient level: the camelyon17 challenge\", \"BEiT: Bert pretraining of image transformers\", \"Vicreg: Variance-invariance-covariance regularization for selfsupervised learning\", \"Latent video prediction for visual representation learning\", \"Self-supervised learning from images with a joint-embedding predictive architecture\", \"Sequential modeling enables scalable learning for large vision models\", \"A simple framework for contrastive learning of visual representations\", \"An empirical study of training self-supervised vision transformers\", \"An image is worth 16 x 16 words: Transformers for image recognition at scale\", \"Are large-scale datasets necessary for self-supervised pre-training?\", \"Mask r-cnn\", \"Training compute-optimal large language models\", \"Lora: Low-rank adaptation of large language models\", \"Deep networks with stochastic depth\", \"Learning multiple layers of features from tiny images\", \"Image transformer\", \"Context encoders: Feature learning by inpainting\", \"The effectiveness of mae pre-pretraining for billionscale pretraining\", \"Llama: Open and efficient foundation language models\", \"Rotation equivariant cnns for digital pathology\", \"Diffusion models as masked autoencoders\", \"Coca: Contrastive captioners are image-text foundation models\", \"Barlow twins: Self-supervised learning via redundancy reduction\"], \"Citation\":[\"V-JEPA: Latent video prediction for visual representation learning\", \"Self-supervised learning from images with a joint-embedding predictive architecture\", \"Sequential modeling enables scalable learning for large vision models\", \"From detection of individual metastases to classification of lymph node status at the patient level: the camelyon17 challenge\", \"BEiT: Bert pretraining of image transformers\", \"Vicreg: Variance-invariance-covariance regularization for selfsupervised learning\", \"Latent video prediction for visual representation learning\", \"Self-supervised learning from images with a joint-embedding predictive architecture\", \"Sequential modeling enables scalable learning for large vision models\", \"A simple framework for contrastive learning of visual representations\", \"An empirical study of training self-supervised vision transformers\", \"An image is worth 16 x 16 words: Transformers for image recognition at scale\", \"Are large-scale datasets necessary for self-supervised pre-training?\", \"Mask r-cnn\", \"Training compute-optimal large language models\", \"Lora: Low-rank adaptation of large language models\", \"Deep networks with stochastic depth\", \"Learning multiple layers of features from tiny images\", \"Image transformer\", \"Context encoders: Feature learning by inpainting\", \"The effectiveness of mae pre-pretraining for billionscale pretraining\", \"Llama: Open and efficient foundation language models\", \"Rotation equivariant cnns for digital pathology\", \"Diffusion models as masked autoencoders\", \"Coca: Contrastive captioners are image-text foundation models\", \"Barlow twins: Self-supervised learning via redundancy reduction\"]}\n[The End of Assistant's Predicted Answer]\n\n[System]\nWe would like to request your feedback on the performance of the AI assistant in response to the user question displayed above according to the gold answer. Please use the following listed aspects and their descriptions as evaluation criteria:\n    - Accuracy and Hallucinations: The assistant's answer is semantically consistent with the gold answer; The numerical value and order need to be accurate, and there should be no hallucinations.\n    - Completeness: Referring to the reference answers, the assistant's answer should contain all the key points needed to answer the user's question; further elaboration on these key points can be omitted.\nPlease rate whether this answer is suitable for the question. Please note that the gold answer can be considered as a correct answer to the question.\n\nThe assistant receives an overall score on a scale of 1 to 100, where a higher score indicates better overall performance.\nPlease note that if the assistant's answer and the gold answer fully meet the above criteria, its overall rating should be the full marks (100).\nPlease first provide a comprehensive explanation of your evaluation, avoiding any potential bias.\nThen, output a line indicating the score of the Assistant.\n\nPLEASE OUTPUT WITH THE FOLLOWING FORMAT, WHERE THE SCORE IS A SCALE OF 1 TO 100 BY STRICTLY FOLLOWING THIS FORMAT: \"[[score]]\", FOR EXAMPLE \"Rating: [[100]]\":\n<start output>\nEvaluation evidence: your evluation explanation here, no more than 100 words\nRating: [[score]]\n<end output>\n\nNow, start your evaluation:",
        "generate_response": "{\"Reference\":[\"V-JEPA: Latent video prediction for visual representation learning\", \"Self-supervised learning from images with a joint-embedding predictive architecture\", \"Sequential modeling enables scalable learning for large vision models\", \"From detection of individual metastases to classification of lymph node status at the patient level: the camelyon17 challenge\", \"BEiT: Bert pretraining of image transformers\", \"Vicreg: Variance-invariance-covariance regularization for selfsupervised learning\", \"Latent video prediction for visual representation learning\", \"Self-supervised learning from images with a joint-embedding predictive architecture\", \"Sequential modeling enables scalable learning for large vision models\", \"A simple framework for contrastive learning of visual representations\", \"An empirical study of training self-supervised vision transformers\", \"An image is worth 16 x 16 words: Transformers for image recognition at scale\", \"Are large-scale datasets necessary for self-supervised pre-training?\", \"Mask r-cnn\", \"Training compute-optimal large language models\", \"Lora: Low-rank adaptation of large language models\", \"Deep networks with stochastic depth\", \"Learning multiple layers of features from tiny images\", \"Image transformer\", \"Context encoders: Feature learning by inpainting\", \"The effectiveness of mae pre-pretraining for billionscale pretraining\", \"Llama: Open and efficient foundation language models\", \"Rotation equivariant cnns for digital pathology\", \"Diffusion models as masked autoencoders\", \"Coca: Contrastive captioners are image-text foundation models\", \"Barlow twins: Self-supervised learning via redundancy reduction\"], \"Citation\":[\"V-JEPA: Latent video prediction for visual representation learning\", \"Self-supervised learning from images with a joint-embedding predictive architecture\", \"Sequential modeling enables scalable learning for large vision models\", \"From detection of individual metastases to classification of lymph node status at the patient level: the camelyon17 challenge\", \"BEiT: Bert pretraining of image transformers\", \"Vicreg: Variance-invariance-covariance regularization for selfsupervised learning\", \"Latent video prediction for visual representation learning\", \"Self-supervised learning from images with a joint-embedding predictive architecture\", \"Sequential modeling enables scalable learning for large vision models\", \"A simple framework for contrastive learning of visual representations\", \"An empirical study of training self-supervised vision transformers\", \"An image is worth 16 x 16 words: Transformers for image recognition at scale\", \"Are large-scale datasets necessary for self-supervised pre-training?\", \"Mask r-cnn\", \"Training compute-optimal large language models\", \"Lora: Low-rank adaptation of large language models\", \"Deep networks with stochastic depth\", \"Learning multiple layers of features from tiny images\", \"Image transformer\", \"Context encoders: Feature learning by inpainting\", \"The effectiveness of mae pre-pretraining for billionscale pretraining\", \"Llama: Open and efficient foundation language models\", \"Rotation equivariant cnns for digital pathology\", \"Diffusion models as masked autoencoders\", \"Coca: Contrastive captioners are image-text foundation models\", \"Barlow twins: Self-supervised learning via redundancy reduction\"]}",
        "eval_response": "Evaluation evidence: The assistant's answer significantly deviates from the gold answer in terms of accuracy and hallucinations. The gold answer lists two specific references and no citations, whereas the assistant's answer includes a long list of both references and citations, many of which are not mentioned in the gold answer. This indicates a high level of hallucination and inaccuracy. The assistant's answer also fails in completeness as it does not accurately reflect the specific references and citations relevant to the given paper.\n\nRating: [[10]]",
        "score": 10
    },
    {
        "level": 3,
        "set": 1,
        "length": 41774,
        "type": "paper",
        "language": "en",
        "question": "KTO: Model Alignment as Prospect Theoretic Optimization",
        "instruction": "We hope you will carefully study the provided papers and determine the citation relationships between them. Please follow the instructions below strictly to complete the task:\n\n#Specific Requirements:\n1. Reference: When a given paper mentions other provided papers, those other papers are considered as \"references\" for the given paper. To summarize in this specific context, references are about what the given paper is using.\n2. Citation: Conversely, when other provided papers mention the given paper in their works, the given paper is being \"cited\" by those other papers. To summarize in this specific context, citations are about who is using the given paper.\n3. Given a paper, you need to determine the citation or reference relationship between this paper and the other papers. Do not consider papers that are not provided.\n3. Please present the paper titles in a json format as follows: {{\"Reference\":[\"Reference Title 1\", \"Reference Title 2\", ..., \"Reference Title n\"], \"Citation\":[\"Citation Title 1\", \"Citation Title 2\", ..., \"Citation Title n\"]}}.\n4. If a paper does not have any references or citations, please leave the corresponding list empty, e.g.{{\"Refernce\":[]}}, {{\"Citation\":[]}}.",
        "prompt_template": "#Papers:\n{docs}\n\n{instruction}\n\n#The paper you need to analyze:\n{question}",
        "doc": [
            "2402.01306.md",
            "2405.20175.md",
            "2310.06825.md"
        ],
        "answer": {
            "Reference": [
                "# Mistral 7B "
            ],
            "Citation": [
                "# InstructionCP: A fast approach to transfer Large Language Models into target language "
            ]
        },
        "shuffle_doc": true,
        "id": "90e446b8-4238-4dd8-b5dc-748c51d52cbb",
        "prompt": "[Question]\n#Papers:\n{docs}\n\nWe hope you will carefully study the provided papers and determine the citation relationships between them. Please follow the instructions below strictly to complete the task:\n\n#Specific Requirements:\n1. Reference: When a given paper mentions other provided papers, those other papers are considered as \"references\" for the given paper. To summarize in this specific context, references are about what the given paper is using.\n2. Citation: Conversely, when other provided papers mention the given paper in their works, the given paper is being \"cited\" by those other papers. To summarize in this specific context, citations are about who is using the given paper.\n3. Given a paper, you need to determine the citation or reference relationship between this paper and the other papers. Do not consider papers that are not provided.\n3. Please present the paper titles in a json format as follows: {{\"Reference\":[\"Reference Title 1\", \"Reference Title 2\", ..., \"Reference Title n\"], \"Citation\":[\"Citation Title 1\", \"Citation Title 2\", ..., \"Citation Title n\"]}}.\n4. If a paper does not have any references or citations, please leave the corresponding list empty, e.g.{{\"Refernce\":[]}}, {{\"Citation\":[]}}.\n\n#The paper you need to analyze:\nKTO: Model Alignment as Prospect Theoretic Optimization\n\n[Gold Answer]\n{'Reference': ['# Mistral 7B '], 'Citation': ['# InstructionCP: A fast approach to transfer Large Language Models into target language ']}\n\n[The Start of Assistant's Predicted Answer]\n{\"Reference\":[\"Benchmarking Large Language Models in Retrieval-Augmented Generation\", \"Pythia: A suite for analyzing large language models across training and scaling\", \"Rank analysis of incomplete block designs: I. the method of paired comparisons\", \"Preference-based reinforcement learning: evolutionary direct policy search using a preference-based racing algorithm\", \"Open problems and fundamental limitations of reinforcement learning from human feedback\", \"Understanding dataset difficulty with $\\\\mathcal{V}$-usable information\", \"Red teaming language models to reduce harms: Methods, scaling behaviors, and lessons learned\", \"Aligning language models with preferences through f-divergence minimization\", \"Decision-making under uncertainty-a field study of cumulative prospect theory\", \"Neural collaborative filtering\", \"Measuring massive multitask language understanding\", \"Constructing stable preferences: A look into dimensions of experience and their impact on preference stability\", \"A simple sequentially rejective multiple test procedure\", \"Learning trajectory preferences for manipulators via iterative improvement\", \"Prospect theory: An analysis of decision under risk\", \"Openassistant conversationsdemocratizing large language model alignment\", \"Reliability and learnability of human bandit feedback for sequence-tosequence reinforcement learning\", \"When humans aren't optimal: Robots that collaborate with risk-aware humans\", \"Alpacaeval: An automatic evaluator of instruction-following models\", \"Training language models to follow instructions with human feedback\", \"Advantage-weighted regression: Simple and scalable off-policy reinforcement learning\", \"Reinforcement learning by rewardweighted regression for operational space control\", \"Direct preference optimization: Your language model is secretly a reward model\", \"Proximal policy optimization algorithms\", \"Beyond the imitation game: Quantifying and extrapolating the capabilities of language models\", \"Learning to summarize with human feedback\", \"Interpretable modelling of driving behaviors in interactive driving scenarios based on cumulative prospect theory\", \"Fine-tuning language models for factuality\", \"Llama: Open and efficient foundation language models\", \"Zephyr: Direct distillation of lm alignment\", \"Advances in prospect theory: Cumulative representation of uncertainty\", \"Neural text generation with unlikelihood training\", \"Self-rewarding language models\", \"Slic-hf: Sequence likelihood calibration with human feedback\", \"Judging llm-as-a-judge with mt-bench and chatbot arena\"], \"Citation\":[\"Benchmarking Large Language Models in Retrieval-Augmented Generation\", \"Pythia: A suite for analyzing large language models across training and scaling\", \"Rank analysis of incomplete block designs: I. the method of paired comparisons\", \"Preference-based reinforcement learning: evolutionary direct policy search using a preference-based racing algorithm\", \"Open problems and fundamental limitations of reinforcement learning from human feedback\", \"Understanding dataset difficulty with $\\\\mathcal{V}$-usable information\", \"Red teaming language models to reduce harms: Methods, scaling behaviors, and lessons learned\", \"Aligning language models with preferences through f-divergence minimization\", \"Decision-making under uncertainty-a field study of cumulative prospect theory\", \"Neural collaborative filtering\", \"Measuring massive multitask language understanding\", \"Constructing stable preferences: A look into dimensions of experience and their impact on preference stability\", \"A simple sequentially rejective multiple test procedure\", \"Learning trajectory preferences for manipulators via iterative improvement\", \"Mistral 7b\", \"Prospect theory: An analysis of decision under risk\", \"Openassistant conversationsdemocratizing large language model alignment\", \"Reliability and learnability of human bandit feedback for sequence-tosequence reinforcement learning\", \"When humans aren't optimal: Robots that collaborate with risk-aware humans\", \"Alpacaeval: An automatic evaluator of instruction-following models\", \"Training language models to follow instructions with human feedback\", \"Advantage-weighted regression: Simple and scalable off-policy reinforcement learning\", \"Reinforcement learning by rewardweighted regression for operational space control\", \"Direct preference optimization: Your language model is secretly a reward model\", \"Proximal policy optimization algorithms\", \"Beyond the imitation game: Quantifying and extrapolating the capabilities of language models\", \"Learning to summarize with human feedback\", \"Interpretable modelling of driving behaviors in interactive driving scenarios based on cumulative prospect theory\", \"Fine-tuning language models for factuality\", \"Llama: Open and efficient foundation language models\", \"Zephyr: Direct distillation of lm alignment\", \"Advances in prospect theory: Cumulative representation of uncertainty\", \"Neural text generation with unlikelihood training\", \"Self-rewarding language models\", \"Slic-hf: Sequence likelihood calibration with human feedback\", \"Judging llm-as-a-judge with mt-bench and chatbot arena\"]}\n[The End of Assistant's Predicted Answer]\n\n[System]\nWe would like to request your feedback on the performance of the AI assistant in response to the user question displayed above according to the gold answer. Please use the following listed aspects and their descriptions as evaluation criteria:\n    - Accuracy and Hallucinations: The assistant's answer is semantically consistent with the gold answer; The numerical value and order need to be accurate, and there should be no hallucinations.\n    - Completeness: Referring to the reference answers, the assistant's answer should contain all the key points needed to answer the user's question; further elaboration on these key points can be omitted.\nPlease rate whether this answer is suitable for the question. Please note that the gold answer can be considered as a correct answer to the question.\n\nThe assistant receives an overall score on a scale of 1 to 100, where a higher score indicates better overall performance.\nPlease note that if the assistant's answer and the gold answer fully meet the above criteria, its overall rating should be the full marks (100).\nPlease first provide a comprehensive explanation of your evaluation, avoiding any potential bias.\nThen, output a line indicating the score of the Assistant.\n\nPLEASE OUTPUT WITH THE FOLLOWING FORMAT, WHERE THE SCORE IS A SCALE OF 1 TO 100 BY STRICTLY FOLLOWING THIS FORMAT: \"[[score]]\", FOR EXAMPLE \"Rating: [[100]]\":\n<start output>\nEvaluation evidence: your evluation explanation here, no more than 100 words\nRating: [[score]]\n<end output>\n\nNow, start your evaluation:",
        "generate_response": "{\"Reference\":[\"Benchmarking Large Language Models in Retrieval-Augmented Generation\", \"Pythia: A suite for analyzing large language models across training and scaling\", \"Rank analysis of incomplete block designs: I. the method of paired comparisons\", \"Preference-based reinforcement learning: evolutionary direct policy search using a preference-based racing algorithm\", \"Open problems and fundamental limitations of reinforcement learning from human feedback\", \"Understanding dataset difficulty with $\\\\mathcal{V}$-usable information\", \"Red teaming language models to reduce harms: Methods, scaling behaviors, and lessons learned\", \"Aligning language models with preferences through f-divergence minimization\", \"Decision-making under uncertainty-a field study of cumulative prospect theory\", \"Neural collaborative filtering\", \"Measuring massive multitask language understanding\", \"Constructing stable preferences: A look into dimensions of experience and their impact on preference stability\", \"A simple sequentially rejective multiple test procedure\", \"Learning trajectory preferences for manipulators via iterative improvement\", \"Prospect theory: An analysis of decision under risk\", \"Openassistant conversationsdemocratizing large language model alignment\", \"Reliability and learnability of human bandit feedback for sequence-tosequence reinforcement learning\", \"When humans aren't optimal: Robots that collaborate with risk-aware humans\", \"Alpacaeval: An automatic evaluator of instruction-following models\", \"Training language models to follow instructions with human feedback\", \"Advantage-weighted regression: Simple and scalable off-policy reinforcement learning\", \"Reinforcement learning by rewardweighted regression for operational space control\", \"Direct preference optimization: Your language model is secretly a reward model\", \"Proximal policy optimization algorithms\", \"Beyond the imitation game: Quantifying and extrapolating the capabilities of language models\", \"Learning to summarize with human feedback\", \"Interpretable modelling of driving behaviors in interactive driving scenarios based on cumulative prospect theory\", \"Fine-tuning language models for factuality\", \"Llama: Open and efficient foundation language models\", \"Zephyr: Direct distillation of lm alignment\", \"Advances in prospect theory: Cumulative representation of uncertainty\", \"Neural text generation with unlikelihood training\", \"Self-rewarding language models\", \"Slic-hf: Sequence likelihood calibration with human feedback\", \"Judging llm-as-a-judge with mt-bench and chatbot arena\"], \"Citation\":[\"Benchmarking Large Language Models in Retrieval-Augmented Generation\", \"Pythia: A suite for analyzing large language models across training and scaling\", \"Rank analysis of incomplete block designs: I. the method of paired comparisons\", \"Preference-based reinforcement learning: evolutionary direct policy search using a preference-based racing algorithm\", \"Open problems and fundamental limitations of reinforcement learning from human feedback\", \"Understanding dataset difficulty with $\\\\mathcal{V}$-usable information\", \"Red teaming language models to reduce harms: Methods, scaling behaviors, and lessons learned\", \"Aligning language models with preferences through f-divergence minimization\", \"Decision-making under uncertainty-a field study of cumulative prospect theory\", \"Neural collaborative filtering\", \"Measuring massive multitask language understanding\", \"Constructing stable preferences: A look into dimensions of experience and their impact on preference stability\", \"A simple sequentially rejective multiple test procedure\", \"Learning trajectory preferences for manipulators via iterative improvement\", \"Mistral 7b\", \"Prospect theory: An analysis of decision under risk\", \"Openassistant conversationsdemocratizing large language model alignment\", \"Reliability and learnability of human bandit feedback for sequence-tosequence reinforcement learning\", \"When humans aren't optimal: Robots that collaborate with risk-aware humans\", \"Alpacaeval: An automatic evaluator of instruction-following models\", \"Training language models to follow instructions with human feedback\", \"Advantage-weighted regression: Simple and scalable off-policy reinforcement learning\", \"Reinforcement learning by rewardweighted regression for operational space control\", \"Direct preference optimization: Your language model is secretly a reward model\", \"Proximal policy optimization algorithms\", \"Beyond the imitation game: Quantifying and extrapolating the capabilities of language models\", \"Learning to summarize with human feedback\", \"Interpretable modelling of driving behaviors in interactive driving scenarios based on cumulative prospect theory\", \"Fine-tuning language models for factuality\", \"Llama: Open and efficient foundation language models\", \"Zephyr: Direct distillation of lm alignment\", \"Advances in prospect theory: Cumulative representation of uncertainty\", \"Neural text generation with unlikelihood training\", \"Self-rewarding language models\", \"Slic-hf: Sequence likelihood calibration with human feedback\", \"Judging llm-as-a-judge with mt-bench and chatbot arena\"]}",
        "eval_response": "Evaluation evidence: The assistant's answer significantly deviates from the gold answer in terms of accuracy and completeness. The gold answer lists one reference and one citation, whereas the assistant's answer includes a long list of references and citations that are not mentioned in the gold answer. This indicates a high level of hallucination and a lack of accuracy. The assistant's response does not align with the specific requirements of the task, which asked for relationships between the given paper and other provided papers only.\n\nRating: [[10]]",
        "score": 10
    },
    {
        "level": 3,
        "set": 3,
        "length": 100582,
        "type": "paper",
        "language": "en",
        "question": "Small LLMs Are Weak Tool Learners: A Multi-LLM Agent",
        "instruction": "We hope you will carefully study the provided papers and determine the citation relationships between them. Please follow the instructions below strictly to complete the task:\n\n#Specific Requirements:\n1. Reference: When a given paper mentions other provided papers, those other papers are considered as \"references\" for the given paper. To summarize in this specific context, references are about what the given paper is using.\n2. Citation: Conversely, when other provided papers mention the given paper in their works, the given paper is being \"cited\" by those other papers. To summarize in this specific context, citations are about who is using the given paper.\n3. Given a paper, you need to determine the citation or reference relationship between this paper and the other papers. Do not consider papers that are not provided.\n3. Please present the paper titles in a json format as follows: {{\"Reference\":[\"Reference Title 1\", \"Reference Title 2\", ..., \"Reference Title n\"], \"Citation\":[\"Citation Title 1\", \"Citation Title 2\", ..., \"Citation Title n\"]}}.\n4. If a paper does not have any references or citations, please leave the corresponding list empty, e.g.{{\"Refernce\":[]}}, {{\"Citation\":[]}}.",
        "prompt_template": "#Papers:\n{docs}\n\n{instruction}\n\n#The paper you need to analyze:\n{question}",
        "doc": [
            "2401.07324.md",
            "2405.16533.md",
            "2405.17935.md",
            "2310.05915.md"
        ],
        "answer": {
            "Reference": [
                "# FireAct: TOWARd LANGUAGE AGENT FinE-TUNING "
            ],
            "Citation": [
                "# Chain of Tools: Large Language Model is an Automatic Multi-tool Learner ",
                "# Tool Learning with Large Language Models: A Survey "
            ]
        },
        "shuffle_doc": true,
        "id": "0b3be2dd-7211-48e4-9cf4-dfad595661fa",
        "prompt": "[Question]\n#Papers:\n{docs}\n\nWe hope you will carefully study the provided papers and determine the citation relationships between them. Please follow the instructions below strictly to complete the task:\n\n#Specific Requirements:\n1. Reference: When a given paper mentions other provided papers, those other papers are considered as \"references\" for the given paper. To summarize in this specific context, references are about what the given paper is using.\n2. Citation: Conversely, when other provided papers mention the given paper in their works, the given paper is being \"cited\" by those other papers. To summarize in this specific context, citations are about who is using the given paper.\n3. Given a paper, you need to determine the citation or reference relationship between this paper and the other papers. Do not consider papers that are not provided.\n3. Please present the paper titles in a json format as follows: {{\"Reference\":[\"Reference Title 1\", \"Reference Title 2\", ..., \"Reference Title n\"], \"Citation\":[\"Citation Title 1\", \"Citation Title 2\", ..., \"Citation Title n\"]}}.\n4. If a paper does not have any references or citations, please leave the corresponding list empty, e.g.{{\"Refernce\":[]}}, {{\"Citation\":[]}}.\n\n#The paper you need to analyze:\nSmall LLMs Are Weak Tool Learners: A Multi-LLM Agent\n\n[Gold Answer]\n{'Reference': ['# FireAct: TOWARd LANGUAGE AGENT FinE-TUNING '], 'Citation': ['# Chain of Tools: Large Language Model is an Automatic Multi-tool Learner ', '# Tool Learning with Large Language Models: A Survey ']}\n\n[The Start of Assistant's Predicted Answer]\n{\"Reference\": [\"A Survey on Evaluation of Large Language Models\", \"Vicuna: An Open-Source Chatbot Impressing GPT-4 with $90 \\% *$ ChatGPT Quality\", \"ChatLaw: Open-Source Legal Large Language Model with Integrated External Knowledge Bases\", \"Compositional Semantic Parsing with Large Language Models\", \"Open LLM Leaderboard\", \"How Close is ChatGPT to Human Experts? Comparison Corpus, Evaluation, and Detection\", \"REALM: Retrieval-Augmented Language Model PreTraining\", \"Rethinking with Retrieval: Faithful Large Language Model Inference\", \"Measuring Massive Multitask Language Understanding\", \"Atlas: Few-shot Learning with Retrieval Augmented Language Models\", \"Chain-of-thought prompting elicits reasoning in large language models\", \"Reflextion: Language agents with verbal reinforcement learning\", \"MemoryBank: Enhancing large language models with long-term memory\", \"Tool learning with foundation models\", \"ToolLLM: Facilitating large language models to master 16000+ real-world APIs\", \"Global-to-local progressive fine-tuning for multi-LLM systems\", \"ToolEval: A method for evaluating the tool invocation capabilities of large language models\", \"BabyAGI\", \"AutoGPT\", \"Voyager: An open-ended embodied agent with large language models\", \"Ghost in the Minecraft: Generally capable agents for open-world environments via large language models with text-based knowledge and memory\", \"MetaGPT\", \"ChatDev\", \"AutoGen: Enabling next-gen LLM applications via multiagent conversation framework\", \"FiREACT: Toward language agent fine-tuning\", \"AgentTuning: Enabling generalized agent abilities for LLMs\", \"Claude-2\", \"GPT-4 Code Interpreter\", \"LLaMA: Open and efficient foundation language models\", \"LLaMA 2: Open foundation and fine-tuned chat models\", \"HuggingGPT: Solving AI tasks with ChatGPT and its friends in Hugging Face\", \"Visual ChatGPT: Talking, drawing and editing with visual foundation models\", \"MMReACT: Prompting ChatGPT for multimodal reasoning and action\", \"Training verifiers to solve math word problems\", \"WebGPT: Browser-assisted question-answering with human feedback\", \"Gorilla: Large language model connected with massive APIs\", \"ToolAlpaca: Generalized tool learning for language models with 3000 simulated cases\", \"Tool learning with foundation models\", \"How abilities in large language models are affected by supervised fine-tuning data composition\", \"Tora: A tool-integrated reasoning agent for mathematical problem solving\", \"Scaling instruction-finetuned language models\", \"ToolFormers: Language models can teach themselves to use tools\", \"Multi-party chat: Conversational agents in group settings with humans and models\", \"ChatGPT is not enough: Enhancing large language models with knowledge graphs for fact-aware language modeling\", \"GPT4Tools: Teaching large language model to use tools via self-instruction\", \"Tree of thoughts: Deliberate problem solving with large language models\", \"REACT: Synergizing reasoning and acting in language models\", \"ModelScope-Agent: Building your customizable agent system with open-source large language models\", \"Communicative agents for software development\"], \"Citation\": []}\n[The End of Assistant's Predicted Answer]\n\n[System]\nWe would like to request your feedback on the performance of the AI assistant in response to the user question displayed above according to the gold answer. Please use the following listed aspects and their descriptions as evaluation criteria:\n    - Accuracy and Hallucinations: The assistant's answer is semantically consistent with the gold answer; The numerical value and order need to be accurate, and there should be no hallucinations.\n    - Completeness: Referring to the reference answers, the assistant's answer should contain all the key points needed to answer the user's question; further elaboration on these key points can be omitted.\nPlease rate whether this answer is suitable for the question. Please note that the gold answer can be considered as a correct answer to the question.\n\nThe assistant receives an overall score on a scale of 1 to 100, where a higher score indicates better overall performance.\nPlease note that if the assistant's answer and the gold answer fully meet the above criteria, its overall rating should be the full marks (100).\nPlease first provide a comprehensive explanation of your evaluation, avoiding any potential bias.\nThen, output a line indicating the score of the Assistant.\n\nPLEASE OUTPUT WITH THE FOLLOWING FORMAT, WHERE THE SCORE IS A SCALE OF 1 TO 100 BY STRICTLY FOLLOWING THIS FORMAT: \"[[score]]\", FOR EXAMPLE \"Rating: [[100]]\":\n<start output>\nEvaluation evidence: your evluation explanation here, no more than 100 words\nRating: [[score]]\n<end output>\n\nNow, start your evaluation:",
        "generate_response": "{\"Reference\": [\"A Survey on Evaluation of Large Language Models\", \"Vicuna: An Open-Source Chatbot Impressing GPT-4 with $90 \\% *$ ChatGPT Quality\", \"ChatLaw: Open-Source Legal Large Language Model with Integrated External Knowledge Bases\", \"Compositional Semantic Parsing with Large Language Models\", \"Open LLM Leaderboard\", \"How Close is ChatGPT to Human Experts? Comparison Corpus, Evaluation, and Detection\", \"REALM: Retrieval-Augmented Language Model PreTraining\", \"Rethinking with Retrieval: Faithful Large Language Model Inference\", \"Measuring Massive Multitask Language Understanding\", \"Atlas: Few-shot Learning with Retrieval Augmented Language Models\", \"Chain-of-thought prompting elicits reasoning in large language models\", \"Reflextion: Language agents with verbal reinforcement learning\", \"MemoryBank: Enhancing large language models with long-term memory\", \"Tool learning with foundation models\", \"ToolLLM: Facilitating large language models to master 16000+ real-world APIs\", \"Global-to-local progressive fine-tuning for multi-LLM systems\", \"ToolEval: A method for evaluating the tool invocation capabilities of large language models\", \"BabyAGI\", \"AutoGPT\", \"Voyager: An open-ended embodied agent with large language models\", \"Ghost in the Minecraft: Generally capable agents for open-world environments via large language models with text-based knowledge and memory\", \"MetaGPT\", \"ChatDev\", \"AutoGen: Enabling next-gen LLM applications via multiagent conversation framework\", \"FiREACT: Toward language agent fine-tuning\", \"AgentTuning: Enabling generalized agent abilities for LLMs\", \"Claude-2\", \"GPT-4 Code Interpreter\", \"LLaMA: Open and efficient foundation language models\", \"LLaMA 2: Open foundation and fine-tuned chat models\", \"HuggingGPT: Solving AI tasks with ChatGPT and its friends in Hugging Face\", \"Visual ChatGPT: Talking, drawing and editing with visual foundation models\", \"MMReACT: Prompting ChatGPT for multimodal reasoning and action\", \"Training verifiers to solve math word problems\", \"WebGPT: Browser-assisted question-answering with human feedback\", \"Gorilla: Large language model connected with massive APIs\", \"ToolAlpaca: Generalized tool learning for language models with 3000 simulated cases\", \"Tool learning with foundation models\", \"How abilities in large language models are affected by supervised fine-tuning data composition\", \"Tora: A tool-integrated reasoning agent for mathematical problem solving\", \"Scaling instruction-finetuned language models\", \"ToolFormers: Language models can teach themselves to use tools\", \"Multi-party chat: Conversational agents in group settings with humans and models\", \"ChatGPT is not enough: Enhancing large language models with knowledge graphs for fact-aware language modeling\", \"GPT4Tools: Teaching large language model to use tools via self-instruction\", \"Tree of thoughts: Deliberate problem solving with large language models\", \"REACT: Synergizing reasoning and acting in language models\", \"ModelScope-Agent: Building your customizable agent system with open-source large language models\", \"Communicative agents for software development\"], \"Citation\": []}",
        "eval_response": "Evaluation evidence: The assistant's answer does not match the gold answer in terms of accuracy and completeness. The gold answer specifies one reference and two citations, while the assistant's answer lists numerous references that are not mentioned in the gold answer and no citations. This discrepancy indicates a significant error in identifying the correct citation and reference relationships, leading to both inaccuracies and hallucinations in the response.\n\nRating: [[10]]",
        "score": 10
    },
    {
        "level": 3,
        "set": 1,
        "length": 45743,
        "type": "paper",
        "language": "en",
        "question": "Extreme Compression of Large Language Models via Additive Quantization",
        "instruction": "We hope you will carefully study the provided papers and determine the citation relationships between them. Please follow the instructions below strictly to complete the task:\n\n#Specific Requirements:\n1. Reference: When a given paper mentions other provided papers, those other papers are considered as \"references\" for the given paper. To summarize in this specific context, references are about what the given paper is using.\n2. Citation: Conversely, when other provided papers mention the given paper in their works, the given paper is being \"cited\" by those other papers. To summarize in this specific context, citations are about who is using the given paper.\n3. Given a paper, you need to determine the citation or reference relationship between this paper and the other papers. Do not consider papers that are not provided.\n3. Please present the paper titles in a json format as follows: {{\"Reference\":[\"Reference Title 1\", \"Reference Title 2\", ..., \"Reference Title n\"], \"Citation\":[\"Citation Title 1\", \"Citation Title 2\", ..., \"Citation Title n\"]}}.\n4. If a paper does not have any references or citations, please leave the corresponding list empty, e.g.{{\"Refernce\":[]}}, {{\"Citation\":[]}}.",
        "prompt_template": "#Papers:\n{docs}\n\n{instruction}\n\n#The paper you need to analyze:\n{question}",
        "doc": [
            "2401.06118.md",
            "2405.15756.md",
            "2310.06825.md"
        ],
        "answer": {
            "Reference": [
                "# Mistral 7B "
            ],
            "Citation": [
                "# Sparse Expansion and Neuronal Disentanglement "
            ]
        },
        "shuffle_doc": true,
        "id": "8878fb61-6996-4984-9774-5ce9fb506d37",
        "prompt": "[Question]\n#Papers:\n{docs}\n\nWe hope you will carefully study the provided papers and determine the citation relationships between them. Please follow the instructions below strictly to complete the task:\n\n#Specific Requirements:\n1. Reference: When a given paper mentions other provided papers, those other papers are considered as \"references\" for the given paper. To summarize in this specific context, references are about what the given paper is using.\n2. Citation: Conversely, when other provided papers mention the given paper in their works, the given paper is being \"cited\" by those other papers. To summarize in this specific context, citations are about who is using the given paper.\n3. Given a paper, you need to determine the citation or reference relationship between this paper and the other papers. Do not consider papers that are not provided.\n3. Please present the paper titles in a json format as follows: {{\"Reference\":[\"Reference Title 1\", \"Reference Title 2\", ..., \"Reference Title n\"], \"Citation\":[\"Citation Title 1\", \"Citation Title 2\", ..., \"Citation Title n\"]}}.\n4. If a paper does not have any references or citations, please leave the corresponding list empty, e.g.{{\"Refernce\":[]}}, {{\"Citation\":[]}}.\n\n#The paper you need to analyze:\nExtreme Compression of Large Language Models via Additive Quantization\n\n[Gold Answer]\n{'Reference': ['# Mistral 7B '], 'Citation': ['# Sparse Expansion and Neuronal Disentanglement ']}\n\n[The Start of Assistant's Predicted Answer]\n{\"Reference\": [\"Mistral 7B\"], \"Citation\": [\"A Survey on Evaluation of Large Language Models\", \"Vicuna: An Open-Source Chatbot Impressing GPT-4 with $90 \\% *$ ChatGPT Quality\", \"ChatLaw: Open-Source Legal Large Language Model with Integrated External Knowledge Bases\", \"Compositional Semantic Parsing with Large Language Models\", \"Open LLM Leaderboard\", \"How Close is ChatGPT to Human Experts? Comparison Corpus, Evaluation, and Detection\", \"REALM: Retrieval-Augmented Language Model PreTraining\", \"Rethinking with Retrieval: Faithful Large Language Model Inference\", \"Measuring Massive Multitask Language Understanding\", \"Atlas: Few-shot Learning with Retrieval Augmented Language Models\", \"Pythia: A suite for analyzing large language models across training and scaling\", \"Spqr: A sparse-quantized representation for near-lossless $11 \\mathrm{~m}$ weight compression\", \"Qmoe: Practical sub-1-bit compression of trillion-parameter models\", \"Gptq: Accurate post-training quantization for generative pretrained transformers\", \"Optimal Brain Compression: A framework for accurate posttraining quantization and pruning\", \"Mixtral of experts\", \"Squeezellm: Dense-and-sparse quantization\", \"Adam: A method for stochastic optimization\", \"Sparse fine-tuning for inference acceleration of large language models\", \"Performance guaranteed network acceleration via high-order residual quantization\", \"Awq: Activation-aware weight quantization for llm compression and acceleration\", \"Lsq++: Lower running time and higher recall in multi-codebook quantization\", \"Look-ups are not (yet) all you need for deep learning inference\", \"Bloom: A 176b-parameter open-access multilingual language model\", \"Llama: Open and efficient foundation language models\", \"Quip\\#: Quip with lattice codebooks\"]}\n[The End of Assistant's Predicted Answer]\n\n[System]\nWe would like to request your feedback on the performance of the AI assistant in response to the user question displayed above according to the gold answer. Please use the following listed aspects and their descriptions as evaluation criteria:\n    - Accuracy and Hallucinations: The assistant's answer is semantically consistent with the gold answer; The numerical value and order need to be accurate, and there should be no hallucinations.\n    - Completeness: Referring to the reference answers, the assistant's answer should contain all the key points needed to answer the user's question; further elaboration on these key points can be omitted.\nPlease rate whether this answer is suitable for the question. Please note that the gold answer can be considered as a correct answer to the question.\n\nThe assistant receives an overall score on a scale of 1 to 100, where a higher score indicates better overall performance.\nPlease note that if the assistant's answer and the gold answer fully meet the above criteria, its overall rating should be the full marks (100).\nPlease first provide a comprehensive explanation of your evaluation, avoiding any potential bias.\nThen, output a line indicating the score of the Assistant.\n\nPLEASE OUTPUT WITH THE FOLLOWING FORMAT, WHERE THE SCORE IS A SCALE OF 1 TO 100 BY STRICTLY FOLLOWING THIS FORMAT: \"[[score]]\", FOR EXAMPLE \"Rating: [[100]]\":\n<start output>\nEvaluation evidence: your evluation explanation here, no more than 100 words\nRating: [[score]]\n<end output>\n\nNow, start your evaluation:",
        "generate_response": "{\"Reference\": [\"Mistral 7B\"], \"Citation\": [\"A Survey on Evaluation of Large Language Models\", \"Vicuna: An Open-Source Chatbot Impressing GPT-4 with $90 \\% *$ ChatGPT Quality\", \"ChatLaw: Open-Source Legal Large Language Model with Integrated External Knowledge Bases\", \"Compositional Semantic Parsing with Large Language Models\", \"Open LLM Leaderboard\", \"How Close is ChatGPT to Human Experts? Comparison Corpus, Evaluation, and Detection\", \"REALM: Retrieval-Augmented Language Model PreTraining\", \"Rethinking with Retrieval: Faithful Large Language Model Inference\", \"Measuring Massive Multitask Language Understanding\", \"Atlas: Few-shot Learning with Retrieval Augmented Language Models\", \"Pythia: A suite for analyzing large language models across training and scaling\", \"Spqr: A sparse-quantized representation for near-lossless $11 \\mathrm{~m}$ weight compression\", \"Qmoe: Practical sub-1-bit compression of trillion-parameter models\", \"Gptq: Accurate post-training quantization for generative pretrained transformers\", \"Optimal Brain Compression: A framework for accurate posttraining quantization and pruning\", \"Mixtral of experts\", \"Squeezellm: Dense-and-sparse quantization\", \"Adam: A method for stochastic optimization\", \"Sparse fine-tuning for inference acceleration of large language models\", \"Performance guaranteed network acceleration via high-order residual quantization\", \"Awq: Activation-aware weight quantization for llm compression and acceleration\", \"Lsq++: Lower running time and higher recall in multi-codebook quantization\", \"Look-ups are not (yet) all you need for deep learning inference\", \"Bloom: A 176b-parameter open-access multilingual language model\", \"Llama: Open and efficient foundation language models\", \"Quip\\#: Quip with lattice codebooks\"]}",
        "eval_response": "Evaluation evidence: The assistant's answer inaccurately lists numerous citations that are not mentioned in the gold answer, indicating a significant hallucination issue. The reference list correctly includes \"Mistral 7B,\" aligning with the gold answer. However, the extensive and incorrect citation list deviates from the required accuracy and introduces irrelevant data, impacting the completeness and accuracy negatively.\n\nRating: [[20]]",
        "score": 20
    },
    {
        "level": 3,
        "set": 2,
        "length": 94152,
        "type": "paper",
        "language": "en",
        "question": "How Johnny Can Persuade LLMs to Jailbreak Them: Rethinking Persuasion to Challenge AI Safety by Humanizing LLMs",
        "instruction": "We hope you will carefully study the provided papers and determine the citation relationships between them. Please follow the instructions below strictly to complete the task:\n\n#Specific Requirements:\n1. Reference: When a given paper mentions other provided papers, those other papers are considered as \"references\" for the given paper. To summarize in this specific context, references are about what the given paper is using.\n2. Citation: Conversely, when other provided papers mention the given paper in their works, the given paper is being \"cited\" by those other papers. To summarize in this specific context, citations are about who is using the given paper.\n3. Given a paper, you need to determine the citation or reference relationship between this paper and the other papers. Do not consider papers that are not provided.\n3. Please present the paper titles in a json format as follows: {{\"Reference\":[\"Reference Title 1\", \"Reference Title 2\", ..., \"Reference Title n\"], \"Citation\":[\"Citation Title 1\", \"Citation Title 2\", ..., \"Citation Title n\"]}}.\n4. If a paper does not have any references or citations, please leave the corresponding list empty, e.g.{{\"Refernce\":[]}}, {{\"Citation\":[]}}.",
        "prompt_template": "#Papers:\n{docs}\n\n{instruction}\n\n#The paper you need to analyze:\n{question}",
        "doc": [
            "2401.06373.md",
            "2405.18718.md",
            "2405.20947.md",
            "2311.03348.md"
        ],
        "answer": {
            "Reference": [
                "# Scalable and Transferable Black-Box Jailbreaks for Language Models via Persona Modulation "
            ],
            "Citation": [
                "# Efficient Model-agnostic Alignment via Bayesian Persuasion ",
                "# OR-Bench: An Over-Refusal Benchmark for Large Language Models "
            ]
        },
        "shuffle_doc": true,
        "id": "32bec2af-b428-45a3-8c7b-e946de1fbaca",
        "prompt": "[Question]\n#Papers:\n{docs}\n\nWe hope you will carefully study the provided papers and determine the citation relationships between them. Please follow the instructions below strictly to complete the task:\n\n#Specific Requirements:\n1. Reference: When a given paper mentions other provided papers, those other papers are considered as \"references\" for the given paper. To summarize in this specific context, references are about what the given paper is using.\n2. Citation: Conversely, when other provided papers mention the given paper in their works, the given paper is being \"cited\" by those other papers. To summarize in this specific context, citations are about who is using the given paper.\n3. Given a paper, you need to determine the citation or reference relationship between this paper and the other papers. Do not consider papers that are not provided.\n3. Please present the paper titles in a json format as follows: {{\"Reference\":[\"Reference Title 1\", \"Reference Title 2\", ..., \"Reference Title n\"], \"Citation\":[\"Citation Title 1\", \"Citation Title 2\", ..., \"Citation Title n\"]}}.\n4. If a paper does not have any references or citations, please leave the corresponding list empty, e.g.{{\"Refernce\":[]}}, {{\"Citation\":[]}}.\n\n#The paper you need to analyze:\nHow Johnny Can Persuade LLMs to Jailbreak Them: Rethinking Persuasion to Challenge AI Safety by Humanizing LLMs\n\n[Gold Answer]\n{'Reference': ['# Scalable and Transferable Black-Box Jailbreaks for Language Models via Persona Modulation '], 'Citation': ['# Efficient Model-agnostic Alignment via Bayesian Persuasion ', '# OR-Bench: An Over-Refusal Benchmark for Large Language Models ']}\n\n[The Start of Assistant's Predicted Answer]\n{\"Reference\":[\"Llama: An Open-Source Chatbot Impressing GPT-4 with $90 \\% *$ ChatGPT Quality\", \"GPT-4 Technical Report\", \"AdvBench: A Benchmark for Adversarial Attacks on Large Language Models\", \"Constitutional AI: Harmlessness from AI Feedback\", \"Dark Patterns at Scale: Findings from a Crawl of 11k Shopping Websites\", \"Jailbreaker: Automated Jailbreak Across Multiple Large Language Model Chatbots\", \"Certifying $11 \\mathrm{~m}$ Safety Against Adversarial Prompting\", \"Open Sesame! Universal Black Box Jailbreaking of Large Language Models\", \"Rain: Your Language Models Can Align Themselves Without Finetuning\", \"Autodan: Generating Stealthy Jailbreak Prompts on Aligned Large Language Models\", \"Jailbreaking ChatGPT via Prompt Engineering: An Empirical Study\", \"Shining a Light on Dark Patterns\", \"Use of llms for Illicit Purposes: Threats, Prevention Measures, and Vulnerabilities\", \"Finetuning Aligned Language Models Compromises Safety, Even When Users Do Not Intend To!\", \"Adversarial Demonstration Attacks on Large Language Models\", \"Jailbreak and Guard Aligned Language Models with Only Few In-Context Demonstrations\", \"The Earth Is Flat Because...: Investigating llms' Belief Towards Misinformation via Persuasive Conversation\", \"Shadow Alignment: The Ease of Subverting Safely-Aligned Language Models\", \"Low-Resource Languages Jailbreak GPT-4\", \"Gptfuzzer: Red Teaming Large Language Models with Auto-Generated Jailbreak Prompts\", \"GPT-4 Is Too Smart to Be Safe: Stealthy Chat with $11 \\mathrm{~ms}$ via Cipher\", \"Universal and Transferable Adversarial Attacks on Aligned Language Models\"], \"Citation\":[\"Efficient Model-agnostic Alignment via Bayesian Persuasion\"]}\n[The End of Assistant's Predicted Answer]\n\n[System]\nWe would like to request your feedback on the performance of the AI assistant in response to the user question displayed above according to the gold answer. Please use the following listed aspects and their descriptions as evaluation criteria:\n    - Accuracy and Hallucinations: The assistant's answer is semantically consistent with the gold answer; The numerical value and order need to be accurate, and there should be no hallucinations.\n    - Completeness: Referring to the reference answers, the assistant's answer should contain all the key points needed to answer the user's question; further elaboration on these key points can be omitted.\nPlease rate whether this answer is suitable for the question. Please note that the gold answer can be considered as a correct answer to the question.\n\nThe assistant receives an overall score on a scale of 1 to 100, where a higher score indicates better overall performance.\nPlease note that if the assistant's answer and the gold answer fully meet the above criteria, its overall rating should be the full marks (100).\nPlease first provide a comprehensive explanation of your evaluation, avoiding any potential bias.\nThen, output a line indicating the score of the Assistant.\n\nPLEASE OUTPUT WITH THE FOLLOWING FORMAT, WHERE THE SCORE IS A SCALE OF 1 TO 100 BY STRICTLY FOLLOWING THIS FORMAT: \"[[score]]\", FOR EXAMPLE \"Rating: [[100]]\":\n<start output>\nEvaluation evidence: your evluation explanation here, no more than 100 words\nRating: [[score]]\n<end output>\n\nNow, start your evaluation:",
        "generate_response": "{\"Reference\":[\"Llama: An Open-Source Chatbot Impressing GPT-4 with $90 \\% *$ ChatGPT Quality\", \"GPT-4 Technical Report\", \"AdvBench: A Benchmark for Adversarial Attacks on Large Language Models\", \"Constitutional AI: Harmlessness from AI Feedback\", \"Dark Patterns at Scale: Findings from a Crawl of 11k Shopping Websites\", \"Jailbreaker: Automated Jailbreak Across Multiple Large Language Model Chatbots\", \"Certifying $11 \\mathrm{~m}$ Safety Against Adversarial Prompting\", \"Open Sesame! Universal Black Box Jailbreaking of Large Language Models\", \"Rain: Your Language Models Can Align Themselves Without Finetuning\", \"Autodan: Generating Stealthy Jailbreak Prompts on Aligned Large Language Models\", \"Jailbreaking ChatGPT via Prompt Engineering: An Empirical Study\", \"Shining a Light on Dark Patterns\", \"Use of llms for Illicit Purposes: Threats, Prevention Measures, and Vulnerabilities\", \"Finetuning Aligned Language Models Compromises Safety, Even When Users Do Not Intend To!\", \"Adversarial Demonstration Attacks on Large Language Models\", \"Jailbreak and Guard Aligned Language Models with Only Few In-Context Demonstrations\", \"The Earth Is Flat Because...: Investigating llms' Belief Towards Misinformation via Persuasive Conversation\", \"Shadow Alignment: The Ease of Subverting Safely-Aligned Language Models\", \"Low-Resource Languages Jailbreak GPT-4\", \"Gptfuzzer: Red Teaming Large Language Models with Auto-Generated Jailbreak Prompts\", \"GPT-4 Is Too Smart to Be Safe: Stealthy Chat with $11 \\mathrm{~ms}$ via Cipher\", \"Universal and Transferable Adversarial Attacks on Aligned Language Models\"], \"Citation\":[\"Efficient Model-agnostic Alignment via Bayesian Persuasion\"]}",
        "eval_response": "Evaluation evidence: The assistant's answer does not align with the gold answer in terms of accuracy and completeness. The references and citations listed by the assistant do not match those in the gold answer, indicating a significant deviation. The assistant's response includes numerous titles not mentioned in the gold answer, suggesting hallucinations or errors in identifying the correct documents. This misalignment shows a lack of accuracy in determining the correct citation and reference relationships for the given paper.\n\nRating: [[20]]",
        "score": 20
    },
    {
        "level": 3,
        "set": 3,
        "length": 193584,
        "type": "paper",
        "language": "en",
        "question": "Hallucination is Inevitable: <br> An Innate Limitation of Large Language Models",
        "instruction": "We hope you will carefully study the provided papers and determine the citation relationships between them. Please follow the instructions below strictly to complete the task:\n\n#Specific Requirements:\n1. Reference: When a given paper mentions other provided papers, those other papers are considered as \"references\" for the given paper. To summarize in this specific context, references are about what the given paper is using.\n2. Citation: Conversely, when other provided papers mention the given paper in their works, the given paper is being \"cited\" by those other papers. To summarize in this specific context, citations are about who is using the given paper.\n3. Given a paper, you need to determine the citation or reference relationship between this paper and the other papers. Do not consider papers that are not provided.\n3. Please present the paper titles in a json format as follows: {{\"Reference\":[\"Reference Title 1\", \"Reference Title 2\", ..., \"Reference Title n\"], \"Citation\":[\"Citation Title 1\", \"Citation Title 2\", ..., \"Citation Title n\"]}}.\n4. If a paper does not have any references or citations, please leave the corresponding list empty, e.g.{{\"Refernce\":[]}}, {{\"Citation\":[]}}.",
        "prompt_template": "#Papers:\n{docs}\n\n{instruction}\n\n#The paper you need to analyze:\n{question}",
        "doc": [
            "2401.11817.md",
            "2404.18852.md",
            "2405.12910.md",
            "2405.12999.md",
            "2405.15652.md",
            "2405.20404.md",
            "2310.10501.md",
            "2311.05232.md",
            "2401.07897.md",
            "2401.08358.md",
            "2401.09395.md"
        ],
        "answer": {
            "Reference": [
                "# NeMo Guardrails: A Toolkit for Controllable and Safe LLM Applications with Programmable Rails ",
                "# The Pitfalls of Defining Hallucination ",
                "# Hallucination Detection and Hallucination Mitigation: An Investigation ",
                "# Caught in the Quicksand of Reasoning, Far from AGI Summit: Evaluating LLMs' Mathematical and Coding Competency through Ontology-guided Interventions "
            ],
            "Citation": [
                "# VERT: Verified Equivalent Rust Transpilation with Large Language Models as Few-Shot Learners ",
                "# Topic Modelling Case Law Using a Large Language Model and a New Taxonomy for UK Law: AI Insights into Summary Judgment ",
                "# An ASSESSMENT OF MODEL-ON-MODEL DECEPTION ",
                "# $\\mathrm{L}^{2} \\cdot \\mathrm{M}=\\mathrm{C}^{2}$ <br> Large Language Models as Covert Channels... a Systematic Analysis ",
                "# XPrompt: Explaining Large Language Model's Generation via Joint Prompt Attribution "
            ]
        },
        "shuffle_doc": true,
        "id": "8d163c29-7699-4d01-a911-a69bd5940d41",
        "prompt": "[Question]\n#Papers:\n{docs}\n\nWe hope you will carefully study the provided papers and determine the citation relationships between them. Please follow the instructions below strictly to complete the task:\n\n#Specific Requirements:\n1. Reference: When a given paper mentions other provided papers, those other papers are considered as \"references\" for the given paper. To summarize in this specific context, references are about what the given paper is using.\n2. Citation: Conversely, when other provided papers mention the given paper in their works, the given paper is being \"cited\" by those other papers. To summarize in this specific context, citations are about who is using the given paper.\n3. Given a paper, you need to determine the citation or reference relationship between this paper and the other papers. Do not consider papers that are not provided.\n3. Please present the paper titles in a json format as follows: {{\"Reference\":[\"Reference Title 1\", \"Reference Title 2\", ..., \"Reference Title n\"], \"Citation\":[\"Citation Title 1\", \"Citation Title 2\", ..., \"Citation Title n\"]}}.\n4. If a paper does not have any references or citations, please leave the corresponding list empty, e.g.{{\"Refernce\":[]}}, {{\"Citation\":[]}}.\n\n#The paper you need to analyze:\nHallucination is Inevitable: <br> An Innate Limitation of Large Language Models\n\n[Gold Answer]\n{'Reference': ['# NeMo Guardrails: A Toolkit for Controllable and Safe LLM Applications with Programmable Rails ', '# The Pitfalls of Defining Hallucination ', '# Hallucination Detection and Hallucination Mitigation: An Investigation ', \"# Caught in the Quicksand of Reasoning, Far from AGI Summit: Evaluating LLMs' Mathematical and Coding Competency through Ontology-guided Interventions \"], 'Citation': ['# VERT: Verified Equivalent Rust Transpilation with Large Language Models as Few-Shot Learners ', '# Topic Modelling Case Law Using a Large Language Model and a New Taxonomy for UK Law: AI Insights into Summary Judgment ', '# An ASSESSMENT OF MODEL-ON-MODEL DECEPTION ', '# $\\\\mathrm{L}^{2} \\\\cdot \\\\mathrm{M}=\\\\mathrm{C}^{2}$ <br> Large Language Models as Covert Channels... a Systematic Analysis ', \"# XPrompt: Explaining Large Language Model's Generation via Joint Prompt Attribution \"]}\n\n[The Start of Assistant's Predicted Answer]\n{\"Reference\":[\"A Simple Recipe towards Reducing Hallucination in Neural Surface Realisation\", \"Chain-of-Verification Reduces Hallucination in Large Language Models\", \"Neural Path Hunter: Reducing Hallucination in Dialogue Systems via Path Grounding\", \"Looking for a Needle in a Haystack: A Comprehensive Study of Hallucinations in Neural Machine Translation\", \"TruthfulQA: Measuring How Models Mimic Human Falsehoods\", \"Exposing Attention Glitches with Flip-Flop Language Modeling\", \"Check Your Facts and Try Again: Improving Large Language Models with External Knowledge and Automated Feedback\", \"Retrieval Augmentation Reduces Hallucination in Conversation\", \"Trust Your Evidence: Hallucinate Less with Context-aware Decoding\", \"Chain-of-Thought Prompting Elicits Reasoning in Large Language Models\", \"Tree of Thoughts: Deliberate Problem Solving with Large Language Models\", \"Don't Pour Cereal into Coffee: Differentiable Temporal Logic for Temporal Action Segmentation\"], \"Citation\":[\"L2M=C2 Large Language Models as Covert Channels... a Systematic Analysis\", \"Hallucination Detection and Hallucination Mitigation: An Investigation\", \"Topic Modelling Case Law Using a Large Language Model and a New Taxonomy for UK Law: AI Insights into Summary Judgment\", \"A Survey on Hallucination in Large Language Models: Principles, Taxonomy, Challenges, and Open Questions\"]}\n[The End of Assistant's Predicted Answer]\n\n[System]\nWe would like to request your feedback on the performance of the AI assistant in response to the user question displayed above according to the gold answer. Please use the following listed aspects and their descriptions as evaluation criteria:\n    - Accuracy and Hallucinations: The assistant's answer is semantically consistent with the gold answer; The numerical value and order need to be accurate, and there should be no hallucinations.\n    - Completeness: Referring to the reference answers, the assistant's answer should contain all the key points needed to answer the user's question; further elaboration on these key points can be omitted.\nPlease rate whether this answer is suitable for the question. Please note that the gold answer can be considered as a correct answer to the question.\n\nThe assistant receives an overall score on a scale of 1 to 100, where a higher score indicates better overall performance.\nPlease note that if the assistant's answer and the gold answer fully meet the above criteria, its overall rating should be the full marks (100).\nPlease first provide a comprehensive explanation of your evaluation, avoiding any potential bias.\nThen, output a line indicating the score of the Assistant.\n\nPLEASE OUTPUT WITH THE FOLLOWING FORMAT, WHERE THE SCORE IS A SCALE OF 1 TO 100 BY STRICTLY FOLLOWING THIS FORMAT: \"[[score]]\", FOR EXAMPLE \"Rating: [[100]]\":\n<start output>\nEvaluation evidence: your evluation explanation here, no more than 100 words\nRating: [[score]]\n<end output>\n\nNow, start your evaluation:",
        "generate_response": "{\"Reference\":[\"A Simple Recipe towards Reducing Hallucination in Neural Surface Realisation\", \"Chain-of-Verification Reduces Hallucination in Large Language Models\", \"Neural Path Hunter: Reducing Hallucination in Dialogue Systems via Path Grounding\", \"Looking for a Needle in a Haystack: A Comprehensive Study of Hallucinations in Neural Machine Translation\", \"TruthfulQA: Measuring How Models Mimic Human Falsehoods\", \"Exposing Attention Glitches with Flip-Flop Language Modeling\", \"Check Your Facts and Try Again: Improving Large Language Models with External Knowledge and Automated Feedback\", \"Retrieval Augmentation Reduces Hallucination in Conversation\", \"Trust Your Evidence: Hallucinate Less with Context-aware Decoding\", \"Chain-of-Thought Prompting Elicits Reasoning in Large Language Models\", \"Tree of Thoughts: Deliberate Problem Solving with Large Language Models\", \"Don't Pour Cereal into Coffee: Differentiable Temporal Logic for Temporal Action Segmentation\"], \"Citation\":[\"L2M=C2 Large Language Models as Covert Channels... a Systematic Analysis\", \"Hallucination Detection and Hallucination Mitigation: An Investigation\", \"Topic Modelling Case Law Using a Large Language Model and a New Taxonomy for UK Law: AI Insights into Summary Judgment\", \"A Survey on Hallucination in Large Language Models: Principles, Taxonomy, Challenges, and Open Questions\"]}",
        "eval_response": "Evaluation evidence: The assistant's answer does not match the gold answer in terms of accuracy and completeness. The references and citations listed in the assistant's answer are entirely different from those in the gold answer, indicating a significant deviation and hallucination. The assistant's response includes titles that are not present in the gold answer, failing to accurately identify the correct citation and reference relationships for the given paper. This results in a lack of semantic consistency and completeness as per the task requirements.\n\nRating: [[10]]",
        "score": 10
    },
    {
        "level": 3,
        "set": 2,
        "length": 98786,
        "type": "paper",
        "language": "en",
        "question": "MedusA: Simple LLM Inference Acceleration Framework with Multiple Decoding Heads",
        "instruction": "We hope you will carefully study the provided papers and determine the citation relationships between them. Please follow the instructions below strictly to complete the task:\n\n#Specific Requirements:\n1. Reference: When a given paper mentions other provided papers, those other papers are considered as \"references\" for the given paper. To summarize in this specific context, references are about what the given paper is using.\n2. Citation: Conversely, when other provided papers mention the given paper in their works, the given paper is being \"cited\" by those other papers. To summarize in this specific context, citations are about who is using the given paper.\n3. Given a paper, you need to determine the citation or reference relationship between this paper and the other papers. Do not consider papers that are not provided.\n3. Please present the paper titles in a json format as follows: {{\"Reference\":[\"Reference Title 1\", \"Reference Title 2\", ..., \"Reference Title n\"], \"Citation\":[\"Citation Title 1\", \"Citation Title 2\", ..., \"Citation Title n\"]}}.\n4. If a paper does not have any references or citations, please leave the corresponding list empty, e.g.{{\"Refernce\":[]}}, {{\"Citation\":[]}}.",
        "prompt_template": "#Papers:\n{docs}\n\n{instruction}\n\n#The paper you need to analyze:\n{question}",
        "doc": [
            "2401.10774.md",
            "2405.17741.md",
            "2405.19261.md",
            "2309.06180.md",
            "2402.02057.md"
        ],
        "answer": {
            "Reference": [
                "# Efficient Memory Management for Large Language Model Serving with PagedAttention ",
                "# Break the Sequential Dependency of LLM Inference Using LOOKAHEAD DECODING "
            ],
            "Citation": [
                "# LoRA-Switch: Boosting the Efficiency of Dynamic LLM Adapters via System-Algorithm Co-design ",
                "# Faster Cascades via Speculative Decoding "
            ]
        },
        "shuffle_doc": true,
        "id": "5d9c9229-de22-4876-b587-6b7628cf1095",
        "prompt": "[Question]\n#Papers:\n{docs}\n\nWe hope you will carefully study the provided papers and determine the citation relationships between them. Please follow the instructions below strictly to complete the task:\n\n#Specific Requirements:\n1. Reference: When a given paper mentions other provided papers, those other papers are considered as \"references\" for the given paper. To summarize in this specific context, references are about what the given paper is using.\n2. Citation: Conversely, when other provided papers mention the given paper in their works, the given paper is being \"cited\" by those other papers. To summarize in this specific context, citations are about who is using the given paper.\n3. Given a paper, you need to determine the citation or reference relationship between this paper and the other papers. Do not consider papers that are not provided.\n3. Please present the paper titles in a json format as follows: {{\"Reference\":[\"Reference Title 1\", \"Reference Title 2\", ..., \"Reference Title n\"], \"Citation\":[\"Citation Title 1\", \"Citation Title 2\", ..., \"Citation Title n\"]}}.\n4. If a paper does not have any references or citations, please leave the corresponding list empty, e.g.{{\"Refernce\":[]}}, {{\"Citation\":[]}}.\n\n#The paper you need to analyze:\nMedusA: Simple LLM Inference Acceleration Framework with Multiple Decoding Heads\n\n[Gold Answer]\n{'Reference': ['# Efficient Memory Management for Large Language Model Serving with PagedAttention ', '# Break the Sequential Dependency of LLM Inference Using LOOKAHEAD DECODING '], 'Citation': ['# LoRA-Switch: Boosting the Efficiency of Dynamic LLM Adapters via System-Algorithm Co-design ', '# Faster Cascades via Speculative Decoding ']}\n\n[The Start of Assistant's Predicted Answer]\n{\"Reference\":[\"Aakanksha Chowdhery, Sharan Narang, Jacob Devlin, Maarten Bosma, Gaurav Mishra, Adam Roberts, Paul Barham, Hyung Won Chung, Charles Sutton, Sebastian Gehrmann, et al. Palm: Scaling language modeling with pathways\", \"Tim Dettmers, M. Lewis, Sam Shleifer, and Luke Zettlemoyer. 8-bit optimizers via block-wise quantization\", \"Tim Dettmers, Mike Lewis, Younes Belkada, and Luke Zettlemoyer. Llm. int8 (): 8-bit matrix multiplication for transformers at scale\", \"Tim Dettmers, Artidoro Pagnoni, Ari Holtzman, and Luke Zettlemoyer. Qlora: Efficient finetuning of quantized llms\", \"Stefan Elfwing, E. Uchibe, and K. Doya. Sigmoid-weighted linear units for neural network function approximation in reinforcement learning\", \"Angela Fan, Mike Lewis, and Yann Dauphin. Hierarchical neural story generation\", \"Elias Frantar, Saleh Ashkboos, Torsten Hoefler, and Dan Alistarh. Gptq: Accurate post-training quantization for generative pre-trained transformers\", \"Yichao Fu, Peter Bailis, Ion Stoica, and Hao Zhang. Breaking the sequential dependency of $11 \\mathrm{~m}$ inference using lookahead decoding\", \"Zhenyu He, Zexuan Zhong, Tianle Cai, Jason D Lee, and Di He. Rest: Retrieval-based speculative decoding\", \"John Hewitt, Christopher D. Manning, and Percy Liang. Truncation sampling as language model desmoothing\", \"Jordan Hoffmann, Sebastian Borgeaud, Arthur Mensch, Elena Buchatskaya, Trevor Cai, Eliza Rutherford, Diego de Las Casas, Lisa Anne Hendricks, Johannes Welbl, Aidan Clark, et al. Training compute-optimal large language models\", \"Ari Holtzman, Jan Buys, Li Du, Maxwell Forbes, and Yejin Choi. The curious case of neural text degeneration\", \"Edward J. Hu, Yelong Shen, Phillip Wallis, Zeyuan Allen-Zhu, Yuanzhi Li, Shean Wang, and Weizhu Chen. Lora: Low-rank adaptation of large language models\", \"Sehoon Kim, Coleman Hooper, Amir Gholami, Zhen Dong, Xiuyu Li, Sheng Shen, Michael W Mahoney, and Kurt Keutzer. Squeezellm: Dense-and-sparse quantization\", \"Yoon Kim and Alexander M. Rush. Sequence-level knowledge distillation\", \"Ananya Kumar, Aditi Raghunathan, Robbie Jones, Tengyu Ma, and Percy Liang. Fine-tuning can distort pretrained features and underperform out-of-distribution\", \"Woosuk Kwon, Zhuohan Li, Siyuan Zhuang, Ying Sheng, Lianmin Zheng, Cody Hao Yu, Joseph E. Gonzalez, Hao Zhang, and Ion Stoica. Efficient memory management for large language model serving with pagedattention\", \"Yaniv Leviathan, Matan Kalman, and Yossi Matias. Fast inference from transformers via speculative decoding\", \"Ji Lin, Jiaming Tang, Haotian Tang, Shang Yang, Xingyu Dang, and Song Han. Awq: Activation-aware weight quantization for $11 \\mathrm{~m}$ compression and acceleration\", \"Xiaoxuan Liu, Lanxiang Hu, Peter Bailis, Ion Stoica, Zhijie Deng, Alvin Cheung, and Hao Zhang. Online speculative decoding\", \"Clara Meister, Gian Wiher, Tiago Pimentel, and Ryan Cotterell. On the probability-quality paradox in language generation\", \"Clara Meister, Tiago Pimentel, Gian Wiher, and Ryan Cotterell. Locally typical sampling\", \"Xupeng Miao, Gabriele Oliaro, Zhihao Zhang, Xinhao Cheng, Zeyu Wang, Rae Ying Yee Wong, Zhuoming Chen, Daiyaan Arfeen, Reyna Abhyankar, and Zhihao Jia. Specinfer: Accelerating generative llm serving with speculative inference and token tree verification\", \"Long Ouyang, Jeff Wu, Xu Jiang, Diogo Almeida, Carroll L Wainwright, Pamela Mishkin, Chong Zhang, Sandhini Agarwal, Katarina Slama, Alex Ray, et al. Training language models to follow instructions with human feedback\", \"Hugo Touvron, Louis Martin, Kevin Stone, Peter Albert, Amjad Almahairi, Yasmine Babaei, Nikolay Bashlykov, Soumya Batra, Prajjwal Bhargava, Shruti Bhosale, et al. Llama 2: Open foundation and fine-tuned chat models\", \"Lewis Tunstall, Edward Beeching, Nathan Lambert, Nazneen Rajani, Kashif Rasul, Younes Belkada, Shengyi Huang, Leandro von Werra, Clmentine Fourrier, Nathan Habib, Nathan Sarrazin, Omar Sanseviero, Alexander M. Rush, and Thomas Wolf. Zephyr: Direct distillation of lm alignment\", \"Heming Xia, Tao Ge, Si-Qing Chen, Furu Wei, and Zhifang Sui. Speculative decoding: Lossless speedup of autoregressive translation\", \"Guangxuan Xiao, Ji Lin, Mickael Seznec, Hao Wu, Julien Demouth, and Song Han. Smoothquant: Accurate and efficient post-training quantization for large language models\", \"Yisheng Xiao, Lijun Wu, Junliang Guo, Juntao Li, Min Zhang, Tao Qin, and Tie-yan Liu. A survey on non-autoregressive generation for neural machine translation and beyond\", \"Chengxuan Ying, Tianle Cai, Shengjie Luo, Shuxin Zheng, Guolin Ke, Di He, Yanming Shen, and Tie-Yan Liu. Do transformers really perform badly for graph representation?\", \"Susan Zhang, Stephen Roller, Naman Goyal, Mikel Artetxe, Moya Chen, Shuohui Chen, Christopher Dewan, Mona Diab, Xian Li, Xi Victoria Lin, et al. Opt: Open pre-trained transformer language models\", \"Zhenyu Zhang, Ying Sheng, Tianyi Zhou, Tianlong Chen, Lianmin Zheng, Ruisi Cai, Zhao Song, Yuandong Tian, Christopher R, Clark Barrett, et al. H _2 o: Heavy-hitter oracle for efficient generative inference of large language models\", \"Yongchao Zhou, Kaifeng Lyu, Ankit Singh Rawat, Adityy Krishna Menon, Afshin Rostamizadeh, Sanjiv Kumar, Jean-Franois Kagy, and Rishabh Agarwal. Distillspec: Improving speculative decoding via knowledge distillation\"]}, \"Citation\":[\"LoRA-Switch: Boosting the Efficiency of Dynamic LLM Adapters via System-Algorithm Co-design\"]}\n[The End of Assistant's Predicted Answer]\n\n[System]\nWe would like to request your feedback on the performance of the AI assistant in response to the user question displayed above according to the gold answer. Please use the following listed aspects and their descriptions as evaluation criteria:\n    - Accuracy and Hallucinations: The assistant's answer is semantically consistent with the gold answer; The numerical value and order need to be accurate, and there should be no hallucinations.\n    - Completeness: Referring to the reference answers, the assistant's answer should contain all the key points needed to answer the user's question; further elaboration on these key points can be omitted.\nPlease rate whether this answer is suitable for the question. Please note that the gold answer can be considered as a correct answer to the question.\n\nThe assistant receives an overall score on a scale of 1 to 100, where a higher score indicates better overall performance.\nPlease note that if the assistant's answer and the gold answer fully meet the above criteria, its overall rating should be the full marks (100).\nPlease first provide a comprehensive explanation of your evaluation, avoiding any potential bias.\nThen, output a line indicating the score of the Assistant.\n\nPLEASE OUTPUT WITH THE FOLLOWING FORMAT, WHERE THE SCORE IS A SCALE OF 1 TO 100 BY STRICTLY FOLLOWING THIS FORMAT: \"[[score]]\", FOR EXAMPLE \"Rating: [[100]]\":\n<start output>\nEvaluation evidence: your evluation explanation here, no more than 100 words\nRating: [[score]]\n<end output>\n\nNow, start your evaluation:",
        "generate_response": "{\"Reference\":[\"Aakanksha Chowdhery, Sharan Narang, Jacob Devlin, Maarten Bosma, Gaurav Mishra, Adam Roberts, Paul Barham, Hyung Won Chung, Charles Sutton, Sebastian Gehrmann, et al. Palm: Scaling language modeling with pathways\", \"Tim Dettmers, M. Lewis, Sam Shleifer, and Luke Zettlemoyer. 8-bit optimizers via block-wise quantization\", \"Tim Dettmers, Mike Lewis, Younes Belkada, and Luke Zettlemoyer. Llm. int8 (): 8-bit matrix multiplication for transformers at scale\", \"Tim Dettmers, Artidoro Pagnoni, Ari Holtzman, and Luke Zettlemoyer. Qlora: Efficient finetuning of quantized llms\", \"Stefan Elfwing, E. Uchibe, and K. Doya. Sigmoid-weighted linear units for neural network function approximation in reinforcement learning\", \"Angela Fan, Mike Lewis, and Yann Dauphin. Hierarchical neural story generation\", \"Elias Frantar, Saleh Ashkboos, Torsten Hoefler, and Dan Alistarh. Gptq: Accurate post-training quantization for generative pre-trained transformers\", \"Yichao Fu, Peter Bailis, Ion Stoica, and Hao Zhang. Breaking the sequential dependency of $11 \\mathrm{~m}$ inference using lookahead decoding\", \"Zhenyu He, Zexuan Zhong, Tianle Cai, Jason D Lee, and Di He. Rest: Retrieval-based speculative decoding\", \"John Hewitt, Christopher D. Manning, and Percy Liang. Truncation sampling as language model desmoothing\", \"Jordan Hoffmann, Sebastian Borgeaud, Arthur Mensch, Elena Buchatskaya, Trevor Cai, Eliza Rutherford, Diego de Las Casas, Lisa Anne Hendricks, Johannes Welbl, Aidan Clark, et al. Training compute-optimal large language models\", \"Ari Holtzman, Jan Buys, Li Du, Maxwell Forbes, and Yejin Choi. The curious case of neural text degeneration\", \"Edward J. Hu, Yelong Shen, Phillip Wallis, Zeyuan Allen-Zhu, Yuanzhi Li, Shean Wang, and Weizhu Chen. Lora: Low-rank adaptation of large language models\", \"Sehoon Kim, Coleman Hooper, Amir Gholami, Zhen Dong, Xiuyu Li, Sheng Shen, Michael W Mahoney, and Kurt Keutzer. Squeezellm: Dense-and-sparse quantization\", \"Yoon Kim and Alexander M. Rush. Sequence-level knowledge distillation\", \"Ananya Kumar, Aditi Raghunathan, Robbie Jones, Tengyu Ma, and Percy Liang. Fine-tuning can distort pretrained features and underperform out-of-distribution\", \"Woosuk Kwon, Zhuohan Li, Siyuan Zhuang, Ying Sheng, Lianmin Zheng, Cody Hao Yu, Joseph E. Gonzalez, Hao Zhang, and Ion Stoica. Efficient memory management for large language model serving with pagedattention\", \"Yaniv Leviathan, Matan Kalman, and Yossi Matias. Fast inference from transformers via speculative decoding\", \"Ji Lin, Jiaming Tang, Haotian Tang, Shang Yang, Xingyu Dang, and Song Han. Awq: Activation-aware weight quantization for $11 \\mathrm{~m}$ compression and acceleration\", \"Xiaoxuan Liu, Lanxiang Hu, Peter Bailis, Ion Stoica, Zhijie Deng, Alvin Cheung, and Hao Zhang. Online speculative decoding\", \"Clara Meister, Gian Wiher, Tiago Pimentel, and Ryan Cotterell. On the probability-quality paradox in language generation\", \"Clara Meister, Tiago Pimentel, Gian Wiher, and Ryan Cotterell. Locally typical sampling\", \"Xupeng Miao, Gabriele Oliaro, Zhihao Zhang, Xinhao Cheng, Zeyu Wang, Rae Ying Yee Wong, Zhuoming Chen, Daiyaan Arfeen, Reyna Abhyankar, and Zhihao Jia. Specinfer: Accelerating generative llm serving with speculative inference and token tree verification\", \"Long Ouyang, Jeff Wu, Xu Jiang, Diogo Almeida, Carroll L Wainwright, Pamela Mishkin, Chong Zhang, Sandhini Agarwal, Katarina Slama, Alex Ray, et al. Training language models to follow instructions with human feedback\", \"Hugo Touvron, Louis Martin, Kevin Stone, Peter Albert, Amjad Almahairi, Yasmine Babaei, Nikolay Bashlykov, Soumya Batra, Prajjwal Bhargava, Shruti Bhosale, et al. Llama 2: Open foundation and fine-tuned chat models\", \"Lewis Tunstall, Edward Beeching, Nathan Lambert, Nazneen Rajani, Kashif Rasul, Younes Belkada, Shengyi Huang, Leandro von Werra, Clmentine Fourrier, Nathan Habib, Nathan Sarrazin, Omar Sanseviero, Alexander M. Rush, and Thomas Wolf. Zephyr: Direct distillation of lm alignment\", \"Heming Xia, Tao Ge, Si-Qing Chen, Furu Wei, and Zhifang Sui. Speculative decoding: Lossless speedup of autoregressive translation\", \"Guangxuan Xiao, Ji Lin, Mickael Seznec, Hao Wu, Julien Demouth, and Song Han. Smoothquant: Accurate and efficient post-training quantization for large language models\", \"Yisheng Xiao, Lijun Wu, Junliang Guo, Juntao Li, Min Zhang, Tao Qin, and Tie-yan Liu. A survey on non-autoregressive generation for neural machine translation and beyond\", \"Chengxuan Ying, Tianle Cai, Shengjie Luo, Shuxin Zheng, Guolin Ke, Di He, Yanming Shen, and Tie-Yan Liu. Do transformers really perform badly for graph representation?\", \"Susan Zhang, Stephen Roller, Naman Goyal, Mikel Artetxe, Moya Chen, Shuohui Chen, Christopher Dewan, Mona Diab, Xian Li, Xi Victoria Lin, et al. Opt: Open pre-trained transformer language models\", \"Zhenyu Zhang, Ying Sheng, Tianyi Zhou, Tianlong Chen, Lianmin Zheng, Ruisi Cai, Zhao Song, Yuandong Tian, Christopher R, Clark Barrett, et al. H _2 o: Heavy-hitter oracle for efficient generative inference of large language models\", \"Yongchao Zhou, Kaifeng Lyu, Ankit Singh Rawat, Adityy Krishna Menon, Afshin Rostamizadeh, Sanjiv Kumar, Jean-Franois Kagy, and Rishabh Agarwal. Distillspec: Improving speculative decoding via knowledge distillation\"]}, \"Citation\":[\"LoRA-Switch: Boosting the Efficiency of Dynamic LLM Adapters via System-Algorithm Co-design\"]}",
        "eval_response": "Evaluation evidence: The assistant's answer does not match the gold answer in terms of accuracy and completeness. The references and citations listed in the assistant's response are largely unrelated and excessively detailed compared to the gold answer, which clearly specifies only two references and two citations. The assistant's answer includes a long list of unrelated papers, indicating a significant hallucination issue. This deviation from the gold answer suggests a misunderstanding of the task requirements.\n\nRating: [[20]]",
        "score": 20
    },
    {
        "level": 3,
        "set": 3,
        "length": 102320,
        "type": "paper",
        "language": "en",
        "question": "Investigating Data Contamination for Pre-training Language Models",
        "instruction": "We hope you will carefully study the provided papers and determine the citation relationships between them. Please follow the instructions below strictly to complete the task:\n\n#Specific Requirements:\n1. Reference: When a given paper mentions other provided papers, those other papers are considered as \"references\" for the given paper. To summarize in this specific context, references are about what the given paper is using.\n2. Citation: Conversely, when other provided papers mention the given paper in their works, the given paper is being \"cited\" by those other papers. To summarize in this specific context, citations are about who is using the given paper.\n3. Given a paper, you need to determine the citation or reference relationship between this paper and the other papers. Do not consider papers that are not provided.\n3. Please present the paper titles in a json format as follows: {{\"Reference\":[\"Reference Title 1\", \"Reference Title 2\", ..., \"Reference Title n\"], \"Citation\":[\"Citation Title 1\", \"Citation Title 2\", ..., \"Citation Title n\"]}}.\n4. If a paper does not have any references or citations, please leave the corresponding list empty, e.g.{{\"Refernce\":[]}}, {{\"Citation\":[]}}.",
        "prompt_template": "#Papers:\n{docs}\n\n{instruction}\n\n#The paper you need to analyze:\n{question}",
        "doc": [
            "2401.06059.md",
            "2404.18824.md",
            "2405.10632.md",
            "2405.19313.md"
        ],
        "answer": {
            "Reference": [],
            "Citation": [
                "# Benchmarking Benchmark Leakage in Large Language Models ",
                "# Beyond Static AI Evaluations: Advancing Human Interaction Evaluations for LLM Harms and Risks ",
                "# Language Models Trained to do Arithmetic Predict Human Risky and Intertemporal Choice "
            ]
        },
        "shuffle_doc": true,
        "id": "8a42f23d-9c6a-41b2-bf09-198401022ce8",
        "prompt": "[Question]\n#Papers:\n{docs}\n\nWe hope you will carefully study the provided papers and determine the citation relationships between them. Please follow the instructions below strictly to complete the task:\n\n#Specific Requirements:\n1. Reference: When a given paper mentions other provided papers, those other papers are considered as \"references\" for the given paper. To summarize in this specific context, references are about what the given paper is using.\n2. Citation: Conversely, when other provided papers mention the given paper in their works, the given paper is being \"cited\" by those other papers. To summarize in this specific context, citations are about who is using the given paper.\n3. Given a paper, you need to determine the citation or reference relationship between this paper and the other papers. Do not consider papers that are not provided.\n3. Please present the paper titles in a json format as follows: {{\"Reference\":[\"Reference Title 1\", \"Reference Title 2\", ..., \"Reference Title n\"], \"Citation\":[\"Citation Title 1\", \"Citation Title 2\", ..., \"Citation Title n\"]}}.\n4. If a paper does not have any references or citations, please leave the corresponding list empty, e.g.{{\"Refernce\":[]}}, {{\"Citation\":[]}}.\n\n#The paper you need to analyze:\nInvestigating Data Contamination for Pre-training Language Models\n\n[Gold Answer]\n{'Reference': [], 'Citation': ['# Benchmarking Benchmark Leakage in Large Language Models ', '# Beyond Static AI Evaluations: Advancing Human Interaction Evaluations for LLM Harms and Risks ', '# Language Models Trained to do Arithmetic Predict Human Risky and Intertemporal Choice ']}\n\n[The Start of Assistant's Predicted Answer]\n{\"Reference\":[\"A Survey on Evaluation of Large Language Models\", \"Vicuna: An Open-Source Chatbot Impressing GPT-4 with $90 \\% *$ ChatGPT Quality\", \"ChatLaw: Open-Source Legal Large Language Model with Integrated External Knowledge Bases\", \"Compositional Semantic Parsing with Large Language Models\", \"Open LLM Leaderboard\", \"How Close is ChatGPT to Human Experts? Comparison Corpus, Evaluation, and Detection\", \"REALM: Retrieval-Augmented Language Model PreTraining\", \"Rethinking with Retrieval: Faithful Large Language Model Inference\", \"Measuring Massive Multitask Language Understanding\", \"Atlas: Few-shot Learning with Retrieval Augmented Language Models\", \"Language models are few-shot learners\", \"Membership inference attacks from first principles\", \"Quantifying memorization across neural language models\", \"Extracting training data from large language models\", \"Palm: Scaling language modeling with pathways\", \"BERT: Pre-training of deep bidirectional transformers for language understanding\", \"Does learning require memorization? a short tale about a long tail\", \"The pile: An $800 \\mathrm{gb}$ dataset of diverse text for language modeling\", \"Time travel in llms: Tracing data contamination in large language models\", \"Textbooks are all you need\", \"Measuring massive multitask language understanding\", \"An empirical analysis of compute-optimal large language model training\", \"Preventing generation of verbatim memorization in language models gives a false sense of privacy\", \"Membership inference attack susceptibility of clinical language models\", \"Pretraining language models with human preferences\", \"Textbooks are all you need ii: phi-1.5 technical report\", \"Holistic evaluation of language models\", \"ROUGE: A package for automatic evaluation of summaries\", \"Data contamination: From memorization to exploitation\", \"Membership inference on word embedding and beyond\", \"Membership inference attacks against language models via neighbourhood comparison\", \"Quantifying privacy risks of masked language models using membership inference attacks\", \"Abstractive text summarization using sequence-to-sequence RNNs and beyond\", \"Scalable extraction of training data from (production) language models\", \"Modern neural networks generalize on small data sets\", \"Gpt-4 technical report\", \"Proving test set contamination in black box language models\", \"Language models are unsupervised multitask learners\", \"SQUAD: 100,000+ questions for machine comprehension of text\", \"Recursive deep models for semantic compositionality over a sentiment treebank\", \"Llama: Open and efficient foundation language models\", \"Llama 2: Open foundation and fine-tuned chat models\", \"How far can camels go? exploring the state of instruction tuning on open resources\", \"Rethinking benchmark and contamination for language models with rephrased samples\", \"Towards a unified multi-dimensional evaluator for text generation\"], \"Citation\":[]}\n[The End of Assistant's Predicted Answer]\n\n[System]\nWe would like to request your feedback on the performance of the AI assistant in response to the user question displayed above according to the gold answer. Please use the following listed aspects and their descriptions as evaluation criteria:\n    - Accuracy and Hallucinations: The assistant's answer is semantically consistent with the gold answer; The numerical value and order need to be accurate, and there should be no hallucinations.\n    - Completeness: Referring to the reference answers, the assistant's answer should contain all the key points needed to answer the user's question; further elaboration on these key points can be omitted.\nPlease rate whether this answer is suitable for the question. Please note that the gold answer can be considered as a correct answer to the question.\n\nThe assistant receives an overall score on a scale of 1 to 100, where a higher score indicates better overall performance.\nPlease note that if the assistant's answer and the gold answer fully meet the above criteria, its overall rating should be the full marks (100).\nPlease first provide a comprehensive explanation of your evaluation, avoiding any potential bias.\nThen, output a line indicating the score of the Assistant.\n\nPLEASE OUTPUT WITH THE FOLLOWING FORMAT, WHERE THE SCORE IS A SCALE OF 1 TO 100 BY STRICTLY FOLLOWING THIS FORMAT: \"[[score]]\", FOR EXAMPLE \"Rating: [[100]]\":\n<start output>\nEvaluation evidence: your evluation explanation here, no more than 100 words\nRating: [[score]]\n<end output>\n\nNow, start your evaluation:",
        "generate_response": "{\"Reference\":[\"A Survey on Evaluation of Large Language Models\", \"Vicuna: An Open-Source Chatbot Impressing GPT-4 with $90 \\% *$ ChatGPT Quality\", \"ChatLaw: Open-Source Legal Large Language Model with Integrated External Knowledge Bases\", \"Compositional Semantic Parsing with Large Language Models\", \"Open LLM Leaderboard\", \"How Close is ChatGPT to Human Experts? Comparison Corpus, Evaluation, and Detection\", \"REALM: Retrieval-Augmented Language Model PreTraining\", \"Rethinking with Retrieval: Faithful Large Language Model Inference\", \"Measuring Massive Multitask Language Understanding\", \"Atlas: Few-shot Learning with Retrieval Augmented Language Models\", \"Language models are few-shot learners\", \"Membership inference attacks from first principles\", \"Quantifying memorization across neural language models\", \"Extracting training data from large language models\", \"Palm: Scaling language modeling with pathways\", \"BERT: Pre-training of deep bidirectional transformers for language understanding\", \"Does learning require memorization? a short tale about a long tail\", \"The pile: An $800 \\mathrm{gb}$ dataset of diverse text for language modeling\", \"Time travel in llms: Tracing data contamination in large language models\", \"Textbooks are all you need\", \"Measuring massive multitask language understanding\", \"An empirical analysis of compute-optimal large language model training\", \"Preventing generation of verbatim memorization in language models gives a false sense of privacy\", \"Membership inference attack susceptibility of clinical language models\", \"Pretraining language models with human preferences\", \"Textbooks are all you need ii: phi-1.5 technical report\", \"Holistic evaluation of language models\", \"ROUGE: A package for automatic evaluation of summaries\", \"Data contamination: From memorization to exploitation\", \"Membership inference on word embedding and beyond\", \"Membership inference attacks against language models via neighbourhood comparison\", \"Quantifying privacy risks of masked language models using membership inference attacks\", \"Abstractive text summarization using sequence-to-sequence RNNs and beyond\", \"Scalable extraction of training data from (production) language models\", \"Modern neural networks generalize on small data sets\", \"Gpt-4 technical report\", \"Proving test set contamination in black box language models\", \"Language models are unsupervised multitask learners\", \"SQUAD: 100,000+ questions for machine comprehension of text\", \"Recursive deep models for semantic compositionality over a sentiment treebank\", \"Llama: Open and efficient foundation language models\", \"Llama 2: Open foundation and fine-tuned chat models\", \"How far can camels go? exploring the state of instruction tuning on open resources\", \"Rethinking benchmark and contamination for language models with rephrased samples\", \"Towards a unified multi-dimensional evaluator for text generation\"], \"Citation\":[]}",
        "eval_response": "Evaluation evidence: The assistant's answer inaccurately lists numerous references that are not mentioned in the gold answer, indicating a significant hallucination issue. The gold answer specifies that the paper in question has no references but is cited by three specific papers. The assistant's answer fails to list any citations and incorrectly lists multiple references, showing a lack of accuracy and completeness in understanding the task requirements.\n\nRating: [[10]]",
        "score": 10
    },
    {
        "level": 3,
        "set": 4,
        "length": 247567,
        "type": "paper",
        "language": "en",
        "question": "DistServe: Disaggregating Prefill and Decoding for Goodput-optimized Large Language Model Serving",
        "instruction": "We hope you will carefully study the provided papers and determine the citation relationships between them. Please follow the instructions below strictly to complete the task:\n\n#Specific Requirements:\n1. Reference: When a given paper mentions other provided papers, those other papers are considered as \"references\" for the given paper. To summarize in this specific context, references are about what the given paper is using.\n2. Citation: Conversely, when other provided papers mention the given paper in their works, the given paper is being \"cited\" by those other papers. To summarize in this specific context, citations are about who is using the given paper.\n3. Given a paper, you need to determine the citation or reference relationship between this paper and the other papers. Do not consider papers that are not provided.\n3. Please present the paper titles in a json format as follows: {{\"Reference\":[\"Reference Title 1\", \"Reference Title 2\", ..., \"Reference Title n\"], \"Citation\":[\"Citation Title 1\", \"Citation Title 2\", ..., \"Citation Title n\"]}}.\n4. If a paper does not have any references or citations, please leave the corresponding list empty, e.g.{{\"Refernce\":[]}}, {{\"Citation\":[]}}.",
        "prompt_template": "#Papers:\n{docs}\n\n{instruction}\n\n#The paper you need to analyze:\n{question}",
        "doc": [
            "2401.09670.md",
            "2405.04437.md",
            "2405.05465.md",
            "2405.06856.md",
            "2405.16444.md",
            "2405.19888.md",
            "2308.12950.md",
            "2309.06180.md",
            "2311.18677.md",
            "2401.11181.md",
            "2403.01876.md"
        ],
        "answer": {
            "Reference": [
                "# Code Llama: Open Foundation Models for Code ",
                "# Efficient Memory Management for Large Language Model Serving with PagedAttention ",
                "# Splitwise: Efficient Generative LLM Inference Using Phase Splitting ",
                "# Inference without Interference: Disaggregate LLM Inference for Mixed Downstream Workloads ",
                "# DjVu: KV-cache Streaming for Fast, Fault-tolerant Generative LLM Serving "
            ],
            "Citation": [
                "# vAttention: Dynamic Memory Management for Serving LLMs without PagedAttention ",
                "# VIDUR: A LARGE-SCALE SIMULATION FRAMEWORK FOR LLM INFERENCE ",
                "# Aladdin: Joint Placement and Scaling for SLO-Aware LLM Serving ",
                "# CacheBlend: Fast Large Language Model Serving for RAG with Cached Knowledge Fusion ",
                "# Parrot: Efficient Serving of LLM-based Applications with Semantic Variable "
            ]
        },
        "shuffle_doc": true,
        "id": "276b22eb-271b-44af-b539-74988a13c219",
        "prompt": "[Question]\n#Papers:\n{docs}\n\nWe hope you will carefully study the provided papers and determine the citation relationships between them. Please follow the instructions below strictly to complete the task:\n\n#Specific Requirements:\n1. Reference: When a given paper mentions other provided papers, those other papers are considered as \"references\" for the given paper. To summarize in this specific context, references are about what the given paper is using.\n2. Citation: Conversely, when other provided papers mention the given paper in their works, the given paper is being \"cited\" by those other papers. To summarize in this specific context, citations are about who is using the given paper.\n3. Given a paper, you need to determine the citation or reference relationship between this paper and the other papers. Do not consider papers that are not provided.\n3. Please present the paper titles in a json format as follows: {{\"Reference\":[\"Reference Title 1\", \"Reference Title 2\", ..., \"Reference Title n\"], \"Citation\":[\"Citation Title 1\", \"Citation Title 2\", ..., \"Citation Title n\"]}}.\n4. If a paper does not have any references or citations, please leave the corresponding list empty, e.g.{{\"Refernce\":[]}}, {{\"Citation\":[]}}.\n\n#The paper you need to analyze:\nDistServe: Disaggregating Prefill and Decoding for Goodput-optimized Large Language Model Serving\n\n[Gold Answer]\n{'Reference': ['# Code Llama: Open Foundation Models for Code ', '# Efficient Memory Management for Large Language Model Serving with PagedAttention ', '# Splitwise: Efficient Generative LLM Inference Using Phase Splitting ', '# Inference without Interference: Disaggregate LLM Inference for Mixed Downstream Workloads ', '# DjVu: KV-cache Streaming for Fast, Fault-tolerant Generative LLM Serving '], 'Citation': ['# vAttention: Dynamic Memory Management for Serving LLMs without PagedAttention ', '# VIDUR: A LARGE-SCALE SIMULATION FRAMEWORK FOR LLM INFERENCE ', '# Aladdin: Joint Placement and Scaling for SLO-Aware LLM Serving ', '# CacheBlend: Fast Large Language Model Serving for RAG with Cached Knowledge Fusion ', '# Parrot: Efficient Serving of LLM-based Applications with Semantic Variable ']}\n\n[The Start of Assistant's Predicted Answer]\n{\"Reference\":[\"A Survey on Evaluation of Large Language Models\", \"Vicuna: An Open-Source Chatbot Impressing GPT-4 with $90 \\% *$ ChatGPT Quality\", \"ChatLaw: Open-Source Legal Large Language Model with Integrated External Knowledge Bases\", \"Compositional Semantic Parsing with Large Language Models\", \"Open LLM Leaderboard\", \"How Close is ChatGPT to Human Experts? Comparison Corpus, Evaluation, and Detection\", \"REALM: Retrieval-Augmented Language Model PreTraining\", \"Rethinking with Retrieval: Faithful Large Language Model Inference\", \"Measuring Massive Multitask Language Understanding\", \"Atlas: Few-shot Learning with Retrieval Augmented Language Models\", \"Introducing chatgpt\", \"Bard, an experiment by google\", \"Inflection tech memo\", \"Lanchain usecase: Summarization\", \"Nvidia collective communications library (nccl)\", \"Serve, optimize and scale pytorch models in production\", \"Sarathi: Efficient llm inference by piggybacking decodes with chunked prefills\", \"Gqa: Training generalized multi-query transformer models from multi-head checkpoints\", \"A case for disaggregation of $\\\\mathrm{ml}$ data processing\", \"Longbench: A bilingual, multitask benchmark for long context understanding\", \"Evaluating large language models trained on code\", \"Vicuna: An open-source chatbot impressing gpt-4 with $90 \\% *$ chatgpt quality\", \"Compute express link\", \"Fastertransformer\", \"Triton inference server: An optimized cloud and edge inferencing solution.\", \"Flashattention: Fast and memoryefficient exact attention with io-awareness\", \"Turbotransformers: an efficient gpu serving system for transformer models\", \"Serving DNNs like clockwork: Performance predictability from the bottom up\", \"Mira: A program-behavior-guided far memory system\", \"Microsecond-scale preemption for concurrent GPU-accelerated DNN inferences\", \"Inference without interference: Disaggregate $11 \\\\mathrm{~m}$ inference for mixed downstream workloads\", \"Gpipe: Efficient training of giant neural networks using pipeline parallelism\", \"Sia: Heterogeneity-aware, goodput-optimized ml-cluster scheduling\", \"Efficient memory management for large language model serving with pagedattention\", \"Alpaserve: Statistical multiplexing with model parallelism for deep learning serving\", \"Ray: A distributed framework for emerging AI applications\", \"Pipedream: Generalized pipeline parallelism for dnn training\", \"Gpt-4 technical report\", \"Splitwise: Efficient generative llm inference using phase splitting\", \"Pollux: Co-adaptive cluster scheduling for goodput-optimized deep learning\", \"Zero: Memory optimizations toward training trillion parameter models\", \"Reuters\", \"Code llama: Open foundation models for code\", \"LegoOS : A disseminated, distributed $\\\\{\\\\mathrm{OS}\\\\}$ for hardware resource disaggregation\", \"Megatron-lm: Training multi-billion parameter language models\", \"Fundamentals of queueing theory\", \"Djvu: Kv-cache streaming for fast, fault-tolerant generative llm serving\", \"Hotgpt: How to make software documentation more useful with a large language model?\", \"Llama: Open and efficient foundation language models\", \"Fast distributed inference serving for large language models\", \"Orca: A distributed serving system for \\\\{Transformer-Based\\\\} generative models\", \"Shepherd: Serving dnns in the wild.\", \"Opt: Open pre-trained transformer language models\", \"Make it real: An end-to-end implementation of a physically disaggregated data center\", \"Ft-cnn: Algorithm-based fault tolerance for convolutional neural networks\", \"Alpa: Automating inter- and Intra-Operator parallelism for distributed deep learning\", \"PetS: A unified framework for Parameter-Efficient transformers serving\"], \"Citation\":[\"CacheBlend: Fast Large Language Model Serving for RAG with Cached Knowledge Fusion\", \"vAttention: Dynamic Memory Management for Serving LLMs without PagedAttention\"]}\n[The End of Assistant's Predicted Answer]\n\n[System]\nWe would like to request your feedback on the performance of the AI assistant in response to the user question displayed above according to the gold answer. Please use the following listed aspects and their descriptions as evaluation criteria:\n    - Accuracy and Hallucinations: The assistant's answer is semantically consistent with the gold answer; The numerical value and order need to be accurate, and there should be no hallucinations.\n    - Completeness: Referring to the reference answers, the assistant's answer should contain all the key points needed to answer the user's question; further elaboration on these key points can be omitted.\nPlease rate whether this answer is suitable for the question. Please note that the gold answer can be considered as a correct answer to the question.\n\nThe assistant receives an overall score on a scale of 1 to 100, where a higher score indicates better overall performance.\nPlease note that if the assistant's answer and the gold answer fully meet the above criteria, its overall rating should be the full marks (100).\nPlease first provide a comprehensive explanation of your evaluation, avoiding any potential bias.\nThen, output a line indicating the score of the Assistant.\n\nPLEASE OUTPUT WITH THE FOLLOWING FORMAT, WHERE THE SCORE IS A SCALE OF 1 TO 100 BY STRICTLY FOLLOWING THIS FORMAT: \"[[score]]\", FOR EXAMPLE \"Rating: [[100]]\":\n<start output>\nEvaluation evidence: your evluation explanation here, no more than 100 words\nRating: [[score]]\n<end output>\n\nNow, start your evaluation:",
        "generate_response": "{\"Reference\":[\"A Survey on Evaluation of Large Language Models\", \"Vicuna: An Open-Source Chatbot Impressing GPT-4 with $90 \\% *$ ChatGPT Quality\", \"ChatLaw: Open-Source Legal Large Language Model with Integrated External Knowledge Bases\", \"Compositional Semantic Parsing with Large Language Models\", \"Open LLM Leaderboard\", \"How Close is ChatGPT to Human Experts? Comparison Corpus, Evaluation, and Detection\", \"REALM: Retrieval-Augmented Language Model PreTraining\", \"Rethinking with Retrieval: Faithful Large Language Model Inference\", \"Measuring Massive Multitask Language Understanding\", \"Atlas: Few-shot Learning with Retrieval Augmented Language Models\", \"Introducing chatgpt\", \"Bard, an experiment by google\", \"Inflection tech memo\", \"Lanchain usecase: Summarization\", \"Nvidia collective communications library (nccl)\", \"Serve, optimize and scale pytorch models in production\", \"Sarathi: Efficient llm inference by piggybacking decodes with chunked prefills\", \"Gqa: Training generalized multi-query transformer models from multi-head checkpoints\", \"A case for disaggregation of $\\\\mathrm{ml}$ data processing\", \"Longbench: A bilingual, multitask benchmark for long context understanding\", \"Evaluating large language models trained on code\", \"Vicuna: An open-source chatbot impressing gpt-4 with $90 \\% *$ chatgpt quality\", \"Compute express link\", \"Fastertransformer\", \"Triton inference server: An optimized cloud and edge inferencing solution.\", \"Flashattention: Fast and memoryefficient exact attention with io-awareness\", \"Turbotransformers: an efficient gpu serving system for transformer models\", \"Serving DNNs like clockwork: Performance predictability from the bottom up\", \"Mira: A program-behavior-guided far memory system\", \"Microsecond-scale preemption for concurrent GPU-accelerated DNN inferences\", \"Inference without interference: Disaggregate $11 \\\\mathrm{~m}$ inference for mixed downstream workloads\", \"Gpipe: Efficient training of giant neural networks using pipeline parallelism\", \"Sia: Heterogeneity-aware, goodput-optimized ml-cluster scheduling\", \"Efficient memory management for large language model serving with pagedattention\", \"Alpaserve: Statistical multiplexing with model parallelism for deep learning serving\", \"Ray: A distributed framework for emerging AI applications\", \"Pipedream: Generalized pipeline parallelism for dnn training\", \"Gpt-4 technical report\", \"Splitwise: Efficient generative llm inference using phase splitting\", \"Pollux: Co-adaptive cluster scheduling for goodput-optimized deep learning\", \"Zero: Memory optimizations toward training trillion parameter models\", \"Reuters\", \"Code llama: Open foundation models for code\", \"LegoOS : A disseminated, distributed $\\\\{\\\\mathrm{OS}\\\\}$ for hardware resource disaggregation\", \"Megatron-lm: Training multi-billion parameter language models\", \"Fundamentals of queueing theory\", \"Djvu: Kv-cache streaming for fast, fault-tolerant generative llm serving\", \"Hotgpt: How to make software documentation more useful with a large language model?\", \"Llama: Open and efficient foundation language models\", \"Fast distributed inference serving for large language models\", \"Orca: A distributed serving system for \\\\{Transformer-Based\\\\} generative models\", \"Shepherd: Serving dnns in the wild.\", \"Opt: Open pre-trained transformer language models\", \"Make it real: An end-to-end implementation of a physically disaggregated data center\", \"Ft-cnn: Algorithm-based fault tolerance for convolutional neural networks\", \"Alpa: Automating inter- and Intra-Operator parallelism for distributed deep learning\", \"PetS: A unified framework for Parameter-Efficient transformers serving\"], \"Citation\":[\"CacheBlend: Fast Large Language Model Serving for RAG with Cached Knowledge Fusion\", \"vAttention: Dynamic Memory Management for Serving LLMs without PagedAttention\"]}",
        "eval_response": "Evaluation evidence: The assistant's answer does not match the gold answer in terms of the specific papers listed under \"Reference\" and \"Citation.\" The assistant's response includes a large number of papers that are not mentioned in the gold answer, indicating significant hallucinations and inaccuracies. Additionally, the assistant fails to include any of the correct papers from the gold answer, leading to a complete lack of accuracy and completeness in addressing the task requirements.\n\nRating: [[1]]",
        "score": 1
    },
    {
        "level": 3,
        "set": 1,
        "length": 45468,
        "type": "paper",
        "language": "en",
        "question": "Hallucination is Inevitable: <br> An Innate Limitation of Large Language Models",
        "instruction": "We hope you will carefully study the provided papers and determine the citation relationships between them. Please follow the instructions below strictly to complete the task:\n\n#Specific Requirements:\n1. Reference: When a given paper mentions other provided papers, those other papers are considered as \"references\" for the given paper. To summarize in this specific context, references are about what the given paper is using.\n2. Citation: Conversely, when other provided papers mention the given paper in their works, the given paper is being \"cited\" by those other papers. To summarize in this specific context, citations are about who is using the given paper.\n3. Given a paper, you need to determine the citation or reference relationship between this paper and the other papers. Do not consider papers that are not provided.\n3. Please present the paper titles in a json format as follows: {{\"Reference\":[\"Reference Title 1\", \"Reference Title 2\", ..., \"Reference Title n\"], \"Citation\":[\"Citation Title 1\", \"Citation Title 2\", ..., \"Citation Title n\"]}}.\n4. If a paper does not have any references or citations, please leave the corresponding list empty, e.g.{{\"Refernce\":[]}}, {{\"Citation\":[]}}.",
        "prompt_template": "#Papers:\n{docs}\n\n{instruction}\n\n#The paper you need to analyze:\n{question}",
        "doc": [
            "2401.11817.md",
            "2405.12999.md",
            "2401.07897.md"
        ],
        "answer": {
            "Reference": [
                "# The Pitfalls of Defining Hallucination "
            ],
            "Citation": [
                "# An ASSESSMENT OF MODEL-ON-MODEL DECEPTION "
            ]
        },
        "shuffle_doc": true,
        "id": "d7adf528-91a9-4978-a25c-29e61562a3be",
        "prompt": "[Question]\n#Papers:\n{docs}\n\nWe hope you will carefully study the provided papers and determine the citation relationships between them. Please follow the instructions below strictly to complete the task:\n\n#Specific Requirements:\n1. Reference: When a given paper mentions other provided papers, those other papers are considered as \"references\" for the given paper. To summarize in this specific context, references are about what the given paper is using.\n2. Citation: Conversely, when other provided papers mention the given paper in their works, the given paper is being \"cited\" by those other papers. To summarize in this specific context, citations are about who is using the given paper.\n3. Given a paper, you need to determine the citation or reference relationship between this paper and the other papers. Do not consider papers that are not provided.\n3. Please present the paper titles in a json format as follows: {{\"Reference\":[\"Reference Title 1\", \"Reference Title 2\", ..., \"Reference Title n\"], \"Citation\":[\"Citation Title 1\", \"Citation Title 2\", ..., \"Citation Title n\"]}}.\n4. If a paper does not have any references or citations, please leave the corresponding list empty, e.g.{{\"Refernce\":[]}}, {{\"Citation\":[]}}.\n\n#The paper you need to analyze:\nHallucination is Inevitable: <br> An Innate Limitation of Large Language Models\n\n[Gold Answer]\n{'Reference': ['# The Pitfalls of Defining Hallucination '], 'Citation': ['# An ASSESSMENT OF MODEL-ON-MODEL DECEPTION ']}\n\n[The Start of Assistant's Predicted Answer]\n{\"Reference\": [\"The Pitfalls of Defining Hallucination\", \"An ASSESSMENT OF MODEL-ON-MODEL DECEPTION\"], \"Citation\": [\"A Survey on Evaluation of Large Language Models\", \"GPT-4 technical report\", \"The internal state of an LLM knows when it's lying\", \"Human-level play in the game of Diplomacy by combining language models with strategic reasoning\", \"Language models are few-shot learners\", \"Discovering latent knowledge in language models without supervision\", \"Large language models vs. search engines: Evaluating user preferences across varied information retrieval scenarios\", \"Deep reinforcement learning from human preferences\", \"Challenges with unsupervised llm knowledge discovery\", \"Deception abilities emerged in large language models\", \"Measuring massive multitask language understanding\", \"Sleeper agents: Training deceptive LLMs that persist through safety training\", \"Comparison of two meta-analysis methods: Inverse-variance-weighted average and weighted sum of z-scores\", \"Still no lie detector for language models: Probing empirical and conceptual roadblocks\", \"Retrieval-augmented generation for knowledge-intensive NLP tasks\", \"Inference-time intervention: Eliciting truthful answers from a language model\", \"TruthfulQA: Measuring how models mimic human falsehoods\", \"The geometry of truth: Emergent linear structure in large language model representations of true/false datasets\", \"Hoodwinked: Deception and cooperation in a text-based game for language models\", \"Introducing ChatGPT\", \"Do the rewards justify the means? measuring trade-offs between rewards and ethical behavior in the MACHIAVELLI benchmark\", \"AI deception: A survey of examples, risks, and potential solutions\", \"Discovering language model behaviors with model-written evaluations\", \"Steering Llama 2 via contrastive activation addition\", \"Technical report: Large language models can strategically deceive their users when put under pressure\", \"Towards understanding sycophancy in language models\", \"Comparing traditional and LLM-based search for consumer choice: A randomized experiment\", \"Llama 2: Open foundation and fine-tuned chat models\", \"Fabrication and errors in the bibliographic citations generated by ChatGPT\", \"Honesty is the best policy: Defining and mitigating AI deception\", \"Chain of thought prompting elicits reasoning in large language models\", \"Tree of thoughts: Deliberate problem solving with large language models\", \"Siren's song in the AI ocean: A survey on hallucination in large language models\", \"Representation engineering: A top-down approach to AI transparency\"]}\n[The End of Assistant's Predicted Answer]\n\n[System]\nWe would like to request your feedback on the performance of the AI assistant in response to the user question displayed above according to the gold answer. Please use the following listed aspects and their descriptions as evaluation criteria:\n    - Accuracy and Hallucinations: The assistant's answer is semantically consistent with the gold answer; The numerical value and order need to be accurate, and there should be no hallucinations.\n    - Completeness: Referring to the reference answers, the assistant's answer should contain all the key points needed to answer the user's question; further elaboration on these key points can be omitted.\nPlease rate whether this answer is suitable for the question. Please note that the gold answer can be considered as a correct answer to the question.\n\nThe assistant receives an overall score on a scale of 1 to 100, where a higher score indicates better overall performance.\nPlease note that if the assistant's answer and the gold answer fully meet the above criteria, its overall rating should be the full marks (100).\nPlease first provide a comprehensive explanation of your evaluation, avoiding any potential bias.\nThen, output a line indicating the score of the Assistant.\n\nPLEASE OUTPUT WITH THE FOLLOWING FORMAT, WHERE THE SCORE IS A SCALE OF 1 TO 100 BY STRICTLY FOLLOWING THIS FORMAT: \"[[score]]\", FOR EXAMPLE \"Rating: [[100]]\":\n<start output>\nEvaluation evidence: your evluation explanation here, no more than 100 words\nRating: [[score]]\n<end output>\n\nNow, start your evaluation:",
        "generate_response": "{\"Reference\": [\"The Pitfalls of Defining Hallucination\", \"An ASSESSMENT OF MODEL-ON-MODEL DECEPTION\"], \"Citation\": [\"A Survey on Evaluation of Large Language Models\", \"GPT-4 technical report\", \"The internal state of an LLM knows when it's lying\", \"Human-level play in the game of Diplomacy by combining language models with strategic reasoning\", \"Language models are few-shot learners\", \"Discovering latent knowledge in language models without supervision\", \"Large language models vs. search engines: Evaluating user preferences across varied information retrieval scenarios\", \"Deep reinforcement learning from human preferences\", \"Challenges with unsupervised llm knowledge discovery\", \"Deception abilities emerged in large language models\", \"Measuring massive multitask language understanding\", \"Sleeper agents: Training deceptive LLMs that persist through safety training\", \"Comparison of two meta-analysis methods: Inverse-variance-weighted average and weighted sum of z-scores\", \"Still no lie detector for language models: Probing empirical and conceptual roadblocks\", \"Retrieval-augmented generation for knowledge-intensive NLP tasks\", \"Inference-time intervention: Eliciting truthful answers from a language model\", \"TruthfulQA: Measuring how models mimic human falsehoods\", \"The geometry of truth: Emergent linear structure in large language model representations of true/false datasets\", \"Hoodwinked: Deception and cooperation in a text-based game for language models\", \"Introducing ChatGPT\", \"Do the rewards justify the means? measuring trade-offs between rewards and ethical behavior in the MACHIAVELLI benchmark\", \"AI deception: A survey of examples, risks, and potential solutions\", \"Discovering language model behaviors with model-written evaluations\", \"Steering Llama 2 via contrastive activation addition\", \"Technical report: Large language models can strategically deceive their users when put under pressure\", \"Towards understanding sycophancy in language models\", \"Comparing traditional and LLM-based search for consumer choice: A randomized experiment\", \"Llama 2: Open foundation and fine-tuned chat models\", \"Fabrication and errors in the bibliographic citations generated by ChatGPT\", \"Honesty is the best policy: Defining and mitigating AI deception\", \"Chain of thought prompting elicits reasoning in large language models\", \"Tree of thoughts: Deliberate problem solving with large language models\", \"Siren's song in the AI ocean: A survey on hallucination in large language models\", \"Representation engineering: A top-down approach to AI transparency\"]}",
        "eval_response": "Evaluation evidence: The assistant's answer includes a significant number of hallucinated titles that are not mentioned in the gold answer, indicating a lack of accuracy and the presence of hallucinations. The gold answer specifies only one reference and one citation, whereas the assistant's answer lists numerous unrelated titles under both categories. This discrepancy shows a failure in meeting the accuracy and completeness criteria as per the task requirements.\n\nRating: [[10]]",
        "score": 10
    },
    {
        "level": 3,
        "set": 2,
        "length": 62226,
        "type": "paper",
        "language": "en",
        "question": "How Johnny Can Persuade LLMs to Jailbreak Them: Rethinking Persuasion to Challenge AI Safety by Humanizing LLMs",
        "instruction": "We hope you will carefully study the provided papers and determine the citation relationships between them. Please follow the instructions below strictly to complete the task:\n\n#Specific Requirements:\n1. Reference: When a given paper mentions other provided papers, those other papers are considered as \"references\" for the given paper. To summarize in this specific context, references are about what the given paper is using.\n2. Citation: Conversely, when other provided papers mention the given paper in their works, the given paper is being \"cited\" by those other papers. To summarize in this specific context, citations are about who is using the given paper.\n3. Given a paper, you need to determine the citation or reference relationship between this paper and the other papers. Do not consider papers that are not provided.\n3. Please present the paper titles in a json format as follows: {{\"Reference\":[\"Reference Title 1\", \"Reference Title 2\", ..., \"Reference Title n\"], \"Citation\":[\"Citation Title 1\", \"Citation Title 2\", ..., \"Citation Title n\"]}}.\n4. If a paper does not have any references or citations, please leave the corresponding list empty, e.g.{{\"Refernce\":[]}}, {{\"Citation\":[]}}.",
        "prompt_template": "#Papers:\n{docs}\n\n{instruction}\n\n#The paper you need to analyze:\n{question}",
        "doc": [
            "2401.06373.md",
            "2405.21018.md",
            "2310.08419.md"
        ],
        "answer": {
            "Reference": [
                "# Jailbreaking Black Box Large Language Models in Twenty Queries "
            ],
            "Citation": [
                "# Improved Techniques for Optimization-Based Jailbreaking on Large Language Models "
            ]
        },
        "shuffle_doc": true,
        "id": "c8d9926c-5f86-4459-9bdf-bccec14f2f88",
        "prompt": "[Question]\n#Papers:\n{docs}\n\nWe hope you will carefully study the provided papers and determine the citation relationships between them. Please follow the instructions below strictly to complete the task:\n\n#Specific Requirements:\n1. Reference: When a given paper mentions other provided papers, those other papers are considered as \"references\" for the given paper. To summarize in this specific context, references are about what the given paper is using.\n2. Citation: Conversely, when other provided papers mention the given paper in their works, the given paper is being \"cited\" by those other papers. To summarize in this specific context, citations are about who is using the given paper.\n3. Given a paper, you need to determine the citation or reference relationship between this paper and the other papers. Do not consider papers that are not provided.\n3. Please present the paper titles in a json format as follows: {{\"Reference\":[\"Reference Title 1\", \"Reference Title 2\", ..., \"Reference Title n\"], \"Citation\":[\"Citation Title 1\", \"Citation Title 2\", ..., \"Citation Title n\"]}}.\n4. If a paper does not have any references or citations, please leave the corresponding list empty, e.g.{{\"Refernce\":[]}}, {{\"Citation\":[]}}.\n\n#The paper you need to analyze:\nHow Johnny Can Persuade LLMs to Jailbreak Them: Rethinking Persuasion to Challenge AI Safety by Humanizing LLMs\n\n[Gold Answer]\n{'Reference': ['# Jailbreaking Black Box Large Language Models in Twenty Queries '], 'Citation': ['# Improved Techniques for Optimization-Based Jailbreaking on Large Language Models ']}\n\n[The Start of Assistant's Predicted Answer]\n{\"Reference\":[\"Meta's Llama-2\", \"OpenAI's GPT series\", \"Optimization-based (Zou et al., 2023; Liu et al., 2023a)\", \"Side-channel-based (Yuan et al., 2023; Deng et al., 2023b)\", \"Distribution-based approaches (Deng et al., 2023a; Yu et al., 2023)\", \"Adlakha, V.; BehnamGhader, P.; Lu, X. H.; Meade, N.; and Reddy, S. 2023. Evaluating Correctness and Faithfulness of Instruction-Following Models for Question Answering\", \"Chang, Y.; Wang, X.; Wang, J.; Wu, Y.; Yang, L.; Zhu, K.; Chen, H.; Yi, X.; Wang, C.; Wang, Y.; Ye, W.; Zhang, Y.; Chang, Y.; Yu, P. S.; Yang, Q.; and Xie, X. 2023. A Survey on Evaluation of Large Language Models\", \"Chiang, W.-L.; Li, Z.; Lin, Z.; Sheng, Y.; Wu, Z.; Zhang, H.; Zheng, L.; Zhuang, S.; Zhuang, Y.; Gonzalez, J. E.; Stoica, I.; and Xing, E. P. 2023. Vicuna: An Open-Source Chatbot Impressing GPT-4 with $90 \\% *$ ChatGPT Quality\", \"Cui, J.; Li, Z.; Yan, Y.; Chen, B.; and Yuan, L. 2023. ChatLaw: Open-Source Legal Large Language Model with Integrated External Knowledge Bases\", \"Drozdov, A.; Schrli, N.; Akyrek, E.; Scales, N.; Song, X.; Chen, X.; Bousquet, O.; and Zhou, D. 2023. Compositional Semantic Parsing with Large Language Models\", \"Edward Beeching, N. H. S. H. N. L. N. R. O. S. L. T. T. W., Clmentine Fourrier. 2023. Open LLM Leaderboard\", \"Guo, B.; Zhang, X.; Wang, Z.; Jiang, M.; Nie, J.; Ding, Y.; Yue, J.; and Wu, Y. 2023. How Close is ChatGPT to Human Experts? Comparison Corpus, Evaluation, and Detection\", \"Guu, K.; Lee, K.; Tung, Z.; Pasupat, P.; and Chang, M.-W. 2020. REALM: Retrieval-Augmented Language Model PreTraining\", \"He, H.; Zhang, H.; and Roth, D. 2022. Rethinking with Retrieval: Faithful Large Language Model Inference\", \"Hendrycks, D.; Burns, C.; Basart, S.; Zou, A.; Mazeika, M.; Song, D.; and Steinhardt, J. 2021. Measuring Massive Multitask Language Understanding\", \"Izacard, G.; Lewis, P.; Lomeli, M.; Hosseini, L.; Petroni, F.; Schick, T.; Dwivedi-Yu, J.; Joulin, A.; Riedel, S.; and Grave, E. 2023. Atlas: Few-shot Learning with Retrieval Augmented Language Models\", \"Praveen Aggarwal, Sung Youl Jun, and Jong Ho Huh. 2011. Scarcity messages\", \"Bochuan Cao, Yuanpu Cao, Lu Lin, and Jinghui Chen. 2023. Defending against alignment-breaking attacks via robustly aligned llm\", \"Gelei Deng, Yi Liu, Yuekang Li, Kailong Wang, Ying Zhang, Zefeng Li, Haoyu Wang, Tianwei Zhang, and Yang Liu. 2023a. Jailbreaker: Automated jailbreak across multiple large language model chatbots\", \"Yue Deng, Wenxuan Zhang, Sinno Jialin Pan, and Lidong Bing. 2023b. Multilingual jailbreak challenges in large language models\", \"Nicholas DiFonzo and Prashant Bordia. 2011. Rumors influence: Toward a dynamic social impact theory of rumor\", \"James Price Dillard and Leanne K Knobloch. 2011. Interpersonal influence\", \"Yanai Elazar, Akshaya Bhagia, Ian Magnusson, Abhilasha Ravichander, Dustin Schwenk, Alane Suhr, Pete Walsh, Dirk Groeneveld, Luca Soldaini, Sameer Singh, et al. 2023. What's in my big data?\", \"Robert H Gass and John S Seiter. 2022. Persuasion: Social influence and compliance gaining\", \"Chuan Guo, Alexandre Sablayrolles, Herv Jgou, and Douwe Kiela. 2021. Gradient-based adversarial attacks against text transformers\", \"Huang, Yangsibo; Gupta, Samyak; Xia, Mengzhou; Li, Kai; and Chen, Danqi. 2023. Catastrophic jailbreak of open-source $1 \\mathrm{lms}$ via exploiting generation\", \"Keise Izuma. 2013. The neural basis of social influence and attitude change\", \"Neel Jain, Avi Schwarzschild, Yuxin Wen, Gowthami Somepalli, John Kirchenbauer, Ping-yeh Chiang, Micah Goldblum, Aniruddha Saha, Jonas Geiping, and Tom Goldstein. 2023. Baseline defenses for adversarial attacks against aligned language models\", \"Erik Jones, Anca Dragan, Aditi Raghunathan, and Jacob Steinhardt. 2023. Automatically auditing large language models via discrete optimization\", \"Daniel Kang, Xuechen Li, Ion Stoica, Carlos Guestrin, Matei Zaharia, and Tatsunori Hashimoto. 2023. Exploiting programmatic behavior of $11 \\mathrm{~ms}$ : Dual-use through standard security attacks\", \"Aounon Kumar, Chirag Agarwal, Suraj Srinivas, Sohein Feizi, and Hima Lakkaraju. 2023. Certifying $11 \\mathrm{~m}$ safety against adversarial prompting\", \"Raz Lapid, Ron Langberg, and Moshe Sipper. 2023. Open sesame! universal black box jailbreaking of large language models\", \"Yuhui Li, Fangyun Wei, Jinjing Zhao, Chao Zhang, and Hongyang Zhang. 2023. Rain: Your language models can align themselves without finetuning\", \"Liu, Xiaogeng; Xu, Nan; Chen, Muhao; and Xiao, Chaowei. 2023a. Autodan: Generating stealthy jailbreak prompts on aligned large language models\", \"Liu, Yi; Deng, Gelei; Xu, Zhengzi; Li, Yuekang; Zheng, Yaowen; Zhang, Ying; Zhao, Lida; Zhang, Tianwei; and Liu, Yang. 2023b. Jailbreaking chatgpt via prompt engineering: An empirical study\", \"Jamie Luguri and Lior Jacob Strahilevitz. 2021. Shining a light on dark patterns\", \"Arunesh Mathur, Gunes Acar, Michael J Friedman, Eli Lucherini, Jonathan Mayer, Marshini Chetty, and Arvind Narayanan. 2019. Dark patterns at scale: Findings from a crawl of $11 \\mathrm{k}$ shopping websites\", \"Arvind Narayanan, Arunesh Mathur, Marshini Chetty, and Mihir Kshirsagar. 2020. Dark patterns: Past, present, and future: The evolution of tricky user interfaces\", \"Alexander Robey, Eric Wong, Hamed Hassani, and George J Pappas. 2023. Smoothllm: Defending large language models against jailbreaking attacks\", \"Rusheb Shah, Quentin Feuillade-Montixi, Soroush Pour, Arush Tagade, Stephen Casper, and Javier Rando. 2023. Scalable and transferable black-box jailbreaks for language models via persona modulation\", \"Hugo Touvron, Louis Martin, Kevin Stone, Peter Albert, Amjad Almahairi, Yasmine Babaei, Nikolay Bashlykov, Soumya Batra, Prajjwal Bhargava, Shruti Bhosale, et al. 2023. Llama 2: Open foundation and fine-tuned chat models\", \"Zeming Wei, Yifei Wang, and Yisen Wang. 2023. Jailbreak and guard aligned language models with only few in-context demonstrations\", \"Timothy D Wilson, JC Olson, and MP Zanna. 2013. Self-persuasion via self-reflection\", \"Rongwu Xu, Brian Lin, Shujian Yang, Tianqi Zhang, Weiyan Shi, Tianwei Zhang, Zhixuan Fang, Wei $\\mathrm{Xu}$, and Han Qiu. 2023. The earth is flat because...: Investigating llms' belief towards misinformation via persuasive conversation\", \"Xianjun Yang, Xiao Wang, Qi Zhang, Linda Petzold, William Yang Wang, Xun Zhao, and Dahua Lin. 2023. Shadow alignment: The ease of subverting safely-aligned language models\", \"Zheng-Xin Yong, Cristina Menghini, and Stephen H Bach. 2023. Low-resource languages jailbreak gpt-4\", \"Jiahao Yu, Xingwei Lin, and Xinyu Xing. 2023. Gptfuzzer: Red teaming large language models with auto-generated jailbreak prompts\", \"Youliang Yuan, Wenxiang Jiao, Wenxuan Wang, Jen-tse Huang, Pinjia He, Shuming Shi, and Zhaopeng Tu. 2023. Gpt-4 is too smart to be safe: Stealthy chat with $11 \\mathrm{~ms}$ via cipher\", \"Andy Zou, Zifan Wang, J Zico Kolter, and Matt Fredrikson. 2023. Universal and transferable adversarial attacks on aligned language models\"], \"Citation\":[]}\n[The End of Assistant's Predicted Answer]\n\n[System]\nWe would like to request your feedback on the performance of the AI assistant in response to the user question displayed above according to the gold answer. Please use the following listed aspects and their descriptions as evaluation criteria:\n    - Accuracy and Hallucinations: The assistant's answer is semantically consistent with the gold answer; The numerical value and order need to be accurate, and there should be no hallucinations.\n    - Completeness: Referring to the reference answers, the assistant's answer should contain all the key points needed to answer the user's question; further elaboration on these key points can be omitted.\nPlease rate whether this answer is suitable for the question. Please note that the gold answer can be considered as a correct answer to the question.\n\nThe assistant receives an overall score on a scale of 1 to 100, where a higher score indicates better overall performance.\nPlease note that if the assistant's answer and the gold answer fully meet the above criteria, its overall rating should be the full marks (100).\nPlease first provide a comprehensive explanation of your evaluation, avoiding any potential bias.\nThen, output a line indicating the score of the Assistant.\n\nPLEASE OUTPUT WITH THE FOLLOWING FORMAT, WHERE THE SCORE IS A SCALE OF 1 TO 100 BY STRICTLY FOLLOWING THIS FORMAT: \"[[score]]\", FOR EXAMPLE \"Rating: [[100]]\":\n<start output>\nEvaluation evidence: your evluation explanation here, no more than 100 words\nRating: [[score]]\n<end output>\n\nNow, start your evaluation:",
        "generate_response": "{\"Reference\":[\"Meta's Llama-2\", \"OpenAI's GPT series\", \"Optimization-based (Zou et al., 2023; Liu et al., 2023a)\", \"Side-channel-based (Yuan et al., 2023; Deng et al., 2023b)\", \"Distribution-based approaches (Deng et al., 2023a; Yu et al., 2023)\", \"Adlakha, V.; BehnamGhader, P.; Lu, X. H.; Meade, N.; and Reddy, S. 2023. Evaluating Correctness and Faithfulness of Instruction-Following Models for Question Answering\", \"Chang, Y.; Wang, X.; Wang, J.; Wu, Y.; Yang, L.; Zhu, K.; Chen, H.; Yi, X.; Wang, C.; Wang, Y.; Ye, W.; Zhang, Y.; Chang, Y.; Yu, P. S.; Yang, Q.; and Xie, X. 2023. A Survey on Evaluation of Large Language Models\", \"Chiang, W.-L.; Li, Z.; Lin, Z.; Sheng, Y.; Wu, Z.; Zhang, H.; Zheng, L.; Zhuang, S.; Zhuang, Y.; Gonzalez, J. E.; Stoica, I.; and Xing, E. P. 2023. Vicuna: An Open-Source Chatbot Impressing GPT-4 with $90 \\% *$ ChatGPT Quality\", \"Cui, J.; Li, Z.; Yan, Y.; Chen, B.; and Yuan, L. 2023. ChatLaw: Open-Source Legal Large Language Model with Integrated External Knowledge Bases\", \"Drozdov, A.; Schrli, N.; Akyrek, E.; Scales, N.; Song, X.; Chen, X.; Bousquet, O.; and Zhou, D. 2023. Compositional Semantic Parsing with Large Language Models\", \"Edward Beeching, N. H. S. H. N. L. N. R. O. S. L. T. T. W., Clmentine Fourrier. 2023. Open LLM Leaderboard\", \"Guo, B.; Zhang, X.; Wang, Z.; Jiang, M.; Nie, J.; Ding, Y.; Yue, J.; and Wu, Y. 2023. How Close is ChatGPT to Human Experts? Comparison Corpus, Evaluation, and Detection\", \"Guu, K.; Lee, K.; Tung, Z.; Pasupat, P.; and Chang, M.-W. 2020. REALM: Retrieval-Augmented Language Model PreTraining\", \"He, H.; Zhang, H.; and Roth, D. 2022. Rethinking with Retrieval: Faithful Large Language Model Inference\", \"Hendrycks, D.; Burns, C.; Basart, S.; Zou, A.; Mazeika, M.; Song, D.; and Steinhardt, J. 2021. Measuring Massive Multitask Language Understanding\", \"Izacard, G.; Lewis, P.; Lomeli, M.; Hosseini, L.; Petroni, F.; Schick, T.; Dwivedi-Yu, J.; Joulin, A.; Riedel, S.; and Grave, E. 2023. Atlas: Few-shot Learning with Retrieval Augmented Language Models\", \"Praveen Aggarwal, Sung Youl Jun, and Jong Ho Huh. 2011. Scarcity messages\", \"Bochuan Cao, Yuanpu Cao, Lu Lin, and Jinghui Chen. 2023. Defending against alignment-breaking attacks via robustly aligned llm\", \"Gelei Deng, Yi Liu, Yuekang Li, Kailong Wang, Ying Zhang, Zefeng Li, Haoyu Wang, Tianwei Zhang, and Yang Liu. 2023a. Jailbreaker: Automated jailbreak across multiple large language model chatbots\", \"Yue Deng, Wenxuan Zhang, Sinno Jialin Pan, and Lidong Bing. 2023b. Multilingual jailbreak challenges in large language models\", \"Nicholas DiFonzo and Prashant Bordia. 2011. Rumors influence: Toward a dynamic social impact theory of rumor\", \"James Price Dillard and Leanne K Knobloch. 2011. Interpersonal influence\", \"Yanai Elazar, Akshaya Bhagia, Ian Magnusson, Abhilasha Ravichander, Dustin Schwenk, Alane Suhr, Pete Walsh, Dirk Groeneveld, Luca Soldaini, Sameer Singh, et al. 2023. What's in my big data?\", \"Robert H Gass and John S Seiter. 2022. Persuasion: Social influence and compliance gaining\", \"Chuan Guo, Alexandre Sablayrolles, Herv Jgou, and Douwe Kiela. 2021. Gradient-based adversarial attacks against text transformers\", \"Huang, Yangsibo; Gupta, Samyak; Xia, Mengzhou; Li, Kai; and Chen, Danqi. 2023. Catastrophic jailbreak of open-source $1 \\mathrm{lms}$ via exploiting generation\", \"Keise Izuma. 2013. The neural basis of social influence and attitude change\", \"Neel Jain, Avi Schwarzschild, Yuxin Wen, Gowthami Somepalli, John Kirchenbauer, Ping-yeh Chiang, Micah Goldblum, Aniruddha Saha, Jonas Geiping, and Tom Goldstein. 2023. Baseline defenses for adversarial attacks against aligned language models\", \"Erik Jones, Anca Dragan, Aditi Raghunathan, and Jacob Steinhardt. 2023. Automatically auditing large language models via discrete optimization\", \"Daniel Kang, Xuechen Li, Ion Stoica, Carlos Guestrin, Matei Zaharia, and Tatsunori Hashimoto. 2023. Exploiting programmatic behavior of $11 \\mathrm{~ms}$ : Dual-use through standard security attacks\", \"Aounon Kumar, Chirag Agarwal, Suraj Srinivas, Sohein Feizi, and Hima Lakkaraju. 2023. Certifying $11 \\mathrm{~m}$ safety against adversarial prompting\", \"Raz Lapid, Ron Langberg, and Moshe Sipper. 2023. Open sesame! universal black box jailbreaking of large language models\", \"Yuhui Li, Fangyun Wei, Jinjing Zhao, Chao Zhang, and Hongyang Zhang. 2023. Rain: Your language models can align themselves without finetuning\", \"Liu, Xiaogeng; Xu, Nan; Chen, Muhao; and Xiao, Chaowei. 2023a. Autodan: Generating stealthy jailbreak prompts on aligned large language models\", \"Liu, Yi; Deng, Gelei; Xu, Zhengzi; Li, Yuekang; Zheng, Yaowen; Zhang, Ying; Zhao, Lida; Zhang, Tianwei; and Liu, Yang. 2023b. Jailbreaking chatgpt via prompt engineering: An empirical study\", \"Jamie Luguri and Lior Jacob Strahilevitz. 2021. Shining a light on dark patterns\", \"Arunesh Mathur, Gunes Acar, Michael J Friedman, Eli Lucherini, Jonathan Mayer, Marshini Chetty, and Arvind Narayanan. 2019. Dark patterns at scale: Findings from a crawl of $11 \\mathrm{k}$ shopping websites\", \"Arvind Narayanan, Arunesh Mathur, Marshini Chetty, and Mihir Kshirsagar. 2020. Dark patterns: Past, present, and future: The evolution of tricky user interfaces\", \"Alexander Robey, Eric Wong, Hamed Hassani, and George J Pappas. 2023. Smoothllm: Defending large language models against jailbreaking attacks\", \"Rusheb Shah, Quentin Feuillade-Montixi, Soroush Pour, Arush Tagade, Stephen Casper, and Javier Rando. 2023. Scalable and transferable black-box jailbreaks for language models via persona modulation\", \"Hugo Touvron, Louis Martin, Kevin Stone, Peter Albert, Amjad Almahairi, Yasmine Babaei, Nikolay Bashlykov, Soumya Batra, Prajjwal Bhargava, Shruti Bhosale, et al. 2023. Llama 2: Open foundation and fine-tuned chat models\", \"Zeming Wei, Yifei Wang, and Yisen Wang. 2023. Jailbreak and guard aligned language models with only few in-context demonstrations\", \"Timothy D Wilson, JC Olson, and MP Zanna. 2013. Self-persuasion via self-reflection\", \"Rongwu Xu, Brian Lin, Shujian Yang, Tianqi Zhang, Weiyan Shi, Tianwei Zhang, Zhixuan Fang, Wei $\\mathrm{Xu}$, and Han Qiu. 2023. The earth is flat because...: Investigating llms' belief towards misinformation via persuasive conversation\", \"Xianjun Yang, Xiao Wang, Qi Zhang, Linda Petzold, William Yang Wang, Xun Zhao, and Dahua Lin. 2023. Shadow alignment: The ease of subverting safely-aligned language models\", \"Zheng-Xin Yong, Cristina Menghini, and Stephen H Bach. 2023. Low-resource languages jailbreak gpt-4\", \"Jiahao Yu, Xingwei Lin, and Xinyu Xing. 2023. Gptfuzzer: Red teaming large language models with auto-generated jailbreak prompts\", \"Youliang Yuan, Wenxiang Jiao, Wenxuan Wang, Jen-tse Huang, Pinjia He, Shuming Shi, and Zhaopeng Tu. 2023. Gpt-4 is too smart to be safe: Stealthy chat with $11 \\mathrm{~ms}$ via cipher\", \"Andy Zou, Zifan Wang, J Zico Kolter, and Matt Fredrikson. 2023. Universal and transferable adversarial attacks on aligned language models\"], \"Citation\":[]}",
        "eval_response": "Evaluation evidence: The assistant's answer does not align with the gold answer in terms of accuracy and hallucinations. The assistant lists numerous references that are not mentioned in the gold answer, indicating a significant amount of hallucinated content. Additionally, the assistant fails to identify any citations, whereas the gold answer specifies one. This discrepancy shows a lack of accuracy in identifying the correct citation relationships. The answer also lacks completeness as it does not correctly reflect the relationships outlined in the gold answer.\n\nRating: [[20]]",
        "score": 20
    },
    {
        "level": 3,
        "set": 2,
        "length": 57363,
        "type": "paper",
        "language": "en",
        "question": "AUtoAcT: Automatic Agent Learning from Scratch for QA via Self-Planning",
        "instruction": "We hope you will carefully study the provided papers and determine the citation relationships between them. Please follow the instructions below strictly to complete the task:\n\n#Specific Requirements:\n1. Reference: When a given paper mentions other provided papers, those other papers are considered as \"references\" for the given paper. To summarize in this specific context, references are about what the given paper is using.\n2. Citation: Conversely, when other provided papers mention the given paper in their works, the given paper is being \"cited\" by those other papers. To summarize in this specific context, citations are about who is using the given paper.\n3. Given a paper, you need to determine the citation or reference relationship between this paper and the other papers. Do not consider papers that are not provided.\n3. Please present the paper titles in a json format as follows: {{\"Reference\":[\"Reference Title 1\", \"Reference Title 2\", ..., \"Reference Title n\"], \"Citation\":[\"Citation Title 1\", \"Citation Title 2\", ..., \"Citation Title n\"]}}.\n4. If a paper does not have any references or citations, please leave the corresponding list empty, e.g.{{\"Refernce\":[]}}, {{\"Citation\":[]}}.",
        "prompt_template": "#Papers:\n{docs}\n\n{instruction}\n\n#The paper you need to analyze:\n{question}",
        "doc": [
            "2401.05268.md",
            "2403.19154.md",
            "2405.14205.md"
        ],
        "answer": {
            "Reference": [],
            "Citation": [
                "# STaR-GATE: <br> Teaching Language Models to Ask Clarifying Questions ",
                "# Agent Planning with World Knowledge Model "
            ]
        },
        "shuffle_doc": true,
        "id": "a05907b0-87b2-4776-8be9-2b3ece704493",
        "prompt": "[Question]\n#Papers:\n{docs}\n\nWe hope you will carefully study the provided papers and determine the citation relationships between them. Please follow the instructions below strictly to complete the task:\n\n#Specific Requirements:\n1. Reference: When a given paper mentions other provided papers, those other papers are considered as \"references\" for the given paper. To summarize in this specific context, references are about what the given paper is using.\n2. Citation: Conversely, when other provided papers mention the given paper in their works, the given paper is being \"cited\" by those other papers. To summarize in this specific context, citations are about who is using the given paper.\n3. Given a paper, you need to determine the citation or reference relationship between this paper and the other papers. Do not consider papers that are not provided.\n3. Please present the paper titles in a json format as follows: {{\"Reference\":[\"Reference Title 1\", \"Reference Title 2\", ..., \"Reference Title n\"], \"Citation\":[\"Citation Title 1\", \"Citation Title 2\", ..., \"Citation Title n\"]}}.\n4. If a paper does not have any references or citations, please leave the corresponding list empty, e.g.{{\"Refernce\":[]}}, {{\"Citation\":[]}}.\n\n#The paper you need to analyze:\nAUtoAcT: Automatic Agent Learning from Scratch for QA via Self-Planning\n\n[Gold Answer]\n{'Reference': [], 'Citation': ['# STaR-GATE: <br> Teaching Language Models to Ask Clarifying Questions ', '# Agent Planning with World Knowledge Model ']}\n\n[The Start of Assistant's Predicted Answer]\n{\"Reference\": [\"A Survey on Evaluation of Large Language Models\", \"Rest meets react: Self-improvement for multistep reasoning llm agent\", \"Fireact: Toward language agent fine-tuning\", \"Reasoning with language model prompting: A survey\", \"Large language models can self-improve\", \"Large language models cannot self-correct reasoning yet\", \"Encouraging divergent thinking in large language models through multi-agent debate\", \"Learn to explain: Multimodal reasoning via thought chains for science question answering\", \"Chameleon: Plug-and-play compositional reasoning with large language models\", \"Self-refine: Iterative refinement with self-feedback\", \"Gorilla: Large language model connected with massive apis\", \"Reasoning with language model prompting: A survey (duplicate)\", \"Making language models better tool learners with execution feedback\", \"Hugginggpt: Solving AI tasks with chatgpt and its friends in huggingface\", \"Reflexion: language agents with verbal reinforcement learning\", \"Alfworld: Aligning text and embodied environments for interactive learning\", \"Llm-planner: Few-shot grounded planning for embodied agents with large language models\", \"Multiagent collaboration: Harnessing the power of intelligent LLM agents\", \"Medagents: Large language models as collaborators for zero-shot medical reasoning\", \"Stanford alpaca: An instruction-following llama model\", \"Xagent: An autonomous agent for complex task solving\", \"Chain-of-thought prompting elicits reasoning in large language models\", \"The rise and potential of large language model based agents: A survey\", \"Decoupling reasoning from observations for efficient augmented language models\", \"Hotpotqa: A dataset for diverse, explainable multi-hop question answering\", \"Webshop: Towards scalable realworld web interaction with grounded language agents\", \"React: Synergizing reasoning and acting in language models\", \"Learning agents with unified data, modular design, and open-source llms\", \"Star: Bootstrapping reasoning with reasoning\", \"Agenttuning: Enabling generalized agent abilities for llms\", \"Igniting language intelligence: The hitchhiker's guide from chain-of-thought reasoning to language agents\", \"Judging llm-as-a-judge with mt-bench and chatbot arena\", \"Webarena: A realistic web environment for building autonomous agents\", \"An open-source framework for autonomous language agents\"], \"Citation\": []}\n[The End of Assistant's Predicted Answer]\n\n[System]\nWe would like to request your feedback on the performance of the AI assistant in response to the user question displayed above according to the gold answer. Please use the following listed aspects and their descriptions as evaluation criteria:\n    - Accuracy and Hallucinations: The assistant's answer is semantically consistent with the gold answer; The numerical value and order need to be accurate, and there should be no hallucinations.\n    - Completeness: Referring to the reference answers, the assistant's answer should contain all the key points needed to answer the user's question; further elaboration on these key points can be omitted.\nPlease rate whether this answer is suitable for the question. Please note that the gold answer can be considered as a correct answer to the question.\n\nThe assistant receives an overall score on a scale of 1 to 100, where a higher score indicates better overall performance.\nPlease note that if the assistant's answer and the gold answer fully meet the above criteria, its overall rating should be the full marks (100).\nPlease first provide a comprehensive explanation of your evaluation, avoiding any potential bias.\nThen, output a line indicating the score of the Assistant.\n\nPLEASE OUTPUT WITH THE FOLLOWING FORMAT, WHERE THE SCORE IS A SCALE OF 1 TO 100 BY STRICTLY FOLLOWING THIS FORMAT: \"[[score]]\", FOR EXAMPLE \"Rating: [[100]]\":\n<start output>\nEvaluation evidence: your evluation explanation here, no more than 100 words\nRating: [[score]]\n<end output>\n\nNow, start your evaluation:",
        "generate_response": "{\"Reference\": [\"A Survey on Evaluation of Large Language Models\", \"Rest meets react: Self-improvement for multistep reasoning llm agent\", \"Fireact: Toward language agent fine-tuning\", \"Reasoning with language model prompting: A survey\", \"Large language models can self-improve\", \"Large language models cannot self-correct reasoning yet\", \"Encouraging divergent thinking in large language models through multi-agent debate\", \"Learn to explain: Multimodal reasoning via thought chains for science question answering\", \"Chameleon: Plug-and-play compositional reasoning with large language models\", \"Self-refine: Iterative refinement with self-feedback\", \"Gorilla: Large language model connected with massive apis\", \"Reasoning with language model prompting: A survey (duplicate)\", \"Making language models better tool learners with execution feedback\", \"Hugginggpt: Solving AI tasks with chatgpt and its friends in huggingface\", \"Reflexion: language agents with verbal reinforcement learning\", \"Alfworld: Aligning text and embodied environments for interactive learning\", \"Llm-planner: Few-shot grounded planning for embodied agents with large language models\", \"Multiagent collaboration: Harnessing the power of intelligent LLM agents\", \"Medagents: Large language models as collaborators for zero-shot medical reasoning\", \"Stanford alpaca: An instruction-following llama model\", \"Xagent: An autonomous agent for complex task solving\", \"Chain-of-thought prompting elicits reasoning in large language models\", \"The rise and potential of large language model based agents: A survey\", \"Decoupling reasoning from observations for efficient augmented language models\", \"Hotpotqa: A dataset for diverse, explainable multi-hop question answering\", \"Webshop: Towards scalable realworld web interaction with grounded language agents\", \"React: Synergizing reasoning and acting in language models\", \"Learning agents with unified data, modular design, and open-source llms\", \"Star: Bootstrapping reasoning with reasoning\", \"Agenttuning: Enabling generalized agent abilities for llms\", \"Igniting language intelligence: The hitchhiker's guide from chain-of-thought reasoning to language agents\", \"Judging llm-as-a-judge with mt-bench and chatbot arena\", \"Webarena: A realistic web environment for building autonomous agents\", \"An open-source framework for autonomous language agents\"], \"Citation\": []}",
        "eval_response": "Evaluation evidence: The assistant's answer does not align with the gold answer. The gold answer specifies two citations and no references for the paper \"AUtoAcT: Automatic Agent Learning from Scratch for QA via Self-Planning,\" whereas the assistant lists numerous references and no citations. This discrepancy indicates a significant error in understanding the task requirements, leading to inaccuracies and hallucinations in the response. The assistant's answer fails to meet the completeness and accuracy criteria.\n\nRating: [[10]]",
        "score": 10
    },
    {
        "level": 3,
        "set": 3,
        "length": 175947,
        "type": "paper",
        "language": "en",
        "question": "Self-Rewarding Language Models",
        "instruction": "We hope you will carefully study the provided papers and determine the citation relationships between them. Please follow the instructions below strictly to complete the task:\n\n#Specific Requirements:\n1. Reference: When a given paper mentions other provided papers, those other papers are considered as \"references\" for the given paper. To summarize in this specific context, references are about what the given paper is using.\n2. Citation: Conversely, when other provided papers mention the given paper in their works, the given paper is being \"cited\" by those other papers. To summarize in this specific context, citations are about who is using the given paper.\n3. Given a paper, you need to determine the citation or reference relationship between this paper and the other papers. Do not consider papers that are not provided.\n3. Please present the paper titles in a json format as follows: {{\"Reference\":[\"Reference Title 1\", \"Reference Title 2\", ..., \"Reference Title n\"], \"Citation\":[\"Citation Title 1\", \"Citation Title 2\", ..., \"Citation Title n\"]}}.\n4. If a paper does not have any references or citations, please leave the corresponding list empty, e.g.{{\"Refernce\":[]}}, {{\"Citation\":[]}}.",
        "prompt_template": "#Papers:\n{docs}\n\n{instruction}\n\n#The paper you need to analyze:\n{question}",
        "doc": [
            "2401.10020.md",
            "2405.16388.md",
            "2405.16714.md",
            "2405.16833.md",
            "2405.17051.md",
            "2405.17220.md",
            "2309.00267.md",
            "2310.08491.md",
            "2310.15123.md",
            "2312.16682.md",
            "2401.01335.md"
        ],
        "answer": {
            "Reference": [
                "# RLAIF: Scaling Reinforcement Learning from Human Feedback with AI Feedback ",
                "# (3) PROMETHEUS: INDUCING FINE-GRAINED EVALUATION CAPABILITY IN LANGUAGE MODELS ",
                "# BRANCH-SOLVE-MERGE IMPROVES LARGE LANGUAGE MODEL EVALUATION AND GENERATION ",
                "# Some things are more CRINGE than others: Iterative Preference Optimization with the Pairwise Cringe Loss ",
                "# Self-Play Fine-Tuning Converts Weak Language Models to Strong Language Models "
            ],
            "Citation": [
                "# Multi-Reference Preference Optimization for Large Language Models ",
                "# Crafting Interpretable Embeddings by Asking LLMs Questions ",
                "# Safe LoRA: the Silver Lining of Reducing Safety Risks when Fine-tuning Large Language Models ",
                "# BeamVQ: Aligning Space-Time Forecasting Model via Self-training on Physics-aware Metrics ",
                "# RLAIF-V: Aligning MLLMs through Open-Source AI Feedback for Super GPT-4V Trustworthiness "
            ]
        },
        "shuffle_doc": true,
        "id": "c9297c9e-8fbb-4aa8-a81d-e8643aa20027",
        "prompt": "[Question]\n#Papers:\n{docs}\n\nWe hope you will carefully study the provided papers and determine the citation relationships between them. Please follow the instructions below strictly to complete the task:\n\n#Specific Requirements:\n1. Reference: When a given paper mentions other provided papers, those other papers are considered as \"references\" for the given paper. To summarize in this specific context, references are about what the given paper is using.\n2. Citation: Conversely, when other provided papers mention the given paper in their works, the given paper is being \"cited\" by those other papers. To summarize in this specific context, citations are about who is using the given paper.\n3. Given a paper, you need to determine the citation or reference relationship between this paper and the other papers. Do not consider papers that are not provided.\n3. Please present the paper titles in a json format as follows: {{\"Reference\":[\"Reference Title 1\", \"Reference Title 2\", ..., \"Reference Title n\"], \"Citation\":[\"Citation Title 1\", \"Citation Title 2\", ..., \"Citation Title n\"]}}.\n4. If a paper does not have any references or citations, please leave the corresponding list empty, e.g.{{\"Refernce\":[]}}, {{\"Citation\":[]}}.\n\n#The paper you need to analyze:\nSelf-Rewarding Language Models\n\n[Gold Answer]\n{'Reference': ['# RLAIF: Scaling Reinforcement Learning from Human Feedback with AI Feedback ', '# (3) PROMETHEUS: INDUCING FINE-GRAINED EVALUATION CAPABILITY IN LANGUAGE MODELS ', '# BRANCH-SOLVE-MERGE IMPROVES LARGE LANGUAGE MODEL EVALUATION AND GENERATION ', '# Some things are more CRINGE than others: Iterative Preference Optimization with the Pairwise Cringe Loss ', '# Self-Play Fine-Tuning Converts Weak Language Models to Strong Language Models '], 'Citation': ['# Multi-Reference Preference Optimization for Large Language Models ', '# Crafting Interpretable Embeddings by Asking LLMs Questions ', '# Safe LoRA: the Silver Lining of Reducing Safety Risks when Fine-tuning Large Language Models ', '# BeamVQ: Aligning Space-Time Forecasting Model via Self-training on Physics-aware Metrics ', '# RLAIF-V: Aligning MLLMs through Open-Source AI Feedback for Super GPT-4V Trustworthiness ']}\n\n[The Start of Assistant's Predicted Answer]\n{\"Reference\":[\"GPT-4 technical report\", \"The CRINGE loss: Learning what language not to model\", \"Claude 2\", \"Training a helpful and harmless assistant with reinforcement learning from human feedback\", \"Constitutional AI: Harmlessness from AI feedback\", \"Benchmarking foundation models with language-model-as-an-examiner\", \"AlpaGasus: Training a better alpaca with fewer data\", \"Self-play fine-tuning converts weak language models to strong language models\", \"Think you have solved question answering? Try ARC, the AI2 reasoning challenge\", \"Training verifiers to solve math word problems\", \"A unified architecture for natural language processing: Deep neural networks with multitask learning\", \"Alpacafarm: A simulation framework for methods that learn from human feedback\", \"Reinforced self-training (rest) for language modeling\", \"Measuring massive multitask language understanding\", \"RLAIF: Scaling reinforcement learning from human feedback with AI feedback\", \"Self-alignment with instruction backtranslation\", \"ROUGE: A package for automatic evaluation of summaries\", \"Can a suit of armor conduct electricity? a new dataset for open book question answering\", \"Training language models to follow instructions with human feedback\", \"Automatically correcting large language models: Surveying the landscape of diverse self-correction strategies\", \"Language models are unsupervised multitask learners\", \"Direct preference optimization: Your language model is secretly a reward model\", \"Learning to summarize with human feedback\", \"Stanford alpaca: An instruction-following llama model\", \"Llama 2: Open foundation and fine-tuned chat models\", \"Visualizing data using t-SNE\", \"Self-instruct: Aligning language models with self-generated instructions\", \"Gibbs sampling from human feedback: A provable kl-constrained framework for rlhf\", \"Some things are more cringe than others: Preference optimization with the pairwise cringe loss\", \"RRHF: Rank responses to align language models with human feedback\", \"Hellaswag: Can a machine really finish your sentence?\", \"Sequence likelihood calibration with human feedback\", \"Click: Controllable text generation with sequence likelihood contrastive learning\", \"Judging LLM-as-a-judge with MT-bench and chatbot arena\", \"Fine-tuning language models from human preferences\"], \"Citation\":[\"BeamVQ: Aligning Space-Time Forecasting Model via Self-training on Physics-aware Metrics\", \"BRANCH-SOLVE-MERGE IMPROVES LARGE LANGUAGE MODEL EVALUATION AND GENERATION\", \"Crafting Interpretable Embeddings by Asking LLMs Questions\", \"Safe LoRA: the Silver Lining of Reducing Safety Risks when Fine-tuning Large Language Models\", \"RLAIF-V: Aligning MLLMs through Open-Source AI Feedback for Super GPT-4V Trustworthiness\"]}\n[The End of Assistant's Predicted Answer]\n\n[System]\nWe would like to request your feedback on the performance of the AI assistant in response to the user question displayed above according to the gold answer. Please use the following listed aspects and their descriptions as evaluation criteria:\n    - Accuracy and Hallucinations: The assistant's answer is semantically consistent with the gold answer; The numerical value and order need to be accurate, and there should be no hallucinations.\n    - Completeness: Referring to the reference answers, the assistant's answer should contain all the key points needed to answer the user's question; further elaboration on these key points can be omitted.\nPlease rate whether this answer is suitable for the question. Please note that the gold answer can be considered as a correct answer to the question.\n\nThe assistant receives an overall score on a scale of 1 to 100, where a higher score indicates better overall performance.\nPlease note that if the assistant's answer and the gold answer fully meet the above criteria, its overall rating should be the full marks (100).\nPlease first provide a comprehensive explanation of your evaluation, avoiding any potential bias.\nThen, output a line indicating the score of the Assistant.\n\nPLEASE OUTPUT WITH THE FOLLOWING FORMAT, WHERE THE SCORE IS A SCALE OF 1 TO 100 BY STRICTLY FOLLOWING THIS FORMAT: \"[[score]]\", FOR EXAMPLE \"Rating: [[100]]\":\n<start output>\nEvaluation evidence: your evluation explanation here, no more than 100 words\nRating: [[score]]\n<end output>\n\nNow, start your evaluation:",
        "generate_response": "{\"Reference\":[\"GPT-4 technical report\", \"The CRINGE loss: Learning what language not to model\", \"Claude 2\", \"Training a helpful and harmless assistant with reinforcement learning from human feedback\", \"Constitutional AI: Harmlessness from AI feedback\", \"Benchmarking foundation models with language-model-as-an-examiner\", \"AlpaGasus: Training a better alpaca with fewer data\", \"Self-play fine-tuning converts weak language models to strong language models\", \"Think you have solved question answering? Try ARC, the AI2 reasoning challenge\", \"Training verifiers to solve math word problems\", \"A unified architecture for natural language processing: Deep neural networks with multitask learning\", \"Alpacafarm: A simulation framework for methods that learn from human feedback\", \"Reinforced self-training (rest) for language modeling\", \"Measuring massive multitask language understanding\", \"RLAIF: Scaling reinforcement learning from human feedback with AI feedback\", \"Self-alignment with instruction backtranslation\", \"ROUGE: A package for automatic evaluation of summaries\", \"Can a suit of armor conduct electricity? a new dataset for open book question answering\", \"Training language models to follow instructions with human feedback\", \"Automatically correcting large language models: Surveying the landscape of diverse self-correction strategies\", \"Language models are unsupervised multitask learners\", \"Direct preference optimization: Your language model is secretly a reward model\", \"Learning to summarize with human feedback\", \"Stanford alpaca: An instruction-following llama model\", \"Llama 2: Open foundation and fine-tuned chat models\", \"Visualizing data using t-SNE\", \"Self-instruct: Aligning language models with self-generated instructions\", \"Gibbs sampling from human feedback: A provable kl-constrained framework for rlhf\", \"Some things are more cringe than others: Preference optimization with the pairwise cringe loss\", \"RRHF: Rank responses to align language models with human feedback\", \"Hellaswag: Can a machine really finish your sentence?\", \"Sequence likelihood calibration with human feedback\", \"Click: Controllable text generation with sequence likelihood contrastive learning\", \"Judging LLM-as-a-judge with MT-bench and chatbot arena\", \"Fine-tuning language models from human preferences\"], \"Citation\":[\"BeamVQ: Aligning Space-Time Forecasting Model via Self-training on Physics-aware Metrics\", \"BRANCH-SOLVE-MERGE IMPROVES LARGE LANGUAGE MODEL EVALUATION AND GENERATION\", \"Crafting Interpretable Embeddings by Asking LLMs Questions\", \"Safe LoRA: the Silver Lining of Reducing Safety Risks when Fine-tuning Large Language Models\", \"RLAIF-V: Aligning MLLMs through Open-Source AI Feedback for Super GPT-4V Trustworthiness\"]}",
        "eval_response": "Evaluation evidence: The assistant's answer does not match the gold answer in terms of the specific papers listed under \"Reference\" and \"Citation.\" The titles mentioned in the assistant's response are largely different and not present in the gold answer, indicating a significant deviation and hallucination of content. This misalignment suggests inaccuracies in identifying the correct citation and reference relationships for the paper \"Self-Rewarding Language Models.\" Therefore, the response fails in terms of accuracy and completeness.\n\nRating: [[20]]",
        "score": 20
    },
    {
        "level": 4,
        "set": 2,
        "length": 82787,
        "type": "paper",
        "language": "en",
        "question": "",
        "instruction": "We kindly ask you to thoroughly review the provided papers and construct a citation chain from them. Please adhere to the following instructions strictly while completing the task:\n\n#Task Instructions:\nGiven several papers, you are required to identify and list the longest citation chain, which demonstrates the citation relationship among the provided papers.\n\n#Specific Requirements:\n1.Please present the titles of the papers in the form of a list, as follows: [\"Title of Paper 1\", \"Title of Paper 2\", ..., \"Title of Paper n\"].\n2.Ensure that the citation chain in the list is linear and continuous, meaning that the first paper title in the list (Paper 1) should not cite any other works. Instead, it should be cited by the next paper in the list (Paper 2); subsequently, each paper should then be cited by the next one in the list, continuing up to the last paper (Paper n).\n3.Consider only the citation relationships within the supplied collection of papers, and ensure that the citation chain accurately reflects the sequential citation order among these documents.\n4.Do not take into account any articles not provided, and disregard other non-linear citation relationships.",
        "prompt_template": "{instruction}\n\n#Papers Provided:\n{docs}",
        "doc": [
            "2309.08532.md",
            "2310.05204.md",
            "2310.12541.md",
            "2310.19046.md",
            "2405.07761.md"
        ],
        "answer": [
            "# ConneCTING LARGE LANGUAGE MODELS WITH EVOLUTIONARY ALGORITHMS YIELDS POWERFUL PROMPT OPTIMIZERS ",
            "# Towards Optimizing with Large Language Model ",
            "# Large Language Model for Multi-objective Evolutionary Optimization ",
            "# Large Language Models as Evolutionary Optimizers ",
            "# LLM4ED: LARGE LANGUAGE MODELS FOR AUTOMATIC EQUATION DISCOVERY "
        ],
        "shuffle_doc": true,
        "id": "9c764a1e-c9eb-4c8c-adc2-859492ba5d84",
        "prompt": "[Question]\nWe kindly ask you to thoroughly review the provided papers and construct a citation chain from them. Please adhere to the following instructions strictly while completing the task:\n\n#Task Instructions:\nGiven several papers, you are required to identify and list the longest citation chain, which demonstrates the citation relationship among the provided papers.\n\n#Specific Requirements:\n1.Please present the titles of the papers in the form of a list, as follows: [\"Title of Paper 1\", \"Title of Paper 2\", ..., \"Title of Paper n\"].\n2.Ensure that the citation chain in the list is linear and continuous, meaning that the first paper title in the list (Paper 1) should not cite any other works. Instead, it should be cited by the next paper in the list (Paper 2); subsequently, each paper should then be cited by the next one in the list, continuing up to the last paper (Paper n).\n3.Consider only the citation relationships within the supplied collection of papers, and ensure that the citation chain accurately reflects the sequential citation order among these documents.\n4.Do not take into account any articles not provided, and disregard other non-linear citation relationships.\n\n#Papers Provided:\n{docs}\n\n[Gold Answer]\n['# ConneCTING LARGE LANGUAGE MODELS WITH EVOLUTIONARY ALGORITHMS YIELDS POWERFUL PROMPT OPTIMIZERS ', '# Towards Optimizing with Large Language Model ', '# Large Language Model for Multi-objective Evolutionary Optimization ', '# Large Language Models as Evolutionary Optimizers ', '# LLM4ED: LARGE LANGUAGE MODELS FOR AUTOMATIC EQUATION DISCOVERY ']\n\n[The Start of Assistant's Predicted Answer]\n[\"Towards Optimizing with Large Language Model\", \"Connecting Large Language Models with Evolutionary Algorithms Yields Powerful Prompt Optimizers\", \"Evoprompting: Language models for code-level neural architecture search\", \"Can gpt-4 perform neural architecture search?\", \"Recent advances in natural language processing via large pre-trained language models: A survey\", \"Benefits, limits, and risks of gpt-4 as an ai chatbot for medicine\"]\n[The End of Assistant's Predicted Answer]\n\n[System]\nWe would like to request your feedback on the performance of the AI assistant in response to the user question displayed above according to the gold answer. Please use the following listed aspects and their descriptions as evaluation criteria:\n    - Accuracy and Hallucinations: The assistant's answer is semantically consistent with the gold answer; The numerical value and order need to be accurate, and there should be no hallucinations.\n    - Completeness: Referring to the reference answers, the assistant's answer should contain all the key points needed to answer the user's question; further elaboration on these key points can be omitted.\nPlease rate whether this answer is suitable for the question. Please note that the gold answer can be considered as a correct answer to the question.\n\nThe assistant receives an overall score on a scale of 1 to 100, where a higher score indicates better overall performance.\nPlease note that if the assistant's answer and the gold answer fully meet the above criteria, its overall rating should be the full marks (100).\nPlease first provide a comprehensive explanation of your evaluation, avoiding any potential bias.\nThen, output a line indicating the score of the Assistant.\n\nPLEASE OUTPUT WITH THE FOLLOWING FORMAT, WHERE THE SCORE IS A SCALE OF 1 TO 100 BY STRICTLY FOLLOWING THIS FORMAT: \"[[score]]\", FOR EXAMPLE \"Rating: [[100]]\":\n<start output>\nEvaluation evidence: your evluation explanation here, no more than 100 words\nRating: [[score]]\n<end output>\n\nNow, start your evaluation:",
        "generate_response": "[\"Towards Optimizing with Large Language Model\", \"Connecting Large Language Models with Evolutionary Algorithms Yields Powerful Prompt Optimizers\", \"Evoprompting: Language models for code-level neural architecture search\", \"Can gpt-4 perform neural architecture search?\", \"Recent advances in natural language processing via large pre-trained language models: A survey\", \"Benefits, limits, and risks of gpt-4 as an ai chatbot for medicine\"]",
        "eval_response": "Evaluation evidence: The assistant's answer does not match the gold answer in terms of the titles and the sequence of the citation chain. The titles listed in the assistant's answer do not appear in the gold answer, indicating a significant deviation from the required task. The citation chain is supposed to be linear and continuous, but the assistant's response includes titles that are not related or cited in sequence as per the task's instructions. This results in both inaccuracies and incompleteness in addressing the user's question.\n\nRating: [[20]]",
        "score": 20
    },
    {
        "level": 3,
        "set": 4,
        "length": 204127,
        "type": "paper",
        "language": "en",
        "question": "Multilingual Instruction Tuning With Just a Pinch of Multilinguality",
        "instruction": "We hope you will carefully study the provided papers and determine the citation relationships between them. Please follow the instructions below strictly to complete the task:\n\n#Specific Requirements:\n1. Reference: When a given paper mentions other provided papers, those other papers are considered as \"references\" for the given paper. To summarize in this specific context, references are about what the given paper is using.\n2. Citation: Conversely, when other provided papers mention the given paper in their works, the given paper is being \"cited\" by those other papers. To summarize in this specific context, citations are about who is using the given paper.\n3. Given a paper, you need to determine the citation or reference relationship between this paper and the other papers. Do not consider papers that are not provided.\n3. Please present the paper titles in a json format as follows: {{\"Reference\":[\"Reference Title 1\", \"Reference Title 2\", ..., \"Reference Title n\"], \"Citation\":[\"Citation Title 1\", \"Citation Title 2\", ..., \"Citation Title n\"]}}.\n4. If a paper does not have any references or citations, please leave the corresponding list empty, e.g.{{\"Refernce\":[]}}, {{\"Citation\":[]}}.",
        "prompt_template": "#Papers:\n{docs}\n\n{instruction}\n\n#The paper you need to analyze:\n{question}",
        "doc": [
            "2401.01854.md",
            "2404.04850.md",
            "2404.04925.md",
            "2404.11932.md",
            "2404.12318.md",
            "2405.15604.md",
            "2306.05685.md",
            "2307.16039.md",
            "2309.08958.md",
            "2311.08711.md",
            "2312.12683.md"
        ],
        "answer": {
            "Reference": [
                "# Judging LLM-as-a-Judge with MT-Bench and Chatbot Arena ",
                "# Okapi: Instruction-tuned Large Language Models in Multiple Languages with Reinforcement Learning from Human Feedback ",
                "# Monolingual or Multilingual Instruction Tuning: Which Makes a Better Alpaca ",
                "# PLUG: Leveraging Pivot Language in Cross-Lingual Instruction Tuning ",
                "# Turning English-centric LLMs Into Polyglots: How Much Multilinguality Is Needed? "
            ],
            "Citation": [
                "# Lucky 52: How Many Languages Are Needed to Instruction Fine-Tune Large Language Models? ",
                "# CrossIn: An Efficient Instruction Tuning Approach for Cross-Lingual Knowledge Alignment ",
                "# Reuse Your Rewards: Reward Model Transfer for Zero-Shot Cross-Lingual Alignment ",
                "# Text Generation: A Systematic Literature Review of Tasks, Evaluation, and Challenges "
            ]
        },
        "shuffle_doc": true,
        "id": "dc0fd947-67dc-4443-888f-334831a31d8b",
        "prompt": "[Question]\n#Papers:\n{docs}\n\nWe hope you will carefully study the provided papers and determine the citation relationships between them. Please follow the instructions below strictly to complete the task:\n\n#Specific Requirements:\n1. Reference: When a given paper mentions other provided papers, those other papers are considered as \"references\" for the given paper. To summarize in this specific context, references are about what the given paper is using.\n2. Citation: Conversely, when other provided papers mention the given paper in their works, the given paper is being \"cited\" by those other papers. To summarize in this specific context, citations are about who is using the given paper.\n3. Given a paper, you need to determine the citation or reference relationship between this paper and the other papers. Do not consider papers that are not provided.\n3. Please present the paper titles in a json format as follows: {{\"Reference\":[\"Reference Title 1\", \"Reference Title 2\", ..., \"Reference Title n\"], \"Citation\":[\"Citation Title 1\", \"Citation Title 2\", ..., \"Citation Title n\"]}}.\n4. If a paper does not have any references or citations, please leave the corresponding list empty, e.g.{{\"Refernce\":[]}}, {{\"Citation\":[]}}.\n\n#The paper you need to analyze:\nMultilingual Instruction Tuning With Just a Pinch of Multilinguality\n\n[Gold Answer]\n{'Reference': ['# Judging LLM-as-a-Judge with MT-Bench and Chatbot Arena ', '# Okapi: Instruction-tuned Large Language Models in Multiple Languages with Reinforcement Learning from Human Feedback ', '# Monolingual or Multilingual Instruction Tuning: Which Makes a Better Alpaca ', '# PLUG: Leveraging Pivot Language in Cross-Lingual Instruction Tuning ', '# Turning English-centric LLMs Into Polyglots: How Much Multilinguality Is Needed? '], 'Citation': ['# Lucky 52: How Many Languages Are Needed to Instruction Fine-Tune Large Language Models? ', '# CrossIn: An Efficient Instruction Tuning Approach for Cross-Lingual Knowledge Alignment ', '# Reuse Your Rewards: Reward Model Transfer for Zero-Shot Cross-Lingual Alignment ', '# Text Generation: A Systematic Literature Review of Tasks, Evaluation, and Challenges ']}\n\n[The Start of Assistant's Predicted Answer]\n{\"Reference\": [\"A Survey on Evaluation of Large Language Models\", \"Vicuna: An Open-Source Chatbot Impressing GPT-4 with $90 \\% *$ ChatGPT Quality\", \"ChatLaw: Open-Source Legal Large Language Model with Integrated External Knowledge Bases\", \"Compositional Semantic Parsing with Large Language Models\", \"Open LLM Leaderboard\", \"How Close is ChatGPT to Human Experts? Comparison Corpus, Evaluation, and Detection\", \"REALM: Retrieval-Augmented Language Model PreTraining\", \"Rethinking with Retrieval: Faithful Large Language Model Inference\", \"Measuring Massive Multitask Language Understanding\", \"Atlas: Few-shot Learning with Retrieval Augmented Language Models\", \"Palm 2 technical report\", \"Massively multilingual sentence embeddings for zeroshot cross-lingual transfer and beyond\", \"Training a helpful and harmless assistant with reinforcement learning from human feedback\", \"Sparks of artificial general intelligence: Early experiments with gpt-4\", \"Monolingual or multilingual instruction tuning: Which makes a better alpaca\", \"How do languages influence each other? studying cross-lingual data sharing during LM fine-tuning\", \"Unsupervised cross-lingual representation learning at scale\", \"Emerging cross-lingual structure in pretrained language models\", \"QLoRA: Efficient finetuning of quantized LLMs\", \"BERT: Pre-training of deep bidirectional transformers for language understanding\", \"Alpacafarm: A simulation framework for methods that learn from human feedback\", \"Scaling laws for multilingual neural machine translation\", \"Koala: A dialogue model for academic research\", \"The false promise of imitating proprietary llms\", \"The curious case of neural text degeneration\", \"LoRA: Low-rank adaptation of large language models\", \"XTREME: A massively multilingual multitask benchmark for evaluating cross-lingual generalisation\", \"Cross-lingual ability of multilingual bert: An empirical study\", \"Turning english-centric llms into polyglots: How much multilinguality is needed?\", \"Openassistant conversations - democratizing large language model alignment\", \"Self-alignment with instruction backtranslation\", \"ROUGE: A package for automatic evaluation of summaries\", \"A balanced data approach for evaluating cross-lingual transfer: Mapping the linguistic blood bank\", \"Cross-task generalization via natural language crowdsourcing instructions\", \"Crosslingual generalization through multitask finetuning\", \"Exploring the limits of transfer learning with a unified text-to-text transformer\", \"Multitask prompted training enables zero-shot task generalization\", \"Causes and cures for interference in multilingual translation\", \"Stanford alpaca: An instruction-following llama model\", \"Llama: Open and efficient foundation language models\", \"Llama 2: Open foundation and fine-tuned chat models\", \"Attention is all you need\", \"Finetuned language models are zero-shot learners\", \"Beto, bentz, becas: The surprising cross-lingual effectiveness of BERT\", \"mT5: A massively multilingual pre-trained text-to-text transformer\", \"Language versatilists vs. specialists: An empirical revisiting on multilingual transfer ability\", \"Judging llm-as-a-judge with mt-bench and chatbot arena\", \"Lima: Less is more for alignment\", \"Plug: Leveraging pivot language in crosslingual instruction tuning\"], \"Citation\": []}\n[The End of Assistant's Predicted Answer]\n\n[System]\nWe would like to request your feedback on the performance of the AI assistant in response to the user question displayed above according to the gold answer. Please use the following listed aspects and their descriptions as evaluation criteria:\n    - Accuracy and Hallucinations: The assistant's answer is semantically consistent with the gold answer; The numerical value and order need to be accurate, and there should be no hallucinations.\n    - Completeness: Referring to the reference answers, the assistant's answer should contain all the key points needed to answer the user's question; further elaboration on these key points can be omitted.\nPlease rate whether this answer is suitable for the question. Please note that the gold answer can be considered as a correct answer to the question.\n\nThe assistant receives an overall score on a scale of 1 to 100, where a higher score indicates better overall performance.\nPlease note that if the assistant's answer and the gold answer fully meet the above criteria, its overall rating should be the full marks (100).\nPlease first provide a comprehensive explanation of your evaluation, avoiding any potential bias.\nThen, output a line indicating the score of the Assistant.\n\nPLEASE OUTPUT WITH THE FOLLOWING FORMAT, WHERE THE SCORE IS A SCALE OF 1 TO 100 BY STRICTLY FOLLOWING THIS FORMAT: \"[[score]]\", FOR EXAMPLE \"Rating: [[100]]\":\n<start output>\nEvaluation evidence: your evluation explanation here, no more than 100 words\nRating: [[score]]\n<end output>\n\nNow, start your evaluation:",
        "generate_response": "{\"Reference\": [\"A Survey on Evaluation of Large Language Models\", \"Vicuna: An Open-Source Chatbot Impressing GPT-4 with $90 \\% *$ ChatGPT Quality\", \"ChatLaw: Open-Source Legal Large Language Model with Integrated External Knowledge Bases\", \"Compositional Semantic Parsing with Large Language Models\", \"Open LLM Leaderboard\", \"How Close is ChatGPT to Human Experts? Comparison Corpus, Evaluation, and Detection\", \"REALM: Retrieval-Augmented Language Model PreTraining\", \"Rethinking with Retrieval: Faithful Large Language Model Inference\", \"Measuring Massive Multitask Language Understanding\", \"Atlas: Few-shot Learning with Retrieval Augmented Language Models\", \"Palm 2 technical report\", \"Massively multilingual sentence embeddings for zeroshot cross-lingual transfer and beyond\", \"Training a helpful and harmless assistant with reinforcement learning from human feedback\", \"Sparks of artificial general intelligence: Early experiments with gpt-4\", \"Monolingual or multilingual instruction tuning: Which makes a better alpaca\", \"How do languages influence each other? studying cross-lingual data sharing during LM fine-tuning\", \"Unsupervised cross-lingual representation learning at scale\", \"Emerging cross-lingual structure in pretrained language models\", \"QLoRA: Efficient finetuning of quantized LLMs\", \"BERT: Pre-training of deep bidirectional transformers for language understanding\", \"Alpacafarm: A simulation framework for methods that learn from human feedback\", \"Scaling laws for multilingual neural machine translation\", \"Koala: A dialogue model for academic research\", \"The false promise of imitating proprietary llms\", \"The curious case of neural text degeneration\", \"LoRA: Low-rank adaptation of large language models\", \"XTREME: A massively multilingual multitask benchmark for evaluating cross-lingual generalisation\", \"Cross-lingual ability of multilingual bert: An empirical study\", \"Turning english-centric llms into polyglots: How much multilinguality is needed?\", \"Openassistant conversations - democratizing large language model alignment\", \"Self-alignment with instruction backtranslation\", \"ROUGE: A package for automatic evaluation of summaries\", \"A balanced data approach for evaluating cross-lingual transfer: Mapping the linguistic blood bank\", \"Cross-task generalization via natural language crowdsourcing instructions\", \"Crosslingual generalization through multitask finetuning\", \"Exploring the limits of transfer learning with a unified text-to-text transformer\", \"Multitask prompted training enables zero-shot task generalization\", \"Causes and cures for interference in multilingual translation\", \"Stanford alpaca: An instruction-following llama model\", \"Llama: Open and efficient foundation language models\", \"Llama 2: Open foundation and fine-tuned chat models\", \"Attention is all you need\", \"Finetuned language models are zero-shot learners\", \"Beto, bentz, becas: The surprising cross-lingual effectiveness of BERT\", \"mT5: A massively multilingual pre-trained text-to-text transformer\", \"Language versatilists vs. specialists: An empirical revisiting on multilingual transfer ability\", \"Judging llm-as-a-judge with mt-bench and chatbot arena\", \"Lima: Less is more for alignment\", \"Plug: Leveraging pivot language in crosslingual instruction tuning\"], \"Citation\": []}",
        "eval_response": "Evaluation evidence: The assistant's answer does not match the gold answer in terms of accuracy and completeness. The references listed in the assistant's answer do not align with those in the gold answer, indicating a significant deviation and inclusion of unrelated titles. Additionally, the assistant's answer includes no citations, whereas the gold answer lists several, showing a lack of completeness. There are also hallucinations present as the assistant lists titles not mentioned in the gold answer or question prompt.\n\nRating: [[20]]",
        "score": 20
    },
    {
        "level": 3,
        "set": 2,
        "length": 80683,
        "type": "paper",
        "language": "en",
        "question": "OpenMoE: An Early Effort on Open Mixture-of-Experts Language Models",
        "instruction": "We hope you will carefully study the provided papers and determine the citation relationships between them. Please follow the instructions below strictly to complete the task:\n\n#Specific Requirements:\n1. Reference: When a given paper mentions other provided papers, those other papers are considered as \"references\" for the given paper. To summarize in this specific context, references are about what the given paper is using.\n2. Citation: Conversely, when other provided papers mention the given paper in their works, the given paper is being \"cited\" by those other papers. To summarize in this specific context, citations are about who is using the given paper.\n3. Given a paper, you need to determine the citation or reference relationship between this paper and the other papers. Do not consider papers that are not provided.\n3. Please present the paper titles in a json format as follows: {{\"Reference\":[\"Reference Title 1\", \"Reference Title 2\", ..., \"Reference Title n\"], \"Citation\":[\"Citation Title 1\", \"Citation Title 2\", ..., \"Citation Title n\"]}}.\n4. If a paper does not have any references or citations, please leave the corresponding list empty, e.g.{{\"Refernce\":[]}}, {{\"Citation\":[]}}.",
        "prompt_template": "#Papers:\n{docs}\n\n{instruction}\n\n#The paper you need to analyze:\n{question}",
        "doc": [
            "2402.01739.md",
            "2404.15247.md",
            "2405.03133.md",
            "2401.02385.md",
            "2401.04088.md"
        ],
        "answer": {
            "Reference": [
                "# TinyLlama: An Open-Source Small Language Model ",
                "# Mixtral of Experts "
            ],
            "Citation": [
                "# $\\mathcal{X}$ FT: Unlocking the Power of Code Instruction Tuning by Simply Merging Upcycled Mixture-of-Experts ",
                "# Lory: Fully Differentiable Mixture-of-Experts for Autoregressive Language Model Pre-training "
            ]
        },
        "shuffle_doc": true,
        "id": "b2257900-a70a-4987-a9ba-3c301497942d",
        "prompt": "[Question]\n#Papers:\n{docs}\n\nWe hope you will carefully study the provided papers and determine the citation relationships between them. Please follow the instructions below strictly to complete the task:\n\n#Specific Requirements:\n1. Reference: When a given paper mentions other provided papers, those other papers are considered as \"references\" for the given paper. To summarize in this specific context, references are about what the given paper is using.\n2. Citation: Conversely, when other provided papers mention the given paper in their works, the given paper is being \"cited\" by those other papers. To summarize in this specific context, citations are about who is using the given paper.\n3. Given a paper, you need to determine the citation or reference relationship between this paper and the other papers. Do not consider papers that are not provided.\n3. Please present the paper titles in a json format as follows: {{\"Reference\":[\"Reference Title 1\", \"Reference Title 2\", ..., \"Reference Title n\"], \"Citation\":[\"Citation Title 1\", \"Citation Title 2\", ..., \"Citation Title n\"]}}.\n4. If a paper does not have any references or citations, please leave the corresponding list empty, e.g.{{\"Refernce\":[]}}, {{\"Citation\":[]}}.\n\n#The paper you need to analyze:\nOpenMoE: An Early Effort on Open Mixture-of-Experts Language Models\n\n[Gold Answer]\n{'Reference': ['# TinyLlama: An Open-Source Small Language Model ', '# Mixtral of Experts '], 'Citation': ['# $\\\\mathcal{X}$ FT: Unlocking the Power of Code Instruction Tuning by Simply Merging Upcycled Mixture-of-Experts ', '# Lory: Fully Differentiable Mixture-of-Experts for Autoregressive Language Model Pre-training ']}\n\n[The Start of Assistant's Predicted Answer]\n{\"Reference\":[\"Palm 2 technical report\", \"inthe)wildchat: 570k chatGPT interaction logs in the wild\", \"Efficient large scale language modeling with mixtures of experts\", \"Beyond the imitation game: Quantifying and extrapolating the capabilities of language models\", \"Efficient training of language models to fill in the middle\", \"Findings of the 2016 conference on machine translation\", \"Language models are few-shot learners\", \"Evaluating large language models trained on code\", \"Palm: Scaling language modeling with pathways\", \"Unimax: Fairer and more effective language sampling for large-scale multilingual pretraining\", \"Megablocks: Efficient sparse training with mixture-of-experts\", \"A framework for few-shot language model evaluation\", \"Openllama: An open reproduction of llama\", \"Measuring massive multitask language understanding\", \"Training compute-optimal large language models\", \"Mixtral of experts\", \"Sparse upcycling: Training mixture-of-experts from dense checkpoints\", \"Gshard: Scaling giant models with conditional computation and automatic sharding\", \"Base layers: Simplifying training of large, sparse models\", \"Self-prompting large language models for open-domain qa\", \"Starcoder: May the source be with you!\", \"Roberta: A robustly optimized bert pretraining approach\", \"Cross-token modeling with conditional computation\", \"Multimodal contrastive learning with limoe: The language-image mixture of experts\", \"Xgen-7b technical report\", \"From sparse to soft mixtures of experts\", \"Scaling language models: Methods, analysis & insights from training gopher\", \"Exploring the limits of transfer learning with a unified text-to-text transformer\", \"Scaling vision with sparse mixture of experts\", \"Hash layers for large sparse models\", \"Code llama: Open foundation models for code\", \"Glu variants improve transformer\", \"Outrageously large neural networks: The sparsely-gated mixture-of-experts layer\", \"Megatron-lm: Training multi-billion parameter language models using model parallelism\", \"Dolma: An Open Corpus of Three Trillion Tokens for Language Model Pretraining Research\", \"Roformer: Enhanced transformer with rotary position embedding\", \"U12: Unifying language learning paradigms\", \"Unifying language learning paradigms\", \"Llama: Open and efficient foundation language models\", \"GPT-J-6B: A 6 Billion Parameter Autoregressive Language Model\", \"CCNet: Extracting high quality monolingual datasets from web crawl data\", \"Gspmd: General and scalable parallelization for ml computation graphs\", \"One student knows all experts know: From sparse to dense\", \"Go wider instead of deeper\", \"Efficient language modeling with sparse all-mlp\", \"Tinyllama: An open-source small language model\", \"Deep long-tailed learning: A survey\", \"Judging llm-as-a-judge with mt-bench and chatbot arena\", \"Brainformers: Trading simplicity for efficiency\", \"Mixture-of-experts with expert choice routing\", \"St-moe: Designing stable and transferable sparse expert models\"], \"Citation\":[]}\n[The End of Assistant's Predicted Answer]\n\n[System]\nWe would like to request your feedback on the performance of the AI assistant in response to the user question displayed above according to the gold answer. Please use the following listed aspects and their descriptions as evaluation criteria:\n    - Accuracy and Hallucinations: The assistant's answer is semantically consistent with the gold answer; The numerical value and order need to be accurate, and there should be no hallucinations.\n    - Completeness: Referring to the reference answers, the assistant's answer should contain all the key points needed to answer the user's question; further elaboration on these key points can be omitted.\nPlease rate whether this answer is suitable for the question. Please note that the gold answer can be considered as a correct answer to the question.\n\nThe assistant receives an overall score on a scale of 1 to 100, where a higher score indicates better overall performance.\nPlease note that if the assistant's answer and the gold answer fully meet the above criteria, its overall rating should be the full marks (100).\nPlease first provide a comprehensive explanation of your evaluation, avoiding any potential bias.\nThen, output a line indicating the score of the Assistant.\n\nPLEASE OUTPUT WITH THE FOLLOWING FORMAT, WHERE THE SCORE IS A SCALE OF 1 TO 100 BY STRICTLY FOLLOWING THIS FORMAT: \"[[score]]\", FOR EXAMPLE \"Rating: [[100]]\":\n<start output>\nEvaluation evidence: your evluation explanation here, no more than 100 words\nRating: [[score]]\n<end output>\n\nNow, start your evaluation:",
        "generate_response": "{\"Reference\":[\"Palm 2 technical report\", \"inthe)wildchat: 570k chatGPT interaction logs in the wild\", \"Efficient large scale language modeling with mixtures of experts\", \"Beyond the imitation game: Quantifying and extrapolating the capabilities of language models\", \"Efficient training of language models to fill in the middle\", \"Findings of the 2016 conference on machine translation\", \"Language models are few-shot learners\", \"Evaluating large language models trained on code\", \"Palm: Scaling language modeling with pathways\", \"Unimax: Fairer and more effective language sampling for large-scale multilingual pretraining\", \"Megablocks: Efficient sparse training with mixture-of-experts\", \"A framework for few-shot language model evaluation\", \"Openllama: An open reproduction of llama\", \"Measuring massive multitask language understanding\", \"Training compute-optimal large language models\", \"Mixtral of experts\", \"Sparse upcycling: Training mixture-of-experts from dense checkpoints\", \"Gshard: Scaling giant models with conditional computation and automatic sharding\", \"Base layers: Simplifying training of large, sparse models\", \"Self-prompting large language models for open-domain qa\", \"Starcoder: May the source be with you!\", \"Roberta: A robustly optimized bert pretraining approach\", \"Cross-token modeling with conditional computation\", \"Multimodal contrastive learning with limoe: The language-image mixture of experts\", \"Xgen-7b technical report\", \"From sparse to soft mixtures of experts\", \"Scaling language models: Methods, analysis & insights from training gopher\", \"Exploring the limits of transfer learning with a unified text-to-text transformer\", \"Scaling vision with sparse mixture of experts\", \"Hash layers for large sparse models\", \"Code llama: Open foundation models for code\", \"Glu variants improve transformer\", \"Outrageously large neural networks: The sparsely-gated mixture-of-experts layer\", \"Megatron-lm: Training multi-billion parameter language models using model parallelism\", \"Dolma: An Open Corpus of Three Trillion Tokens for Language Model Pretraining Research\", \"Roformer: Enhanced transformer with rotary position embedding\", \"U12: Unifying language learning paradigms\", \"Unifying language learning paradigms\", \"Llama: Open and efficient foundation language models\", \"GPT-J-6B: A 6 Billion Parameter Autoregressive Language Model\", \"CCNet: Extracting high quality monolingual datasets from web crawl data\", \"Gspmd: General and scalable parallelization for ml computation graphs\", \"One student knows all experts know: From sparse to dense\", \"Go wider instead of deeper\", \"Efficient language modeling with sparse all-mlp\", \"Tinyllama: An open-source small language model\", \"Deep long-tailed learning: A survey\", \"Judging llm-as-a-judge with mt-bench and chatbot arena\", \"Brainformers: Trading simplicity for efficiency\", \"Mixture-of-experts with expert choice routing\", \"St-moe: Designing stable and transferable sparse expert models\"], \"Citation\":[]}",
        "eval_response": "Evaluation evidence: The assistant's answer does not match the gold answer in terms of accuracy and completeness. The references listed in the assistant's answer do not align with those in the gold answer, and it includes many titles not mentioned in the gold answer. Additionally, the assistant's answer fails to list any citations, whereas the gold answer specifies two citations. This discrepancy indicates a significant error in identifying the correct relationships between the papers, leading to both inaccuracies and hallucinations.\n\nRating: [[20]]",
        "score": 20
    },
    {
        "level": 4,
        "set": 1,
        "length": 40179,
        "type": "paper",
        "language": "en",
        "question": "",
        "instruction": "We kindly ask you to thoroughly review the provided papers and construct a citation chain from them. Please adhere to the following instructions strictly while completing the task:\n\n#Task Instructions:\nGiven several papers, you are required to identify and list the longest citation chain, which demonstrates the citation relationship among the provided papers.\n\n#Specific Requirements:\n1.Please present the titles of the papers in the form of a list, as follows: [\"Title of Paper 1\", \"Title of Paper 2\", ..., \"Title of Paper n\"].\n2.Ensure that the citation chain in the list is linear and continuous, meaning that the first paper title in the list (Paper 1) should not cite any other works. Instead, it should be cited by the next paper in the list (Paper 2); subsequently, each paper should then be cited by the next one in the list, continuing up to the last paper (Paper n).\n3.Consider only the citation relationships within the supplied collection of papers, and ensure that the citation chain accurately reflects the sequential citation order among these documents.\n4.Do not take into account any articles not provided, and disregard other non-linear citation relationships.",
        "prompt_template": "{instruction}\n\n#Papers Provided:\n{docs}",
        "doc": [
            "1508.01745.md",
            "1709.03082.md",
            "1803.08375.md",
            "2405.07657.md"
        ],
        "answer": [
            "# Semantically Conditioned LSTM-based Natural Language Generation for Spoken Dialogue Systems ",
            "# A Neural Network Architecture Combining Gated Recurrent Unit (GRU) and Support Vector Machine (SVM) for Intrusion Detection in Network Traffic Data ",
            "# Deep Learning using Rectified Linear Units (ReLU) ",
            "# BEYOND TRADITIONAL MAGNETIC RESONANCE PROCESSING WITH ARTIFICIAL INTELLIGENCE * "
        ],
        "shuffle_doc": true,
        "id": "f7116f92-a07c-4278-9333-b63ed5965907",
        "prompt": "[Question]\nWe kindly ask you to thoroughly review the provided papers and construct a citation chain from them. Please adhere to the following instructions strictly while completing the task:\n\n#Task Instructions:\nGiven several papers, you are required to identify and list the longest citation chain, which demonstrates the citation relationship among the provided papers.\n\n#Specific Requirements:\n1.Please present the titles of the papers in the form of a list, as follows: [\"Title of Paper 1\", \"Title of Paper 2\", ..., \"Title of Paper n\"].\n2.Ensure that the citation chain in the list is linear and continuous, meaning that the first paper title in the list (Paper 1) should not cite any other works. Instead, it should be cited by the next paper in the list (Paper 2); subsequently, each paper should then be cited by the next one in the list, continuing up to the last paper (Paper n).\n3.Consider only the citation relationships within the supplied collection of papers, and ensure that the citation chain accurately reflects the sequential citation order among these documents.\n4.Do not take into account any articles not provided, and disregard other non-linear citation relationships.\n\n#Papers Provided:\n{docs}\n\n[Gold Answer]\n['# Semantically Conditioned LSTM-based Natural Language Generation for Spoken Dialogue Systems ', '# A Neural Network Architecture Combining Gated Recurrent Unit (GRU) and Support Vector Machine (SVM) for Intrusion Detection in Network Traffic Data ', '# Deep Learning using Rectified Linear Units (ReLU) ', '# BEYOND TRADITIONAL MAGNETIC RESONANCE PROCESSING WITH ARTIFICIAL INTELLIGENCE * ']\n\n[The Start of Assistant's Predicted Answer]\n[\"A Neural Network Architecture Combining Gated Recurrent Unit (GRU) and Support Vector Machine (SVM) for Intrusion Detection in Network Traffic Data\", \"A Novel Approach Combining Recurrent Neural Network and Support Vector Machines for Time Series Classification\"]\n[The End of Assistant's Predicted Answer]\n\n[System]\nWe would like to request your feedback on the performance of the AI assistant in response to the user question displayed above according to the gold answer. Please use the following listed aspects and their descriptions as evaluation criteria:\n    - Accuracy and Hallucinations: The assistant's answer is semantically consistent with the gold answer; The numerical value and order need to be accurate, and there should be no hallucinations.\n    - Completeness: Referring to the reference answers, the assistant's answer should contain all the key points needed to answer the user's question; further elaboration on these key points can be omitted.\nPlease rate whether this answer is suitable for the question. Please note that the gold answer can be considered as a correct answer to the question.\n\nThe assistant receives an overall score on a scale of 1 to 100, where a higher score indicates better overall performance.\nPlease note that if the assistant's answer and the gold answer fully meet the above criteria, its overall rating should be the full marks (100).\nPlease first provide a comprehensive explanation of your evaluation, avoiding any potential bias.\nThen, output a line indicating the score of the Assistant.\n\nPLEASE OUTPUT WITH THE FOLLOWING FORMAT, WHERE THE SCORE IS A SCALE OF 1 TO 100 BY STRICTLY FOLLOWING THIS FORMAT: \"[[score]]\", FOR EXAMPLE \"Rating: [[100]]\":\n<start output>\nEvaluation evidence: your evluation explanation here, no more than 100 words\nRating: [[score]]\n<end output>\n\nNow, start your evaluation:",
        "generate_response": "[\"A Neural Network Architecture Combining Gated Recurrent Unit (GRU) and Support Vector Machine (SVM) for Intrusion Detection in Network Traffic Data\", \"A Novel Approach Combining Recurrent Neural Network and Support Vector Machines for Time Series Classification\"]",
        "eval_response": "Evaluation evidence: The assistant's answer does not match the gold answer in terms of the titles of the papers listed. The assistant's response includes only two titles, neither of which appear in the gold answer. This indicates a significant deviation from the required task, as it neither accurately reflects the citation chain nor includes the correct titles from the provided papers. The response is incomplete and contains inaccuracies, failing to meet the task requirements.\n\nRating: [[10]]",
        "score": 10
    },
    {
        "level": 3,
        "set": 3,
        "length": 186724,
        "type": "paper",
        "language": "en",
        "question": "Eyes Wide Shut? Exploring the Visual Shortcomings of Multimodal LLMs",
        "instruction": "We hope you will carefully study the provided papers and determine the citation relationships between them. Please follow the instructions below strictly to complete the task:\n\n#Specific Requirements:\n1. Reference: When a given paper mentions other provided papers, those other papers are considered as \"references\" for the given paper. To summarize in this specific context, references are about what the given paper is using.\n2. Citation: Conversely, when other provided papers mention the given paper in their works, the given paper is being \"cited\" by those other papers. To summarize in this specific context, citations are about who is using the given paper.\n3. Given a paper, you need to determine the citation or reference relationship between this paper and the other papers. Do not consider papers that are not provided.\n3. Please present the paper titles in a json format as follows: {{\"Reference\":[\"Reference Title 1\", \"Reference Title 2\", ..., \"Reference Title n\"], \"Citation\":[\"Citation Title 1\", \"Citation Title 2\", ..., \"Citation Title n\"]}}.\n4. If a paper does not have any references or citations, please leave the corresponding list empty, e.g.{{\"Refernce\":[]}}, {{\"Citation\":[]}}.",
        "prompt_template": "#Papers:\n{docs}\n\n{instruction}\n\n#The paper you need to analyze:\n{question}",
        "doc": [
            "2401.06209.md",
            "2405.14974.md",
            "2405.18415.md",
            "2309.17421.md",
            "2311.09215.md"
        ],
        "answer": {
            "Reference": [
                "# The Dawn of LMMs: Preliminary Explorations with GPT-4V(ision) ",
                "# ConvNet vs Transformer, Supervised vs CLIP: Beyond ImageNet Accuracy "
            ],
            "Citation": [
                "# LOVA ${ }^{3}$ : Learning to Visual Question Answering, Asking and Assessment ",
                "# Why are Visually-Grounded Language Models Bad at Image Classification? "
            ]
        },
        "shuffle_doc": true,
        "id": "8a5edaad-689a-4abb-a1a5-754c2ac97522",
        "prompt": "[Question]\n#Papers:\n{docs}\n\nWe hope you will carefully study the provided papers and determine the citation relationships between them. Please follow the instructions below strictly to complete the task:\n\n#Specific Requirements:\n1. Reference: When a given paper mentions other provided papers, those other papers are considered as \"references\" for the given paper. To summarize in this specific context, references are about what the given paper is using.\n2. Citation: Conversely, when other provided papers mention the given paper in their works, the given paper is being \"cited\" by those other papers. To summarize in this specific context, citations are about who is using the given paper.\n3. Given a paper, you need to determine the citation or reference relationship between this paper and the other papers. Do not consider papers that are not provided.\n3. Please present the paper titles in a json format as follows: {{\"Reference\":[\"Reference Title 1\", \"Reference Title 2\", ..., \"Reference Title n\"], \"Citation\":[\"Citation Title 1\", \"Citation Title 2\", ..., \"Citation Title n\"]}}.\n4. If a paper does not have any references or citations, please leave the corresponding list empty, e.g.{{\"Refernce\":[]}}, {{\"Citation\":[]}}.\n\n#The paper you need to analyze:\nEyes Wide Shut? Exploring the Visual Shortcomings of Multimodal LLMs\n\n[Gold Answer]\n{'Reference': ['# The Dawn of LMMs: Preliminary Explorations with GPT-4V(ision) ', '# ConvNet vs Transformer, Supervised vs CLIP: Beyond ImageNet Accuracy '], 'Citation': ['# LOVA ${ }^{3}$ : Learning to Visual Question Answering, Asking and Assessment ', '# Why are Visually-Grounded Language Models Bad at Image Classification? ']}\n\n[The Start of Assistant's Predicted Answer]\n{\"Reference\":[\"Realms: Retrieval-Augmented Language Model PreTraining\", \"Compositional Semantic Parsing with Large Language Models\", \"Open LLM Leaderboard\", \"How Close is ChatGPT to Human Experts? Comparison Corpus, Evaluation, and Detection\", \"Measuring Massive Multitask Language Understanding\", \"Atlas: Few-shot Learning with Retrieval Augmented Language Models\", \"Contrastive Language-Image PreTraining\", \"DINOv2: Vision-Only Self-Supervised Learning\", \"LLaVA: Large Language and Vision Adapter\", \"InstructBLIP: Instruction-Tuned Large-Scale Vision and Language Pre-Training\", \"Mini-GPT4: Multimodal Pre-Training with Instruction Following\", \"GEMINI: General Multimodal Large Language Model\", \"BARD: Bidirectional and Auto-Regressive Denoising Transformer for Multimodal Pre-Training\", \"EVA01: Exploring the Limits of Masked Visual Representation Learning at Scale\", \"EVA02: Exploring the Limits of Masked Visual Representation Learning at Scale\", \"SigL: Sign Language Multimodal Pre-Training\", \"MM-Vet: Multimodal Visual Entailment Test\", \"POPE: Probing Open-Ended Perception in Multimodal Language Models\", \"MM-Bench: A Benchmark for Multimodal Reasoning\", \"GQA: Grounded Question Answering\", \"TextVQA: TextVQA: A Benchmark for Reasoning About Text in Images\", \"SLIP: Scaling Language and Vision Pre-training to the Internet\", \"Thrush: A Benchmark for Evaluating and Improving Binding in Multimodal Models\", \"Yuksekgonul: Investigating Binding Problems in Multimodal Models\", \"Hsieh: Understanding the Limitations of Multimodal Models in Attribute Binding\", \"Tschannen: Image Captioners as Alternatives to CLIP for Attribute Binding\", \"A Survey on Evaluation of Large Language Models\", \"Vicuna: An Open-Source Chatbot Impressing GPT-4 with $90 \\% *$ ChatGPT Quality\", \"ChatLaw: Open-Source Legal Large Language Model with Integrated External Knowledge Bases\", \"Mixture of Features for Multimodal Representation Learning\", \"Masked Visual Representation Learning at Scale\", \"Self-Supervised Learning with Masked Image Modeling\", \"Self-Supervised Learning with Contrastive Predictive Coding\", \"Self-Supervised Learning with Deep Clustering\", \"Self-Supervised Learning with Instance Discrimination\", \"Self-Supervised Learning with Hierarchical Clustering\", \"Self-Supervised Learning with MoCoV3\", \"Self-Supervised Learning with MAE\", \"Self-Supervised Learning with Vision-Only Models for MLLMs\", \"Self-Supervised Learning with Diverse Evaluations in Visual Representation Learning\", \"Self-Supervised Learning with Hyperbolic Labs\", \"Self-Supervised Learning with NYU IT High Performance Computing\"], \"Citation\":[]}\n[The End of Assistant's Predicted Answer]\n\n[System]\nWe would like to request your feedback on the performance of the AI assistant in response to the user question displayed above according to the gold answer. Please use the following listed aspects and their descriptions as evaluation criteria:\n    - Accuracy and Hallucinations: The assistant's answer is semantically consistent with the gold answer; The numerical value and order need to be accurate, and there should be no hallucinations.\n    - Completeness: Referring to the reference answers, the assistant's answer should contain all the key points needed to answer the user's question; further elaboration on these key points can be omitted.\nPlease rate whether this answer is suitable for the question. Please note that the gold answer can be considered as a correct answer to the question.\n\nThe assistant receives an overall score on a scale of 1 to 100, where a higher score indicates better overall performance.\nPlease note that if the assistant's answer and the gold answer fully meet the above criteria, its overall rating should be the full marks (100).\nPlease first provide a comprehensive explanation of your evaluation, avoiding any potential bias.\nThen, output a line indicating the score of the Assistant.\n\nPLEASE OUTPUT WITH THE FOLLOWING FORMAT, WHERE THE SCORE IS A SCALE OF 1 TO 100 BY STRICTLY FOLLOWING THIS FORMAT: \"[[score]]\", FOR EXAMPLE \"Rating: [[100]]\":\n<start output>\nEvaluation evidence: your evluation explanation here, no more than 100 words\nRating: [[score]]\n<end output>\n\nNow, start your evaluation:",
        "generate_response": "{\"Reference\":[\"Realms: Retrieval-Augmented Language Model PreTraining\", \"Compositional Semantic Parsing with Large Language Models\", \"Open LLM Leaderboard\", \"How Close is ChatGPT to Human Experts? Comparison Corpus, Evaluation, and Detection\", \"Measuring Massive Multitask Language Understanding\", \"Atlas: Few-shot Learning with Retrieval Augmented Language Models\", \"Contrastive Language-Image PreTraining\", \"DINOv2: Vision-Only Self-Supervised Learning\", \"LLaVA: Large Language and Vision Adapter\", \"InstructBLIP: Instruction-Tuned Large-Scale Vision and Language Pre-Training\", \"Mini-GPT4: Multimodal Pre-Training with Instruction Following\", \"GEMINI: General Multimodal Large Language Model\", \"BARD: Bidirectional and Auto-Regressive Denoising Transformer for Multimodal Pre-Training\", \"EVA01: Exploring the Limits of Masked Visual Representation Learning at Scale\", \"EVA02: Exploring the Limits of Masked Visual Representation Learning at Scale\", \"SigL: Sign Language Multimodal Pre-Training\", \"MM-Vet: Multimodal Visual Entailment Test\", \"POPE: Probing Open-Ended Perception in Multimodal Language Models\", \"MM-Bench: A Benchmark for Multimodal Reasoning\", \"GQA: Grounded Question Answering\", \"TextVQA: TextVQA: A Benchmark for Reasoning About Text in Images\", \"SLIP: Scaling Language and Vision Pre-training to the Internet\", \"Thrush: A Benchmark for Evaluating and Improving Binding in Multimodal Models\", \"Yuksekgonul: Investigating Binding Problems in Multimodal Models\", \"Hsieh: Understanding the Limitations of Multimodal Models in Attribute Binding\", \"Tschannen: Image Captioners as Alternatives to CLIP for Attribute Binding\", \"A Survey on Evaluation of Large Language Models\", \"Vicuna: An Open-Source Chatbot Impressing GPT-4 with $90 \\% *$ ChatGPT Quality\", \"ChatLaw: Open-Source Legal Large Language Model with Integrated External Knowledge Bases\", \"Mixture of Features for Multimodal Representation Learning\", \"Masked Visual Representation Learning at Scale\", \"Self-Supervised Learning with Masked Image Modeling\", \"Self-Supervised Learning with Contrastive Predictive Coding\", \"Self-Supervised Learning with Deep Clustering\", \"Self-Supervised Learning with Instance Discrimination\", \"Self-Supervised Learning with Hierarchical Clustering\", \"Self-Supervised Learning with MoCoV3\", \"Self-Supervised Learning with MAE\", \"Self-Supervised Learning with Vision-Only Models for MLLMs\", \"Self-Supervised Learning with Diverse Evaluations in Visual Representation Learning\", \"Self-Supervised Learning with Hyperbolic Labs\", \"Self-Supervised Learning with NYU IT High Performance Computing\"], \"Citation\":[]}",
        "eval_response": "Evaluation evidence: The assistant's answer is incorrect and does not align with the gold answer. The references and citations listed in the assistant's response do not match those in the gold answer, indicating a significant error in identifying the correct papers. The assistant's response includes a long list of unrelated papers, which is a clear example of hallucination. The answer fails both in accuracy and completeness as it does not correctly identify any of the papers mentioned in the gold answer.\n\nRating: [[10]]",
        "score": 10
    },
    {
        "level": 4,
        "set": 2,
        "length": 81776,
        "type": "paper",
        "language": "en",
        "question": "",
        "instruction": "We kindly ask you to thoroughly review the provided papers and construct a citation chain from them. Please adhere to the following instructions strictly while completing the task:\n\n#Task Instructions:\nGiven several papers, you are required to identify and list the longest citation chain, which demonstrates the citation relationship among the provided papers.\n\n#Specific Requirements:\n1.Please present the titles of the papers in the form of a list, as follows: [\"Title of Paper 1\", \"Title of Paper 2\", ..., \"Title of Paper n\"].\n2.Ensure that the citation chain in the list is linear and continuous, meaning that the first paper title in the list (Paper 1) should not cite any other works. Instead, it should be cited by the next paper in the list (Paper 2); subsequently, each paper should then be cited by the next one in the list, continuing up to the last paper (Paper n).\n3.Consider only the citation relationships within the supplied collection of papers, and ensure that the citation chain accurately reflects the sequential citation order among these documents.\n4.Do not take into account any articles not provided, and disregard other non-linear citation relationships.",
        "prompt_template": "{instruction}\n\n#Papers Provided:\n{docs}",
        "doc": [
            "2011.09533.md",
            "2103.01955.md",
            "2209.12660.md",
            "2308.13937.md",
            "2405.09255.md"
        ],
        "answer": [
            "# Is Independent Learning All You Need in the StarCraft Multi-Agent Challenge? ",
            "# The Surprising Effectiveness of PPO in Cooperative Multi-Agent Games ",
            "# MARLUI: Multi-Agent Reinforcement Learning for Adaptive UIs ",
            "# A Comparative Study on Reward Models for UI Adaptation with Reinforcement Learning ",
            "# Reinforcement Learning-Based Framework for the Intelligent Adaptation of User Interfaces "
        ],
        "shuffle_doc": true,
        "id": "194e3b4c-8059-4731-830b-b8835d8a611a",
        "prompt": "[Question]\nWe kindly ask you to thoroughly review the provided papers and construct a citation chain from them. Please adhere to the following instructions strictly while completing the task:\n\n#Task Instructions:\nGiven several papers, you are required to identify and list the longest citation chain, which demonstrates the citation relationship among the provided papers.\n\n#Specific Requirements:\n1.Please present the titles of the papers in the form of a list, as follows: [\"Title of Paper 1\", \"Title of Paper 2\", ..., \"Title of Paper n\"].\n2.Ensure that the citation chain in the list is linear and continuous, meaning that the first paper title in the list (Paper 1) should not cite any other works. Instead, it should be cited by the next paper in the list (Paper 2); subsequently, each paper should then be cited by the next one in the list, continuing up to the last paper (Paper n).\n3.Consider only the citation relationships within the supplied collection of papers, and ensure that the citation chain accurately reflects the sequential citation order among these documents.\n4.Do not take into account any articles not provided, and disregard other non-linear citation relationships.\n\n#Papers Provided:\n{docs}\n\n[Gold Answer]\n['# Is Independent Learning All You Need in the StarCraft Multi-Agent Challenge? ', '# The Surprising Effectiveness of PPO in Cooperative Multi-Agent Games ', '# MARLUI: Multi-Agent Reinforcement Learning for Adaptive UIs ', '# A Comparative Study on Reward Models for UI Adaptation with Reinforcement Learning ', '# Reinforcement Learning-Based Framework for the Intelligent Adaptation of User Interfaces ']\n\n[The Start of Assistant's Predicted Answer]\n[\"MARLUI: Multi-Agent Reinforcement Learning for Adaptive UIs\", \"A Comparative Study on Reward Models for UI Adaptation with Reinforcement Learning\"]\n[The End of Assistant's Predicted Answer]\n\n[System]\nWe would like to request your feedback on the performance of the AI assistant in response to the user question displayed above according to the gold answer. Please use the following listed aspects and their descriptions as evaluation criteria:\n    - Accuracy and Hallucinations: The assistant's answer is semantically consistent with the gold answer; The numerical value and order need to be accurate, and there should be no hallucinations.\n    - Completeness: Referring to the reference answers, the assistant's answer should contain all the key points needed to answer the user's question; further elaboration on these key points can be omitted.\nPlease rate whether this answer is suitable for the question. Please note that the gold answer can be considered as a correct answer to the question.\n\nThe assistant receives an overall score on a scale of 1 to 100, where a higher score indicates better overall performance.\nPlease note that if the assistant's answer and the gold answer fully meet the above criteria, its overall rating should be the full marks (100).\nPlease first provide a comprehensive explanation of your evaluation, avoiding any potential bias.\nThen, output a line indicating the score of the Assistant.\n\nPLEASE OUTPUT WITH THE FOLLOWING FORMAT, WHERE THE SCORE IS A SCALE OF 1 TO 100 BY STRICTLY FOLLOWING THIS FORMAT: \"[[score]]\", FOR EXAMPLE \"Rating: [[100]]\":\n<start output>\nEvaluation evidence: your evluation explanation here, no more than 100 words\nRating: [[score]]\n<end output>\n\nNow, start your evaluation:",
        "generate_response": "[\"MARLUI: Multi-Agent Reinforcement Learning for Adaptive UIs\", \"A Comparative Study on Reward Models for UI Adaptation with Reinforcement Learning\"]",
        "eval_response": "Evaluation evidence: The assistant's answer is incomplete and does not match the gold answer in terms of the length and content of the citation chain. The gold answer lists a chain of five papers, whereas the assistant's answer only includes two papers. Additionally, the papers listed by the assistant do not align with the beginning of the citation chain as outlined in the gold answer. This discrepancy indicates a lack of accuracy and completeness in the assistant's response.\n\nRating: [[20]]",
        "score": 20
    },
    {
        "level": 3,
        "set": 4,
        "length": 207646,
        "type": "paper",
        "language": "en",
        "question": "EasyTool: Enhancing LLM-based Agents with Concise Tool Instruction",
        "instruction": "We hope you will carefully study the provided papers and determine the citation relationships between them. Please follow the instructions below strictly to complete the task:\n\n#Specific Requirements:\n1. Reference: When a given paper mentions other provided papers, those other papers are considered as \"references\" for the given paper. To summarize in this specific context, references are about what the given paper is using.\n2. Citation: Conversely, when other provided papers mention the given paper in their works, the given paper is being \"cited\" by those other papers. To summarize in this specific context, citations are about who is using the given paper.\n3. Given a paper, you need to determine the citation or reference relationship between this paper and the other papers. Do not consider papers that are not provided.\n3. Please present the paper titles in a json format as follows: {{\"Reference\":[\"Reference Title 1\", \"Reference Title 2\", ..., \"Reference Title n\"], \"Citation\":[\"Citation Title 1\", \"Citation Title 2\", ..., \"Citation Title n\"]}}.\n4. If a paper does not have any references or citations, please leave the corresponding list empty, e.g.{{\"Refernce\":[]}}, {{\"Citation\":[]}}.",
        "prompt_template": "#Papers:\n{docs}\n\n{instruction}\n\n#The paper you need to analyze:\n{question}",
        "doc": [
            "2401.06201.md",
            "2405.05955.md",
            "2405.16510.md",
            "2405.16533.md",
            "2405.17935.md",
            "2405.18208.md",
            "2306.06624.md",
            "2307.16789.md",
            "2310.07075.md",
            "2310.13227.md",
            "2311.18760.md"
        ],
        "answer": {
            "Reference": [
                "# RestGPT: Connecting Large Language Models with Real-World RESTful APIs ",
                "# ToolLLM: Facilitating Large LanguAGE MODELS TO MASTER 16000+ REAL-WORLD APIS ",
                "# Don't Fine-Tune, Decode: Syntax Error-Free Tool Use via Constrained Decoding ",
                "# ToolChaIN*: EFFiCIENT ACtion SpaCe NAVIGATION in Large Language ModelS With A* Search ",
                "# TASKBENCH: BENCHMARKING LARGE "
            ],
            "Citation": [
                "# Smurfs: Leveraging Multiple Proficiency Agents with Context-Efficiency for Tool Planning ",
                "# Meta-Task Planning for Language Agents ",
                "# Chain of Tools: Large Language Model is an Automatic Multi-tool Learner ",
                "# A Human-Like Reasoning Framework for Multi-Phases Planning Task with Large Language Models "
            ]
        },
        "shuffle_doc": true,
        "id": "ab93653a-5b35-40e7-91dc-99e10e771765",
        "prompt": "[Question]\n#Papers:\n{docs}\n\nWe hope you will carefully study the provided papers and determine the citation relationships between them. Please follow the instructions below strictly to complete the task:\n\n#Specific Requirements:\n1. Reference: When a given paper mentions other provided papers, those other papers are considered as \"references\" for the given paper. To summarize in this specific context, references are about what the given paper is using.\n2. Citation: Conversely, when other provided papers mention the given paper in their works, the given paper is being \"cited\" by those other papers. To summarize in this specific context, citations are about who is using the given paper.\n3. Given a paper, you need to determine the citation or reference relationship between this paper and the other papers. Do not consider papers that are not provided.\n3. Please present the paper titles in a json format as follows: {{\"Reference\":[\"Reference Title 1\", \"Reference Title 2\", ..., \"Reference Title n\"], \"Citation\":[\"Citation Title 1\", \"Citation Title 2\", ..., \"Citation Title n\"]}}.\n4. If a paper does not have any references or citations, please leave the corresponding list empty, e.g.{{\"Refernce\":[]}}, {{\"Citation\":[]}}.\n\n#The paper you need to analyze:\nEasyTool: Enhancing LLM-based Agents with Concise Tool Instruction\n\n[Gold Answer]\n{'Reference': ['# RestGPT: Connecting Large Language Models with Real-World RESTful APIs ', '# ToolLLM: Facilitating Large LanguAGE MODELS TO MASTER 16000+ REAL-WORLD APIS ', \"# Don't Fine-Tune, Decode: Syntax Error-Free Tool Use via Constrained Decoding \", '# ToolChaIN*: EFFiCIENT ACtion SpaCe NAVIGATION in Large Language ModelS With A* Search ', '# TASKBENCH: BENCHMARKING LARGE '], 'Citation': ['# Smurfs: Leveraging Multiple Proficiency Agents with Context-Efficiency for Tool Planning ', '# Meta-Task Planning for Language Agents ', '# Chain of Tools: Large Language Model is an Automatic Multi-tool Learner ', '# A Human-Like Reasoning Framework for Multi-Phases Planning Task with Large Language Models ']}\n\n[The Start of Assistant's Predicted Answer]\n{\"Reference\":[\"A Survey on Evaluation of Large Language Models\", \"Vicuna: An Open-Source Chatbot Impressing GPT-4 with $90 \\% *$ ChatGPT Quality\", \"ChatLaw: Open-Source Legal Large Language Model with Integrated External Knowledge Bases\", \"Compositional Semantic Parsing with Large Language Models\", \"Open LLM Leaderboard\", \"How Close is ChatGPT to Human Experts? Comparison Corpus, Evaluation, and Detection\", \"REALM: Retrieval-Augmented Language Model PreTraining\", \"Rethinking with Retrieval: Faithful Large Language Model Inference\", \"Measuring Massive Multitask Language Understanding\", \"Atlas: Few-shot Learning with Retrieval Augmented Language Models\", \"BERT: Pre-training of Deep Bidirectional Transformers for Language Understanding\", \"Auto-gpt: An Autonomous GPT-4 Experiment\", \"ToolkenGPT: Augmenting Frozen Language Models with Massive Tools via Tool Embeddings\", \"Tool Documentation Enables Zero-Shot Tool-Usage with Large Language Models\", \"LLMLingua: Compressing Prompts for Accelerated Inference of Large Language Models\", \"API-Bank: A Comprehensive Benchmark for Tool-Augmented LLMs\", \"Chameleon: Plug-and-play Compositional Reasoning with Large Language Models\", \"Learning to Compress Prompts with Gist Tokens\", \"Chatgpt\", \"GPT-4 Technical Report\", \"Training Language Models to Follow Instructions with Human Feedback\", \"TALM: Tool Augmented Language Models\", \"Gorilla: Large Language Model Connected with Massive APIs\", \"Toolformer: Language Models Can Teach Themselves to Use Tools\", \"Toolllm: Facilitating Large Language Models to Master 16000+ Real-World APIs\", \"HuggingGPT: Solving AI Tasks with ChatGPT and Its Friends in Hugging Face\", \"TaskBench: Benchmarking Large Language Models for Task Automation\", \"Restgpt: Connecting Large Language Models with Real-World RESTful APIs\", \"ToolAlpaca: Generalized Tool Learning for Language Models with 3000 Simulated Cases\", \"Gemini: A Family of Highly Capable Multimodal Models\", \"Llama: Open and Efficient Foundation Language Models\", \"Llama 2: Open Foundation and Fine-Tuned Chat Models\", \"On the Tool Manipulation Capability of Open-Source Large Language Models\", \"React: Synergizing Reasoning and Acting in Language Models\", \"Syntax Error-Free and Generalizable Tool Use for LLMs via Finite-State Decoding\", \"Toolchain*: Efficient Action Space Navigation in Large Language Models with A* Search\"], \"Citation\":[\"Don't Fine-Tune, Decode: Syntax Error-Free Tool Use via Constrained Decoding\"]}\n[The End of Assistant's Predicted Answer]\n\n[System]\nWe would like to request your feedback on the performance of the AI assistant in response to the user question displayed above according to the gold answer. Please use the following listed aspects and their descriptions as evaluation criteria:\n    - Accuracy and Hallucinations: The assistant's answer is semantically consistent with the gold answer; The numerical value and order need to be accurate, and there should be no hallucinations.\n    - Completeness: Referring to the reference answers, the assistant's answer should contain all the key points needed to answer the user's question; further elaboration on these key points can be omitted.\nPlease rate whether this answer is suitable for the question. Please note that the gold answer can be considered as a correct answer to the question.\n\nThe assistant receives an overall score on a scale of 1 to 100, where a higher score indicates better overall performance.\nPlease note that if the assistant's answer and the gold answer fully meet the above criteria, its overall rating should be the full marks (100).\nPlease first provide a comprehensive explanation of your evaluation, avoiding any potential bias.\nThen, output a line indicating the score of the Assistant.\n\nPLEASE OUTPUT WITH THE FOLLOWING FORMAT, WHERE THE SCORE IS A SCALE OF 1 TO 100 BY STRICTLY FOLLOWING THIS FORMAT: \"[[score]]\", FOR EXAMPLE \"Rating: [[100]]\":\n<start output>\nEvaluation evidence: your evluation explanation here, no more than 100 words\nRating: [[score]]\n<end output>\n\nNow, start your evaluation:",
        "generate_response": "{\"Reference\":[\"A Survey on Evaluation of Large Language Models\", \"Vicuna: An Open-Source Chatbot Impressing GPT-4 with $90 \\% *$ ChatGPT Quality\", \"ChatLaw: Open-Source Legal Large Language Model with Integrated External Knowledge Bases\", \"Compositional Semantic Parsing with Large Language Models\", \"Open LLM Leaderboard\", \"How Close is ChatGPT to Human Experts? Comparison Corpus, Evaluation, and Detection\", \"REALM: Retrieval-Augmented Language Model PreTraining\", \"Rethinking with Retrieval: Faithful Large Language Model Inference\", \"Measuring Massive Multitask Language Understanding\", \"Atlas: Few-shot Learning with Retrieval Augmented Language Models\", \"BERT: Pre-training of Deep Bidirectional Transformers for Language Understanding\", \"Auto-gpt: An Autonomous GPT-4 Experiment\", \"ToolkenGPT: Augmenting Frozen Language Models with Massive Tools via Tool Embeddings\", \"Tool Documentation Enables Zero-Shot Tool-Usage with Large Language Models\", \"LLMLingua: Compressing Prompts for Accelerated Inference of Large Language Models\", \"API-Bank: A Comprehensive Benchmark for Tool-Augmented LLMs\", \"Chameleon: Plug-and-play Compositional Reasoning with Large Language Models\", \"Learning to Compress Prompts with Gist Tokens\", \"Chatgpt\", \"GPT-4 Technical Report\", \"Training Language Models to Follow Instructions with Human Feedback\", \"TALM: Tool Augmented Language Models\", \"Gorilla: Large Language Model Connected with Massive APIs\", \"Toolformer: Language Models Can Teach Themselves to Use Tools\", \"Toolllm: Facilitating Large Language Models to Master 16000+ Real-World APIs\", \"HuggingGPT: Solving AI Tasks with ChatGPT and Its Friends in Hugging Face\", \"TaskBench: Benchmarking Large Language Models for Task Automation\", \"Restgpt: Connecting Large Language Models with Real-World RESTful APIs\", \"ToolAlpaca: Generalized Tool Learning for Language Models with 3000 Simulated Cases\", \"Gemini: A Family of Highly Capable Multimodal Models\", \"Llama: Open and Efficient Foundation Language Models\", \"Llama 2: Open Foundation and Fine-Tuned Chat Models\", \"On the Tool Manipulation Capability of Open-Source Large Language Models\", \"React: Synergizing Reasoning and Acting in Language Models\", \"Syntax Error-Free and Generalizable Tool Use for LLMs via Finite-State Decoding\", \"Toolchain*: Efficient Action Space Navigation in Large Language Models with A* Search\"], \"Citation\":[\"Don't Fine-Tune, Decode: Syntax Error-Free Tool Use via Constrained Decoding\"]}",
        "eval_response": "Evaluation evidence: The assistant's answer does not match the gold answer in terms of accuracy and completeness. The references and citations listed are largely incorrect or hallucinated, not aligning with those specified in the gold answer. The assistant's response includes many titles that are not mentioned in the gold answer, indicating a significant deviation from the required task. This results in a failure to meet the criteria of accuracy and completeness.\n\nRating: [[10]]",
        "score": 10
    },
    {
        "level": 3,
        "set": 3,
        "length": 152524,
        "type": "paper",
        "language": "en",
        "question": "Investigating Data Contamination for Pre-training Language Models",
        "instruction": "We hope you will carefully study the provided papers and determine the citation relationships between them. Please follow the instructions below strictly to complete the task:\n\n#Specific Requirements:\n1. Reference: When a given paper mentions other provided papers, those other papers are considered as \"references\" for the given paper. To summarize in this specific context, references are about what the given paper is using.\n2. Citation: Conversely, when other provided papers mention the given paper in their works, the given paper is being \"cited\" by those other papers. To summarize in this specific context, citations are about who is using the given paper.\n3. Given a paper, you need to determine the citation or reference relationship between this paper and the other papers. Do not consider papers that are not provided.\n3. Please present the paper titles in a json format as follows: {{\"Reference\":[\"Reference Title 1\", \"Reference Title 2\", ..., \"Reference Title n\"], \"Citation\":[\"Citation Title 1\", \"Citation Title 2\", ..., \"Citation Title n\"]}}.\n4. If a paper does not have any references or citations, please leave the corresponding list empty, e.g.{{\"Refernce\":[]}}, {{\"Citation\":[]}}.",
        "prompt_template": "#Papers:\n{docs}\n\n{instruction}\n\n#The paper you need to analyze:\n{question}",
        "doc": [
            "2401.06059.md",
            "2404.18824.md",
            "2405.17512.md",
            "2308.08493.md",
            "2311.04850.md"
        ],
        "answer": {
            "Reference": [
                "# TiME TRAVEL IN LLMs: TRACING DATA CONTAMINATION IN LARGE LANGUAGE MODELS ",
                "# Rethinking Benchmark and Contamination for Language Models with Rephrased Samples "
            ],
            "Citation": [
                "# Benchmarking Benchmark Leakage in Large Language Models ",
                "# On Fairness of Low-Rank Adaptation of Large Models "
            ]
        },
        "shuffle_doc": true,
        "id": "1fec6e37-572d-4dee-bd25-02dbeb08a9c8",
        "prompt": "[Question]\n#Papers:\n{docs}\n\nWe hope you will carefully study the provided papers and determine the citation relationships between them. Please follow the instructions below strictly to complete the task:\n\n#Specific Requirements:\n1. Reference: When a given paper mentions other provided papers, those other papers are considered as \"references\" for the given paper. To summarize in this specific context, references are about what the given paper is using.\n2. Citation: Conversely, when other provided papers mention the given paper in their works, the given paper is being \"cited\" by those other papers. To summarize in this specific context, citations are about who is using the given paper.\n3. Given a paper, you need to determine the citation or reference relationship between this paper and the other papers. Do not consider papers that are not provided.\n3. Please present the paper titles in a json format as follows: {{\"Reference\":[\"Reference Title 1\", \"Reference Title 2\", ..., \"Reference Title n\"], \"Citation\":[\"Citation Title 1\", \"Citation Title 2\", ..., \"Citation Title n\"]}}.\n4. If a paper does not have any references or citations, please leave the corresponding list empty, e.g.{{\"Refernce\":[]}}, {{\"Citation\":[]}}.\n\n#The paper you need to analyze:\nInvestigating Data Contamination for Pre-training Language Models\n\n[Gold Answer]\n{'Reference': ['# TiME TRAVEL IN LLMs: TRACING DATA CONTAMINATION IN LARGE LANGUAGE MODELS ', '# Rethinking Benchmark and Contamination for Language Models with Rephrased Samples '], 'Citation': ['# Benchmarking Benchmark Leakage in Large Language Models ', '# On Fairness of Low-Rank Adaptation of Large Models ']}\n\n[The Start of Assistant's Predicted Answer]\n{\"Reference\": [\"A Survey on Evaluation of Large Language Models\", \"Vicuna: An Open-Source Chatbot Impressing GPT-4 with $90 \\% *$ ChatGPT Quality\", \"Compositional Semantic Parsing with Large Language Models\", \"Open LLM Leaderboard\", \"How Close is ChatGPT to Human Experts? Comparison Corpus, Evaluation, and Detection\", \"REALM: Retrieval-Augmented Language Model PreTraining\", \"Rethinking with Retrieval: Faithful Large Language Model Inference\", \"Measuring Massive Multitask Language Understanding\", \"Evaluating Correctness and Faithfulness of Instruction-Following Models for Question Answering\", \"An image is worth $16 \\\\times 16$ words: Transformers for image recognition at scale\", \"Robbie: Robust bias evaluation of large generative language models\", \"Foundation models and fair use\", \"The unreasonable ineffectiveness of the deeper layers\", \"Decoding compressed trust: Scrutinizing the trustworthiness of efficient llms under compression\", \"Evaluating the output of machine translation systems\", \"Few-shot parameter-efficient fine-tuning is better and cheaper than in-context learning\", \"Swin transformer v2: Scaling up capacity and resolution\", \"Making llms worth every penny: Resource-limited text classification in banking\", \"Gpt-4 technical report\", \"Adversarial robustness via robust low rank representations\", \"The power of scale for parameter-efficient prompt tuning\", \"Prefix-tuning: Optimizing continuous prompts for generation\", \"Bleu: a method for automatic evaluation of machine translation\", \"Pytorch: An imperative style, high-performance deep learning library\", \"Decodingtrust: A comprehensive assessment of trustworthiness in gpt models\", \"Fairness with overlapping groups; a probabilistic perspective\", \"Privacy risk in machine learning: Analyzing the connection to overfitting\", \"Leveraging generative ai and large language models: a comprehensive roadmap for healthcare integration\", \"A careful examination of large language model performance on grade school arithmetic\", \"Age progression regression by conditional adversarial autoencoder\", \"Everybody prune now: Structured pruning of llms with only forward passes\", \"Qlora: Efficient finetuning of quantized llms\", \"Does learning require memorization? a short tale about a long tail\", \"Concept sliders: Lora adaptors for precise control in diffusion models\", \"The frontiers of fairness in machine learning\", \"Training verifiers to solve math word problems\", \"Lora: Low-rank adaptation of large language models\", \"Mistral 7b\", \"Proving test set contamination for black-box language models\", \"Evaluating gender bias in machine translation\", \"Unsupervised contrast-consistent ranking with language models\", \"Multiple-attribute text style transfer\", \"Llama 2: Open foundation and fine-tuned chat models\", \"Pruning has a disparate impact on model accuracy\", \"How to fine-tune large language models for enterprise use cases\", \"Large language models are not robust multiple choice selectors\", \"Representation engineering: A top-down approach to ai transparency\"], \"Citation\": [\"TiME TRAVEL IN LLMs: TRACING DATA CONTAMINATION IN LARGE LANGUAGE MODELS\", \"Rethinking Benchmark and Contamination for Language Models with Rephrased Samples\", \"On Fairness of Low-Rank Adaptation of Large Models\", \"Benchmarking Benchmark Leakage in Large Language Models\"]}\n[The End of Assistant's Predicted Answer]\n\n[System]\nWe would like to request your feedback on the performance of the AI assistant in response to the user question displayed above according to the gold answer. Please use the following listed aspects and their descriptions as evaluation criteria:\n    - Accuracy and Hallucinations: The assistant's answer is semantically consistent with the gold answer; The numerical value and order need to be accurate, and there should be no hallucinations.\n    - Completeness: Referring to the reference answers, the assistant's answer should contain all the key points needed to answer the user's question; further elaboration on these key points can be omitted.\nPlease rate whether this answer is suitable for the question. Please note that the gold answer can be considered as a correct answer to the question.\n\nThe assistant receives an overall score on a scale of 1 to 100, where a higher score indicates better overall performance.\nPlease note that if the assistant's answer and the gold answer fully meet the above criteria, its overall rating should be the full marks (100).\nPlease first provide a comprehensive explanation of your evaluation, avoiding any potential bias.\nThen, output a line indicating the score of the Assistant.\n\nPLEASE OUTPUT WITH THE FOLLOWING FORMAT, WHERE THE SCORE IS A SCALE OF 1 TO 100 BY STRICTLY FOLLOWING THIS FORMAT: \"[[score]]\", FOR EXAMPLE \"Rating: [[100]]\":\n<start output>\nEvaluation evidence: your evluation explanation here, no more than 100 words\nRating: [[score]]\n<end output>\n\nNow, start your evaluation:",
        "generate_response": "{\"Reference\": [\"A Survey on Evaluation of Large Language Models\", \"Vicuna: An Open-Source Chatbot Impressing GPT-4 with $90 \\% *$ ChatGPT Quality\", \"Compositional Semantic Parsing with Large Language Models\", \"Open LLM Leaderboard\", \"How Close is ChatGPT to Human Experts? Comparison Corpus, Evaluation, and Detection\", \"REALM: Retrieval-Augmented Language Model PreTraining\", \"Rethinking with Retrieval: Faithful Large Language Model Inference\", \"Measuring Massive Multitask Language Understanding\", \"Evaluating Correctness and Faithfulness of Instruction-Following Models for Question Answering\", \"An image is worth $16 \\\\times 16$ words: Transformers for image recognition at scale\", \"Robbie: Robust bias evaluation of large generative language models\", \"Foundation models and fair use\", \"The unreasonable ineffectiveness of the deeper layers\", \"Decoding compressed trust: Scrutinizing the trustworthiness of efficient llms under compression\", \"Evaluating the output of machine translation systems\", \"Few-shot parameter-efficient fine-tuning is better and cheaper than in-context learning\", \"Swin transformer v2: Scaling up capacity and resolution\", \"Making llms worth every penny: Resource-limited text classification in banking\", \"Gpt-4 technical report\", \"Adversarial robustness via robust low rank representations\", \"The power of scale for parameter-efficient prompt tuning\", \"Prefix-tuning: Optimizing continuous prompts for generation\", \"Bleu: a method for automatic evaluation of machine translation\", \"Pytorch: An imperative style, high-performance deep learning library\", \"Decodingtrust: A comprehensive assessment of trustworthiness in gpt models\", \"Fairness with overlapping groups; a probabilistic perspective\", \"Privacy risk in machine learning: Analyzing the connection to overfitting\", \"Leveraging generative ai and large language models: a comprehensive roadmap for healthcare integration\", \"A careful examination of large language model performance on grade school arithmetic\", \"Age progression regression by conditional adversarial autoencoder\", \"Everybody prune now: Structured pruning of llms with only forward passes\", \"Qlora: Efficient finetuning of quantized llms\", \"Does learning require memorization? a short tale about a long tail\", \"Concept sliders: Lora adaptors for precise control in diffusion models\", \"The frontiers of fairness in machine learning\", \"Training verifiers to solve math word problems\", \"Lora: Low-rank adaptation of large language models\", \"Mistral 7b\", \"Proving test set contamination for black-box language models\", \"Evaluating gender bias in machine translation\", \"Unsupervised contrast-consistent ranking with language models\", \"Multiple-attribute text style transfer\", \"Llama 2: Open foundation and fine-tuned chat models\", \"Pruning has a disparate impact on model accuracy\", \"How to fine-tune large language models for enterprise use cases\", \"Large language models are not robust multiple choice selectors\", \"Representation engineering: A top-down approach to ai transparency\"], \"Citation\": [\"TiME TRAVEL IN LLMs: TRACING DATA CONTAMINATION IN LARGE LANGUAGE MODELS\", \"Rethinking Benchmark and Contamination for Language Models with Rephrased Samples\", \"On Fairness of Low-Rank Adaptation of Large Models\", \"Benchmarking Benchmark Leakage in Large Language Models\"]}",
        "eval_response": "Evaluation evidence: The assistant's answer does not match the gold answer in terms of accuracy and completeness. The assistant lists numerous references and citations that are not mentioned in the gold answer, indicating hallucinations and incorrect information. The correct references and citations from the gold answer are not clearly identified in the assistant's response, leading to a lack of completeness and accuracy.\n\nRating: [[20]]",
        "score": 20
    },
    {
        "level": 3,
        "set": 2,
        "length": 59966,
        "type": "paper",
        "language": "en",
        "question": "RAG VS Fine-TUNing: PiPELINES, TRADEOFFS, AND A CASE STUDY ON AGRICULTURE",
        "instruction": "We hope you will carefully study the provided papers and determine the citation relationships between them. Please follow the instructions below strictly to complete the task:\n\n#Specific Requirements:\n1. Reference: When a given paper mentions other provided papers, those other papers are considered as \"references\" for the given paper. To summarize in this specific context, references are about what the given paper is using.\n2. Citation: Conversely, when other provided papers mention the given paper in their works, the given paper is being \"cited\" by those other papers. To summarize in this specific context, citations are about who is using the given paper.\n3. Given a paper, you need to determine the citation or reference relationship between this paper and the other papers. Do not consider papers that are not provided.\n3. Please present the paper titles in a json format as follows: {{\"Reference\":[\"Reference Title 1\", \"Reference Title 2\", ..., \"Reference Title n\"], \"Citation\":[\"Citation Title 1\", \"Citation Title 2\", ..., \"Citation Title n\"]}}.\n4. If a paper does not have any references or citations, please leave the corresponding list empty, e.g.{{\"Refernce\":[]}}, {{\"Citation\":[]}}.",
        "prompt_template": "#Papers:\n{docs}\n\n{instruction}\n\n#The paper you need to analyze:\n{question}",
        "doc": [
            "2401.08406.md",
            "2307.03875.md",
            "2310.06225.md"
        ],
        "answer": {
            "Reference": [
                "# Large Language Models for Supply Chain Optimization ",
                "# GPT-4 AS AN AGRONOMIST ASSISTANT? ANSWERING AGRICULTURE QUESTIONS USING LARGE LANGUAGE MODELS "
            ],
            "Citation": []
        },
        "shuffle_doc": true,
        "id": "5c4dd39f-44b7-49b2-bceb-dbe20c2a06a2",
        "prompt": "[Question]\n#Papers:\n{docs}\n\nWe hope you will carefully study the provided papers and determine the citation relationships between them. Please follow the instructions below strictly to complete the task:\n\n#Specific Requirements:\n1. Reference: When a given paper mentions other provided papers, those other papers are considered as \"references\" for the given paper. To summarize in this specific context, references are about what the given paper is using.\n2. Citation: Conversely, when other provided papers mention the given paper in their works, the given paper is being \"cited\" by those other papers. To summarize in this specific context, citations are about who is using the given paper.\n3. Given a paper, you need to determine the citation or reference relationship between this paper and the other papers. Do not consider papers that are not provided.\n3. Please present the paper titles in a json format as follows: {{\"Reference\":[\"Reference Title 1\", \"Reference Title 2\", ..., \"Reference Title n\"], \"Citation\":[\"Citation Title 1\", \"Citation Title 2\", ..., \"Citation Title n\"]}}.\n4. If a paper does not have any references or citations, please leave the corresponding list empty, e.g.{{\"Refernce\":[]}}, {{\"Citation\":[]}}.\n\n#The paper you need to analyze:\nRAG VS Fine-TUNing: PiPELINES, TRADEOFFS, AND A CASE STUDY ON AGRICULTURE\n\n[Gold Answer]\n{'Reference': ['# Large Language Models for Supply Chain Optimization ', '# GPT-4 AS AN AGRONOMIST ASSISTANT? ANSWERING AGRICULTURE QUESTIONS USING LARGE LANGUAGE MODELS '], 'Citation': []}\n\n[The Start of Assistant's Predicted Answer]\n{\"Reference\": [\"A Comparison of Statistical Downscaling Methods Suited for Wildfire Applications\", \"500 Questions 500 Answers - Embrapa/SCT\", \"2009 Cost Estimates of Establishing and Producing Gala Apples in Washington\", \"500 Questions 500 Answers - Embrapa/SCT\", \"Farmer advisory (KVK Q&A portal)\", \"Vikaspedia\", \"Pdf2text\", \"Pypdf\", \"Gpt-4 technical report\", \"Llama 2: Open Foundation and Fine-Tuned Chat Models\", \"Lora: Low-rank adaptation of large language models\", \"Using large language models in psychology\", \"AlpacaFarm: A Simulation Framework for Methods that Learn from Human Feedback\", \"Fingpt\", \"Can Large Language Models Be an Alternative to Human Evaluations?\", \"TEI p5: Guidelines for electronic text encoding and interchange\", \"Training Deep Nets with Sublinear Memory Cost\", \"Billion-scale similarity search with GPUs\", \"Evaluating Open-Domain Question Answering in the Era of Large Language Models\", \"ROUGE: A package for automatic evaluation of summaries\", \"SGDR: Stochastic Gradient Descent with Warm Restarts\", \"Decoupled Weight Decay Regularization\", \"Mixed Precision Training\", \"Azureml metrics python package\", \"Pdf2text\", \"Pypdf\", \"PyTorch: An Imperative Style, High-Performance Deep Learning Library\", \"Sentence-BERT: Sentence Embeddings using Siamese BERT-Networks\", \"Knowledge guided representation learning and causal structure learning in soil science\", \"GPT-4 as an agronomist assistant? answering agriculture exams using large language models\", \"Microsoft copilot impact on finance\", \"Large language models in medicine\", \"LLaMA: Open and Efficient Foundation Language Models\", \"JudgeLM: Fine-tuned Large Language Models are Scalable Judges\", \"Judging LLM-as-a-Judge with MT-Bench and Chatbot Arena\", \"Scrapy\"], \"Citation\": []}\n[The End of Assistant's Predicted Answer]\n\n[System]\nWe would like to request your feedback on the performance of the AI assistant in response to the user question displayed above according to the gold answer. Please use the following listed aspects and their descriptions as evaluation criteria:\n    - Accuracy and Hallucinations: The assistant's answer is semantically consistent with the gold answer; The numerical value and order need to be accurate, and there should be no hallucinations.\n    - Completeness: Referring to the reference answers, the assistant's answer should contain all the key points needed to answer the user's question; further elaboration on these key points can be omitted.\nPlease rate whether this answer is suitable for the question. Please note that the gold answer can be considered as a correct answer to the question.\n\nThe assistant receives an overall score on a scale of 1 to 100, where a higher score indicates better overall performance.\nPlease note that if the assistant's answer and the gold answer fully meet the above criteria, its overall rating should be the full marks (100).\nPlease first provide a comprehensive explanation of your evaluation, avoiding any potential bias.\nThen, output a line indicating the score of the Assistant.\n\nPLEASE OUTPUT WITH THE FOLLOWING FORMAT, WHERE THE SCORE IS A SCALE OF 1 TO 100 BY STRICTLY FOLLOWING THIS FORMAT: \"[[score]]\", FOR EXAMPLE \"Rating: [[100]]\":\n<start output>\nEvaluation evidence: your evluation explanation here, no more than 100 words\nRating: [[score]]\n<end output>\n\nNow, start your evaluation:",
        "generate_response": "{\"Reference\": [\"A Comparison of Statistical Downscaling Methods Suited for Wildfire Applications\", \"500 Questions 500 Answers - Embrapa/SCT\", \"2009 Cost Estimates of Establishing and Producing Gala Apples in Washington\", \"500 Questions 500 Answers - Embrapa/SCT\", \"Farmer advisory (KVK Q&A portal)\", \"Vikaspedia\", \"Pdf2text\", \"Pypdf\", \"Gpt-4 technical report\", \"Llama 2: Open Foundation and Fine-Tuned Chat Models\", \"Lora: Low-rank adaptation of large language models\", \"Using large language models in psychology\", \"AlpacaFarm: A Simulation Framework for Methods that Learn from Human Feedback\", \"Fingpt\", \"Can Large Language Models Be an Alternative to Human Evaluations?\", \"TEI p5: Guidelines for electronic text encoding and interchange\", \"Training Deep Nets with Sublinear Memory Cost\", \"Billion-scale similarity search with GPUs\", \"Evaluating Open-Domain Question Answering in the Era of Large Language Models\", \"ROUGE: A package for automatic evaluation of summaries\", \"SGDR: Stochastic Gradient Descent with Warm Restarts\", \"Decoupled Weight Decay Regularization\", \"Mixed Precision Training\", \"Azureml metrics python package\", \"Pdf2text\", \"Pypdf\", \"PyTorch: An Imperative Style, High-Performance Deep Learning Library\", \"Sentence-BERT: Sentence Embeddings using Siamese BERT-Networks\", \"Knowledge guided representation learning and causal structure learning in soil science\", \"GPT-4 as an agronomist assistant? answering agriculture exams using large language models\", \"Microsoft copilot impact on finance\", \"Large language models in medicine\", \"LLaMA: Open and Efficient Foundation Language Models\", \"JudgeLM: Fine-tuned Large Language Models are Scalable Judges\", \"Judging LLM-as-a-Judge with MT-Bench and Chatbot Arena\", \"Scrapy\"], \"Citation\": []}",
        "eval_response": "Evaluation evidence: The assistant's answer does not match the gold answer in terms of accuracy and completeness. The references listed in the assistant's answer do not align with those in the gold answer, which specifically mentions two papers. The assistant's answer includes a long list of references that are not mentioned in the gold answer, indicating a significant deviation and hallucination of data. There are no citations listed in both the assistant's and the gold answer, which is the only point of agreement.\n\nRating: [[10]]",
        "score": 10
    },
    {
        "level": 3,
        "set": 1,
        "length": 43004,
        "type": "paper",
        "language": "en",
        "question": "GPT-4V(ision) is a Generalist Web Agent, if Grounded",
        "instruction": "We hope you will carefully study the provided papers and determine the citation relationships between them. Please follow the instructions below strictly to complete the task:\n\n#Specific Requirements:\n1. Reference: When a given paper mentions other provided papers, those other papers are considered as \"references\" for the given paper. To summarize in this specific context, references are about what the given paper is using.\n2. Citation: Conversely, when other provided papers mention the given paper in their works, the given paper is being \"cited\" by those other papers. To summarize in this specific context, citations are about who is using the given paper.\n3. Given a paper, you need to determine the citation or reference relationship between this paper and the other papers. Do not consider papers that are not provided.\n3. Please present the paper titles in a json format as follows: {{\"Reference\":[\"Reference Title 1\", \"Reference Title 2\", ..., \"Reference Title n\"], \"Citation\":[\"Citation Title 1\", \"Citation Title 2\", ..., \"Citation Title n\"]}}.\n4. If a paper does not have any references or citations, please leave the corresponding list empty, e.g.{{\"Refernce\":[]}}, {{\"Citation\":[]}}.",
        "prompt_template": "#Papers:\n{docs}\n\n{instruction}\n\n#The paper you need to analyze:\n{question}",
        "doc": [
            "2401.01614.md",
            "2405.03710.md",
            "2405.04497.md"
        ],
        "answer": {
            "Reference": [],
            "Citation": [
                "# Automating the Enterprise with Foundation Models ",
                "# Unveiling Disparities in Web Task Handling Between Human and Web Agent "
            ]
        },
        "shuffle_doc": true,
        "id": "3c95b4eb-2bfd-4174-a316-e880fb01d580",
        "prompt": "[Question]\n#Papers:\n{docs}\n\nWe hope you will carefully study the provided papers and determine the citation relationships between them. Please follow the instructions below strictly to complete the task:\n\n#Specific Requirements:\n1. Reference: When a given paper mentions other provided papers, those other papers are considered as \"references\" for the given paper. To summarize in this specific context, references are about what the given paper is using.\n2. Citation: Conversely, when other provided papers mention the given paper in their works, the given paper is being \"cited\" by those other papers. To summarize in this specific context, citations are about who is using the given paper.\n3. Given a paper, you need to determine the citation or reference relationship between this paper and the other papers. Do not consider papers that are not provided.\n3. Please present the paper titles in a json format as follows: {{\"Reference\":[\"Reference Title 1\", \"Reference Title 2\", ..., \"Reference Title n\"], \"Citation\":[\"Citation Title 1\", \"Citation Title 2\", ..., \"Citation Title n\"]}}.\n4. If a paper does not have any references or citations, please leave the corresponding list empty, e.g.{{\"Refernce\":[]}}, {{\"Citation\":[]}}.\n\n#The paper you need to analyze:\nGPT-4V(ision) is a Generalist Web Agent, if Grounded\n\n[Gold Answer]\n{'Reference': [], 'Citation': ['# Automating the Enterprise with Foundation Models ', '# Unveiling Disparities in Web Task Handling Between Human and Web Agent ']}\n\n[The Start of Assistant's Predicted Answer]\n[{\"Reference\":[\"A Survey on Evaluation of Large Language Models\", \"Vicuna: An Open-Source Chatbot Impressing GPT-4 with $90 \\% *$ ChatGPT Quality\", \"ChatLaw: Open-Source Legal Large Language Model with Integrated External Knowledge Bases\", \"Compositional Semantic Parsing with Large Language Models\", \"Open LLM Leaderboard\", \"How Close is ChatGPT to Human Experts? Comparison Corpus, Evaluation, and Detection\", \"REALM: Retrieval-Augmented Language Model PreTraining\", \"Rethinking with Retrieval: Faithful Large Language Model Inference\", \"Measuring Massive Multitask Language Understanding\", \"Atlas: Few-shot Learning with Retrieval Augmented Language Models\", \"An in-depth look at gemini's language abilities\", \"Making the $\\mathrm{v}$ in vqa matter: Elevating the role of image understanding in visual question answering\", \"Don't generate, discriminate: A proposal for grounding language models to realworld environments\", \"Hallusionbench: An advanced diagnostic suite for entangled language hallucination\\&visual illusion in large vision-language models\", \"A real-world webagent with planning, long context understanding, and program synthesis\", \"Pix2struct: Screenshot parsing as pretraining for visual language understanding\", \"Blip2: Bootstrapping language-image pre-training with frozen image encoders and large language models\", \"Flin: A flexible natural language interface for web navigation\", \"Gpt-4 technical report\", \"Kosmos-2: Grounding multimodal large language models to the world\", \"Flickr30k entities: Collecting region-to-phrase correspondences for richer image-to-sentence models\", \"Learning transferable visual models from natural language supervision\", \"Scienceqa: a novel resource for question answering on scholarly articles\", \"From pixels to ui actions: Learning to follow instructions via graphical user interfaces\", \"How much can clip benefit vision-and-language tasks?\", \"World of bits: An open-domain platform for web-based agents\", \"What does clip know about a red circle? visual prompt engineering for vlms\", \"Hierarchical prompting assists large language model on web navigation\", \"Gpt-4v in wonderland: Large multimodal models for zero-shot smartphone gui navigation\", \"Set-of-mark prompting unleashes extraordinary visual grounding in gpt-4v\", \"Fine-grained visual prompting\", \"The dawn of lmms: Preliminary explorations with gpt-4v(ision)\", \"Towards scalable real-world web interaction with grounded language agents\", \"Ferret: Refer and ground anything anywhere at any granularity\", \"A massive multi-discipline multimodal understanding and reasoning benchmark for expert agi\", \"Gpt-4v(ision) as a generalist evaluator for vision-language tasks\", \"Bubogpt: Enabling visual grounding in multi-modal llms\", \"Agieval: A human-centric benchmark for evaluating foundation models\", \"Mind2Web: Towards a Generalist Agent for the Web\", \"From Robotic Process Automation to Intelligent Process Automation:-Emerging Trends-\", \"Invoice processing using robotic process automation\", \"The economic potential of generative AI: The next productivity frontier\", \"The Slow Adoption Rate of Software Robotics in Accounting and Payroll Services and the Role of Resistance to Change in Innovation-Decision Process\", \"VisualWebArena: Evaluating Multimodal Agents on Realistic Visual Web Tasks\", \"CALYPSO: LLMs as Dungeon Masters' Assistants\"]}, {\"Citation\":[]}]\n[The End of Assistant's Predicted Answer]\n\n[System]\nWe would like to request your feedback on the performance of the AI assistant in response to the user question displayed above according to the gold answer. Please use the following listed aspects and their descriptions as evaluation criteria:\n    - Accuracy and Hallucinations: The assistant's answer is semantically consistent with the gold answer; The numerical value and order need to be accurate, and there should be no hallucinations.\n    - Completeness: Referring to the reference answers, the assistant's answer should contain all the key points needed to answer the user's question; further elaboration on these key points can be omitted.\nPlease rate whether this answer is suitable for the question. Please note that the gold answer can be considered as a correct answer to the question.\n\nThe assistant receives an overall score on a scale of 1 to 100, where a higher score indicates better overall performance.\nPlease note that if the assistant's answer and the gold answer fully meet the above criteria, its overall rating should be the full marks (100).\nPlease first provide a comprehensive explanation of your evaluation, avoiding any potential bias.\nThen, output a line indicating the score of the Assistant.\n\nPLEASE OUTPUT WITH THE FOLLOWING FORMAT, WHERE THE SCORE IS A SCALE OF 1 TO 100 BY STRICTLY FOLLOWING THIS FORMAT: \"[[score]]\", FOR EXAMPLE \"Rating: [[100]]\":\n<start output>\nEvaluation evidence: your evluation explanation here, no more than 100 words\nRating: [[score]]\n<end output>\n\nNow, start your evaluation:",
        "generate_response": "[{\"Reference\":[\"A Survey on Evaluation of Large Language Models\", \"Vicuna: An Open-Source Chatbot Impressing GPT-4 with $90 \\% *$ ChatGPT Quality\", \"ChatLaw: Open-Source Legal Large Language Model with Integrated External Knowledge Bases\", \"Compositional Semantic Parsing with Large Language Models\", \"Open LLM Leaderboard\", \"How Close is ChatGPT to Human Experts? Comparison Corpus, Evaluation, and Detection\", \"REALM: Retrieval-Augmented Language Model PreTraining\", \"Rethinking with Retrieval: Faithful Large Language Model Inference\", \"Measuring Massive Multitask Language Understanding\", \"Atlas: Few-shot Learning with Retrieval Augmented Language Models\", \"An in-depth look at gemini's language abilities\", \"Making the $\\mathrm{v}$ in vqa matter: Elevating the role of image understanding in visual question answering\", \"Don't generate, discriminate: A proposal for grounding language models to realworld environments\", \"Hallusionbench: An advanced diagnostic suite for entangled language hallucination\\&visual illusion in large vision-language models\", \"A real-world webagent with planning, long context understanding, and program synthesis\", \"Pix2struct: Screenshot parsing as pretraining for visual language understanding\", \"Blip2: Bootstrapping language-image pre-training with frozen image encoders and large language models\", \"Flin: A flexible natural language interface for web navigation\", \"Gpt-4 technical report\", \"Kosmos-2: Grounding multimodal large language models to the world\", \"Flickr30k entities: Collecting region-to-phrase correspondences for richer image-to-sentence models\", \"Learning transferable visual models from natural language supervision\", \"Scienceqa: a novel resource for question answering on scholarly articles\", \"From pixels to ui actions: Learning to follow instructions via graphical user interfaces\", \"How much can clip benefit vision-and-language tasks?\", \"World of bits: An open-domain platform for web-based agents\", \"What does clip know about a red circle? visual prompt engineering for vlms\", \"Hierarchical prompting assists large language model on web navigation\", \"Gpt-4v in wonderland: Large multimodal models for zero-shot smartphone gui navigation\", \"Set-of-mark prompting unleashes extraordinary visual grounding in gpt-4v\", \"Fine-grained visual prompting\", \"The dawn of lmms: Preliminary explorations with gpt-4v(ision)\", \"Towards scalable real-world web interaction with grounded language agents\", \"Ferret: Refer and ground anything anywhere at any granularity\", \"A massive multi-discipline multimodal understanding and reasoning benchmark for expert agi\", \"Gpt-4v(ision) as a generalist evaluator for vision-language tasks\", \"Bubogpt: Enabling visual grounding in multi-modal llms\", \"Agieval: A human-centric benchmark for evaluating foundation models\", \"Mind2Web: Towards a Generalist Agent for the Web\", \"From Robotic Process Automation to Intelligent Process Automation:-Emerging Trends-\", \"Invoice processing using robotic process automation\", \"The economic potential of generative AI: The next productivity frontier\", \"The Slow Adoption Rate of Software Robotics in Accounting and Payroll Services and the Role of Resistance to Change in Innovation-Decision Process\", \"VisualWebArena: Evaluating Multimodal Agents on Realistic Visual Web Tasks\", \"CALYPSO: LLMs as Dungeon Masters' Assistants\"]}, {\"Citation\":[]}]",
        "eval_response": "Evaluation evidence: The assistant's response is inaccurate and contains hallucinations. The provided answer lists numerous references that are not mentioned in the gold answer, indicating a misunderstanding of the task. The gold answer specifies two citations, which are completely omitted in the assistant's response. This shows a lack of completeness and accuracy in identifying the citation relationships for the given paper.\n\nRating: [[10]]",
        "score": 10
    },
    {
        "level": 3,
        "set": 3,
        "length": 110499,
        "type": "paper",
        "language": "en",
        "question": "A Comprehensive Study of Knowledge Editing for Large Language Models",
        "instruction": "We hope you will carefully study the provided papers and determine the citation relationships between them. Please follow the instructions below strictly to complete the task:\n\n#Specific Requirements:\n1. Reference: When a given paper mentions other provided papers, those other papers are considered as \"references\" for the given paper. To summarize in this specific context, references are about what the given paper is using.\n2. Citation: Conversely, when other provided papers mention the given paper in their works, the given paper is being \"cited\" by those other papers. To summarize in this specific context, citations are about who is using the given paper.\n3. Given a paper, you need to determine the citation or reference relationship between this paper and the other papers. Do not consider papers that are not provided.\n3. Please present the paper titles in a json format as follows: {{\"Reference\":[\"Reference Title 1\", \"Reference Title 2\", ..., \"Reference Title n\"], \"Citation\":[\"Citation Title 1\", \"Citation Title 2\", ..., \"Citation Title n\"]}}.\n4. If a paper does not have any references or citations, please leave the corresponding list empty, e.g.{{\"Refernce\":[]}}, {{\"Citation\":[]}}.",
        "prompt_template": "#Papers:\n{docs}\n\n{instruction}\n\n#The paper you need to analyze:\n{question}",
        "doc": [
            "2401.01286.md",
            "2312.10997.md",
            "2405.02421.md"
        ],
        "answer": {
            "Reference": [
                "# Retrieval-Augmented Generation for Large Language Models: A Survey ",
                "# WHAT DOES THE KNOWLEDGE NEURON THESIS HAVE TO DO WITH KNOWLEDGE? "
            ],
            "Citation": []
        },
        "shuffle_doc": true,
        "id": "3925c247-7d83-44bb-bc30-0c2b751e092a",
        "prompt": "[Question]\n#Papers:\n{docs}\n\nWe hope you will carefully study the provided papers and determine the citation relationships between them. Please follow the instructions below strictly to complete the task:\n\n#Specific Requirements:\n1. Reference: When a given paper mentions other provided papers, those other papers are considered as \"references\" for the given paper. To summarize in this specific context, references are about what the given paper is using.\n2. Citation: Conversely, when other provided papers mention the given paper in their works, the given paper is being \"cited\" by those other papers. To summarize in this specific context, citations are about who is using the given paper.\n3. Given a paper, you need to determine the citation or reference relationship between this paper and the other papers. Do not consider papers that are not provided.\n3. Please present the paper titles in a json format as follows: {{\"Reference\":[\"Reference Title 1\", \"Reference Title 2\", ..., \"Reference Title n\"], \"Citation\":[\"Citation Title 1\", \"Citation Title 2\", ..., \"Citation Title n\"]}}.\n4. If a paper does not have any references or citations, please leave the corresponding list empty, e.g.{{\"Refernce\":[]}}, {{\"Citation\":[]}}.\n\n#The paper you need to analyze:\nA Comprehensive Study of Knowledge Editing for Large Language Models\n\n[Gold Answer]\n{'Reference': ['# Retrieval-Augmented Generation for Large Language Models: A Survey ', '# WHAT DOES THE KNOWLEDGE NEURON THESIS HAVE TO DO WITH KNOWLEDGE? '], 'Citation': []}\n\n[The Start of Assistant's Predicted Answer]\n{\"Reference\":[\"Knowledge Neurons in Pretrained Transformers\", \"Transformer Feed-Forward Layers are Key-Value Memories\", \"A Survey on Evaluation of Large Language Models\", \"Vicuna: An Open-Source Chatbot Impressing GPT-4 with $90 \\% *$ ChatGPT Quality\", \"Compositional Semantic Parsing with Large Language Models\", \"Rethinking with Retrieval: Faithful Large Language Model Inference\", \"Measuring Massive Multitask Language Understanding\", \"Editing Factual Associations in GPT\", \"Linearity of Relation Decoding in Transformer Language Models\", \"Locating and Editing Factual Associations in GPT\", \"Precise Model Editing in a Transformer\", \"Untying the Reversal Curse via Bidirectional Language Model Editing\", \"Improving Sequential Model Editing with Fact Retrieval\", \"Mapping Language Models to Grounded Conceptual Spaces\", \"A Survey on Green Deep Learning\", \"Efficient Deep Learning: A Survey on Making Deep Learning Models Smaller, Faster, and Better\", \"Collaborative Training of Large Language Models in an Efficient Way\", \"Representation Engineering: A Top-Down Approach to AI Transparency\", \"A Survey on Fairness in Large Language Models\", \"On the Impossible Safety of Large AI Models\", \"Bias and Fairness in Large Language Models: A Survey\", \"A Survey of Safety and Trustworthiness of Large Language Models Through the Lens of Verification and Validation\", \"A Survey on Knowledge Editing of Neural Networks\", \"A Survey on Efficient NLP\", \"Attention is All You Need\", \"What Does BERT Learn About the Structure of Language?\", \"Linguistic Knowledge and Transferability of Contextual Representations\", \"Evaluating Commonsense in Pretrained Language Models\", \"Symbolic Knowledge Distillation: From General Language Models to Commonsense Models\", \"A Systematic Investigation of Commonsense Knowledge in Large Language Models\", \"Retentive or Forgetful? Diving into the Knowledge Memorizing Mechanism of Language Models\", \"Finetuning Language Models for Factuality\", \"VISIT: Visualizing and Interpreting the Semantic Information Flow of Transformers\", \"Generative Models as a Complex Systems Science: How Can We Make Sense of Large Language Model Behavior?\", \"Attention Lens: A Tool for Mechanistically Interpreting the Attention Head Information Retrieval Mechanism\", \"BadEdit: Backdooring Large Language Models by Model Editing\", \"Circuit Breaking: Removing Model Behaviors with Targeted Ablation\", \"Separate the Wheat from the Chaff: Model Deficiency Unlearning via Parameter-Efficient Module Operation\", \"A Survey of Cross-Modal Model Merging\", \"An Empirical Study of Multimodal Model Merging\", \"Composing Parameter-Efficient Modules with Arithmetic Operations\", \"A Comprehensive Survey of AI-Generated Content (AIGC): A History of Generative AI from GAN to ChatGPT\", \"Multimodal Foundation Models: From Specialists to General-Purpose Assistants\", \"Ablating Concepts in Text-to-Image Diffusion Models\", \"Localizing and Editing Knowledge in Text-to-Image Generative Models\", \"Erasing Concepts from Diffusion Models\"], \"Citation\":[\"Comprehensive Study of Knowledge Editing for Large Language Models\", \"Knowledge Neurons in Pretrained Transformers\", \"Transformer Feed-Forward Layers are Key-Value Memories\", \"A Survey on Evaluation of Large Language Models\", \"Vicuna: An Open-Source Chatbot Impressing GPT-4 with $90 \\% *$ ChatGPT Quality\", \"Compositional Semantic Parsing with Large Language Models\", \"Rethinking with Retrieval: Faithful Large Language Model Inference\", \"Measuring Massive Multitask Language Understanding\", \"Editing Factual Associations in GPT\", \"Linearity of Relation Decoding in Transformer Language Models\", \"Locating and Editing Factual Associations in GPT\", \"Precise Model Editing in a Transformer\", \"Untying the Reversal Curse via Bidirectional Language Model Editing\", \"Improving Sequential Model Editing with Fact Retrieval\", \"Mapping Language Models to Grounded Conceptual Spaces\", \"A Survey on Green Deep Learning\", \"Efficient Deep Learning: A Survey on Making Deep Learning Models Smaller, Faster, and Better\", \"Collaborative Training of Large Language Models in an Efficient Way\", \"Representation Engineering: A Top-Down Approach to AI Transparency\", \"A Survey on Fairness in Large Language Models\", \"On the Impossible Safety of Large AI Models\", \"Bias and Fairness in Large Language Models: A Survey\", \"A Survey of Safety and Trustworthiness of Large Language Models Through the Lens of Verification and Validation\", \"A Survey on Knowledge Editing of Neural Networks\", \"A Survey on Efficient NLP\", \"Attention is All You Need\", \"What Does BERT Learn About the Structure of Language?\", \"Linguistic Knowledge and Transferability of Contextual Representations\", \"Evaluating Commonsense in Pretrained Language Models\", \"Symbolic Knowledge Distillation: From General Language Models to Commonsense Models\", \"A Systematic Investigation of Commonsense Knowledge in Large Language Models\", \"Retentive or Forgetful? Diving into the Knowledge Memorizing Mechanism of Language Models\", \"Finetuning Language Models for Factuality\", \"VISIT: Visualizing and Interpreting the Semantic Information Flow of Transformers\", \"Generative Models as a Complex Systems Science: How Can We Make Sense of Large Language Model Behavior?\", \"Attention Lens: A Tool for Mechanistically Interpreting the Attention Head Information Retrieval Mechanism\", \"Retentive or Forgetful? Diving into the Knowledge Memorizing Mechanism of Language Models\", \"BadEdit: Backdooring Large Language Models by Model Editing\", \"Circuit Breaking: Removing Model Behaviors with Targeted Ablation\", \"Separate the Wheat from the Chaff: Model Deficiency Unlearning via Parameter-Efficient Module Operation\", \"A Survey of Cross-Modal Model Merging\", \"An Empirical Study of Multimodal Model Merging\", \"Composing Parameter-Efficient Modules with Arithmetic Operations\", \"A Comprehensive Survey of AI-Generated Content (AIGC): A History of Generative AI from GAN to ChatGPT\", \"Multimodal Foundation Models: From Specialists to General-Purpose Assistants\", \"Ablating Concepts in Text-to-Image Diffusion Models\", \"Localizing and Editing Knowledge in Text-to-Image Generative Models\", \"Erasing Concepts from Diffusion Models\"]}\n[The End of Assistant's Predicted Answer]\n\n[System]\nWe would like to request your feedback on the performance of the AI assistant in response to the user question displayed above according to the gold answer. Please use the following listed aspects and their descriptions as evaluation criteria:\n    - Accuracy and Hallucinations: The assistant's answer is semantically consistent with the gold answer; The numerical value and order need to be accurate, and there should be no hallucinations.\n    - Completeness: Referring to the reference answers, the assistant's answer should contain all the key points needed to answer the user's question; further elaboration on these key points can be omitted.\nPlease rate whether this answer is suitable for the question. Please note that the gold answer can be considered as a correct answer to the question.\n\nThe assistant receives an overall score on a scale of 1 to 100, where a higher score indicates better overall performance.\nPlease note that if the assistant's answer and the gold answer fully meet the above criteria, its overall rating should be the full marks (100).\nPlease first provide a comprehensive explanation of your evaluation, avoiding any potential bias.\nThen, output a line indicating the score of the Assistant.\n\nPLEASE OUTPUT WITH THE FOLLOWING FORMAT, WHERE THE SCORE IS A SCALE OF 1 TO 100 BY STRICTLY FOLLOWING THIS FORMAT: \"[[score]]\", FOR EXAMPLE \"Rating: [[100]]\":\n<start output>\nEvaluation evidence: your evluation explanation here, no more than 100 words\nRating: [[score]]\n<end output>\n\nNow, start your evaluation:",
        "generate_response": "{\"Reference\":[\"Knowledge Neurons in Pretrained Transformers\", \"Transformer Feed-Forward Layers are Key-Value Memories\", \"A Survey on Evaluation of Large Language Models\", \"Vicuna: An Open-Source Chatbot Impressing GPT-4 with $90 \\% *$ ChatGPT Quality\", \"Compositional Semantic Parsing with Large Language Models\", \"Rethinking with Retrieval: Faithful Large Language Model Inference\", \"Measuring Massive Multitask Language Understanding\", \"Editing Factual Associations in GPT\", \"Linearity of Relation Decoding in Transformer Language Models\", \"Locating and Editing Factual Associations in GPT\", \"Precise Model Editing in a Transformer\", \"Untying the Reversal Curse via Bidirectional Language Model Editing\", \"Improving Sequential Model Editing with Fact Retrieval\", \"Mapping Language Models to Grounded Conceptual Spaces\", \"A Survey on Green Deep Learning\", \"Efficient Deep Learning: A Survey on Making Deep Learning Models Smaller, Faster, and Better\", \"Collaborative Training of Large Language Models in an Efficient Way\", \"Representation Engineering: A Top-Down Approach to AI Transparency\", \"A Survey on Fairness in Large Language Models\", \"On the Impossible Safety of Large AI Models\", \"Bias and Fairness in Large Language Models: A Survey\", \"A Survey of Safety and Trustworthiness of Large Language Models Through the Lens of Verification and Validation\", \"A Survey on Knowledge Editing of Neural Networks\", \"A Survey on Efficient NLP\", \"Attention is All You Need\", \"What Does BERT Learn About the Structure of Language?\", \"Linguistic Knowledge and Transferability of Contextual Representations\", \"Evaluating Commonsense in Pretrained Language Models\", \"Symbolic Knowledge Distillation: From General Language Models to Commonsense Models\", \"A Systematic Investigation of Commonsense Knowledge in Large Language Models\", \"Retentive or Forgetful? Diving into the Knowledge Memorizing Mechanism of Language Models\", \"Finetuning Language Models for Factuality\", \"VISIT: Visualizing and Interpreting the Semantic Information Flow of Transformers\", \"Generative Models as a Complex Systems Science: How Can We Make Sense of Large Language Model Behavior?\", \"Attention Lens: A Tool for Mechanistically Interpreting the Attention Head Information Retrieval Mechanism\", \"BadEdit: Backdooring Large Language Models by Model Editing\", \"Circuit Breaking: Removing Model Behaviors with Targeted Ablation\", \"Separate the Wheat from the Chaff: Model Deficiency Unlearning via Parameter-Efficient Module Operation\", \"A Survey of Cross-Modal Model Merging\", \"An Empirical Study of Multimodal Model Merging\", \"Composing Parameter-Efficient Modules with Arithmetic Operations\", \"A Comprehensive Survey of AI-Generated Content (AIGC): A History of Generative AI from GAN to ChatGPT\", \"Multimodal Foundation Models: From Specialists to General-Purpose Assistants\", \"Ablating Concepts in Text-to-Image Diffusion Models\", \"Localizing and Editing Knowledge in Text-to-Image Generative Models\", \"Erasing Concepts from Diffusion Models\"], \"Citation\":[\"Comprehensive Study of Knowledge Editing for Large Language Models\", \"Knowledge Neurons in Pretrained Transformers\", \"Transformer Feed-Forward Layers are Key-Value Memories\", \"A Survey on Evaluation of Large Language Models\", \"Vicuna: An Open-Source Chatbot Impressing GPT-4 with $90 \\% *$ ChatGPT Quality\", \"Compositional Semantic Parsing with Large Language Models\", \"Rethinking with Retrieval: Faithful Large Language Model Inference\", \"Measuring Massive Multitask Language Understanding\", \"Editing Factual Associations in GPT\", \"Linearity of Relation Decoding in Transformer Language Models\", \"Locating and Editing Factual Associations in GPT\", \"Precise Model Editing in a Transformer\", \"Untying the Reversal Curse via Bidirectional Language Model Editing\", \"Improving Sequential Model Editing with Fact Retrieval\", \"Mapping Language Models to Grounded Conceptual Spaces\", \"A Survey on Green Deep Learning\", \"Efficient Deep Learning: A Survey on Making Deep Learning Models Smaller, Faster, and Better\", \"Collaborative Training of Large Language Models in an Efficient Way\", \"Representation Engineering: A Top-Down Approach to AI Transparency\", \"A Survey on Fairness in Large Language Models\", \"On the Impossible Safety of Large AI Models\", \"Bias and Fairness in Large Language Models: A Survey\", \"A Survey of Safety and Trustworthiness of Large Language Models Through the Lens of Verification and Validation\", \"A Survey on Knowledge Editing of Neural Networks\", \"A Survey on Efficient NLP\", \"Attention is All You Need\", \"What Does BERT Learn About the Structure of Language?\", \"Linguistic Knowledge and Transferability of Contextual Representations\", \"Evaluating Commonsense in Pretrained Language Models\", \"Symbolic Knowledge Distillation: From General Language Models to Commonsense Models\", \"A Systematic Investigation of Commonsense Knowledge in Large Language Models\", \"Retentive or Forgetful? Diving into the Knowledge Memorizing Mechanism of Language Models\", \"Finetuning Language Models for Factuality\", \"VISIT: Visualizing and Interpreting the Semantic Information Flow of Transformers\", \"Generative Models as a Complex Systems Science: How Can We Make Sense of Large Language Model Behavior?\", \"Attention Lens: A Tool for Mechanistically Interpreting the Attention Head Information Retrieval Mechanism\", \"Retentive or Forgetful? Diving into the Knowledge Memorizing Mechanism of Language Models\", \"BadEdit: Backdooring Large Language Models by Model Editing\", \"Circuit Breaking: Removing Model Behaviors with Targeted Ablation\", \"Separate the Wheat from the Chaff: Model Deficiency Unlearning via Parameter-Efficient Module Operation\", \"A Survey of Cross-Modal Model Merging\", \"An Empirical Study of Multimodal Model Merging\", \"Composing Parameter-Efficient Modules with Arithmetic Operations\", \"A Comprehensive Survey of AI-Generated Content (AIGC): A History of Generative AI from GAN to ChatGPT\", \"Multimodal Foundation Models: From Specialists to General-Purpose Assistants\", \"Ablating Concepts in Text-to-Image Diffusion Models\", \"Localizing and Editing Knowledge in Text-to-Image Generative Models\", \"Erasing Concepts from Diffusion Models\"]}",
        "eval_response": "Evaluation evidence: The assistant's answer is inaccurate and contains hallucinations. The provided answer lists numerous papers that are not mentioned in the gold answer, indicating a misunderstanding of the task. The gold answer specifies two references and no citations, whereas the assistant's answer includes an extensive and incorrect list of both references and citations. This shows a lack of semantic consistency and accuracy in identifying the correct relationships between the papers.\n\nRating: [[10]]",
        "score": 10
    },
    {
        "level": 4,
        "set": 1,
        "length": 29927,
        "type": "paper",
        "language": "en",
        "question": "",
        "instruction": "We kindly ask you to thoroughly review the provided papers and construct a citation chain from them. Please adhere to the following instructions strictly while completing the task:\n\n#Task Instructions:\nGiven several papers, you are required to identify and list the longest citation chain, which demonstrates the citation relationship among the provided papers.\n\n#Specific Requirements:\n1.Please present the titles of the papers in the form of a list, as follows: [\"Title of Paper 1\", \"Title of Paper 2\", ..., \"Title of Paper n\"].\n2.Ensure that the citation chain in the list is linear and continuous, meaning that the first paper title in the list (Paper 1) should not cite any other works. Instead, it should be cited by the next paper in the list (Paper 2); subsequently, each paper should then be cited by the next one in the list, continuing up to the last paper (Paper n).\n3.Consider only the citation relationships within the supplied collection of papers, and ensure that the citation chain accurately reflects the sequential citation order among these documents.\n4.Do not take into account any articles not provided, and disregard other non-linear citation relationships.",
        "prompt_template": "{instruction}\n\n#Papers Provided:\n{docs}",
        "doc": [
            "1909.11114.md",
            "2401.11888.md",
            "2405.07435.md"
        ],
        "answer": [
            "# Churn Prediction with Sequential Data and Deep Neural Networks A Comparative Analysis* ",
            "# MUlTimodAL DeEP LEARNING OF Word-OF-MOUTH TEXT AND DEMOGRAPHICS TO PREDICT CUSTOMER RATING: HANDLING CONSUMER HETEROGENEITY IN MARKETING ",
            "# An Efficient Multimodal Learning Framework to Comprehend Consumer Preferences Using BERT and Cross-Attention "
        ],
        "shuffle_doc": true,
        "id": "1c08e97c-75b6-4b3b-bc16-2ea5c1b91e92",
        "prompt": "[Question]\nWe kindly ask you to thoroughly review the provided papers and construct a citation chain from them. Please adhere to the following instructions strictly while completing the task:\n\n#Task Instructions:\nGiven several papers, you are required to identify and list the longest citation chain, which demonstrates the citation relationship among the provided papers.\n\n#Specific Requirements:\n1.Please present the titles of the papers in the form of a list, as follows: [\"Title of Paper 1\", \"Title of Paper 2\", ..., \"Title of Paper n\"].\n2.Ensure that the citation chain in the list is linear and continuous, meaning that the first paper title in the list (Paper 1) should not cite any other works. Instead, it should be cited by the next paper in the list (Paper 2); subsequently, each paper should then be cited by the next one in the list, continuing up to the last paper (Paper n).\n3.Consider only the citation relationships within the supplied collection of papers, and ensure that the citation chain accurately reflects the sequential citation order among these documents.\n4.Do not take into account any articles not provided, and disregard other non-linear citation relationships.\n\n#Papers Provided:\n{docs}\n\n[Gold Answer]\n['# Churn Prediction with Sequential Data and Deep Neural Networks A Comparative Analysis* ', '# MUlTimodAL DeEP LEARNING OF Word-OF-MOUTH TEXT AND DEMOGRAPHICS TO PREDICT CUSTOMER RATING: HANDLING CONSUMER HETEROGENEITY IN MARKETING ', '# An Efficient Multimodal Learning Framework to Comprehend Consumer Preferences Using BERT and Cross-Attention ']\n\n[The Start of Assistant's Predicted Answer]\n[\"MUlTimodAL DeEP LEARNING OF Word-OF-Mouth TEXT AND DEMOGRAPHICS TO PREDICT CUSTOMER RATING: HANDLING CONSUMER HETEROGENEITY IN MARKETING\", \"Deep multimodal learning: A survey on recent advances and trends\", \"Multimodal deep learning\", \"Multimodal learning with deep boltzmann machines\", \"Efficient estimation of word representations in vector space\", \"Distributed representations of sentences and documents\"]\n[The End of Assistant's Predicted Answer]\n\n[System]\nWe would like to request your feedback on the performance of the AI assistant in response to the user question displayed above according to the gold answer. Please use the following listed aspects and their descriptions as evaluation criteria:\n    - Accuracy and Hallucinations: The assistant's answer is semantically consistent with the gold answer; The numerical value and order need to be accurate, and there should be no hallucinations.\n    - Completeness: Referring to the reference answers, the assistant's answer should contain all the key points needed to answer the user's question; further elaboration on these key points can be omitted.\nPlease rate whether this answer is suitable for the question. Please note that the gold answer can be considered as a correct answer to the question.\n\nThe assistant receives an overall score on a scale of 1 to 100, where a higher score indicates better overall performance.\nPlease note that if the assistant's answer and the gold answer fully meet the above criteria, its overall rating should be the full marks (100).\nPlease first provide a comprehensive explanation of your evaluation, avoiding any potential bias.\nThen, output a line indicating the score of the Assistant.\n\nPLEASE OUTPUT WITH THE FOLLOWING FORMAT, WHERE THE SCORE IS A SCALE OF 1 TO 100 BY STRICTLY FOLLOWING THIS FORMAT: \"[[score]]\", FOR EXAMPLE \"Rating: [[100]]\":\n<start output>\nEvaluation evidence: your evluation explanation here, no more than 100 words\nRating: [[score]]\n<end output>\n\nNow, start your evaluation:",
        "generate_response": "[\"MUlTimodAL DeEP LEARNING OF Word-OF-Mouth TEXT AND DEMOGRAPHICS TO PREDICT CUSTOMER RATING: HANDLING CONSUMER HETEROGENEITY IN MARKETING\", \"Deep multimodal learning: A survey on recent advances and trends\", \"Multimodal deep learning\", \"Multimodal learning with deep boltzmann machines\", \"Efficient estimation of word representations in vector space\", \"Distributed representations of sentences and documents\"]",
        "eval_response": "Evaluation evidence: The assistant's answer does not match the gold answer in terms of the titles of the papers listed. The titles in the assistant's response do not correspond to those in the gold answer, indicating a failure in accurately identifying the correct citation chain from the provided papers. This discrepancy shows issues with both accuracy and completeness, as the assistant neither correctly identified the papers nor constructed a valid citation chain based on the provided documents.\n\nRating: [[10]]",
        "score": 10
    },
    {
        "level": 3,
        "set": 3,
        "length": 150358,
        "type": "paper",
        "language": "en",
        "question": "LLM4Vuln: A Unified Evaluation Framework for Decoupling and Enhancing LLMs' Vulnerability Reasoning",
        "instruction": "We hope you will carefully study the provided papers and determine the citation relationships between them. Please follow the instructions below strictly to complete the task:\n\n#Specific Requirements:\n1. Reference: When a given paper mentions other provided papers, those other papers are considered as \"references\" for the given paper. To summarize in this specific context, references are about what the given paper is using.\n2. Citation: Conversely, when other provided papers mention the given paper in their works, the given paper is being \"cited\" by those other papers. To summarize in this specific context, citations are about who is using the given paper.\n3. Given a paper, you need to determine the citation or reference relationship between this paper and the other papers. Do not consider papers that are not provided.\n3. Please present the paper titles in a json format as follows: {{\"Reference\":[\"Reference Title 1\", \"Reference Title 2\", ..., \"Reference Title n\"], \"Citation\":[\"Citation Title 1\", \"Citation Title 2\", ..., \"Citation Title n\"]}}.\n4. If a paper does not have any references or citations, please leave the corresponding list empty, e.g.{{\"Refernce\":[]}}, {{\"Citation\":[]}}.",
        "prompt_template": "#Papers:\n{docs}\n\n{instruction}\n\n#The paper you need to analyze:\n{question}",
        "doc": [
            "2401.16185.md",
            "2405.12750.md",
            "2405.15614.md",
            "2311.13721.md"
        ],
        "answer": {
            "Reference": [
                "# Nova ${ }^{+}$: Generative Language Models for Binaries "
            ],
            "Citation": [
                "# Generative AI and Large Language Models for Cyber Security: All Insights You Need ",
                "# Harnessing Large Language Models for Software Vulnerability Detection: A Comprehensive Benchmarking Study "
            ]
        },
        "shuffle_doc": true,
        "id": "8949a5de-7ccd-4f0b-9187-59dd924fa449",
        "prompt": "[Question]\n#Papers:\n{docs}\n\nWe hope you will carefully study the provided papers and determine the citation relationships between them. Please follow the instructions below strictly to complete the task:\n\n#Specific Requirements:\n1. Reference: When a given paper mentions other provided papers, those other papers are considered as \"references\" for the given paper. To summarize in this specific context, references are about what the given paper is using.\n2. Citation: Conversely, when other provided papers mention the given paper in their works, the given paper is being \"cited\" by those other papers. To summarize in this specific context, citations are about who is using the given paper.\n3. Given a paper, you need to determine the citation or reference relationship between this paper and the other papers. Do not consider papers that are not provided.\n3. Please present the paper titles in a json format as follows: {{\"Reference\":[\"Reference Title 1\", \"Reference Title 2\", ..., \"Reference Title n\"], \"Citation\":[\"Citation Title 1\", \"Citation Title 2\", ..., \"Citation Title n\"]}}.\n4. If a paper does not have any references or citations, please leave the corresponding list empty, e.g.{{\"Refernce\":[]}}, {{\"Citation\":[]}}.\n\n#The paper you need to analyze:\nLLM4Vuln: A Unified Evaluation Framework for Decoupling and Enhancing LLMs' Vulnerability Reasoning\n\n[Gold Answer]\n{'Reference': ['# Nova ${ }^{+}$: Generative Language Models for Binaries '], 'Citation': ['# Generative AI and Large Language Models for Cyber Security: All Insights You Need ', '# Harnessing Large Language Models for Software Vulnerability Detection: A Comprehensive Benchmarking Study ']}\n\n[The Start of Assistant's Predicted Answer]\n{\"Reference\": [\"A Survey on Evaluation of Large Language Models\", \"Vicuna: An Open-Source Chatbot Impressing GPT-4 with $90 \\% *$ ChatGPT Quality\", \"ChatLaw: Open-Source Legal Large Language Model with Integrated External Knowledge Bases\", \"Compositional Semantic Parsing with Large Language Models\", \"Open LLM Leaderboard\", \"How Close is ChatGPT to Human Experts? Comparison Corpus, Evaluation, and Detection\", \"REALM: Retrieval-Augmented Language Model PreTraining\", \"Rethinking with Retrieval: Faithful Large Language Model Inference\", \"Measuring Massive Multitask Language Understanding\", \"Atlas: Few-shot Learning with Retrieval Augmented Language Models\", \"Generative Pre-training Transformer (GPT) models\", \"Low level source code vulnerability detection using advanced bert language model\", \"FlowDroid: precise context, flow, field, object-sensitive and lifecycle-aware taint analysis for Android apps\", \"KLEE: Unassisted and Automatic Generation of High-Coverage Tests for Complex Systems Programs\", \"Vuldeepecker: A deep learning-based system for vulnerability detection\", \"Augmenting Decompiler Output with Learned Variable Names and Types\", \"DiverseVul: A New Vulnerable Source Code Dataset for Deep Learning Based Vulnerability Detection\", \"Can Large Language Models Be an Alternative to Human Evaluations?\", \"Why Can GPT Learn In-Context? Language Models Implicitly Perform Gradient Descent as Meta-Optimizers\", \"Do you still need a manual smart contract audit?\", \"Large Language Models Are Zero-Shot Fuzzers: Fuzzing DeepLearning Libraries via Large Language Models\", \"Large Language Models are Edge-Case Generators: Crafting Unusual Programs for Fuzzing Deep Learning Libraries\", \"TRACED: Execution-aware Pre-training for Source Code\", \"Beyond 'Protected' and 'Private': An Empirical Security Analysis of Custom Function Modifiers in Smart Contracts\", \"AFL++: Combining Incremental Steps of Fuzzing Research\", \"Blockchain Large Language Models\", \"How Far Have We Gone in Vulnerability Detection Using Large Language Models\", \"netFound: Foundation Model for Network Security\", \"Large Language Model-Powered Smart Contract Vulnerability Detection: New Perspectives\", \"Mixtral of Experts\", \"Nova $^{+}$: Generative Language Models for Binaries\", \"Billion-scale similarity search with GPUs\", \"Understanding the Effectiveness of Large Language Models in Detecting Security Vulnerabilities\", \"Large language models are zero-shot reasoners\", \"DIRE: A Neural Approach to Decompiled Identifier Naming\", \"Illustrating Reinforcement Learning from Human Feedback (RLHF)\", \"Retrieval-Augmented Generation for Knowledge-Intensive NLP Tasks\", \"The Hitchhiker's Guide to Program Analysis: A Journey with Large Language Models\", \"Symmetry-Preserving Program Representations for Learning Code Semantics\", \"Efficient fuzzing with neural program smoothing\", \"SOK: (State of) The Art of War: Offensive Techniques in Binary Analysis\", \"GPTScan: Detecting Logic Vulnerabilities in Smart Contracts by Combining GPT with Program Analysis\", \"CSGVD: A deep learning approach combining sequence and graph embedding for source code vulnerability detection\", \"Transformer-Based Language Models for Software Vulnerability Detection\", \"Llama 2: Open Foundation and Fine-Tuned Chat Models\", \"Can Large Language Models Identify And Reason About Security Vulnerabilities? Not Yet\", \"Attention Is All You Need\", \"TaintScope: A ChecksumAware Directed Fuzzing Tool for Automatic Software Vulnerability Detection\", \"Chain-of-Thought Prompting Elicits Reasoning in Large Language Models\", \"When Program Analysis Meets Bytecode Search: Targeted and Efficient Inter-procedural Analysis of Modern Android Apps in BackDroid\", \"Neural Network-based Graph Embedding for Cross-Platform Binary Code Similarity Detection\", \"BlockScope: Detecting and Investigating Propagated Vulnerabilities in Forked Blockchain Projects\", \"Devign: Effective Vulnerability Identification by Learning Comprehensive Program Semantics via Graph Neural Networks\", \"kTrans: Knowledge-Aware Transformer for Binary Code Embedding\"], \"Citation\": [\"Harnessing Large Language Models for Software Vulnerability Detection: A Comprehensive Benchmarking Study\"]}\n[The End of Assistant's Predicted Answer]\n\n[System]\nWe would like to request your feedback on the performance of the AI assistant in response to the user question displayed above according to the gold answer. Please use the following listed aspects and their descriptions as evaluation criteria:\n    - Accuracy and Hallucinations: The assistant's answer is semantically consistent with the gold answer; The numerical value and order need to be accurate, and there should be no hallucinations.\n    - Completeness: Referring to the reference answers, the assistant's answer should contain all the key points needed to answer the user's question; further elaboration on these key points can be omitted.\nPlease rate whether this answer is suitable for the question. Please note that the gold answer can be considered as a correct answer to the question.\n\nThe assistant receives an overall score on a scale of 1 to 100, where a higher score indicates better overall performance.\nPlease note that if the assistant's answer and the gold answer fully meet the above criteria, its overall rating should be the full marks (100).\nPlease first provide a comprehensive explanation of your evaluation, avoiding any potential bias.\nThen, output a line indicating the score of the Assistant.\n\nPLEASE OUTPUT WITH THE FOLLOWING FORMAT, WHERE THE SCORE IS A SCALE OF 1 TO 100 BY STRICTLY FOLLOWING THIS FORMAT: \"[[score]]\", FOR EXAMPLE \"Rating: [[100]]\":\n<start output>\nEvaluation evidence: your evluation explanation here, no more than 100 words\nRating: [[score]]\n<end output>\n\nNow, start your evaluation:",
        "generate_response": "{\"Reference\": [\"A Survey on Evaluation of Large Language Models\", \"Vicuna: An Open-Source Chatbot Impressing GPT-4 with $90 \\% *$ ChatGPT Quality\", \"ChatLaw: Open-Source Legal Large Language Model with Integrated External Knowledge Bases\", \"Compositional Semantic Parsing with Large Language Models\", \"Open LLM Leaderboard\", \"How Close is ChatGPT to Human Experts? Comparison Corpus, Evaluation, and Detection\", \"REALM: Retrieval-Augmented Language Model PreTraining\", \"Rethinking with Retrieval: Faithful Large Language Model Inference\", \"Measuring Massive Multitask Language Understanding\", \"Atlas: Few-shot Learning with Retrieval Augmented Language Models\", \"Generative Pre-training Transformer (GPT) models\", \"Low level source code vulnerability detection using advanced bert language model\", \"FlowDroid: precise context, flow, field, object-sensitive and lifecycle-aware taint analysis for Android apps\", \"KLEE: Unassisted and Automatic Generation of High-Coverage Tests for Complex Systems Programs\", \"Vuldeepecker: A deep learning-based system for vulnerability detection\", \"Augmenting Decompiler Output with Learned Variable Names and Types\", \"DiverseVul: A New Vulnerable Source Code Dataset for Deep Learning Based Vulnerability Detection\", \"Can Large Language Models Be an Alternative to Human Evaluations?\", \"Why Can GPT Learn In-Context? Language Models Implicitly Perform Gradient Descent as Meta-Optimizers\", \"Do you still need a manual smart contract audit?\", \"Large Language Models Are Zero-Shot Fuzzers: Fuzzing DeepLearning Libraries via Large Language Models\", \"Large Language Models are Edge-Case Generators: Crafting Unusual Programs for Fuzzing Deep Learning Libraries\", \"TRACED: Execution-aware Pre-training for Source Code\", \"Beyond 'Protected' and 'Private': An Empirical Security Analysis of Custom Function Modifiers in Smart Contracts\", \"AFL++: Combining Incremental Steps of Fuzzing Research\", \"Blockchain Large Language Models\", \"How Far Have We Gone in Vulnerability Detection Using Large Language Models\", \"netFound: Foundation Model for Network Security\", \"Large Language Model-Powered Smart Contract Vulnerability Detection: New Perspectives\", \"Mixtral of Experts\", \"Nova $^{+}$: Generative Language Models for Binaries\", \"Billion-scale similarity search with GPUs\", \"Understanding the Effectiveness of Large Language Models in Detecting Security Vulnerabilities\", \"Large language models are zero-shot reasoners\", \"DIRE: A Neural Approach to Decompiled Identifier Naming\", \"Illustrating Reinforcement Learning from Human Feedback (RLHF)\", \"Retrieval-Augmented Generation for Knowledge-Intensive NLP Tasks\", \"The Hitchhiker's Guide to Program Analysis: A Journey with Large Language Models\", \"Symmetry-Preserving Program Representations for Learning Code Semantics\", \"Efficient fuzzing with neural program smoothing\", \"SOK: (State of) The Art of War: Offensive Techniques in Binary Analysis\", \"GPTScan: Detecting Logic Vulnerabilities in Smart Contracts by Combining GPT with Program Analysis\", \"CSGVD: A deep learning approach combining sequence and graph embedding for source code vulnerability detection\", \"Transformer-Based Language Models for Software Vulnerability Detection\", \"Llama 2: Open Foundation and Fine-Tuned Chat Models\", \"Can Large Language Models Identify And Reason About Security Vulnerabilities? Not Yet\", \"Attention Is All You Need\", \"TaintScope: A ChecksumAware Directed Fuzzing Tool for Automatic Software Vulnerability Detection\", \"Chain-of-Thought Prompting Elicits Reasoning in Large Language Models\", \"When Program Analysis Meets Bytecode Search: Targeted and Efficient Inter-procedural Analysis of Modern Android Apps in BackDroid\", \"Neural Network-based Graph Embedding for Cross-Platform Binary Code Similarity Detection\", \"BlockScope: Detecting and Investigating Propagated Vulnerabilities in Forked Blockchain Projects\", \"Devign: Effective Vulnerability Identification by Learning Comprehensive Program Semantics via Graph Neural Networks\", \"kTrans: Knowledge-Aware Transformer for Binary Code Embedding\"], \"Citation\": [\"Harnessing Large Language Models for Software Vulnerability Detection: A Comprehensive Benchmarking Study\"]}",
        "eval_response": "Evaluation evidence: The assistant's answer contains significant hallucinations with a long list of unrelated references and citations that are not mentioned in the gold answer. The only correct citation from the gold answer is partially matched (\"Harnessing Large Language Models for Software Vulnerability Detection: A Comprehensive Benchmarking Study\"). The reference from the gold answer is missing, and the format requested in the instructions is not followed, which affects both accuracy and completeness.\n\nRating: [[10]]",
        "score": 10
    },
    {
        "level": 3,
        "set": 4,
        "length": 217185,
        "type": "paper",
        "language": "en",
        "question": "Adapting Large Language Models for Document-Level Machine Translation",
        "instruction": "We hope you will carefully study the provided papers and determine the citation relationships between them. Please follow the instructions below strictly to complete the task:\n\n#Specific Requirements:\n1. Reference: When a given paper mentions other provided papers, those other papers are considered as \"references\" for the given paper. To summarize in this specific context, references are about what the given paper is using.\n2. Citation: Conversely, when other provided papers mention the given paper in their works, the given paper is being \"cited\" by those other papers. To summarize in this specific context, citations are about who is using the given paper.\n3. Given a paper, you need to determine the citation or reference relationship between this paper and the other papers. Do not consider papers that are not provided.\n3. Please present the paper titles in a json format as follows: {{\"Reference\":[\"Reference Title 1\", \"Reference Title 2\", ..., \"Reference Title n\"], \"Citation\":[\"Citation Title 1\", \"Citation Title 2\", ..., \"Citation Title n\"]}}.\n4. If a paper does not have any references or citations, please leave the corresponding list empty, e.g.{{\"Refernce\":[]}}, {{\"Citation\":[]}}.",
        "prompt_template": "#Papers:\n{docs}\n\n{instruction}\n\n#The paper you need to analyze:\n{question}",
        "doc": [
            "2401.06468.md",
            "2404.14122.md",
            "2404.19505.md",
            "2405.11804.md",
            "2405.12819.md",
            "2405.18922.md",
            "2309.08958.md",
            "2309.09400.md",
            "2310.10631.md",
            "2310.13988.md",
            "2401.15360.md"
        ],
        "answer": {
            "Reference": [
                "# Monolingual or Multilingual Instruction Tuning: Which Makes a Better Alpaca ",
                "# CulturaX: A Cleaned, Enormous, and Multilingual Dataset for Large Language Models in 167 Languages ",
                "# LLEMMA: AN OPEN LANGUAGE MODEL FOR MATHEMATICS ",
                "# GEMBA-MQM: Detecting Translation Quality Error Spans with GPT-4 ",
                "# Importance-Aware Data Augmentation for Document-Level Neural Machine Translation "
            ],
            "Citation": [
                "# Fine-Tuning Large Language Models to Translate: Will a Touch of Noisy Data in Misaligned Languages Suffice? ",
                "# Context-Aware Machine Translation with Source Coreference Explanation ",
                "# (PERHAPs) BEYOND Human TranSlation: HarNESSING MULTI-AGENT COLLABORATION FOR TRANSLATING ULTRA-LONG LITERARY TEXTS ",
                "# Large Language Models Meet NLP: A Survey ",
                "# Understanding and Addressing the Under-Translation Problem from the Perspective of Decoding Objective "
            ]
        },
        "shuffle_doc": true,
        "id": "518968ec-aa45-426c-8583-ee0f0e4189ec",
        "prompt": "[Question]\n#Papers:\n{docs}\n\nWe hope you will carefully study the provided papers and determine the citation relationships between them. Please follow the instructions below strictly to complete the task:\n\n#Specific Requirements:\n1. Reference: When a given paper mentions other provided papers, those other papers are considered as \"references\" for the given paper. To summarize in this specific context, references are about what the given paper is using.\n2. Citation: Conversely, when other provided papers mention the given paper in their works, the given paper is being \"cited\" by those other papers. To summarize in this specific context, citations are about who is using the given paper.\n3. Given a paper, you need to determine the citation or reference relationship between this paper and the other papers. Do not consider papers that are not provided.\n3. Please present the paper titles in a json format as follows: {{\"Reference\":[\"Reference Title 1\", \"Reference Title 2\", ..., \"Reference Title n\"], \"Citation\":[\"Citation Title 1\", \"Citation Title 2\", ..., \"Citation Title n\"]}}.\n4. If a paper does not have any references or citations, please leave the corresponding list empty, e.g.{{\"Refernce\":[]}}, {{\"Citation\":[]}}.\n\n#The paper you need to analyze:\nAdapting Large Language Models for Document-Level Machine Translation\n\n[Gold Answer]\n{'Reference': ['# Monolingual or Multilingual Instruction Tuning: Which Makes a Better Alpaca ', '# CulturaX: A Cleaned, Enormous, and Multilingual Dataset for Large Language Models in 167 Languages ', '# LLEMMA: AN OPEN LANGUAGE MODEL FOR MATHEMATICS ', '# GEMBA-MQM: Detecting Translation Quality Error Spans with GPT-4 ', '# Importance-Aware Data Augmentation for Document-Level Neural Machine Translation '], 'Citation': ['# Fine-Tuning Large Language Models to Translate: Will a Touch of Noisy Data in Misaligned Languages Suffice? ', '# Context-Aware Machine Translation with Source Coreference Explanation ', '# (PERHAPs) BEYOND Human TranSlation: HarNESSING MULTI-AGENT COLLABORATION FOR TRANSLATING ULTRA-LONG LITERARY TEXTS ', '# Large Language Models Meet NLP: A Survey ', '# Understanding and Addressing the Under-Translation Problem from the Perspective of Decoding Objective ']}\n\n[The Start of Assistant's Predicted Answer]\n{\"Reference\":[\"Palm 2 technical report\", \"Llemma: An open language model for mathematics\", \"Evaluating discourse phenomena in neural machine translation\", \"Enriching word vectors with subword information\", \"Language models are few-shot learners\", \"Multidimensional quality metrics: a flexible system for assessing translation quality\", \"Overview of the IWSLT 2017 evaluation campaign\", \"Monolingual or multilingual instruction tuning: Which makes a better alpaca\", \"A large-scale test set for the evaluation of context-aware pronoun translation in neural machine translation\", \"Culturax: A Cleaned, Enormous, and Multilingual Dataset for Large Language Models in 167 languages\", \"GPT-4 technical report\", \"Training language models to follow instructions with human feedback\", \"Bleu: a method for automatic evaluation of machine translation\", \"A call for clarity in reporting BLEU scores\", \"COMET: A neural framework for MT evaluation\", \"How good are GPT models at machine translation? A comprehensive evaluation\", \"Lora: Low-rank adaptation of large language models\", \"Diving deep into context-aware neural machine translation\", \"Mistral 7b\", \"Is chatgpt A good translator? A preliminary study\", \"GEMBAMQM: Detecting translation quality error spans with GPT-4\", \"When less is more: Investigating data pruning for pretraining llms at scale\", \"A survey on document-level neural machine translation: Methods and evaluation\", \"BLOOM: A 176b-parameter open-access multilingual language model\", \"Document flattening: Beyond concatenating context for document-level neural machine translation\", \"Importance-aware data augmentation for document-level neural machine translation\", \"Adaptive machine translation with large language models\", \"Chain-of-dictionary prompting elicits translation in large language models\", \"Wizardcoder: Empowering code large language models with evolinstruct\", \"Using whole document context in neural machine translation\", \"Document context neural machine translation with memory networks\", \"Adaptive machine translation with large language models\", \"A large-scale test set for the evaluation of context-aware pronoun translation in neural machine translation\", \"A survey on document-level neural machine translation: Methods and evaluation\", \"Multitask prompted training enables zero-shot task generalization\", \"BLOOM: A 176b-parameter open-access multilingual language model\", \"Rethinking document-level neural machine translation\", \"Neural machine translation with extended context\", \"When a good translation is wrong in context: Context-aware machine translation improves on deixis, ellipsis, and lexical cohesion\", \"Context-aware neural machine translation learns anaphora resolution\", \"A survey on zero pronoun translation\", \"Document-level machine translation with large language models\", \"Exploiting cross-sentence context for neural machine translation\", \"Super-NaturalInstructions: Generalization via declarative instructions on 1600+ NLP tasks\", \"Finetuned language models are zero-shot learners\", \"Chain-of-thought prompting elicits reasoning in large language models\", \"Learning from task descriptions\", \"Contextual neural machine translation improves translation of cataphoric pronouns\", \"Style over substance: Evaluation biases for large language models\", \"A massively multilingual pre-trained text-to-text transformer\", \"Bigtrans: Augmenting large language models with multilingual translation capability over 100 languages\", \"Multilingual document-level translation enables zero-shot transfer from sentences to documents\", \"Improving the transformer translation model with document-level context\", \"Long-short term masking transformer: A simple but effective baseline for document-level neural machine translation\", \"Bayling: Bridging cross-lingual alignment and instruction following through interactive translation for large language models\", \"Multilingual machine translation with large language models: Empirical results and analysis\"], \"Citation\":[\"(PERHAPs) BEYOND Human TranSlation: HarNESSING MULTI-AGENT COLLABORATION FOR TRANSLATING ULTRA-LONG LITERARY TEXTS\", \"Fine-Tuning Large Language Models to Translate: Will a Touch of Noisy Data in Misaligned Languages Suffice?\", \"A large-scale test set for the evaluation of context-aware pronoun translation in neural machine translation\"]}\n[The End of Assistant's Predicted Answer]\n\n[System]\nWe would like to request your feedback on the performance of the AI assistant in response to the user question displayed above according to the gold answer. Please use the following listed aspects and their descriptions as evaluation criteria:\n    - Accuracy and Hallucinations: The assistant's answer is semantically consistent with the gold answer; The numerical value and order need to be accurate, and there should be no hallucinations.\n    - Completeness: Referring to the reference answers, the assistant's answer should contain all the key points needed to answer the user's question; further elaboration on these key points can be omitted.\nPlease rate whether this answer is suitable for the question. Please note that the gold answer can be considered as a correct answer to the question.\n\nThe assistant receives an overall score on a scale of 1 to 100, where a higher score indicates better overall performance.\nPlease note that if the assistant's answer and the gold answer fully meet the above criteria, its overall rating should be the full marks (100).\nPlease first provide a comprehensive explanation of your evaluation, avoiding any potential bias.\nThen, output a line indicating the score of the Assistant.\n\nPLEASE OUTPUT WITH THE FOLLOWING FORMAT, WHERE THE SCORE IS A SCALE OF 1 TO 100 BY STRICTLY FOLLOWING THIS FORMAT: \"[[score]]\", FOR EXAMPLE \"Rating: [[100]]\":\n<start output>\nEvaluation evidence: your evluation explanation here, no more than 100 words\nRating: [[score]]\n<end output>\n\nNow, start your evaluation:",
        "generate_response": "{\"Reference\":[\"Palm 2 technical report\", \"Llemma: An open language model for mathematics\", \"Evaluating discourse phenomena in neural machine translation\", \"Enriching word vectors with subword information\", \"Language models are few-shot learners\", \"Multidimensional quality metrics: a flexible system for assessing translation quality\", \"Overview of the IWSLT 2017 evaluation campaign\", \"Monolingual or multilingual instruction tuning: Which makes a better alpaca\", \"A large-scale test set for the evaluation of context-aware pronoun translation in neural machine translation\", \"Culturax: A Cleaned, Enormous, and Multilingual Dataset for Large Language Models in 167 languages\", \"GPT-4 technical report\", \"Training language models to follow instructions with human feedback\", \"Bleu: a method for automatic evaluation of machine translation\", \"A call for clarity in reporting BLEU scores\", \"COMET: A neural framework for MT evaluation\", \"How good are GPT models at machine translation? A comprehensive evaluation\", \"Lora: Low-rank adaptation of large language models\", \"Diving deep into context-aware neural machine translation\", \"Mistral 7b\", \"Is chatgpt A good translator? A preliminary study\", \"GEMBAMQM: Detecting translation quality error spans with GPT-4\", \"When less is more: Investigating data pruning for pretraining llms at scale\", \"A survey on document-level neural machine translation: Methods and evaluation\", \"BLOOM: A 176b-parameter open-access multilingual language model\", \"Document flattening: Beyond concatenating context for document-level neural machine translation\", \"Importance-aware data augmentation for document-level neural machine translation\", \"Adaptive machine translation with large language models\", \"Chain-of-dictionary prompting elicits translation in large language models\", \"Wizardcoder: Empowering code large language models with evolinstruct\", \"Using whole document context in neural machine translation\", \"Document context neural machine translation with memory networks\", \"Adaptive machine translation with large language models\", \"A large-scale test set for the evaluation of context-aware pronoun translation in neural machine translation\", \"A survey on document-level neural machine translation: Methods and evaluation\", \"Multitask prompted training enables zero-shot task generalization\", \"BLOOM: A 176b-parameter open-access multilingual language model\", \"Rethinking document-level neural machine translation\", \"Neural machine translation with extended context\", \"When a good translation is wrong in context: Context-aware machine translation improves on deixis, ellipsis, and lexical cohesion\", \"Context-aware neural machine translation learns anaphora resolution\", \"A survey on zero pronoun translation\", \"Document-level machine translation with large language models\", \"Exploiting cross-sentence context for neural machine translation\", \"Super-NaturalInstructions: Generalization via declarative instructions on 1600+ NLP tasks\", \"Finetuned language models are zero-shot learners\", \"Chain-of-thought prompting elicits reasoning in large language models\", \"Learning from task descriptions\", \"Contextual neural machine translation improves translation of cataphoric pronouns\", \"Style over substance: Evaluation biases for large language models\", \"A massively multilingual pre-trained text-to-text transformer\", \"Bigtrans: Augmenting large language models with multilingual translation capability over 100 languages\", \"Multilingual document-level translation enables zero-shot transfer from sentences to documents\", \"Improving the transformer translation model with document-level context\", \"Long-short term masking transformer: A simple but effective baseline for document-level neural machine translation\", \"Bayling: Bridging cross-lingual alignment and instruction following through interactive translation for large language models\", \"Multilingual machine translation with large language models: Empirical results and analysis\"], \"Citation\":[\"(PERHAPs) BEYOND Human TranSlation: HarNESSING MULTI-AGENT COLLABORATION FOR TRANSLATING ULTRA-LONG LITERARY TEXTS\", \"Fine-Tuning Large Language Models to Translate: Will a Touch of Noisy Data in Misaligned Languages Suffice?\", \"A large-scale test set for the evaluation of context-aware pronoun translation in neural machine translation\"]}",
        "eval_response": "Evaluation evidence: The assistant's answer does not match the gold answer in terms of accuracy and completeness. The references and citations listed in the assistant's answer are significantly different from those in the gold answer, indicating a high level of hallucination and a lack of accuracy. The assistant also fails to capture all the key points from the gold answer, leading to an incomplete response.\n\nRating: [[20]]",
        "score": 20
    },
    {
        "level": 4,
        "set": 1,
        "length": 33919,
        "type": "paper",
        "language": "en",
        "question": "",
        "instruction": "We kindly ask you to thoroughly review the provided papers and construct a citation chain from them. Please adhere to the following instructions strictly while completing the task:\n\n#Task Instructions:\nGiven several papers, you are required to identify and list the longest citation chain, which demonstrates the citation relationship among the provided papers.\n\n#Specific Requirements:\n1.Please present the titles of the papers in the form of a list, as follows: [\"Title of Paper 1\", \"Title of Paper 2\", ..., \"Title of Paper n\"].\n2.Ensure that the citation chain in the list is linear and continuous, meaning that the first paper title in the list (Paper 1) should not cite any other works. Instead, it should be cited by the next paper in the list (Paper 2); subsequently, each paper should then be cited by the next one in the list, continuing up to the last paper (Paper n).\n3.Consider only the citation relationships within the supplied collection of papers, and ensure that the citation chain accurately reflects the sequential citation order among these documents.\n4.Do not take into account any articles not provided, and disregard other non-linear citation relationships.",
        "prompt_template": "{instruction}\n\n#Papers Provided:\n{docs}",
        "doc": [
            "1604.05377.md",
            "1909.11114.md",
            "2401.11888.md",
            "2405.07435.md"
        ],
        "answer": [
            "# Churn analysis using deep convolutional neural networks and autoencoders ",
            "# Churn Prediction with Sequential Data and Deep Neural Networks A Comparative Analysis* ",
            "# MUlTimodAL DeEP LEARNING OF Word-OF-MOUTH TEXT AND DEMOGRAPHICS TO PREDICT CUSTOMER RATING: HANDLING CONSUMER HETEROGENEITY IN MARKETING ",
            "# An Efficient Multimodal Learning Framework to Comprehend Consumer Preferences Using BERT and Cross-Attention "
        ],
        "shuffle_doc": true,
        "id": "31ee8539-2e9e-40e2-adc3-082c872698f7",
        "prompt": "[Question]\nWe kindly ask you to thoroughly review the provided papers and construct a citation chain from them. Please adhere to the following instructions strictly while completing the task:\n\n#Task Instructions:\nGiven several papers, you are required to identify and list the longest citation chain, which demonstrates the citation relationship among the provided papers.\n\n#Specific Requirements:\n1.Please present the titles of the papers in the form of a list, as follows: [\"Title of Paper 1\", \"Title of Paper 2\", ..., \"Title of Paper n\"].\n2.Ensure that the citation chain in the list is linear and continuous, meaning that the first paper title in the list (Paper 1) should not cite any other works. Instead, it should be cited by the next paper in the list (Paper 2); subsequently, each paper should then be cited by the next one in the list, continuing up to the last paper (Paper n).\n3.Consider only the citation relationships within the supplied collection of papers, and ensure that the citation chain accurately reflects the sequential citation order among these documents.\n4.Do not take into account any articles not provided, and disregard other non-linear citation relationships.\n\n#Papers Provided:\n{docs}\n\n[Gold Answer]\n['# Churn analysis using deep convolutional neural networks and autoencoders ', '# Churn Prediction with Sequential Data and Deep Neural Networks A Comparative Analysis* ', '# MUlTimodAL DeEP LEARNING OF Word-OF-MOUTH TEXT AND DEMOGRAPHICS TO PREDICT CUSTOMER RATING: HANDLING CONSUMER HETEROGENEITY IN MARKETING ', '# An Efficient Multimodal Learning Framework to Comprehend Consumer Preferences Using BERT and Cross-Attention ']\n\n[The Start of Assistant's Predicted Answer]\n[\"An Efficient Multimodal Learning Framework to Comprehend Consumer Preferences Using BERT and Cross-Attention\", \"Deep learning based recommender system: A survey and new perspectives\", \"A survey of recommendation systems: recommendation models, techniques, and application fields\", \"Attention is all you need\"]\n[The End of Assistant's Predicted Answer]\n\n[System]\nWe would like to request your feedback on the performance of the AI assistant in response to the user question displayed above according to the gold answer. Please use the following listed aspects and their descriptions as evaluation criteria:\n    - Accuracy and Hallucinations: The assistant's answer is semantically consistent with the gold answer; The numerical value and order need to be accurate, and there should be no hallucinations.\n    - Completeness: Referring to the reference answers, the assistant's answer should contain all the key points needed to answer the user's question; further elaboration on these key points can be omitted.\nPlease rate whether this answer is suitable for the question. Please note that the gold answer can be considered as a correct answer to the question.\n\nThe assistant receives an overall score on a scale of 1 to 100, where a higher score indicates better overall performance.\nPlease note that if the assistant's answer and the gold answer fully meet the above criteria, its overall rating should be the full marks (100).\nPlease first provide a comprehensive explanation of your evaluation, avoiding any potential bias.\nThen, output a line indicating the score of the Assistant.\n\nPLEASE OUTPUT WITH THE FOLLOWING FORMAT, WHERE THE SCORE IS A SCALE OF 1 TO 100 BY STRICTLY FOLLOWING THIS FORMAT: \"[[score]]\", FOR EXAMPLE \"Rating: [[100]]\":\n<start output>\nEvaluation evidence: your evluation explanation here, no more than 100 words\nRating: [[score]]\n<end output>\n\nNow, start your evaluation:",
        "generate_response": "[\"An Efficient Multimodal Learning Framework to Comprehend Consumer Preferences Using BERT and Cross-Attention\", \"Deep learning based recommender system: A survey and new perspectives\", \"A survey of recommendation systems: recommendation models, techniques, and application fields\", \"Attention is all you need\"]",
        "eval_response": "Evaluation evidence: The assistant's answer does not match the gold answer in terms of the titles of the papers listed. The titles in the assistant's answer do not appear in the gold answer, indicating a complete mismatch and failure to identify the correct citation chain from the provided papers. This results in inaccuracies and hallucinations as the titles and citation chain mentioned by the assistant are incorrect. The answer is also incomplete as it does not reflect the correct citation relationships among the provided papers.\n\nRating: [[1]]",
        "score": 1
    },
    {
        "level": 4,
        "set": 2,
        "length": 93857,
        "type": "paper",
        "language": "en",
        "question": "",
        "instruction": "We kindly ask you to thoroughly review the provided papers and construct a citation chain from them. Please adhere to the following instructions strictly while completing the task:\n\n#Task Instructions:\nGiven several papers, you are required to identify and list the longest citation chain, which demonstrates the citation relationship among the provided papers.\n\n#Specific Requirements:\n1.Please present the titles of the papers in the form of a list, as follows: [\"Title of Paper 1\", \"Title of Paper 2\", ..., \"Title of Paper n\"].\n2.Ensure that the citation chain in the list is linear and continuous, meaning that the first paper title in the list (Paper 1) should not cite any other works. Instead, it should be cited by the next paper in the list (Paper 2); subsequently, each paper should then be cited by the next one in the list, continuing up to the last paper (Paper n).\n3.Consider only the citation relationships within the supplied collection of papers, and ensure that the citation chain accurately reflects the sequential citation order among these documents.\n4.Do not take into account any articles not provided, and disregard other non-linear citation relationships.",
        "prompt_template": "{instruction}\n\n#Papers Provided:\n{docs}",
        "doc": [
            "2312.10997.md",
            "2312.17122.md",
            "2402.12348.md",
            "2404.09699.md",
            "2405.08289.md"
        ],
        "answer": [
            "# Retrieval-Augmented Generation for Large Language Models: A Survey ",
            "# LLM4CAUSAL: LARGE LANGUAGE MODEL FOR CAUSAL DECISION MAKING ",
            "![](https://cdn.mathpix.com/cropped/2024_06_04_791d7ec1c70f2f7bcd44g-01.jpg?height=135&width=174&top_left_y=320&top_left_x=260)",
            "# Generative AI for Game Theory-based Mobile Networking ",
            "# Exploring Equilibrium Strategies in Network Games with Generative AI "
        ],
        "shuffle_doc": true,
        "id": "6a02dee6-fef8-4e3d-92e8-5bd6123eccf6",
        "prompt": "[Question]\nWe kindly ask you to thoroughly review the provided papers and construct a citation chain from them. Please adhere to the following instructions strictly while completing the task:\n\n#Task Instructions:\nGiven several papers, you are required to identify and list the longest citation chain, which demonstrates the citation relationship among the provided papers.\n\n#Specific Requirements:\n1.Please present the titles of the papers in the form of a list, as follows: [\"Title of Paper 1\", \"Title of Paper 2\", ..., \"Title of Paper n\"].\n2.Ensure that the citation chain in the list is linear and continuous, meaning that the first paper title in the list (Paper 1) should not cite any other works. Instead, it should be cited by the next paper in the list (Paper 2); subsequently, each paper should then be cited by the next one in the list, continuing up to the last paper (Paper n).\n3.Consider only the citation relationships within the supplied collection of papers, and ensure that the citation chain accurately reflects the sequential citation order among these documents.\n4.Do not take into account any articles not provided, and disregard other non-linear citation relationships.\n\n#Papers Provided:\n{docs}\n\n[Gold Answer]\n['# Retrieval-Augmented Generation for Large Language Models: A Survey ', '# LLM4CAUSAL: LARGE LANGUAGE MODEL FOR CAUSAL DECISION MAKING ', '![](https://cdn.mathpix.com/cropped/2024_06_04_791d7ec1c70f2f7bcd44g-01.jpg?height=135&width=174&top_left_y=320&top_left_x=260)', '# Generative AI for Game Theory-based Mobile Networking ', '# Exploring Equilibrium Strategies in Network Games with Generative AI ']\n\n[The Start of Assistant's Predicted Answer]\n[\"GTBENCH: Uncovering the Strategic Reasoning Limitations of LLMs via Game-Theoretic Evaluations\"]\n[The End of Assistant's Predicted Answer]\n\n[System]\nWe would like to request your feedback on the performance of the AI assistant in response to the user question displayed above according to the gold answer. Please use the following listed aspects and their descriptions as evaluation criteria:\n    - Accuracy and Hallucinations: The assistant's answer is semantically consistent with the gold answer; The numerical value and order need to be accurate, and there should be no hallucinations.\n    - Completeness: Referring to the reference answers, the assistant's answer should contain all the key points needed to answer the user's question; further elaboration on these key points can be omitted.\nPlease rate whether this answer is suitable for the question. Please note that the gold answer can be considered as a correct answer to the question.\n\nThe assistant receives an overall score on a scale of 1 to 100, where a higher score indicates better overall performance.\nPlease note that if the assistant's answer and the gold answer fully meet the above criteria, its overall rating should be the full marks (100).\nPlease first provide a comprehensive explanation of your evaluation, avoiding any potential bias.\nThen, output a line indicating the score of the Assistant.\n\nPLEASE OUTPUT WITH THE FOLLOWING FORMAT, WHERE THE SCORE IS A SCALE OF 1 TO 100 BY STRICTLY FOLLOWING THIS FORMAT: \"[[score]]\", FOR EXAMPLE \"Rating: [[100]]\":\n<start output>\nEvaluation evidence: your evluation explanation here, no more than 100 words\nRating: [[score]]\n<end output>\n\nNow, start your evaluation:",
        "generate_response": "[\"GTBENCH: Uncovering the Strategic Reasoning Limitations of LLMs via Game-Theoretic Evaluations\"]",
        "eval_response": "Evaluation evidence: The assistant's answer does not match the gold answer and fails to meet the task requirements. The provided answer lists only one paper title, which does not form a citation chain. The gold answer lists a sequence of five paper titles that form a citation chain. The assistant's response lacks accuracy, completeness, and does not adhere to the specific instructions given for constructing a citation chain from the provided papers.\n\nRating: [[10]]",
        "score": 10
    },
    {
        "level": 3,
        "set": 1,
        "length": 48345,
        "type": "paper",
        "language": "en",
        "question": "Evolution of Heuristics: Towards Efficient Automatic Algorithm Design Using Large Language Model",
        "instruction": "We hope you will carefully study the provided papers and determine the citation relationships between them. Please follow the instructions below strictly to complete the task:\n\n#Specific Requirements:\n1. Reference: When a given paper mentions other provided papers, those other papers are considered as \"references\" for the given paper. To summarize in this specific context, references are about what the given paper is using.\n2. Citation: Conversely, when other provided papers mention the given paper in their works, the given paper is being \"cited\" by those other papers. To summarize in this specific context, citations are about who is using the given paper.\n3. Given a paper, you need to determine the citation or reference relationship between this paper and the other papers. Do not consider papers that are not provided.\n3. Please present the paper titles in a json format as follows: {{\"Reference\":[\"Reference Title 1\", \"Reference Title 2\", ..., \"Reference Title n\"], \"Citation\":[\"Citation Title 1\", \"Citation Title 2\", ..., \"Citation Title n\"]}}.\n4. If a paper does not have any references or citations, please leave the corresponding list empty, e.g.{{\"Refernce\":[]}}, {{\"Citation\":[]}}.",
        "prompt_template": "#Papers:\n{docs}\n\n{instruction}\n\n#The paper you need to analyze:\n{question}",
        "doc": [
            "2401.02051.md",
            "2403.07559.md",
            "2312.01797.md"
        ],
        "answer": {
            "Reference": [
                "# LLM A*: Human in the Loop Large Language Models Enabled A* Search for Robotics "
            ],
            "Citation": [
                "# Ensembling Prioritized Hybrid Policies for Multi-agent Pathfinding "
            ]
        },
        "shuffle_doc": true,
        "id": "7b104919-2fba-44d2-91dd-a56bd2f30e91",
        "prompt": "[Question]\n#Papers:\n{docs}\n\nWe hope you will carefully study the provided papers and determine the citation relationships between them. Please follow the instructions below strictly to complete the task:\n\n#Specific Requirements:\n1. Reference: When a given paper mentions other provided papers, those other papers are considered as \"references\" for the given paper. To summarize in this specific context, references are about what the given paper is using.\n2. Citation: Conversely, when other provided papers mention the given paper in their works, the given paper is being \"cited\" by those other papers. To summarize in this specific context, citations are about who is using the given paper.\n3. Given a paper, you need to determine the citation or reference relationship between this paper and the other papers. Do not consider papers that are not provided.\n3. Please present the paper titles in a json format as follows: {{\"Reference\":[\"Reference Title 1\", \"Reference Title 2\", ..., \"Reference Title n\"], \"Citation\":[\"Citation Title 1\", \"Citation Title 2\", ..., \"Citation Title n\"]}}.\n4. If a paper does not have any references or citations, please leave the corresponding list empty, e.g.{{\"Refernce\":[]}}, {{\"Citation\":[]}}.\n\n#The paper you need to analyze:\nEvolution of Heuristics: Towards Efficient Automatic Algorithm Design Using Large Language Model\n\n[Gold Answer]\n{'Reference': ['# LLM A*: Human in the Loop Large Language Models Enabled A* Search for Robotics '], 'Citation': ['# Ensembling Prioritized Hybrid Policies for Multi-agent Pathfinding ']}\n\n[The Start of Assistant's Predicted Answer]\n{\"Reference\":[\"Optuna: A next-generation hyperparameter optimization framework\", \"Guided local search\", \"Online bin packing with predictions\", \"Concorde tsp solver\", \"Knowledge-guided local search for the vehicle routing problem\", \"Program synthesis with large language models\", \"Handbook of evolutionary computation\", \"Neural combinatorial optimization with reinforcement learning\", \"Machine learning for combinatorial optimization: a methodological tour d'horizon\", \"Mo-paramils: A multi-objective automatic algorithm configuration framework\", \"Iterated local search\", \"Neural combinatorial optimization with heavy decoder: Toward large scale generalization\", \"Eureka: Human-level reward design via coding large language models\", \"Mathematical discoveries from program search with large language models\", \"Dimes: A differentiable meta solver for combinatorial optimization problems\", \"An improved guided local search based on the big valley structure\", \"Applying iterated local search to the permutation flow shop problem\", \"Evolutionary computation in the era of large language model: Survey and roadmap\", \"LLM a*: Human in the loop large language models enabled a* search for robotics\", \"Step-wise deep learning models for solving routing problems\", \"Wizardlm: Empowering large language models to follow complex instructions\", \"Reprompting: Automated chain-of-thought prompt inference through gibbs sampling\", \"Large language models as optimizers\", \"Beyond chain-of-thought, effective graph-of-thought reasoning in large language models\", \"Survey on genetic programming and machine learning techniques for heuristic design in job shop scheduling\"], \"Citation\":[]}\n[The End of Assistant's Predicted Answer]\n\n[System]\nWe would like to request your feedback on the performance of the AI assistant in response to the user question displayed above according to the gold answer. Please use the following listed aspects and their descriptions as evaluation criteria:\n    - Accuracy and Hallucinations: The assistant's answer is semantically consistent with the gold answer; The numerical value and order need to be accurate, and there should be no hallucinations.\n    - Completeness: Referring to the reference answers, the assistant's answer should contain all the key points needed to answer the user's question; further elaboration on these key points can be omitted.\nPlease rate whether this answer is suitable for the question. Please note that the gold answer can be considered as a correct answer to the question.\n\nThe assistant receives an overall score on a scale of 1 to 100, where a higher score indicates better overall performance.\nPlease note that if the assistant's answer and the gold answer fully meet the above criteria, its overall rating should be the full marks (100).\nPlease first provide a comprehensive explanation of your evaluation, avoiding any potential bias.\nThen, output a line indicating the score of the Assistant.\n\nPLEASE OUTPUT WITH THE FOLLOWING FORMAT, WHERE THE SCORE IS A SCALE OF 1 TO 100 BY STRICTLY FOLLOWING THIS FORMAT: \"[[score]]\", FOR EXAMPLE \"Rating: [[100]]\":\n<start output>\nEvaluation evidence: your evluation explanation here, no more than 100 words\nRating: [[score]]\n<end output>\n\nNow, start your evaluation:",
        "generate_response": "{\"Reference\":[\"Optuna: A next-generation hyperparameter optimization framework\", \"Guided local search\", \"Online bin packing with predictions\", \"Concorde tsp solver\", \"Knowledge-guided local search for the vehicle routing problem\", \"Program synthesis with large language models\", \"Handbook of evolutionary computation\", \"Neural combinatorial optimization with reinforcement learning\", \"Machine learning for combinatorial optimization: a methodological tour d'horizon\", \"Mo-paramils: A multi-objective automatic algorithm configuration framework\", \"Iterated local search\", \"Neural combinatorial optimization with heavy decoder: Toward large scale generalization\", \"Eureka: Human-level reward design via coding large language models\", \"Mathematical discoveries from program search with large language models\", \"Dimes: A differentiable meta solver for combinatorial optimization problems\", \"An improved guided local search based on the big valley structure\", \"Applying iterated local search to the permutation flow shop problem\", \"Evolutionary computation in the era of large language model: Survey and roadmap\", \"LLM a*: Human in the loop large language models enabled a* search for robotics\", \"Step-wise deep learning models for solving routing problems\", \"Wizardlm: Empowering large language models to follow complex instructions\", \"Reprompting: Automated chain-of-thought prompt inference through gibbs sampling\", \"Large language models as optimizers\", \"Beyond chain-of-thought, effective graph-of-thought reasoning in large language models\", \"Survey on genetic programming and machine learning techniques for heuristic design in job shop scheduling\"], \"Citation\":[]}",
        "eval_response": "Evaluation evidence: The assistant's answer inaccurately lists numerous references that are not mentioned in the gold answer, indicating a significant hallucination issue. The gold answer specifies only one reference, which the assistant correctly identifies, but it erroneously adds many others. Additionally, the assistant fails to identify any citations, which is consistent with the gold answer. However, the excessive incorrect references overshadow the correct elements, leading to a low accuracy score.\n\nRating: [[20]]",
        "score": 20
    },
    {
        "level": 3,
        "set": 2,
        "length": 82469,
        "type": "paper",
        "language": "en",
        "question": "Contrastive Preference Optimization: Pushing the Boundaries of LLM Performance in Machine Translation",
        "instruction": "We hope you will carefully study the provided papers and determine the citation relationships between them. Please follow the instructions below strictly to complete the task:\n\n#Specific Requirements:\n1. Reference: When a given paper mentions other provided papers, those other papers are considered as \"references\" for the given paper. To summarize in this specific context, references are about what the given paper is using.\n2. Citation: Conversely, when other provided papers mention the given paper in their works, the given paper is being \"cited\" by those other papers. To summarize in this specific context, citations are about who is using the given paper.\n3. Given a paper, you need to determine the citation or reference relationship between this paper and the other papers. Do not consider papers that are not provided.\n3. Please present the paper titles in a json format as follows: {{\"Reference\":[\"Reference Title 1\", \"Reference Title 2\", ..., \"Reference Title n\"], \"Citation\":[\"Citation Title 1\", \"Citation Title 2\", ..., \"Citation Title n\"]}}.\n4. If a paper does not have any references or citations, please leave the corresponding list empty, e.g.{{\"Refernce\":[]}}, {{\"Citation\":[]}}.",
        "prompt_template": "#Papers:\n{docs}\n\n{instruction}\n\n#The paper you need to analyze:\n{question}",
        "doc": [
            "2401.08417.md",
            "2405.19534.md",
            "2405.20512.md"
        ],
        "answer": {
            "Reference": [],
            "Citation": [
                "# Preference Learning Algorithms Do Not Learn Preference Rankings ",
                "# How Multilingual Are Large Language Models Fine-Tuned for Translation? "
            ]
        },
        "shuffle_doc": true,
        "id": "83ebaa6f-655b-4d8e-ba8a-8fff6a7417e6",
        "prompt": "[Question]\n#Papers:\n{docs}\n\nWe hope you will carefully study the provided papers and determine the citation relationships between them. Please follow the instructions below strictly to complete the task:\n\n#Specific Requirements:\n1. Reference: When a given paper mentions other provided papers, those other papers are considered as \"references\" for the given paper. To summarize in this specific context, references are about what the given paper is using.\n2. Citation: Conversely, when other provided papers mention the given paper in their works, the given paper is being \"cited\" by those other papers. To summarize in this specific context, citations are about who is using the given paper.\n3. Given a paper, you need to determine the citation or reference relationship between this paper and the other papers. Do not consider papers that are not provided.\n3. Please present the paper titles in a json format as follows: {{\"Reference\":[\"Reference Title 1\", \"Reference Title 2\", ..., \"Reference Title n\"], \"Citation\":[\"Citation Title 1\", \"Citation Title 2\", ..., \"Citation Title n\"]}}.\n4. If a paper does not have any references or citations, please leave the corresponding list empty, e.g.{{\"Refernce\":[]}}, {{\"Citation\":[]}}.\n\n#The paper you need to analyze:\nContrastive Preference Optimization: Pushing the Boundaries of LLM Performance in Machine Translation\n\n[Gold Answer]\n{'Reference': [], 'Citation': ['# Preference Learning Algorithms Do Not Learn Preference Rankings ', '# How Multilingual Are Large Language Models Fine-Tuned for Translation? ']}\n\n[The Start of Assistant's Predicted Answer]\n{\"Reference\":[\"A Survey on Evaluation of Large Language Models\", \"Vicuna: An Open-Source Chatbot Impressing GPT-4 with $90 \\% *$ ChatGPT Quality\", \"ChatLaw: Open-Source Legal Large Language Model with Integrated External Knowledge Bases\", \"Compositional Semantic Parsing with Large Language Models\", \"Open LLM Leaderboard\", \"How Close is ChatGPT to Human Experts? Comparison Corpus, Evaluation, and Detection\", \"Rethinking with Retrieval: Faithful Large Language Model Inference\", \"Measuring Massive Multitask Language Understanding\", \"Atlas: Few-shot Learning with Retrieval Augmented Language Models\", \"Beyond english-centric multilingual machine translation\", \"No language left behind: Scaling human-centered machine translation\", \"Language models are few-shot learners\", \"A simple framework for contrastive learning of visual representations\", \"Momentum contrast for unsupervised visual representation learning\", \"Contrastive prefence learning: Learning from human feedback without rl\", \"How good are gpt models at machine translation? a comprehensive evaluation\", \"LoRA: Low-rank adaptation of large language models\", \"Mistral 7b\", \"Is chatgpt a good translator? a preliminary study\", \"Madlad-400: A multilingual and document-level large audited dataset\", \"Eliciting the translation ability of large language models via multilingual finetuning with translation instructions\", \"Small data, big impact: Leveraging minimal data for effective machine translation\", \"Representation learning with contrastive predictive coding\", \"Gpt-4 technical report\", \"Training language models to follow instructions with human feedback\", \"Deepspeed: System optimizations enable training deep learning models with over 100 billion parameters\", \"T. COMET-22: Unbabel-IST 2022 submission for the metrics shared task\", \"Scaling up cometkiwi: Unbabel-ist 2023 submission for the quality estimation shared task\", \"Contrastive learning with hard negative samples\", \"Proximal policy optimization algorithms\", \"BLEURT: Learning robust metrics for text generation\", \"Multilingual representation distillation with contrastive learning\", \"Llama: Open and efficient foundation language models\", \"Llama 2: Open foundation and finetuned chat models\", \"Attention is all you need\", \"BERT, mBERT, or BiBERT? a study on contextualized embeddings for neural machine translation\", \"A paradigm shift in machine translation: Boosting translation performance of large language models\", \"mT5: A massively multilingual pre-trained text-to-text transformer\", \"Bigtrans: Augmenting large language models with multilingual translation capability over 100 languages\", \"Bayling: Bridging cross-lingual alignment and instruction following through interactive translation for large language models\", \"Multilingual machine translation with large language models: Empirical results and analysis\", \"Extrapolating large language models to non-english by aligning languages\", \"Fine-tuning language models from human preferences\"], \"Citation\":[\"How Multilingual Are Large Language Models Fine-Tuned for Translation?\", \"Tower: An open multilingual large language model for translation-related tasks\", \"How Good Are GPT Models at Machine Translation? A Comprehensive Evaluation\", \"Searching for Needles in a Haystack: On the Role of Incidental Bilingualism in PaLM's Translation Capability\", \"Do All Languages Cost the Same? Tokenization in the Era of Commercial Language Models\", \"Tokenization Impacts Multilingual Language Modeling: Assessing Vocabulary Allocation and Overlap Across Languages\", \"XLM-V: Overcoming the vocabulary bottleneck in multilingual masked language models\", \"A Survey of Multilingual Neural Machine Translation\", \"Learning an artificial language for knowledge-sharing in multilingual translation\", \"Bleu: a method for automatic evaluation of machine translation\", \"Improving Zeroshot Multilingual Neural Machine Translation by Leveraging Cross-lingual Consistency Regularization\", \"Towards Boosting Many-to-Many Multilingual Machine Translation with Large Language Models\", \"Hallucinations in Large Multilingual Translation Models\", \"The Missing Ingredient in Zero-Shot Neural Machine Translation\", \"The State and Fate of Linguistic Diversity and Inclusion in the NLP World\", \"Language models are few-shot learners\", \"A simple framework for contrastive learning of visual representations\", \"Momentum contrast for unsupervised visual representation learning\", \"Contrastive prefence learning: Learning from human feedback without rl\", \"How good are gpt models at machine translation? a comprehensive evaluation\", \"LoRA: Low-rank adaptation of large language models\", \"Mistral 7b\", \"Is chatgpt a good translator? a preliminary study\", \"Madlad-400: A multilingual and document-level large audited dataset\", \"Eliciting the translation ability of large language models via multilingual finetuning with translation instructions\", \"Small data, big impact: Leveraging minimal data for effective machine translation\", \"Representation learning with contrastive predictive coding\", \"Gpt-4 technical report\", \"Training language models to follow instructions with human feedback\", \"Deepspeed: System optimizations enable training deep learning models with over 100 billion parameters\", \"T. COMET-22: Unbabel-IST 2022 submission for the metrics shared task\", \"Scaling up cometkiwi: Unbabel-ist 2023 submission for the quality estimation shared task\", \"Contrastive learning with hard negative samples\", \"Proximal policy optimization algorithms\", \"BLEURT: Learning robust metrics for text generation\", \"Multilingual representation distillation with contrastive learning\", \"Llama: Open and efficient foundation language models\", \"Llama 2: Open foundation and finetuned chat models\", \"Attention is all you need\", \"BERT, mBERT, or BiBERT? a study on contextualized embeddings for neural machine translation\", \"A paradigm shift in machine translation: Boosting translation performance of large language models\", \"mT5: A massively multilingual pre-trained text-to-text transformer\", \"Bigtrans: Augmenting large language models with multilingual translation capability over 100 languages\", \"Bayling: Bridging cross-lingual alignment and instruction following through interactive translation for large language models\", \"Multilingual machine translation with large language models: Empirical results and analysis\", \"Extrapolating large language models to non-english by aligning languages\", \"Fine-tuning language models from human preferences\"]}\n[The End of Assistant's Predicted Answer]\n\n[System]\nWe would like to request your feedback on the performance of the AI assistant in response to the user question displayed above according to the gold answer. Please use the following listed aspects and their descriptions as evaluation criteria:\n    - Accuracy and Hallucinations: The assistant's answer is semantically consistent with the gold answer; The numerical value and order need to be accurate, and there should be no hallucinations.\n    - Completeness: Referring to the reference answers, the assistant's answer should contain all the key points needed to answer the user's question; further elaboration on these key points can be omitted.\nPlease rate whether this answer is suitable for the question. Please note that the gold answer can be considered as a correct answer to the question.\n\nThe assistant receives an overall score on a scale of 1 to 100, where a higher score indicates better overall performance.\nPlease note that if the assistant's answer and the gold answer fully meet the above criteria, its overall rating should be the full marks (100).\nPlease first provide a comprehensive explanation of your evaluation, avoiding any potential bias.\nThen, output a line indicating the score of the Assistant.\n\nPLEASE OUTPUT WITH THE FOLLOWING FORMAT, WHERE THE SCORE IS A SCALE OF 1 TO 100 BY STRICTLY FOLLOWING THIS FORMAT: \"[[score]]\", FOR EXAMPLE \"Rating: [[100]]\":\n<start output>\nEvaluation evidence: your evluation explanation here, no more than 100 words\nRating: [[score]]\n<end output>\n\nNow, start your evaluation:",
        "generate_response": "{\"Reference\":[\"A Survey on Evaluation of Large Language Models\", \"Vicuna: An Open-Source Chatbot Impressing GPT-4 with $90 \\% *$ ChatGPT Quality\", \"ChatLaw: Open-Source Legal Large Language Model with Integrated External Knowledge Bases\", \"Compositional Semantic Parsing with Large Language Models\", \"Open LLM Leaderboard\", \"How Close is ChatGPT to Human Experts? Comparison Corpus, Evaluation, and Detection\", \"Rethinking with Retrieval: Faithful Large Language Model Inference\", \"Measuring Massive Multitask Language Understanding\", \"Atlas: Few-shot Learning with Retrieval Augmented Language Models\", \"Beyond english-centric multilingual machine translation\", \"No language left behind: Scaling human-centered machine translation\", \"Language models are few-shot learners\", \"A simple framework for contrastive learning of visual representations\", \"Momentum contrast for unsupervised visual representation learning\", \"Contrastive prefence learning: Learning from human feedback without rl\", \"How good are gpt models at machine translation? a comprehensive evaluation\", \"LoRA: Low-rank adaptation of large language models\", \"Mistral 7b\", \"Is chatgpt a good translator? a preliminary study\", \"Madlad-400: A multilingual and document-level large audited dataset\", \"Eliciting the translation ability of large language models via multilingual finetuning with translation instructions\", \"Small data, big impact: Leveraging minimal data for effective machine translation\", \"Representation learning with contrastive predictive coding\", \"Gpt-4 technical report\", \"Training language models to follow instructions with human feedback\", \"Deepspeed: System optimizations enable training deep learning models with over 100 billion parameters\", \"T. COMET-22: Unbabel-IST 2022 submission for the metrics shared task\", \"Scaling up cometkiwi: Unbabel-ist 2023 submission for the quality estimation shared task\", \"Contrastive learning with hard negative samples\", \"Proximal policy optimization algorithms\", \"BLEURT: Learning robust metrics for text generation\", \"Multilingual representation distillation with contrastive learning\", \"Llama: Open and efficient foundation language models\", \"Llama 2: Open foundation and finetuned chat models\", \"Attention is all you need\", \"BERT, mBERT, or BiBERT? a study on contextualized embeddings for neural machine translation\", \"A paradigm shift in machine translation: Boosting translation performance of large language models\", \"mT5: A massively multilingual pre-trained text-to-text transformer\", \"Bigtrans: Augmenting large language models with multilingual translation capability over 100 languages\", \"Bayling: Bridging cross-lingual alignment and instruction following through interactive translation for large language models\", \"Multilingual machine translation with large language models: Empirical results and analysis\", \"Extrapolating large language models to non-english by aligning languages\", \"Fine-tuning language models from human preferences\"], \"Citation\":[\"How Multilingual Are Large Language Models Fine-Tuned for Translation?\", \"Tower: An open multilingual large language model for translation-related tasks\", \"How Good Are GPT Models at Machine Translation? A Comprehensive Evaluation\", \"Searching for Needles in a Haystack: On the Role of Incidental Bilingualism in PaLM's Translation Capability\", \"Do All Languages Cost the Same? Tokenization in the Era of Commercial Language Models\", \"Tokenization Impacts Multilingual Language Modeling: Assessing Vocabulary Allocation and Overlap Across Languages\", \"XLM-V: Overcoming the vocabulary bottleneck in multilingual masked language models\", \"A Survey of Multilingual Neural Machine Translation\", \"Learning an artificial language for knowledge-sharing in multilingual translation\", \"Bleu: a method for automatic evaluation of machine translation\", \"Improving Zeroshot Multilingual Neural Machine Translation by Leveraging Cross-lingual Consistency Regularization\", \"Towards Boosting Many-to-Many Multilingual Machine Translation with Large Language Models\", \"Hallucinations in Large Multilingual Translation Models\", \"The Missing Ingredient in Zero-Shot Neural Machine Translation\", \"The State and Fate of Linguistic Diversity and Inclusion in the NLP World\", \"Language models are few-shot learners\", \"A simple framework for contrastive learning of visual representations\", \"Momentum contrast for unsupervised visual representation learning\", \"Contrastive prefence learning: Learning from human feedback without rl\", \"How good are gpt models at machine translation? a comprehensive evaluation\", \"LoRA: Low-rank adaptation of large language models\", \"Mistral 7b\", \"Is chatgpt a good translator? a preliminary study\", \"Madlad-400: A multilingual and document-level large audited dataset\", \"Eliciting the translation ability of large language models via multilingual finetuning with translation instructions\", \"Small data, big impact: Leveraging minimal data for effective machine translation\", \"Representation learning with contrastive predictive coding\", \"Gpt-4 technical report\", \"Training language models to follow instructions with human feedback\", \"Deepspeed: System optimizations enable training deep learning models with over 100 billion parameters\", \"T. COMET-22: Unbabel-IST 2022 submission for the metrics shared task\", \"Scaling up cometkiwi: Unbabel-ist 2023 submission for the quality estimation shared task\", \"Contrastive learning with hard negative samples\", \"Proximal policy optimization algorithms\", \"BLEURT: Learning robust metrics for text generation\", \"Multilingual representation distillation with contrastive learning\", \"Llama: Open and efficient foundation language models\", \"Llama 2: Open foundation and finetuned chat models\", \"Attention is all you need\", \"BERT, mBERT, or BiBERT? a study on contextualized embeddings for neural machine translation\", \"A paradigm shift in machine translation: Boosting translation performance of large language models\", \"mT5: A massively multilingual pre-trained text-to-text transformer\", \"Bigtrans: Augmenting large language models with multilingual translation capability over 100 languages\", \"Bayling: Bridging cross-lingual alignment and instruction following through interactive translation for large language models\", \"Multilingual machine translation with large language models: Empirical results and analysis\", \"Extrapolating large language models to non-english by aligning languages\", \"Fine-tuning language models from human preferences\"]}",
        "eval_response": "Evaluation evidence: The assistant's answer is inaccurate and contains hallucinations. The provided answer lists numerous references and citations that are not mentioned in the gold answer, indicating a significant deviation from the correct information. The gold answer specifies no references and two citations, which the assistant's response fails to accurately reflect. This discrepancy shows a lack of accuracy and introduces irrelevant data, which does not meet the task requirements.\n\nRating: [[10]]",
        "score": 10
    },
    {
        "level": 3,
        "set": 1,
        "length": 43754,
        "type": "paper",
        "language": "en",
        "question": "AUtoAcT: Automatic Agent Learning from Scratch for QA via Self-Planning",
        "instruction": "We hope you will carefully study the provided papers and determine the citation relationships between them. Please follow the instructions below strictly to complete the task:\n\n#Specific Requirements:\n1. Reference: When a given paper mentions other provided papers, those other papers are considered as \"references\" for the given paper. To summarize in this specific context, references are about what the given paper is using.\n2. Citation: Conversely, when other provided papers mention the given paper in their works, the given paper is being \"cited\" by those other papers. To summarize in this specific context, citations are about who is using the given paper.\n3. Given a paper, you need to determine the citation or reference relationship between this paper and the other papers. Do not consider papers that are not provided.\n3. Please present the paper titles in a json format as follows: {{\"Reference\":[\"Reference Title 1\", \"Reference Title 2\", ..., \"Reference Title n\"], \"Citation\":[\"Citation Title 1\", \"Citation Title 2\", ..., \"Citation Title n\"]}}.\n4. If a paper does not have any references or citations, please leave the corresponding list empty, e.g.{{\"Refernce\":[]}}, {{\"Citation\":[]}}.",
        "prompt_template": "#Papers:\n{docs}\n\n{instruction}\n\n#The paper you need to analyze:\n{question}",
        "doc": [
            "2401.05268.md",
            "2403.19962.md",
            "2402.02716.md"
        ],
        "answer": {
            "Reference": [
                "# Understanding the planning of LLM agents: A survey "
            ],
            "Citation": [
                "# Enhancing the General Agent Capabilities of Low-Parameter LLMs through Tuning and Multi-Branch Reasoning "
            ]
        },
        "shuffle_doc": true,
        "id": "42872573-11e0-4406-855c-ad0d83ce0bc4",
        "prompt": "[Question]\n#Papers:\n{docs}\n\nWe hope you will carefully study the provided papers and determine the citation relationships between them. Please follow the instructions below strictly to complete the task:\n\n#Specific Requirements:\n1. Reference: When a given paper mentions other provided papers, those other papers are considered as \"references\" for the given paper. To summarize in this specific context, references are about what the given paper is using.\n2. Citation: Conversely, when other provided papers mention the given paper in their works, the given paper is being \"cited\" by those other papers. To summarize in this specific context, citations are about who is using the given paper.\n3. Given a paper, you need to determine the citation or reference relationship between this paper and the other papers. Do not consider papers that are not provided.\n3. Please present the paper titles in a json format as follows: {{\"Reference\":[\"Reference Title 1\", \"Reference Title 2\", ..., \"Reference Title n\"], \"Citation\":[\"Citation Title 1\", \"Citation Title 2\", ..., \"Citation Title n\"]}}.\n4. If a paper does not have any references or citations, please leave the corresponding list empty, e.g.{{\"Refernce\":[]}}, {{\"Citation\":[]}}.\n\n#The paper you need to analyze:\nAUtoAcT: Automatic Agent Learning from Scratch for QA via Self-Planning\n\n[Gold Answer]\n{'Reference': ['# Understanding the planning of LLM agents: A survey '], 'Citation': ['# Enhancing the General Agent Capabilities of Low-Parameter LLMs through Tuning and Multi-Branch Reasoning ']}\n\n[The Start of Assistant's Predicted Answer]\n{\"Reference\":[\"Fireact: Toward language agent fine-tuning\", \"Making language models better tool learners with execution feedback\", \"Reasoning with language model prompting: A survey\", \"Stanford alpaca: An instruction-following llama model\", \"Chain-of-thought prompting elicits reasoning in large language models\", \"Self-refine: Iterative refinement with self-feedback\", \"Gorilla: Large language model connected with massive apis\", \"Hugginggpt: Solving AI tasks with chatgpt and its friends in huggingface\", \"Medagents: Large language models as collaborators for zero-shot medical reasoning\", \"Llama 2: Open foundation and fine-tuned chat models\", \"A survey on large language model based autonomous agents\", \"The rise and potential of large language model based agents: A survey\", \"Decoupling reasoning from observations for efficient augmented language models\", \"Learning agents with unified data, modular design, and open-source llms\", \"Agenttuning: Enabling generalized agent abilities for llms\", \"Igniting language intelligence: The hitchhiker's guide from chain-of-thought reasoning to language agents\"], \"Citation\":[\"Agentbench: Evaluating llms as agents\"]}}\n[The End of Assistant's Predicted Answer]\n\n[System]\nWe would like to request your feedback on the performance of the AI assistant in response to the user question displayed above according to the gold answer. Please use the following listed aspects and their descriptions as evaluation criteria:\n    - Accuracy and Hallucinations: The assistant's answer is semantically consistent with the gold answer; The numerical value and order need to be accurate, and there should be no hallucinations.\n    - Completeness: Referring to the reference answers, the assistant's answer should contain all the key points needed to answer the user's question; further elaboration on these key points can be omitted.\nPlease rate whether this answer is suitable for the question. Please note that the gold answer can be considered as a correct answer to the question.\n\nThe assistant receives an overall score on a scale of 1 to 100, where a higher score indicates better overall performance.\nPlease note that if the assistant's answer and the gold answer fully meet the above criteria, its overall rating should be the full marks (100).\nPlease first provide a comprehensive explanation of your evaluation, avoiding any potential bias.\nThen, output a line indicating the score of the Assistant.\n\nPLEASE OUTPUT WITH THE FOLLOWING FORMAT, WHERE THE SCORE IS A SCALE OF 1 TO 100 BY STRICTLY FOLLOWING THIS FORMAT: \"[[score]]\", FOR EXAMPLE \"Rating: [[100]]\":\n<start output>\nEvaluation evidence: your evluation explanation here, no more than 100 words\nRating: [[score]]\n<end output>\n\nNow, start your evaluation:",
        "generate_response": "{\"Reference\":[\"Fireact: Toward language agent fine-tuning\", \"Making language models better tool learners with execution feedback\", \"Reasoning with language model prompting: A survey\", \"Stanford alpaca: An instruction-following llama model\", \"Chain-of-thought prompting elicits reasoning in large language models\", \"Self-refine: Iterative refinement with self-feedback\", \"Gorilla: Large language model connected with massive apis\", \"Hugginggpt: Solving AI tasks with chatgpt and its friends in huggingface\", \"Medagents: Large language models as collaborators for zero-shot medical reasoning\", \"Llama 2: Open foundation and fine-tuned chat models\", \"A survey on large language model based autonomous agents\", \"The rise and potential of large language model based agents: A survey\", \"Decoupling reasoning from observations for efficient augmented language models\", \"Learning agents with unified data, modular design, and open-source llms\", \"Agenttuning: Enabling generalized agent abilities for llms\", \"Igniting language intelligence: The hitchhiker's guide from chain-of-thought reasoning to language agents\"], \"Citation\":[\"Agentbench: Evaluating llms as agents\"]}}",
        "eval_response": "Evaluation evidence: The assistant's answer does not match the gold answer in terms of accuracy. The references and citations listed by the assistant are entirely different from those in the gold answer, indicating a significant hallucination issue. The assistant's response includes a long list of unrelated titles, which are not mentioned in the gold answer, showing a lack of accuracy and relevance. This deviation from the gold answer suggests a misunderstanding of the task requirements.\n\nRating: [[10]]",
        "score": 10
    },
    {
        "level": 3,
        "set": 1,
        "length": 38636,
        "type": "paper",
        "language": "en",
        "question": "LLM Maybe LongLM: SelfExtend LLM Context Window Without Tuning",
        "instruction": "We hope you will carefully study the provided papers and determine the citation relationships between them. Please follow the instructions below strictly to complete the task:\n\n#Specific Requirements:\n1. Reference: When a given paper mentions other provided papers, those other papers are considered as \"references\" for the given paper. To summarize in this specific context, references are about what the given paper is using.\n2. Citation: Conversely, when other provided papers mention the given paper in their works, the given paper is being \"cited\" by those other papers. To summarize in this specific context, citations are about who is using the given paper.\n3. Given a paper, you need to determine the citation or reference relationship between this paper and the other papers. Do not consider papers that are not provided.\n3. Please present the paper titles in a json format as follows: {{\"Reference\":[\"Reference Title 1\", \"Reference Title 2\", ..., \"Reference Title n\"], \"Citation\":[\"Citation Title 1\", \"Citation Title 2\", ..., \"Citation Title n\"]}}.\n4. If a paper does not have any references or citations, please leave the corresponding list empty, e.g.{{\"Refernce\":[]}}, {{\"Citation\":[]}}.",
        "prompt_template": "#Papers:\n{docs}\n\n{instruction}\n\n#The paper you need to analyze:\n{question}",
        "doc": [
            "2401.01325.md",
            "2405.04028.md",
            "2310.06825.md"
        ],
        "answer": {
            "Reference": [
                "# Mistral 7B "
            ],
            "Citation": [
                "# Masked Graph Transformer for Large-Scale Recommendation "
            ]
        },
        "shuffle_doc": true,
        "id": "db2e44e2-6407-43c7-aca3-d0ba0d627b11",
        "prompt": "[Question]\n#Papers:\n{docs}\n\nWe hope you will carefully study the provided papers and determine the citation relationships between them. Please follow the instructions below strictly to complete the task:\n\n#Specific Requirements:\n1. Reference: When a given paper mentions other provided papers, those other papers are considered as \"references\" for the given paper. To summarize in this specific context, references are about what the given paper is using.\n2. Citation: Conversely, when other provided papers mention the given paper in their works, the given paper is being \"cited\" by those other papers. To summarize in this specific context, citations are about who is using the given paper.\n3. Given a paper, you need to determine the citation or reference relationship between this paper and the other papers. Do not consider papers that are not provided.\n3. Please present the paper titles in a json format as follows: {{\"Reference\":[\"Reference Title 1\", \"Reference Title 2\", ..., \"Reference Title n\"], \"Citation\":[\"Citation Title 1\", \"Citation Title 2\", ..., \"Citation Title n\"]}}.\n4. If a paper does not have any references or citations, please leave the corresponding list empty, e.g.{{\"Refernce\":[]}}, {{\"Citation\":[]}}.\n\n#The paper you need to analyze:\nLLM Maybe LongLM: SelfExtend LLM Context Window Without Tuning\n\n[Gold Answer]\n{'Reference': ['# Mistral 7B '], 'Citation': ['# Masked Graph Transformer for Large-Scale Recommendation ']}\n\n[The Start of Assistant's Predicted Answer]\n{\"Reference\": [\"A Survey on Evaluation of Large Language Models\", \"Vicuna: An Open-Source Chatbot Impressing GPT-4 with $90 \\% *$ ChatGPT Quality\", \"Measuring Massive Multitask Language Understanding\", \"A Survey of Large Language Models\", \"Compressive Transformers for Long-Range Sequence Modelling\", \"Exploring the Limits of Transfer Learning with a Unified Text-to-Text Transformer\", \"Code Llama: Open Foundation Models for Code\", \"SparseBERT: Rethinking the Importance Analysis in Self-Attention\", \"Rectified Rotary Position Embeddings\", \"RoFormer: Enhanced Transformer with Rotary Position Embedding\", \"A LengthExtrapolatable Transformer\", \"Llama 2: Open Foundation and Fine-tuned Chat Models\", \"Attention is All You Need\", \"Rethinking and Improving Relative Position Encoding for Vision Transformer\", \"Efficient Streaming Language Models with Attention Sinks\", \"Effective Long-Context Scaling of Foundation Models\", \"mt5: A Massively Multilingual Pre-trained Text-to-Text Transformer\", \"Harnessing the Power of LLMs in Practice: A Survey on ChatGPT and Beyond\", \"Mistrallite Model\", \"L-Eval: Instituting Standardized Evaluation for Long Context Language Models\", \"Long Context Prompting for Claude 2.1\", \"Recent Advances in Adversarial Training for Adversarial Robustness\", \"LongBench: A Bilingual, Multitask Benchmark for Long Context Understanding\", \"Language Models are Few-Shot Learners\", \"Continuous Length Extrapolation for Large Language Models\", \"Extending Context Window of Large Language Models via Positional Interpolation\", \"Efficient Fine-tuning of Long-Context Large Language Models\", \"The Surprising Power of Small Language Models\", \"Mistral 7B\", \"Landmark Attention: Random-Access Infinite Context Length for Transformers\", \"Giraffe: Adventures in Expanding Context Lengths in LLMs\", \"Yarn: Efficient Context Window Extension of Large Language Models\", \"Train Short, Test Long: Attention with Linear Biases Enables Input Length Extrapolation\", \"Compressive Transformers for Long-Range Sequence Modelling\", \"Exploring the Limits of Transfer Learning with a Unified Text-to-Text Transformer\", \"Code Llama: Open Foundation Models for Code\", \"Towards Out-of-Distribution Generalization: A Survey\", \"SparseBERT: Rethinking the Importance Analysis in Self-Attention\", \"Rectified Rotary Position Embeddings\", \"RoFormer: Enhanced Transformer with Rotary Position Embedding\", \"A LengthExtrapolatable Transformer\", \"Llama 2: Open Foundation and Fine-tuned Chat Models\", \"Attention is All You Need\", \"Rethinking and Improving Relative Position Encoding for Vision Transformer\", \"Efficient Streaming Language Models with Attention Sinks\", \"Effective Long-Context Scaling of Foundation Models\", \"mt5: A Massively Multilingual Pre-trained Text-to-Text Transformer\", \"When Neural Networks Fail to Generalize? A Model Sensitivity Perspective\", \"OPT: Open Pre-trained Transformer Language Models\"], \"Citation\": []}\n[The End of Assistant's Predicted Answer]\n\n[System]\nWe would like to request your feedback on the performance of the AI assistant in response to the user question displayed above according to the gold answer. Please use the following listed aspects and their descriptions as evaluation criteria:\n    - Accuracy and Hallucinations: The assistant's answer is semantically consistent with the gold answer; The numerical value and order need to be accurate, and there should be no hallucinations.\n    - Completeness: Referring to the reference answers, the assistant's answer should contain all the key points needed to answer the user's question; further elaboration on these key points can be omitted.\nPlease rate whether this answer is suitable for the question. Please note that the gold answer can be considered as a correct answer to the question.\n\nThe assistant receives an overall score on a scale of 1 to 100, where a higher score indicates better overall performance.\nPlease note that if the assistant's answer and the gold answer fully meet the above criteria, its overall rating should be the full marks (100).\nPlease first provide a comprehensive explanation of your evaluation, avoiding any potential bias.\nThen, output a line indicating the score of the Assistant.\n\nPLEASE OUTPUT WITH THE FOLLOWING FORMAT, WHERE THE SCORE IS A SCALE OF 1 TO 100 BY STRICTLY FOLLOWING THIS FORMAT: \"[[score]]\", FOR EXAMPLE \"Rating: [[100]]\":\n<start output>\nEvaluation evidence: your evluation explanation here, no more than 100 words\nRating: [[score]]\n<end output>\n\nNow, start your evaluation:",
        "generate_response": "{\"Reference\": [\"A Survey on Evaluation of Large Language Models\", \"Vicuna: An Open-Source Chatbot Impressing GPT-4 with $90 \\% *$ ChatGPT Quality\", \"Measuring Massive Multitask Language Understanding\", \"A Survey of Large Language Models\", \"Compressive Transformers for Long-Range Sequence Modelling\", \"Exploring the Limits of Transfer Learning with a Unified Text-to-Text Transformer\", \"Code Llama: Open Foundation Models for Code\", \"SparseBERT: Rethinking the Importance Analysis in Self-Attention\", \"Rectified Rotary Position Embeddings\", \"RoFormer: Enhanced Transformer with Rotary Position Embedding\", \"A LengthExtrapolatable Transformer\", \"Llama 2: Open Foundation and Fine-tuned Chat Models\", \"Attention is All You Need\", \"Rethinking and Improving Relative Position Encoding for Vision Transformer\", \"Efficient Streaming Language Models with Attention Sinks\", \"Effective Long-Context Scaling of Foundation Models\", \"mt5: A Massively Multilingual Pre-trained Text-to-Text Transformer\", \"Harnessing the Power of LLMs in Practice: A Survey on ChatGPT and Beyond\", \"Mistrallite Model\", \"L-Eval: Instituting Standardized Evaluation for Long Context Language Models\", \"Long Context Prompting for Claude 2.1\", \"Recent Advances in Adversarial Training for Adversarial Robustness\", \"LongBench: A Bilingual, Multitask Benchmark for Long Context Understanding\", \"Language Models are Few-Shot Learners\", \"Continuous Length Extrapolation for Large Language Models\", \"Extending Context Window of Large Language Models via Positional Interpolation\", \"Efficient Fine-tuning of Long-Context Large Language Models\", \"The Surprising Power of Small Language Models\", \"Mistral 7B\", \"Landmark Attention: Random-Access Infinite Context Length for Transformers\", \"Giraffe: Adventures in Expanding Context Lengths in LLMs\", \"Yarn: Efficient Context Window Extension of Large Language Models\", \"Train Short, Test Long: Attention with Linear Biases Enables Input Length Extrapolation\", \"Compressive Transformers for Long-Range Sequence Modelling\", \"Exploring the Limits of Transfer Learning with a Unified Text-to-Text Transformer\", \"Code Llama: Open Foundation Models for Code\", \"Towards Out-of-Distribution Generalization: A Survey\", \"SparseBERT: Rethinking the Importance Analysis in Self-Attention\", \"Rectified Rotary Position Embeddings\", \"RoFormer: Enhanced Transformer with Rotary Position Embedding\", \"A LengthExtrapolatable Transformer\", \"Llama 2: Open Foundation and Fine-tuned Chat Models\", \"Attention is All You Need\", \"Rethinking and Improving Relative Position Encoding for Vision Transformer\", \"Efficient Streaming Language Models with Attention Sinks\", \"Effective Long-Context Scaling of Foundation Models\", \"mt5: A Massively Multilingual Pre-trained Text-to-Text Transformer\", \"When Neural Networks Fail to Generalize? A Model Sensitivity Perspective\", \"OPT: Open Pre-trained Transformer Language Models\"], \"Citation\": []}",
        "eval_response": "Evaluation evidence: The assistant's answer inaccurately lists numerous references that are not mentioned in the gold answer, indicating a significant hallucination issue. The gold answer specifies only one reference, \"Mistral 7B,\" which the assistant fails to include correctly. Additionally, the assistant's answer does not list any citations, which aligns with the gold answer. However, the extensive hallucination in the reference section severely impacts the accuracy of the response.\n\nRating: [[20]]",
        "score": 20
    },
    {
        "level": 3,
        "set": 2,
        "length": 69000,
        "type": "paper",
        "language": "en",
        "question": "OpenMoE: An Early Effort on Open Mixture-of-Experts Language Models",
        "instruction": "We hope you will carefully study the provided papers and determine the citation relationships between them. Please follow the instructions below strictly to complete the task:\n\n#Specific Requirements:\n1. Reference: When a given paper mentions other provided papers, those other papers are considered as \"references\" for the given paper. To summarize in this specific context, references are about what the given paper is using.\n2. Citation: Conversely, when other provided papers mention the given paper in their works, the given paper is being \"cited\" by those other papers. To summarize in this specific context, citations are about who is using the given paper.\n3. Given a paper, you need to determine the citation or reference relationship between this paper and the other papers. Do not consider papers that are not provided.\n3. Please present the paper titles in a json format as follows: {{\"Reference\":[\"Reference Title 1\", \"Reference Title 2\", ..., \"Reference Title n\"], \"Citation\":[\"Citation Title 1\", \"Citation Title 2\", ..., \"Citation Title n\"]}}.\n4. If a paper does not have any references or citations, please leave the corresponding list empty, e.g.{{\"Refernce\":[]}}, {{\"Citation\":[]}}.",
        "prompt_template": "#Papers:\n{docs}\n\n{instruction}\n\n#The paper you need to analyze:\n{question}",
        "doc": [
            "2402.01739.md",
            "2309.03450.md",
            "2401.06066.md"
        ],
        "answer": {
            "Reference": [
                "# XGen-7B Technical Report ",
                "# DeepSeekMoE: Towards Ultimate Expert Specialization in Mixture-of-Experts Language Models "
            ],
            "Citation": []
        },
        "shuffle_doc": true,
        "id": "75bf7c5f-fc49-44b3-93df-6c4b15fb42c1",
        "prompt": "[Question]\n#Papers:\n{docs}\n\nWe hope you will carefully study the provided papers and determine the citation relationships between them. Please follow the instructions below strictly to complete the task:\n\n#Specific Requirements:\n1. Reference: When a given paper mentions other provided papers, those other papers are considered as \"references\" for the given paper. To summarize in this specific context, references are about what the given paper is using.\n2. Citation: Conversely, when other provided papers mention the given paper in their works, the given paper is being \"cited\" by those other papers. To summarize in this specific context, citations are about who is using the given paper.\n3. Given a paper, you need to determine the citation or reference relationship between this paper and the other papers. Do not consider papers that are not provided.\n3. Please present the paper titles in a json format as follows: {{\"Reference\":[\"Reference Title 1\", \"Reference Title 2\", ..., \"Reference Title n\"], \"Citation\":[\"Citation Title 1\", \"Citation Title 2\", ..., \"Citation Title n\"]}}.\n4. If a paper does not have any references or citations, please leave the corresponding list empty, e.g.{{\"Refernce\":[]}}, {{\"Citation\":[]}}.\n\n#The paper you need to analyze:\nOpenMoE: An Early Effort on Open Mixture-of-Experts Language Models\n\n[Gold Answer]\n{'Reference': ['# XGen-7B Technical Report ', '# DeepSeekMoE: Towards Ultimate Expert Specialization in Mixture-of-Experts Language Models '], 'Citation': []}\n\n[The Start of Assistant's Predicted Answer]\n{\"Reference\":[\"Palm 2 technical report\", \"inthe)wildchat: 570k chatGPT interaction logs in the wild\", \"Efficient large scale language modeling with mixtures of experts\", \"Beyond the imitation game: Quantifying and extrapolating the capabilities of language models\", \"Efficient training of language models to fill in the middle\", \"Findings of the 2016 conference on machine translation\", \"Language models are few-shot learners\", \"Evaluating large language models trained on code\", \"Palm: Scaling language modeling with pathways\", \"Megablocks: Efficient sparse training with mixture-of-experts\", \"A framework for few-shot language model evaluation\", \"Openllama: An open reproduction of llama\", \"Measuring massive multitask language understanding\", \"Training compute-optimal large language models\", \"Mixtral of experts\", \"Sparse upcycling: Training mixture-of-experts from dense checkpoints\", \"Gshard: Scaling giant models with conditional computation and automatic sharding\", \"Base layers: Simplifying training of large, sparse models\", \"Self-prompting large language models for open-domain qa\", \"Starcoder: May the source be with you!\", \"Roberta: A robustly optimized bert pretraining approach\", \"Cross-token modeling with conditional computation\", \"Multimodal contrastive learning with limoe: The language-image mixture of experts\", \"Xgen-7b technical report\", \"From sparse to soft mixtures of experts\", \"Scaling language models: Methods, analysis \\u0026 insights from training gopher\", \"Exploring the limits of transfer learning with a unified text-to-text transformer\", \"Scaling vision with sparse mixture of experts\", \"Hash layers for large sparse models\", \"Code llama: Open foundation models for code\", \"Glu variants improve transformer\", \"Outrageously large neural networks: The sparsely-gated mixture-of-experts layer\", \"Megatron-lm: Training multi-billion parameter language models using model parallelism\", \"Dolma: An Open Corpus of Three Trillion Tokens for Language Model Pretraining Research\", \"Roformer: Enhanced transformer with rotary position embedding\", \"U12: Unifying language learning paradigms\", \"Unifying language learning paradigms\", \"Llama: Open and efficient foundation language models\", \"GPT-J-6B: A 6 Billion Parameter Autoregressive Language Model\", \"CCNet: Extracting high quality monolingual datasets from web crawl data\", \"Gspmd: General and scalable parallelization for ml computation graphs\", \"One student knows all experts know: From sparse to dense\", \"Go wider instead of deeper\", \"Efficient language modeling with sparse all-mlp\", \"Tinyllama: An open-source small language model\", \"Deep long-tailed learning: A survey\", \"Judging llm-as-a-judge with mt-bench and chatbot arena\", \"Brainformers: Trading simplicity for efficiency\", \"Mixture-of-experts with expert choice routing\", \"St-moe: Designing stable and transferable sparse expert models\"], \"Citation\":[]}\n[The End of Assistant's Predicted Answer]\n\n[System]\nWe would like to request your feedback on the performance of the AI assistant in response to the user question displayed above according to the gold answer. Please use the following listed aspects and their descriptions as evaluation criteria:\n    - Accuracy and Hallucinations: The assistant's answer is semantically consistent with the gold answer; The numerical value and order need to be accurate, and there should be no hallucinations.\n    - Completeness: Referring to the reference answers, the assistant's answer should contain all the key points needed to answer the user's question; further elaboration on these key points can be omitted.\nPlease rate whether this answer is suitable for the question. Please note that the gold answer can be considered as a correct answer to the question.\n\nThe assistant receives an overall score on a scale of 1 to 100, where a higher score indicates better overall performance.\nPlease note that if the assistant's answer and the gold answer fully meet the above criteria, its overall rating should be the full marks (100).\nPlease first provide a comprehensive explanation of your evaluation, avoiding any potential bias.\nThen, output a line indicating the score of the Assistant.\n\nPLEASE OUTPUT WITH THE FOLLOWING FORMAT, WHERE THE SCORE IS A SCALE OF 1 TO 100 BY STRICTLY FOLLOWING THIS FORMAT: \"[[score]]\", FOR EXAMPLE \"Rating: [[100]]\":\n<start output>\nEvaluation evidence: your evluation explanation here, no more than 100 words\nRating: [[score]]\n<end output>\n\nNow, start your evaluation:",
        "generate_response": "{\"Reference\":[\"Palm 2 technical report\", \"inthe)wildchat: 570k chatGPT interaction logs in the wild\", \"Efficient large scale language modeling with mixtures of experts\", \"Beyond the imitation game: Quantifying and extrapolating the capabilities of language models\", \"Efficient training of language models to fill in the middle\", \"Findings of the 2016 conference on machine translation\", \"Language models are few-shot learners\", \"Evaluating large language models trained on code\", \"Palm: Scaling language modeling with pathways\", \"Megablocks: Efficient sparse training with mixture-of-experts\", \"A framework for few-shot language model evaluation\", \"Openllama: An open reproduction of llama\", \"Measuring massive multitask language understanding\", \"Training compute-optimal large language models\", \"Mixtral of experts\", \"Sparse upcycling: Training mixture-of-experts from dense checkpoints\", \"Gshard: Scaling giant models with conditional computation and automatic sharding\", \"Base layers: Simplifying training of large, sparse models\", \"Self-prompting large language models for open-domain qa\", \"Starcoder: May the source be with you!\", \"Roberta: A robustly optimized bert pretraining approach\", \"Cross-token modeling with conditional computation\", \"Multimodal contrastive learning with limoe: The language-image mixture of experts\", \"Xgen-7b technical report\", \"From sparse to soft mixtures of experts\", \"Scaling language models: Methods, analysis \\u0026 insights from training gopher\", \"Exploring the limits of transfer learning with a unified text-to-text transformer\", \"Scaling vision with sparse mixture of experts\", \"Hash layers for large sparse models\", \"Code llama: Open foundation models for code\", \"Glu variants improve transformer\", \"Outrageously large neural networks: The sparsely-gated mixture-of-experts layer\", \"Megatron-lm: Training multi-billion parameter language models using model parallelism\", \"Dolma: An Open Corpus of Three Trillion Tokens for Language Model Pretraining Research\", \"Roformer: Enhanced transformer with rotary position embedding\", \"U12: Unifying language learning paradigms\", \"Unifying language learning paradigms\", \"Llama: Open and efficient foundation language models\", \"GPT-J-6B: A 6 Billion Parameter Autoregressive Language Model\", \"CCNet: Extracting high quality monolingual datasets from web crawl data\", \"Gspmd: General and scalable parallelization for ml computation graphs\", \"One student knows all experts know: From sparse to dense\", \"Go wider instead of deeper\", \"Efficient language modeling with sparse all-mlp\", \"Tinyllama: An open-source small language model\", \"Deep long-tailed learning: A survey\", \"Judging llm-as-a-judge with mt-bench and chatbot arena\", \"Brainformers: Trading simplicity for efficiency\", \"Mixture-of-experts with expert choice routing\", \"St-moe: Designing stable and transferable sparse expert models\"], \"Citation\":[]}",
        "eval_response": "Evaluation evidence: The assistant's answer includes a long list of references that are not mentioned in the gold answer, indicating a significant hallucination issue. The gold answer specifies only two references, which are not included in the assistant's response. This discrepancy shows a lack of accuracy and introduces irrelevant information, failing to meet the completeness criterion as well. The assistant's answer does not align with the gold answer's requirements, leading to a low score.\n\nRating: [[10]]",
        "score": 10
    },
    {
        "level": 3,
        "set": 1,
        "length": 37040,
        "type": "paper",
        "language": "en",
        "question": "Break the Sequential Dependency of LLM Inference Using LOOKAHEAD DECODING",
        "instruction": "We hope you will carefully study the provided papers and determine the citation relationships between them. Please follow the instructions below strictly to complete the task:\n\n#Specific Requirements:\n1. Reference: When a given paper mentions other provided papers, those other papers are considered as \"references\" for the given paper. To summarize in this specific context, references are about what the given paper is using.\n2. Citation: Conversely, when other provided papers mention the given paper in their works, the given paper is being \"cited\" by those other papers. To summarize in this specific context, citations are about who is using the given paper.\n3. Given a paper, you need to determine the citation or reference relationship between this paper and the other papers. Do not consider papers that are not provided.\n3. Please present the paper titles in a json format as follows: {{\"Reference\":[\"Reference Title 1\", \"Reference Title 2\", ..., \"Reference Title n\"], \"Citation\":[\"Citation Title 1\", \"Citation Title 2\", ..., \"Citation Title n\"]}}.\n4. If a paper does not have any references or citations, please leave the corresponding list empty, e.g.{{\"Refernce\":[]}}, {{\"Citation\":[]}}.",
        "prompt_template": "#Papers:\n{docs}\n\n{instruction}\n\n#The paper you need to analyze:\n{question}",
        "doc": [
            "2402.02057.md",
            "2404.18911.md",
            "2310.06825.md"
        ],
        "answer": {
            "Reference": [
                "# Mistral 7B "
            ],
            "Citation": [
                "# Kangaroo: Lossless Self-Speculative Decoding via Double Early Exiting "
            ]
        },
        "shuffle_doc": true,
        "id": "1df3da1b-87c8-4692-9ec7-83d1b0e9caa3",
        "prompt": "[Question]\n#Papers:\n{docs}\n\nWe hope you will carefully study the provided papers and determine the citation relationships between them. Please follow the instructions below strictly to complete the task:\n\n#Specific Requirements:\n1. Reference: When a given paper mentions other provided papers, those other papers are considered as \"references\" for the given paper. To summarize in this specific context, references are about what the given paper is using.\n2. Citation: Conversely, when other provided papers mention the given paper in their works, the given paper is being \"cited\" by those other papers. To summarize in this specific context, citations are about who is using the given paper.\n3. Given a paper, you need to determine the citation or reference relationship between this paper and the other papers. Do not consider papers that are not provided.\n3. Please present the paper titles in a json format as follows: {{\"Reference\":[\"Reference Title 1\", \"Reference Title 2\", ..., \"Reference Title n\"], \"Citation\":[\"Citation Title 1\", \"Citation Title 2\", ..., \"Citation Title n\"]}}.\n4. If a paper does not have any references or citations, please leave the corresponding list empty, e.g.{{\"Refernce\":[]}}, {{\"Citation\":[]}}.\n\n#The paper you need to analyze:\nBreak the Sequential Dependency of LLM Inference Using LOOKAHEAD DECODING\n\n[Gold Answer]\n{'Reference': ['# Mistral 7B '], 'Citation': ['# Kangaroo: Lossless Self-Speculative Decoding via Double Early Exiting ']}\n\n[The Start of Assistant's Predicted Answer]\n{\"Reference\": [\"A Survey on Evaluation of Large Language Models\", \"Vicuna: An Open-Source Chatbot Impressing GPT-4 with $90 \\% *$ ChatGPT Quality\", \"ChatLaw: Open-Source Legal Large Language Model with Integrated External Knowledge Bases\", \"Compositional Semantic Parsing with Large Language Models\", \"Open LLM Leaderboard\", \"How Close is ChatGPT to Human Experts? Comparison Corpus, Evaluation, and Detection\", \"Rethinking with Retrieval: Faithful Large Language Model Inference\", \"Measuring Massive Multitask Language Understanding\", \"Atlas: Few-shot Learning with Retrieval Augmented Language Models\", \"Accelerating large language model decoding with speculative sampling\", \"Program synthesis with large language models\", \"Layer normalization\", \"A framework for the evaluation of code generation models\", \"Medusa: Simple llm inference acceleration framework with multiple decoding heads\", \"Evaluating large language models trained on code\", \"Training verifiers to solve math word problems\", \"FlashAttention-2: Faster attention with better parallelism and work partitioning\", \"FlashAttention: Fast and memory-efficient exact attention with IO-awareness\", \"Classeval: A manuallycrafted benchmark for evaluating llms on class-level code generation\", \"Accelerate: Training and inference at scale made simple, efficient and adaptable\", \"Rest: Retrieval-based speculative decoding\", \"The curious case of neural text degeneration\", \"Mistral 7b\", \"Ancestral gumbeltop-k sampling for sampling without replacement\", \"Fast inference from transformers via speculative decoding\", \"Eagle: Lossless acceleration of llm decoding by feature extrapolation\", \"ROUGE: A package for automatic evaluation of summaries\", \"Online speculative decoding\", \"Specinfer: Accelerating generative large language model serving with speculative inference and token tree verification\", \"Don't give me the details, just the summary! Topic-aware convolutional neural networks for extreme summarization\", \"Efficient large-scale language model training on gpu clusters using megatron-lm\", \"Gpt-4 technical report\", \"Training language models to follow instructions with human feedback\", \"Code llama: Open foundation models for code\", \"Best prompting practices for using the llama 2 chat llm through amazon sagemaker jumpstart\", \"Accelerating transformer inference for translation via parallel decoding\", \"Prompt lookup decoding\", \"Get to the point: Summarization with pointer-generator networks\", \"Megatron-lm: Training multibillion parameter language models using model parallelism\", \"Accelerating feedforward computation via parallel nonlinear equation solving\", \"Blockwise parallel decoding for deep autoregressive models\", \"Gemini: a family of highly capable multimodal models\", \"Attention is all you need\", \"Transformers: State-ofthe-art natural language processing\", \"Inference with reference: Lossless acceleration of large language models\", \"Root mean square layer normalization\", \"Judging llm-as-a-judge with mt-bench and chatbot arena\"], \"Citation\": []}\n[The End of Assistant's Predicted Answer]\n\n[System]\nWe would like to request your feedback on the performance of the AI assistant in response to the user question displayed above according to the gold answer. Please use the following listed aspects and their descriptions as evaluation criteria:\n    - Accuracy and Hallucinations: The assistant's answer is semantically consistent with the gold answer; The numerical value and order need to be accurate, and there should be no hallucinations.\n    - Completeness: Referring to the reference answers, the assistant's answer should contain all the key points needed to answer the user's question; further elaboration on these key points can be omitted.\nPlease rate whether this answer is suitable for the question. Please note that the gold answer can be considered as a correct answer to the question.\n\nThe assistant receives an overall score on a scale of 1 to 100, where a higher score indicates better overall performance.\nPlease note that if the assistant's answer and the gold answer fully meet the above criteria, its overall rating should be the full marks (100).\nPlease first provide a comprehensive explanation of your evaluation, avoiding any potential bias.\nThen, output a line indicating the score of the Assistant.\n\nPLEASE OUTPUT WITH THE FOLLOWING FORMAT, WHERE THE SCORE IS A SCALE OF 1 TO 100 BY STRICTLY FOLLOWING THIS FORMAT: \"[[score]]\", FOR EXAMPLE \"Rating: [[100]]\":\n<start output>\nEvaluation evidence: your evluation explanation here, no more than 100 words\nRating: [[score]]\n<end output>\n\nNow, start your evaluation:",
        "generate_response": "{\"Reference\": [\"A Survey on Evaluation of Large Language Models\", \"Vicuna: An Open-Source Chatbot Impressing GPT-4 with $90 \\% *$ ChatGPT Quality\", \"ChatLaw: Open-Source Legal Large Language Model with Integrated External Knowledge Bases\", \"Compositional Semantic Parsing with Large Language Models\", \"Open LLM Leaderboard\", \"How Close is ChatGPT to Human Experts? Comparison Corpus, Evaluation, and Detection\", \"Rethinking with Retrieval: Faithful Large Language Model Inference\", \"Measuring Massive Multitask Language Understanding\", \"Atlas: Few-shot Learning with Retrieval Augmented Language Models\", \"Accelerating large language model decoding with speculative sampling\", \"Program synthesis with large language models\", \"Layer normalization\", \"A framework for the evaluation of code generation models\", \"Medusa: Simple llm inference acceleration framework with multiple decoding heads\", \"Evaluating large language models trained on code\", \"Training verifiers to solve math word problems\", \"FlashAttention-2: Faster attention with better parallelism and work partitioning\", \"FlashAttention: Fast and memory-efficient exact attention with IO-awareness\", \"Classeval: A manuallycrafted benchmark for evaluating llms on class-level code generation\", \"Accelerate: Training and inference at scale made simple, efficient and adaptable\", \"Rest: Retrieval-based speculative decoding\", \"The curious case of neural text degeneration\", \"Mistral 7b\", \"Ancestral gumbeltop-k sampling for sampling without replacement\", \"Fast inference from transformers via speculative decoding\", \"Eagle: Lossless acceleration of llm decoding by feature extrapolation\", \"ROUGE: A package for automatic evaluation of summaries\", \"Online speculative decoding\", \"Specinfer: Accelerating generative large language model serving with speculative inference and token tree verification\", \"Don't give me the details, just the summary! Topic-aware convolutional neural networks for extreme summarization\", \"Efficient large-scale language model training on gpu clusters using megatron-lm\", \"Gpt-4 technical report\", \"Training language models to follow instructions with human feedback\", \"Code llama: Open foundation models for code\", \"Best prompting practices for using the llama 2 chat llm through amazon sagemaker jumpstart\", \"Accelerating transformer inference for translation via parallel decoding\", \"Prompt lookup decoding\", \"Get to the point: Summarization with pointer-generator networks\", \"Megatron-lm: Training multibillion parameter language models using model parallelism\", \"Accelerating feedforward computation via parallel nonlinear equation solving\", \"Blockwise parallel decoding for deep autoregressive models\", \"Gemini: a family of highly capable multimodal models\", \"Attention is all you need\", \"Transformers: State-ofthe-art natural language processing\", \"Inference with reference: Lossless acceleration of large language models\", \"Root mean square layer normalization\", \"Judging llm-as-a-judge with mt-bench and chatbot arena\"], \"Citation\": []}",
        "eval_response": "Evaluation evidence: The assistant's answer does not match the gold answer in terms of accuracy. The reference paper listed in the gold answer is not included in the assistant's response, and the citation paper mentioned in the gold answer is also missing from the assistant's response. Instead, the assistant provided a long list of unrelated papers, indicating a significant hallucination issue. This shows a lack of accuracy and introduces irrelevant information, which affects the completeness of the answer.\n\nRating: [[10]]",
        "score": 10
    },
    {
        "level": 3,
        "set": 3,
        "length": 112540,
        "type": "paper",
        "language": "en",
        "question": "VisualWebArena: EvalUATING MULTIMODAL AGENTS ON REALISTIC VISUAL WEB TASKS",
        "instruction": "We hope you will carefully study the provided papers and determine the citation relationships between them. Please follow the instructions below strictly to complete the task:\n\n#Specific Requirements:\n1. Reference: When a given paper mentions other provided papers, those other papers are considered as \"references\" for the given paper. To summarize in this specific context, references are about what the given paper is using.\n2. Citation: Conversely, when other provided papers mention the given paper in their works, the given paper is being \"cited\" by those other papers. To summarize in this specific context, citations are about who is using the given paper.\n3. Given a paper, you need to determine the citation or reference relationship between this paper and the other papers. Do not consider papers that are not provided.\n3. Please present the paper titles in a json format as follows: {{\"Reference\":[\"Reference Title 1\", \"Reference Title 2\", ..., \"Reference Title n\"], \"Citation\":[\"Citation Title 1\", \"Citation Title 2\", ..., \"Citation Title n\"]}}.\n4. If a paper does not have any references or citations, please leave the corresponding list empty, e.g.{{\"Refernce\":[]}}, {{\"Citation\":[]}}.",
        "prompt_template": "#Papers:\n{docs}\n\n{instruction}\n\n#The paper you need to analyze:\n{question}",
        "doc": [
            "2401.13649.md",
            "2405.15341.md",
            "2405.15638.md",
            "2312.08914.md",
            "2401.06209.md"
        ],
        "answer": {
            "Reference": [
                "# CogAgent: A Visual Language Model for GUI Agents ",
                "# Eyes Wide Shut? Exploring the Visual Shortcomings of Multimodal LLMs "
            ],
            "Citation": [
                "# V-Zen: Efficient GUI Understanding and Precise Grounding With A Novel Multimodal LLM ",
                "# M4U: Evaluating Multilingual Understanding and Reasoning for Large Multimodal Models "
            ]
        },
        "shuffle_doc": true,
        "id": "5144320f-e23d-4bf0-a772-e06556ca09b6",
        "prompt": "[Question]\n#Papers:\n{docs}\n\nWe hope you will carefully study the provided papers and determine the citation relationships between them. Please follow the instructions below strictly to complete the task:\n\n#Specific Requirements:\n1. Reference: When a given paper mentions other provided papers, those other papers are considered as \"references\" for the given paper. To summarize in this specific context, references are about what the given paper is using.\n2. Citation: Conversely, when other provided papers mention the given paper in their works, the given paper is being \"cited\" by those other papers. To summarize in this specific context, citations are about who is using the given paper.\n3. Given a paper, you need to determine the citation or reference relationship between this paper and the other papers. Do not consider papers that are not provided.\n3. Please present the paper titles in a json format as follows: {{\"Reference\":[\"Reference Title 1\", \"Reference Title 2\", ..., \"Reference Title n\"], \"Citation\":[\"Citation Title 1\", \"Citation Title 2\", ..., \"Citation Title n\"]}}.\n4. If a paper does not have any references or citations, please leave the corresponding list empty, e.g.{{\"Refernce\":[]}}, {{\"Citation\":[]}}.\n\n#The paper you need to analyze:\nVisualWebArena: EvalUATING MULTIMODAL AGENTS ON REALISTIC VISUAL WEB TASKS\n\n[Gold Answer]\n{'Reference': ['# CogAgent: A Visual Language Model for GUI Agents ', '# Eyes Wide Shut? Exploring the Visual Shortcomings of Multimodal LLMs '], 'Citation': ['# V-Zen: Efficient GUI Understanding and Precise Grounding With A Novel Multimodal LLM ', '# M4U: Evaluating Multilingual Understanding and Reasoning for Large Multimodal Models ']}\n\n[The Start of Assistant's Predicted Answer]\n{\"Reference\":[\"A Survey on Evaluation of Large Language Models\", \"Vicuna: An Open-Source Chatbot Impressing GPT-4 with $90 \\% *$ ChatGPT Quality\", \"ChatLaw: Open-Source Legal Large Language Model with Integrated External Knowledge Bases\", \"Compositional Semantic Parsing with Large Language Models\", \"Open LLM Leaderboard\", \"How Close is ChatGPT to Human Experts? Comparison Corpus, Evaluation, and Detection\", \"REALM: Retrieval-Augmented Language Model PreTraining\", \"Rethinking with Retrieval: Faithful Large Language Model Inference\", \"Measuring Massive Multitask Language Understanding\", \"Atlas: Few-shot Learning with Retrieval Augmented Language Models\", \"Flamingo: a visual language model for few-shot learning\", \"VQA: Visual Question Answering\", \"UnlimiFormer: Long-Range Transformers with Unlimited Length Input\", \"Graph of Thoughts: Solving Elaborate Problems with Large Language Models\", \"Mind2Web: Towards a Generalist Agent for the Web\", \"Is it an Agent, or just a Program?: A Taxonomy for Autonomous Agents\", \"Gemini: A Family of Highly Capable Multimodal Models\", \"A Real-World WebAgent with Planning, Long Context Understanding, and Program Synthesis\", \"The Curious Case of Neural Text Degeneration\", \"CogAgent: A Visual Language Model for GUI Agents\", \"A Roadmap of Agent Research and Development\", \"Mistral 7B\", \"MixTal of Experts\", \"Language Models Can Solve Computer Tasks\", \"World of Bits: An Open-Domain Platform for Web-Based Agents\", \"Long Range Arena: A Benchmark for Efficient Transformers\", \"Eyes Wide Shut? Exploring the Visual Shortcomings of Multimodal LLMs\", \"LLaMA: Open and Efficient Foundation Language Models\", \"Llama 2: Open Foundation and Fine-Tuned Chat Models\", \"Multimodal Few-Shot Learning with Frozen Language Models\", \"Focused Transformer: Contrastive Training for Context Scaling\", \"Attention Is All You Need\", \"Show and Tell: A Neural Image Caption Generator\", \"The Rise and Potential of Large Language Model Based Agents: A Survey\", \"GPT-4V in Wonderland: Large Multimodal Models for Zero-Shot Smartphone GUI Navigation\", \"Set-of-Mark Prompting Unleashes Extraordinary Visual Grounding in GPT-4V\", \"AppAgent: Multimodal Agents as Smartphone Users\", \"The Dawn of LMMS: Preliminary Explorations with GPT-4V (ision)\", \"WebShop: Towards Scalable Real-World Web Interaction with Grounded Language Agents\", \"Tree of Thoughts: Deliberate Problem Solving with Large Language Models\", \"MMMU: A Massive Multi-Discipline Multimodal Understanding and Reasoning Benchmark for Expert AGI\", \"AgentTuning: Enabling Generalized Agent Abilities for LLMs\", \"You Only Look at Screens: Multimodal Chain-of-Action Agents\", \"GPT-4V (ision) Is a Generalist Web Agent, if Grounded\"], \"Citation\":[]}\n[The End of Assistant's Predicted Answer]\n\n[System]\nWe would like to request your feedback on the performance of the AI assistant in response to the user question displayed above according to the gold answer. Please use the following listed aspects and their descriptions as evaluation criteria:\n    - Accuracy and Hallucinations: The assistant's answer is semantically consistent with the gold answer; The numerical value and order need to be accurate, and there should be no hallucinations.\n    - Completeness: Referring to the reference answers, the assistant's answer should contain all the key points needed to answer the user's question; further elaboration on these key points can be omitted.\nPlease rate whether this answer is suitable for the question. Please note that the gold answer can be considered as a correct answer to the question.\n\nThe assistant receives an overall score on a scale of 1 to 100, where a higher score indicates better overall performance.\nPlease note that if the assistant's answer and the gold answer fully meet the above criteria, its overall rating should be the full marks (100).\nPlease first provide a comprehensive explanation of your evaluation, avoiding any potential bias.\nThen, output a line indicating the score of the Assistant.\n\nPLEASE OUTPUT WITH THE FOLLOWING FORMAT, WHERE THE SCORE IS A SCALE OF 1 TO 100 BY STRICTLY FOLLOWING THIS FORMAT: \"[[score]]\", FOR EXAMPLE \"Rating: [[100]]\":\n<start output>\nEvaluation evidence: your evluation explanation here, no more than 100 words\nRating: [[score]]\n<end output>\n\nNow, start your evaluation:",
        "generate_response": "{\"Reference\":[\"A Survey on Evaluation of Large Language Models\", \"Vicuna: An Open-Source Chatbot Impressing GPT-4 with $90 \\% *$ ChatGPT Quality\", \"ChatLaw: Open-Source Legal Large Language Model with Integrated External Knowledge Bases\", \"Compositional Semantic Parsing with Large Language Models\", \"Open LLM Leaderboard\", \"How Close is ChatGPT to Human Experts? Comparison Corpus, Evaluation, and Detection\", \"REALM: Retrieval-Augmented Language Model PreTraining\", \"Rethinking with Retrieval: Faithful Large Language Model Inference\", \"Measuring Massive Multitask Language Understanding\", \"Atlas: Few-shot Learning with Retrieval Augmented Language Models\", \"Flamingo: a visual language model for few-shot learning\", \"VQA: Visual Question Answering\", \"UnlimiFormer: Long-Range Transformers with Unlimited Length Input\", \"Graph of Thoughts: Solving Elaborate Problems with Large Language Models\", \"Mind2Web: Towards a Generalist Agent for the Web\", \"Is it an Agent, or just a Program?: A Taxonomy for Autonomous Agents\", \"Gemini: A Family of Highly Capable Multimodal Models\", \"A Real-World WebAgent with Planning, Long Context Understanding, and Program Synthesis\", \"The Curious Case of Neural Text Degeneration\", \"CogAgent: A Visual Language Model for GUI Agents\", \"A Roadmap of Agent Research and Development\", \"Mistral 7B\", \"MixTal of Experts\", \"Language Models Can Solve Computer Tasks\", \"World of Bits: An Open-Domain Platform for Web-Based Agents\", \"Long Range Arena: A Benchmark for Efficient Transformers\", \"Eyes Wide Shut? Exploring the Visual Shortcomings of Multimodal LLMs\", \"LLaMA: Open and Efficient Foundation Language Models\", \"Llama 2: Open Foundation and Fine-Tuned Chat Models\", \"Multimodal Few-Shot Learning with Frozen Language Models\", \"Focused Transformer: Contrastive Training for Context Scaling\", \"Attention Is All You Need\", \"Show and Tell: A Neural Image Caption Generator\", \"The Rise and Potential of Large Language Model Based Agents: A Survey\", \"GPT-4V in Wonderland: Large Multimodal Models for Zero-Shot Smartphone GUI Navigation\", \"Set-of-Mark Prompting Unleashes Extraordinary Visual Grounding in GPT-4V\", \"AppAgent: Multimodal Agents as Smartphone Users\", \"The Dawn of LMMS: Preliminary Explorations with GPT-4V (ision)\", \"WebShop: Towards Scalable Real-World Web Interaction with Grounded Language Agents\", \"Tree of Thoughts: Deliberate Problem Solving with Large Language Models\", \"MMMU: A Massive Multi-Discipline Multimodal Understanding and Reasoning Benchmark for Expert AGI\", \"AgentTuning: Enabling Generalized Agent Abilities for LLMs\", \"You Only Look at Screens: Multimodal Chain-of-Action Agents\", \"GPT-4V (ision) Is a Generalist Web Agent, if Grounded\"], \"Citation\":[]}",
        "eval_response": "Evaluation evidence: The assistant's answer does not match the gold answer in terms of accuracy and completeness. The references and citations listed in the assistant's response do not align with those in the gold answer. The assistant incorrectly lists numerous unrelated papers and fails to include the correct references and citations as specified in the gold answer. This indicates a significant deviation from the required task, leading to a low score.\n\nRating: [[10]]",
        "score": 10
    },
    {
        "level": 3,
        "set": 4,
        "length": 227006,
        "type": "paper",
        "language": "en",
        "question": "Self-Discover: Large Language Models Self-Compose Reasoning Structures",
        "instruction": "We hope you will carefully study the provided papers and determine the citation relationships between them. Please follow the instructions below strictly to complete the task:\n\n#Specific Requirements:\n1. Reference: When a given paper mentions other provided papers, those other papers are considered as \"references\" for the given paper. To summarize in this specific context, references are about what the given paper is using.\n2. Citation: Conversely, when other provided papers mention the given paper in their works, the given paper is being \"cited\" by those other papers. To summarize in this specific context, citations are about who is using the given paper.\n3. Given a paper, you need to determine the citation or reference relationship between this paper and the other papers. Do not consider papers that are not provided.\n3. Please present the paper titles in a json format as follows: {{\"Reference\":[\"Reference Title 1\", \"Reference Title 2\", ..., \"Reference Title n\"], \"Citation\":[\"Citation Title 1\", \"Citation Title 2\", ..., \"Citation Title n\"]}}.\n4. If a paper does not have any references or citations, please leave the corresponding list empty, e.g.{{\"Refernce\":[]}}, {{\"Citation\":[]}}.",
        "prompt_template": "#Papers:\n{docs}\n\n{instruction}\n\n#The paper you need to analyze:\n{question}",
        "doc": [
            "2402.03620.md",
            "2402.15205.md",
            "2403.03101.md",
            "2404.02575.md",
            "2404.15488.md",
            "2405.14804.md",
            "2310.03051.md",
            "2310.06117.md",
            "2310.14628.md",
            "2310.15123.md",
            "2311.08803.md"
        ],
        "answer": {
            "Reference": [
                "# How FaR ARE Large LanguAGE MoDELS FROM AGENTS WITH THEORY-OF-MIND? ",
                "# TAKE A STEP BACK: EVOKING REASONING VIA ABSTRACTION IN LARGE LANGUAGE MODELS ",
                "# Plan, Verify and Switch: Integrated Reasoning with Diverse X-of-Thoughts ",
                "# BRANCH-SOLVE-MERGE IMPROVES LARGE LANGUAGE MODEL EVALUATION AND GENERATION ",
                "# StrategyLLM: Large Language Models as Strategy Generators, Executors, Optimizers, and Evaluators for Problem Solving "
            ],
            "Citation": [
                "# Enhancing ICU Patient Recovery: Using LLMs to Assist Nurses in Diary Writing ",
                "# KnowAgent: Knowledge-Augmented Planning for LLM-Based Agents ",
                "# Language Models as Compilers: Simulating Pseudocode Execution Improves Algorithmic Reasoning in Language Models ",
                "# IryNLP at MEDIQA-CORR 2024: Tackling the Medical Error Detection \\& Correction Task On the Shoulders of Medical Agents ",
                "# Can LLMs Solve Longer Math Word Problems Better? "
            ]
        },
        "shuffle_doc": true,
        "id": "9ecfffba-6cf7-44f0-9d2f-1928aba36cae",
        "prompt": "[Question]\n#Papers:\n{docs}\n\nWe hope you will carefully study the provided papers and determine the citation relationships between them. Please follow the instructions below strictly to complete the task:\n\n#Specific Requirements:\n1. Reference: When a given paper mentions other provided papers, those other papers are considered as \"references\" for the given paper. To summarize in this specific context, references are about what the given paper is using.\n2. Citation: Conversely, when other provided papers mention the given paper in their works, the given paper is being \"cited\" by those other papers. To summarize in this specific context, citations are about who is using the given paper.\n3. Given a paper, you need to determine the citation or reference relationship between this paper and the other papers. Do not consider papers that are not provided.\n3. Please present the paper titles in a json format as follows: {{\"Reference\":[\"Reference Title 1\", \"Reference Title 2\", ..., \"Reference Title n\"], \"Citation\":[\"Citation Title 1\", \"Citation Title 2\", ..., \"Citation Title n\"]}}.\n4. If a paper does not have any references or citations, please leave the corresponding list empty, e.g.{{\"Refernce\":[]}}, {{\"Citation\":[]}}.\n\n#The paper you need to analyze:\nSelf-Discover: Large Language Models Self-Compose Reasoning Structures\n\n[Gold Answer]\n{'Reference': ['# How FaR ARE Large LanguAGE MoDELS FROM AGENTS WITH THEORY-OF-MIND? ', '# TAKE A STEP BACK: EVOKING REASONING VIA ABSTRACTION IN LARGE LANGUAGE MODELS ', '# Plan, Verify and Switch: Integrated Reasoning with Diverse X-of-Thoughts ', '# BRANCH-SOLVE-MERGE IMPROVES LARGE LANGUAGE MODEL EVALUATION AND GENERATION ', '# StrategyLLM: Large Language Models as Strategy Generators, Executors, Optimizers, and Evaluators for Problem Solving '], 'Citation': ['# Enhancing ICU Patient Recovery: Using LLMs to Assist Nurses in Diary Writing ', '# KnowAgent: Knowledge-Augmented Planning for LLM-Based Agents ', '# Language Models as Compilers: Simulating Pseudocode Execution Improves Algorithmic Reasoning in Language Models ', '# IryNLP at MEDIQA-CORR 2024: Tackling the Medical Error Detection \\\\& Correction Task On the Shoulders of Medical Agents ', '# Can LLMs Solve Longer Math Word Problems Better? ']}\n\n[The Start of Assistant's Predicted Answer]\n{\"Reference\":[\"BIG-Bench Hard (BBH) (Suzgun et al., 2022)\", \"Thinking for Doing (T4D) (Zhou et al., 2023)\", \"MATH (Hendrycks et al., 2021)\", \"GPT-4 (gpt-4-turbopreview) (OpenAI, 2023b)\", \"GPT-3.5-turbo (ChatGPT) (OpenAI, 2022)\", \"instruction-tuned PaLM 2-L (Anil et al., 2023)\", \"Llama2-70B (Touvron et al., 2023)\", \"Direct Prompting\", \"CoT (Wei et al., 2022; Kojima et al., 2022)\", \"Plan-and-Solve (Wang et al., 2023)\", \"CoT-Self-Consistency (Wang et al., 2022)\", \"Majority voting of each RM\", \"Best of each RM\", \"LLMs as optimizers (OPRO) (Yang et al., 2023)\", \"Palm 2 technical report (Anil et al., 2023)\", \"Graph of thoughts: Solving elaborate problems with large language models (Besta et al., 2023)\", \"Language models are few-shot learners (Brown et al., 2020)\", \"Skills-in-context prompting: Unlocking compositionality in large language models (Chen et al., 2023)\", \"Program of thoughts prompting: Disentangling computation from reasoning for numerical reasoning tasks (Chen et al., 2022)\", \"Palm: Scaling language modeling with pathways (Chowdhery et al., 2022)\", \"Scaling instruction-finetuned language models (Chung et al., 2022)\", \"Training verifiers to solve math word problems (Cobbe et al., 2021)\", \"Compositional semantic parsing with large language models (Drozdov et al., 2022)\", \"Measuring mathematical problem solving with the math dataset (Hendrycks et al., 2021)\", \"Decomposed prompting: A modular approach for solving complex tasks (Khot et al., 2022)\", \"Large language models are zero-shot reasoners (Kojima et al., 2022)\", \"Less is more: Summary of long instructions is better for program synthesis (Kuznia et al., 2022)\", \"Plan, verify and switch: Integrated reasoning with diverse x-of-thoughts (Liu et al., 2023)\", \"HELP ME THINK: A simple prompting strategy for non-experts to create customized content with models (Mishra et al., 2023)\", \"Lila: A unified benchmark for mathematical reasoning (Mishra et al., 2022a)\", \"Reframing instructional prompts to gptk's language (Mishra et al., 2022b)\", \"Crosstask generalization via natural language crowdsourcing instructions (Mishra et al., 2022c)\", \"Elements of a theory of human problem solving (Newell et al., 1958)\", \"Show your work: Scratchpads for intermediate computation with language models (Nye et al., 2021)\", \"Chatgpt: Optimizing language models for dialogue (OpenAI, 2022)\", \"Json generation mode (OpenAI, 2023a)\", \"Gpt-4 technical report (OpenAI, 2023b)\", \"Training language models to follow instructions with human feedback (Ouyang et al., 2022)\", \"Is a question decomposition unit all we need? (Patel et al., 2022)\", \"How to solve it: A new aspect of mathematical method (Polya, 2004)\", \"Skills, rules, and knowledge; signals, signs, and symbols, and other distinctions in human performance models (Rasmussen, 1983)\", \"Branch-solve-merge improves large language model evaluation and generation (Saha et al., 2023)\", \"Beyond the imitation game: Quantifying and extrapolating the capabilities of language models (Srivastava et al., 2023)\", \"Challenging big-bench tasks and whether chain-of-thought can solve them (Suzgun et al., 2022)\", \"Llama 2: Open foundation and finetuned chat models (Touvron et al., 2023)\", \"Attention is all you need (Vaswani et al., 2017)\", \"Plan-and-solve prompting: Improving zero-shot chain-of-thought reasoning by large language models (Wang et al., 2023)\", \"Selfconsistency improves chain of thought reasoning in language models (Wang et al., 2022)\", \"Finetuned language models are zero-shot learners (Wei et al., 2021)\", \"Chain-of-thought prompting elicits reasoning in large language models (Wei et al., 2022)\", \"Large language models as optimizers (Yang et al., 2023)\", \"Tree of thoughts: Deliberate problem solving with large language models (Yao et al., 2023a)\", \"Beyond chain-of-thought, effective graph-of-thought reasoning in large language models (Yao et al., 2023b)\", \"Large language models as analogical reasoners (Yasunaga et al., 2023)\", \"Take a step back: Evoking reasoning via abstraction in large language models (Zheng et al., 2023)\", \"Adapting language models for zero-shot learning by metatuning on dataset and prompt collections (Zhong et al., 2021)\", \"Least-to-most prompting enables complex reasoning in large language models (Zhou et al., 2022a)\", \"How far are large language models from agents with theory-of-mind? (Zhou et al., 2023)\", \"Large language models are humanlevel prompt engineers (Zhou et al., 2022b)\"], \"Citation\":[\"TAKE A STEP BACK: EVOKING REASONING VIA ABSTRACTION IN LARGE LANGUAGE MODELS\", \"Can LLMs Solve Longer Math Word Problems Better?\", \"Enhancing ICU Patient Recovery: Using LLMs to Assist Nurses in Diary Writing\"]}\n[The End of Assistant's Predicted Answer]\n\n[System]\nWe would like to request your feedback on the performance of the AI assistant in response to the user question displayed above according to the gold answer. Please use the following listed aspects and their descriptions as evaluation criteria:\n    - Accuracy and Hallucinations: The assistant's answer is semantically consistent with the gold answer; The numerical value and order need to be accurate, and there should be no hallucinations.\n    - Completeness: Referring to the reference answers, the assistant's answer should contain all the key points needed to answer the user's question; further elaboration on these key points can be omitted.\nPlease rate whether this answer is suitable for the question. Please note that the gold answer can be considered as a correct answer to the question.\n\nThe assistant receives an overall score on a scale of 1 to 100, where a higher score indicates better overall performance.\nPlease note that if the assistant's answer and the gold answer fully meet the above criteria, its overall rating should be the full marks (100).\nPlease first provide a comprehensive explanation of your evaluation, avoiding any potential bias.\nThen, output a line indicating the score of the Assistant.\n\nPLEASE OUTPUT WITH THE FOLLOWING FORMAT, WHERE THE SCORE IS A SCALE OF 1 TO 100 BY STRICTLY FOLLOWING THIS FORMAT: \"[[score]]\", FOR EXAMPLE \"Rating: [[100]]\":\n<start output>\nEvaluation evidence: your evluation explanation here, no more than 100 words\nRating: [[score]]\n<end output>\n\nNow, start your evaluation:",
        "generate_response": "{\"Reference\":[\"BIG-Bench Hard (BBH) (Suzgun et al., 2022)\", \"Thinking for Doing (T4D) (Zhou et al., 2023)\", \"MATH (Hendrycks et al., 2021)\", \"GPT-4 (gpt-4-turbopreview) (OpenAI, 2023b)\", \"GPT-3.5-turbo (ChatGPT) (OpenAI, 2022)\", \"instruction-tuned PaLM 2-L (Anil et al., 2023)\", \"Llama2-70B (Touvron et al., 2023)\", \"Direct Prompting\", \"CoT (Wei et al., 2022; Kojima et al., 2022)\", \"Plan-and-Solve (Wang et al., 2023)\", \"CoT-Self-Consistency (Wang et al., 2022)\", \"Majority voting of each RM\", \"Best of each RM\", \"LLMs as optimizers (OPRO) (Yang et al., 2023)\", \"Palm 2 technical report (Anil et al., 2023)\", \"Graph of thoughts: Solving elaborate problems with large language models (Besta et al., 2023)\", \"Language models are few-shot learners (Brown et al., 2020)\", \"Skills-in-context prompting: Unlocking compositionality in large language models (Chen et al., 2023)\", \"Program of thoughts prompting: Disentangling computation from reasoning for numerical reasoning tasks (Chen et al., 2022)\", \"Palm: Scaling language modeling with pathways (Chowdhery et al., 2022)\", \"Scaling instruction-finetuned language models (Chung et al., 2022)\", \"Training verifiers to solve math word problems (Cobbe et al., 2021)\", \"Compositional semantic parsing with large language models (Drozdov et al., 2022)\", \"Measuring mathematical problem solving with the math dataset (Hendrycks et al., 2021)\", \"Decomposed prompting: A modular approach for solving complex tasks (Khot et al., 2022)\", \"Large language models are zero-shot reasoners (Kojima et al., 2022)\", \"Less is more: Summary of long instructions is better for program synthesis (Kuznia et al., 2022)\", \"Plan, verify and switch: Integrated reasoning with diverse x-of-thoughts (Liu et al., 2023)\", \"HELP ME THINK: A simple prompting strategy for non-experts to create customized content with models (Mishra et al., 2023)\", \"Lila: A unified benchmark for mathematical reasoning (Mishra et al., 2022a)\", \"Reframing instructional prompts to gptk's language (Mishra et al., 2022b)\", \"Crosstask generalization via natural language crowdsourcing instructions (Mishra et al., 2022c)\", \"Elements of a theory of human problem solving (Newell et al., 1958)\", \"Show your work: Scratchpads for intermediate computation with language models (Nye et al., 2021)\", \"Chatgpt: Optimizing language models for dialogue (OpenAI, 2022)\", \"Json generation mode (OpenAI, 2023a)\", \"Gpt-4 technical report (OpenAI, 2023b)\", \"Training language models to follow instructions with human feedback (Ouyang et al., 2022)\", \"Is a question decomposition unit all we need? (Patel et al., 2022)\", \"How to solve it: A new aspect of mathematical method (Polya, 2004)\", \"Skills, rules, and knowledge; signals, signs, and symbols, and other distinctions in human performance models (Rasmussen, 1983)\", \"Branch-solve-merge improves large language model evaluation and generation (Saha et al., 2023)\", \"Beyond the imitation game: Quantifying and extrapolating the capabilities of language models (Srivastava et al., 2023)\", \"Challenging big-bench tasks and whether chain-of-thought can solve them (Suzgun et al., 2022)\", \"Llama 2: Open foundation and finetuned chat models (Touvron et al., 2023)\", \"Attention is all you need (Vaswani et al., 2017)\", \"Plan-and-solve prompting: Improving zero-shot chain-of-thought reasoning by large language models (Wang et al., 2023)\", \"Selfconsistency improves chain of thought reasoning in language models (Wang et al., 2022)\", \"Finetuned language models are zero-shot learners (Wei et al., 2021)\", \"Chain-of-thought prompting elicits reasoning in large language models (Wei et al., 2022)\", \"Large language models as optimizers (Yang et al., 2023)\", \"Tree of thoughts: Deliberate problem solving with large language models (Yao et al., 2023a)\", \"Beyond chain-of-thought, effective graph-of-thought reasoning in large language models (Yao et al., 2023b)\", \"Large language models as analogical reasoners (Yasunaga et al., 2023)\", \"Take a step back: Evoking reasoning via abstraction in large language models (Zheng et al., 2023)\", \"Adapting language models for zero-shot learning by metatuning on dataset and prompt collections (Zhong et al., 2021)\", \"Least-to-most prompting enables complex reasoning in large language models (Zhou et al., 2022a)\", \"How far are large language models from agents with theory-of-mind? (Zhou et al., 2023)\", \"Large language models are humanlevel prompt engineers (Zhou et al., 2022b)\"], \"Citation\":[\"TAKE A STEP BACK: EVOKING REASONING VIA ABSTRACTION IN LARGE LANGUAGE MODELS\", \"Can LLMs Solve Longer Math Word Problems Better?\", \"Enhancing ICU Patient Recovery: Using LLMs to Assist Nurses in Diary Writing\"]}",
        "eval_response": "Evaluation evidence: The assistant's answer does not match the gold answer in terms of the specific titles listed under \"Reference\" and \"Citation.\" The titles provided by the assistant are not found in the gold answer, indicating a significant discrepancy and possible hallucination of content. This results in both inaccuracies and incompleteness as the assistant fails to identify the correct papers that reference or cite the given paper according to the provided gold answer.\n\nRating: [[20]]",
        "score": 20
    },
    {
        "level": 4,
        "set": 2,
        "length": 50239,
        "type": "paper",
        "language": "en",
        "question": "",
        "instruction": "We kindly ask you to thoroughly review the provided papers and construct a citation chain from them. Please adhere to the following instructions strictly while completing the task:\n\n#Task Instructions:\nGiven several papers, you are required to identify and list the longest citation chain, which demonstrates the citation relationship among the provided papers.\n\n#Specific Requirements:\n1.Please present the titles of the papers in the form of a list, as follows: [\"Title of Paper 1\", \"Title of Paper 2\", ..., \"Title of Paper n\"].\n2.Ensure that the citation chain in the list is linear and continuous, meaning that the first paper title in the list (Paper 1) should not cite any other works. Instead, it should be cited by the next paper in the list (Paper 2); subsequently, each paper should then be cited by the next one in the list, continuing up to the last paper (Paper n).\n3.Consider only the citation relationships within the supplied collection of papers, and ensure that the citation chain accurately reflects the sequential citation order among these documents.\n4.Do not take into account any articles not provided, and disregard other non-linear citation relationships.",
        "prompt_template": "{instruction}\n\n#Papers Provided:\n{docs}",
        "doc": [
            "2211.04894.md",
            "2105.14550.md",
            "2402.09178.md"
        ],
        "answer": [
            "# Exploring Video Quality Assessment on User Generated Contents from Aesthetic and Technical Perspectives ",
            "# Blind Quality Assessment for in-the-Wild Images via Hierarchical Feature Fusion and Iterative Mixed Database Training ",
            "# Generalized Portrait Quality Assessment "
        ],
        "shuffle_doc": true,
        "id": "bd9ccf84-5fb3-47c4-827e-5e73f8f112ef",
        "prompt": "[Question]\nWe kindly ask you to thoroughly review the provided papers and construct a citation chain from them. Please adhere to the following instructions strictly while completing the task:\n\n#Task Instructions:\nGiven several papers, you are required to identify and list the longest citation chain, which demonstrates the citation relationship among the provided papers.\n\n#Specific Requirements:\n1.Please present the titles of the papers in the form of a list, as follows: [\"Title of Paper 1\", \"Title of Paper 2\", ..., \"Title of Paper n\"].\n2.Ensure that the citation chain in the list is linear and continuous, meaning that the first paper title in the list (Paper 1) should not cite any other works. Instead, it should be cited by the next paper in the list (Paper 2); subsequently, each paper should then be cited by the next one in the list, continuing up to the last paper (Paper n).\n3.Consider only the citation relationships within the supplied collection of papers, and ensure that the citation chain accurately reflects the sequential citation order among these documents.\n4.Do not take into account any articles not provided, and disregard other non-linear citation relationships.\n\n#Papers Provided:\n{docs}\n\n[Gold Answer]\n['# Exploring Video Quality Assessment on User Generated Contents from Aesthetic and Technical Perspectives ', '# Blind Quality Assessment for in-the-Wild Images via Hierarchical Feature Fusion and Iterative Mixed Database Training ', '# Generalized Portrait Quality Assessment ']\n\n[The Start of Assistant's Predicted Answer]\n[\"Blind Quality Assessment for in-the-Wild Images via Hierarchical Feature Fusion and Iterative Mixed Database Training\", \"Deep residual learning for image recognition\", \"Convolutional neural networks for no-reference image quality assessment\", \"From patches to pictures (paq-2-piq): Mapping the perceptual space of picture quality\", \"Visualizing and understanding convolutional networks\", \"Hyperface: A deep multitask learning framework for face detection, landmark localization, pose estimation, and gender recognition\", \"Unsupervised blind image quality evaluation via statistical measurements of structure, naturalness, and perception\", \"Blind image quality assessment based on high order statistics aggregation\", \"Multitask rank learning for image quality assessment\", \"Deep neural networks for no-reference and full-reference image quality assessment\", \"End-toend blind image quality assessment using deep neural networks\", \"Blind image quality assessment using a deep bilinear convolutional neural network\", \"Blindly assess image quality in the wild guided by a self-adaptive hyper network\", \"Generalizable no-reference image quality assessment via deep meta-learning\", \"Musiq: Multiscale image quality transformer\", \"Unsupervised feature learning framework for no-reference image quality assessment\", \"Training quality-aware filters for no-reference image quality assessment\", \"Rankiqa: Learning from rankings for no-reference image quality assessment\", \"dipiq: Blind image quality assessment by learning-to-rank discriminable image pairs\", \"Learning a no-reference quality assessment model of enhanced images with big data\", \"Beyond human opinion scores: Blind image quality assessment based on synthetic scores\", \"A novel patch variance biased convolutional neural network for no-reference image quality assessment\", \"Training objective image and video quality estimators using multiple databases\", \"Unified quality assessment of in-the-wild videos with mixed datasets training\", \"Uncertainty-aware blind image quality assessment in the laboratory and wild\", \"Going deeper with convolutions\", \"Faster r-cnn: Towards real-time object detection with region proposal networks\", \"Deepsim: Deep similarity for image quality assessment\", \"Image quality assessment by comparing cnn features between images\", \"Adam: A method for stochastic optimization\", \"Massive online crowdsourced study of subjective and objective picture quality\", \"No-reference blur assessment of digital pictures based on multifeature classifiers\", \"Koniq-10k: An ecologically valid database for deep learning of blind image quality assessment\", \"Perceptual quality assessment of smartphone photography\", \"Ava: A large-scale database for aesthetic visual analysis\", \"The pascal visual object classes (voc) challenge\", \"Emotic: Emotions in context dataset\", \"No-reference blur assessment in natural images using fourier transform and spatial pyramids\", \"Live image quality assessment database release 2 (2005)\", \"Most apparent distortion: fullreference image quality assessment and the role of strategy\", \"Kadid-10k: A large-scale artificially distorted iqa database\", \"Objective quality assessment of multiply distorted images\", \"Learning without human scores for blind image quality assessment\", \"Making a 'completely blind' image quality analyzer\", \"A feature-enriched completely blind image quality evaluator\", \"Which has better visual quality: The clear blue sky or a blurry animal?\", \"Fine-grained image quality assessment: A revisit and further thinking\", \"A statistical evaluation of recent full reference image quality assessment algorithms\", \"Mobilenetv2: Inverted residuals and linear bottlenecks\", \"Aggregated residual transformations for deep neural networks\", \"Metaiqa: deep metalearning for no-reference image quality assessment\", \"Continual learning for blind image quality assessment\", \"Task-specific normalization for continual learning of blind image quality models\"]\n[The End of Assistant's Predicted Answer]\n\n[System]\nWe would like to request your feedback on the performance of the AI assistant in response to the user question displayed above according to the gold answer. Please use the following listed aspects and their descriptions as evaluation criteria:\n    - Accuracy and Hallucinations: The assistant's answer is semantically consistent with the gold answer; The numerical value and order need to be accurate, and there should be no hallucinations.\n    - Completeness: Referring to the reference answers, the assistant's answer should contain all the key points needed to answer the user's question; further elaboration on these key points can be omitted.\nPlease rate whether this answer is suitable for the question. Please note that the gold answer can be considered as a correct answer to the question.\n\nThe assistant receives an overall score on a scale of 1 to 100, where a higher score indicates better overall performance.\nPlease note that if the assistant's answer and the gold answer fully meet the above criteria, its overall rating should be the full marks (100).\nPlease first provide a comprehensive explanation of your evaluation, avoiding any potential bias.\nThen, output a line indicating the score of the Assistant.\n\nPLEASE OUTPUT WITH THE FOLLOWING FORMAT, WHERE THE SCORE IS A SCALE OF 1 TO 100 BY STRICTLY FOLLOWING THIS FORMAT: \"[[score]]\", FOR EXAMPLE \"Rating: [[100]]\":\n<start output>\nEvaluation evidence: your evluation explanation here, no more than 100 words\nRating: [[score]]\n<end output>\n\nNow, start your evaluation:",
        "generate_response": "[\"Blind Quality Assessment for in-the-Wild Images via Hierarchical Feature Fusion and Iterative Mixed Database Training\", \"Deep residual learning for image recognition\", \"Convolutional neural networks for no-reference image quality assessment\", \"From patches to pictures (paq-2-piq): Mapping the perceptual space of picture quality\", \"Visualizing and understanding convolutional networks\", \"Hyperface: A deep multitask learning framework for face detection, landmark localization, pose estimation, and gender recognition\", \"Unsupervised blind image quality evaluation via statistical measurements of structure, naturalness, and perception\", \"Blind image quality assessment based on high order statistics aggregation\", \"Multitask rank learning for image quality assessment\", \"Deep neural networks for no-reference and full-reference image quality assessment\", \"End-toend blind image quality assessment using deep neural networks\", \"Blind image quality assessment using a deep bilinear convolutional neural network\", \"Blindly assess image quality in the wild guided by a self-adaptive hyper network\", \"Generalizable no-reference image quality assessment via deep meta-learning\", \"Musiq: Multiscale image quality transformer\", \"Unsupervised feature learning framework for no-reference image quality assessment\", \"Training quality-aware filters for no-reference image quality assessment\", \"Rankiqa: Learning from rankings for no-reference image quality assessment\", \"dipiq: Blind image quality assessment by learning-to-rank discriminable image pairs\", \"Learning a no-reference quality assessment model of enhanced images with big data\", \"Beyond human opinion scores: Blind image quality assessment based on synthetic scores\", \"A novel patch variance biased convolutional neural network for no-reference image quality assessment\", \"Training objective image and video quality estimators using multiple databases\", \"Unified quality assessment of in-the-wild videos with mixed datasets training\", \"Uncertainty-aware blind image quality assessment in the laboratory and wild\", \"Going deeper with convolutions\", \"Faster r-cnn: Towards real-time object detection with region proposal networks\", \"Deepsim: Deep similarity for image quality assessment\", \"Image quality assessment by comparing cnn features between images\", \"Adam: A method for stochastic optimization\", \"Massive online crowdsourced study of subjective and objective picture quality\", \"No-reference blur assessment of digital pictures based on multifeature classifiers\", \"Koniq-10k: An ecologically valid database for deep learning of blind image quality assessment\", \"Perceptual quality assessment of smartphone photography\", \"Ava: A large-scale database for aesthetic visual analysis\", \"The pascal visual object classes (voc) challenge\", \"Emotic: Emotions in context dataset\", \"No-reference blur assessment in natural images using fourier transform and spatial pyramids\", \"Live image quality assessment database release 2 (2005)\", \"Most apparent distortion: fullreference image quality assessment and the role of strategy\", \"Kadid-10k: A large-scale artificially distorted iqa database\", \"Objective quality assessment of multiply distorted images\", \"Learning without human scores for blind image quality assessment\", \"Making a 'completely blind' image quality analyzer\", \"A feature-enriched completely blind image quality evaluator\", \"Which has better visual quality: The clear blue sky or a blurry animal?\", \"Fine-grained image quality assessment: A revisit and further thinking\", \"A statistical evaluation of recent full reference image quality assessment algorithms\", \"Mobilenetv2: Inverted residuals and linear bottlenecks\", \"Aggregated residual transformations for deep neural networks\", \"Metaiqa: deep metalearning for no-reference image quality assessment\", \"Continual learning for blind image quality assessment\", \"Task-specific normalization for continual learning of blind image quality models\"]",
        "eval_response": "Evaluation evidence: The assistant's response does not align with the gold answer and fails to meet the task requirements. The gold answer lists a specific citation chain of three papers, while the assistant's response includes a lengthy list of unrelated paper titles, which does not form a citation chain. This indicates a lack of accuracy and completeness in addressing the task of identifying and listing the longest citation chain among the provided papers.\n\nRating: [[10]]",
        "score": 10
    },
    {
        "level": 3,
        "set": 2,
        "length": 92078,
        "type": "paper",
        "language": "en",
        "question": "Contrastive Preference Optimization: Pushing the Boundaries of LLM Performance in Machine Translation",
        "instruction": "We hope you will carefully study the provided papers and determine the citation relationships between them. Please follow the instructions below strictly to complete the task:\n\n#Specific Requirements:\n1. Reference: When a given paper mentions other provided papers, those other papers are considered as \"references\" for the given paper. To summarize in this specific context, references are about what the given paper is using.\n2. Citation: Conversely, when other provided papers mention the given paper in their works, the given paper is being \"cited\" by those other papers. To summarize in this specific context, citations are about who is using the given paper.\n3. Given a paper, you need to determine the citation or reference relationship between this paper and the other papers. Do not consider papers that are not provided.\n3. Please present the paper titles in a json format as follows: {{\"Reference\":[\"Reference Title 1\", \"Reference Title 2\", ..., \"Reference Title n\"], \"Citation\":[\"Citation Title 1\", \"Citation Title 2\", ..., \"Citation Title n\"]}}.\n4. If a paper does not have any references or citations, please leave the corresponding list empty, e.g.{{\"Refernce\":[]}}, {{\"Citation\":[]}}.",
        "prompt_template": "#Papers:\n{docs}\n\n{instruction}\n\n#The paper you need to analyze:\n{question}",
        "doc": [
            "2401.08417.md",
            "2405.19534.md",
            "2309.11925.md",
            "2401.06760.md"
        ],
        "answer": {
            "Reference": [
                "# Scaling up CoMETKIWI: Unbabel-IST 2023 Submission for the Quality Estimation Shared Task ",
                "# Navigating the Metrics Maze: Reconciling Score Magnitudes and Accuracies "
            ],
            "Citation": [
                "# Preference Learning Algorithms Do Not Learn Preference Rankings "
            ]
        },
        "shuffle_doc": true,
        "id": "85891a45-feed-4a6c-a2d3-763fa04ab0cd",
        "prompt": "[Question]\n#Papers:\n{docs}\n\nWe hope you will carefully study the provided papers and determine the citation relationships between them. Please follow the instructions below strictly to complete the task:\n\n#Specific Requirements:\n1. Reference: When a given paper mentions other provided papers, those other papers are considered as \"references\" for the given paper. To summarize in this specific context, references are about what the given paper is using.\n2. Citation: Conversely, when other provided papers mention the given paper in their works, the given paper is being \"cited\" by those other papers. To summarize in this specific context, citations are about who is using the given paper.\n3. Given a paper, you need to determine the citation or reference relationship between this paper and the other papers. Do not consider papers that are not provided.\n3. Please present the paper titles in a json format as follows: {{\"Reference\":[\"Reference Title 1\", \"Reference Title 2\", ..., \"Reference Title n\"], \"Citation\":[\"Citation Title 1\", \"Citation Title 2\", ..., \"Citation Title n\"]}}.\n4. If a paper does not have any references or citations, please leave the corresponding list empty, e.g.{{\"Refernce\":[]}}, {{\"Citation\":[]}}.\n\n#The paper you need to analyze:\nContrastive Preference Optimization: Pushing the Boundaries of LLM Performance in Machine Translation\n\n[Gold Answer]\n{'Reference': ['# Scaling up CoMETKIWI: Unbabel-IST 2023 Submission for the Quality Estimation Shared Task ', '# Navigating the Metrics Maze: Reconciling Score Magnitudes and Accuracies '], 'Citation': ['# Preference Learning Algorithms Do Not Learn Preference Rankings ']}\n\n[The Start of Assistant's Predicted Answer]\n{\"Reference\": [\"A Survey on Evaluation of Large Language Models\", \"Vicuna: An Open-Source Chatbot Impressing GPT-4 with $90 \\% *$ ChatGPT Quality\", \"ChatLaw: Open-Source Legal Large Language Model with Integrated External Knowledge Bases\", \"Compositional Semantic Parsing with Large Language Models\", \"Open LLM Leaderboard\", \"How Close is ChatGPT to Human Experts? Comparison Corpus, Evaluation, and Detection\", \"REALM: Retrieval-Augmented Language Model PreTraining\", \"Rethinking with Retrieval: Faithful Large Language Model Inference\", \"Measuring Massive Multitask Language Understanding\", \"Atlas: Few-shot Learning with Retrieval Augmented Language Models\", \"Falcon-40B: An Open Large Language Model with State-of-the-Art Performance\", \"Beyond English-Centric Multilingual Machine Translation\", \"LoRA: Low-Rank Adaptation of Large Language Models\", \"Mistral 7b\", \"Is ChatGPT a Good Translator? A Preliminary Study\", \"A Simple Framework for Contrastive Learning of Visual Representations\", \"Improving Translation Faithfulness of Large Language Models via Augmenting Instructions\", \"No Language Left Behind: Scaling Human-Centered Machine Translation\", \"Representation Learning with Contrastive Predictive Coding\", \"GPT-4 Technical Report\", \"Training Language Models to Follow Instructions with Human Feedback\", \"A Call for Clarity in Reporting BLEU Scores\", \"Direct Preference Optimization: Your Language Model is Secretly a Reward Model\", \"DeepSpeed: System Optimizations Enable Training Deep Learning Models with Over 100 Billion Parameters\", \"Unbabel-IST 2022 Submission for the Metrics Shared Task\", \"Scaling up COMETKiwi: Unbabel-IST 2023 Submission for the Quality Estimation Shared Task\", \"Contrastive Learning with Hard Negative Samples\", \"Proximal Policy Optimization Algorithms\", \"BLEURT: Learning Robust Metrics for Text Generation\", \"Multilingual Representation Distillation with Contrastive Learning\", \"Llama: Open and Efficient Foundation Language Models\", \"Llama 2: Open Foundation and Fine-tuned Chat Models\", \"Attention is All You Need\", \"BERT, mBERT, or BiBERT? A Study on Contextualized Embeddings for Neural Machine Translation\", \"mT5: A Massively Multilingual Pre-Trained Text-to-Text Transformer\", \"BigTrans: Augmenting Large Language Models with Multilingual Translation Capability Over 100 Languages\", \"TIM: Teaching Large Language Models to Translate with Comparison\", \"BayLing: Bridging Cross-lingual Alignment and Instruction Following through Interactive Translation for Large Language Models\", \"Multilingual Machine Translation with Large Language Models: Empirical Results and Analysis\", \"Extrapolating Large Language Models to Non-English by Aligning Languages\"], \"Citation\": []}\n[The End of Assistant's Predicted Answer]\n\n[System]\nWe would like to request your feedback on the performance of the AI assistant in response to the user question displayed above according to the gold answer. Please use the following listed aspects and their descriptions as evaluation criteria:\n    - Accuracy and Hallucinations: The assistant's answer is semantically consistent with the gold answer; The numerical value and order need to be accurate, and there should be no hallucinations.\n    - Completeness: Referring to the reference answers, the assistant's answer should contain all the key points needed to answer the user's question; further elaboration on these key points can be omitted.\nPlease rate whether this answer is suitable for the question. Please note that the gold answer can be considered as a correct answer to the question.\n\nThe assistant receives an overall score on a scale of 1 to 100, where a higher score indicates better overall performance.\nPlease note that if the assistant's answer and the gold answer fully meet the above criteria, its overall rating should be the full marks (100).\nPlease first provide a comprehensive explanation of your evaluation, avoiding any potential bias.\nThen, output a line indicating the score of the Assistant.\n\nPLEASE OUTPUT WITH THE FOLLOWING FORMAT, WHERE THE SCORE IS A SCALE OF 1 TO 100 BY STRICTLY FOLLOWING THIS FORMAT: \"[[score]]\", FOR EXAMPLE \"Rating: [[100]]\":\n<start output>\nEvaluation evidence: your evluation explanation here, no more than 100 words\nRating: [[score]]\n<end output>\n\nNow, start your evaluation:",
        "generate_response": "{\"Reference\": [\"A Survey on Evaluation of Large Language Models\", \"Vicuna: An Open-Source Chatbot Impressing GPT-4 with $90 \\% *$ ChatGPT Quality\", \"ChatLaw: Open-Source Legal Large Language Model with Integrated External Knowledge Bases\", \"Compositional Semantic Parsing with Large Language Models\", \"Open LLM Leaderboard\", \"How Close is ChatGPT to Human Experts? Comparison Corpus, Evaluation, and Detection\", \"REALM: Retrieval-Augmented Language Model PreTraining\", \"Rethinking with Retrieval: Faithful Large Language Model Inference\", \"Measuring Massive Multitask Language Understanding\", \"Atlas: Few-shot Learning with Retrieval Augmented Language Models\", \"Falcon-40B: An Open Large Language Model with State-of-the-Art Performance\", \"Beyond English-Centric Multilingual Machine Translation\", \"LoRA: Low-Rank Adaptation of Large Language Models\", \"Mistral 7b\", \"Is ChatGPT a Good Translator? A Preliminary Study\", \"A Simple Framework for Contrastive Learning of Visual Representations\", \"Improving Translation Faithfulness of Large Language Models via Augmenting Instructions\", \"No Language Left Behind: Scaling Human-Centered Machine Translation\", \"Representation Learning with Contrastive Predictive Coding\", \"GPT-4 Technical Report\", \"Training Language Models to Follow Instructions with Human Feedback\", \"A Call for Clarity in Reporting BLEU Scores\", \"Direct Preference Optimization: Your Language Model is Secretly a Reward Model\", \"DeepSpeed: System Optimizations Enable Training Deep Learning Models with Over 100 Billion Parameters\", \"Unbabel-IST 2022 Submission for the Metrics Shared Task\", \"Scaling up COMETKiwi: Unbabel-IST 2023 Submission for the Quality Estimation Shared Task\", \"Contrastive Learning with Hard Negative Samples\", \"Proximal Policy Optimization Algorithms\", \"BLEURT: Learning Robust Metrics for Text Generation\", \"Multilingual Representation Distillation with Contrastive Learning\", \"Llama: Open and Efficient Foundation Language Models\", \"Llama 2: Open Foundation and Fine-tuned Chat Models\", \"Attention is All You Need\", \"BERT, mBERT, or BiBERT? A Study on Contextualized Embeddings for Neural Machine Translation\", \"mT5: A Massively Multilingual Pre-Trained Text-to-Text Transformer\", \"BigTrans: Augmenting Large Language Models with Multilingual Translation Capability Over 100 Languages\", \"TIM: Teaching Large Language Models to Translate with Comparison\", \"BayLing: Bridging Cross-lingual Alignment and Instruction Following through Interactive Translation for Large Language Models\", \"Multilingual Machine Translation with Large Language Models: Empirical Results and Analysis\", \"Extrapolating Large Language Models to Non-English by Aligning Languages\"], \"Citation\": []}",
        "eval_response": "Evaluation evidence: The assistant's answer does not align with the gold answer in terms of accuracy and completeness. The assistant lists numerous references that are not mentioned in the gold answer, indicating a significant hallucination issue. Additionally, the assistant fails to include the correct citation from the gold answer, further impacting the accuracy and completeness of the response. The provided answer does not meet the requirements set by the task instructions.\n\nRating: [[10]]",
        "score": 10
    },
    {
        "level": 3,
        "set": 3,
        "length": 104205,
        "type": "paper",
        "language": "en",
        "question": "Executable Code Actions Elicit Better LLM Agents",
        "instruction": "We hope you will carefully study the provided papers and determine the citation relationships between them. Please follow the instructions below strictly to complete the task:\n\n#Specific Requirements:\n1. Reference: When a given paper mentions other provided papers, those other papers are considered as \"references\" for the given paper. To summarize in this specific context, references are about what the given paper is using.\n2. Citation: Conversely, when other provided papers mention the given paper in their works, the given paper is being \"cited\" by those other papers. To summarize in this specific context, citations are about who is using the given paper.\n3. Given a paper, you need to determine the citation or reference relationship between this paper and the other papers. Do not consider papers that are not provided.\n3. Please present the paper titles in a json format as follows: {{\"Reference\":[\"Reference Title 1\", \"Reference Title 2\", ..., \"Reference Title n\"], \"Citation\":[\"Citation Title 1\", \"Citation Title 2\", ..., \"Citation Title n\"]}}.\n4. If a paper does not have any references or citations, please leave the corresponding list empty, e.g.{{\"Refernce\":[]}}, {{\"Citation\":[]}}.",
        "prompt_template": "#Papers:\n{docs}\n\n{instruction}\n\n#The paper you need to analyze:\n{question}",
        "doc": [
            "2402.01030.md",
            "2405.16533.md",
            "2311.10081.md",
            "2402.14658.md"
        ],
        "answer": {
            "Reference": [
                "# DRESS : Instructing Large Vision-Language Models to Align and Interact with Humans via Natural Language Feedback ",
                "# OpenCodeInterpreter: Integrating Code Generation with Execution and Refinement "
            ],
            "Citation": [
                "# Chain of Tools: Large Language Model is an Automatic Multi-tool Learner "
            ]
        },
        "shuffle_doc": true,
        "id": "e01bf5a6-8431-427c-9042-af73be75fef5",
        "prompt": "[Question]\n#Papers:\n{docs}\n\nWe hope you will carefully study the provided papers and determine the citation relationships between them. Please follow the instructions below strictly to complete the task:\n\n#Specific Requirements:\n1. Reference: When a given paper mentions other provided papers, those other papers are considered as \"references\" for the given paper. To summarize in this specific context, references are about what the given paper is using.\n2. Citation: Conversely, when other provided papers mention the given paper in their works, the given paper is being \"cited\" by those other papers. To summarize in this specific context, citations are about who is using the given paper.\n3. Given a paper, you need to determine the citation or reference relationship between this paper and the other papers. Do not consider papers that are not provided.\n3. Please present the paper titles in a json format as follows: {{\"Reference\":[\"Reference Title 1\", \"Reference Title 2\", ..., \"Reference Title n\"], \"Citation\":[\"Citation Title 1\", \"Citation Title 2\", ..., \"Citation Title n\"]}}.\n4. If a paper does not have any references or citations, please leave the corresponding list empty, e.g.{{\"Refernce\":[]}}, {{\"Citation\":[]}}.\n\n#The paper you need to analyze:\nExecutable Code Actions Elicit Better LLM Agents\n\n[Gold Answer]\n{'Reference': ['# DRESS : Instructing Large Vision-Language Models to Align and Interact with Humans via Natural Language Feedback ', '# OpenCodeInterpreter: Integrating Code Generation with Execution and Refinement '], 'Citation': ['# Chain of Tools: Large Language Model is an Automatic Multi-tool Learner ']}\n\n[The Start of Assistant's Predicted Answer]\n{\"Reference\": [\"A Survey on Evaluation of Large Language Models\", \"Do as i can and not as i say: Grounding language in robotic affordances\", \"Webgpt: Browser-assisted question-answering with human feedback\", \"Augmented language models: a survey\", \"Measuring massive multitask language understanding\", \"Measuring coding challenge competence with apps\", \"Measuring mathematical problem solving with the math dataset\", \"Composable 3d value maps for robotic manipulation with language models\", \"Llama 2: Open foundation and fine-tuned chat models\", \"Voyager: An openended embodied agent with large language models\", \"Learning to generate from textual interactions\", \"Leti: Learning to generate from textual interactions\", \"Evaluating llms in multi-turn interaction with tools and language feedback\", \"Interactive planning with large language models enables open-world multi-task agents\", \"Chain-of-thought prompting elicits reasoning in large language models\", \"On the tool manipulation capability of open-source large language models\", \"Intercode: Standardizing and benchmarking interactive coding with execution feedback\", \"If llm is the wizard, then code is the wand: A survey on how code empowers large language models to serve as intelligent agents\", \"Towards unified alignment between agents, humans, and environment\", \"Webshop: Towards scalable real-world web interaction with grounded language agents\", \"React: Synergizing reasoning and acting in language models\", \"Tree of thoughts: Deliberate problem solving with large language models\", \"Retroformer: Retrospective large language agents with policy gradient optimization\", \"Customizing llms by creating and retrieving from specialized toolsets\", \"Agenttuning: Enabling generalized agent abilities for llms\", \"Prompt ensemble learning via feedback-reflect-refine\", \"Integrating code generation with execution and refinement\", \"Generally capable agents for open-world enviroments via large language models with text-based knowledge and memory\", \"Executable Python code for the LLM agent's action\", \"Collecting CodeAct-focused multi-turn interaction trajectories CodeActInstruct for instruction tuning\", \"Training language models to follow instructions with human feedback\", \"A code-first agent framework\", \"Tool learning with foundation models\", \"Facilitating large language models to master 16000+ real-world apis\", \"Toolformer: Language models can teach themselves to use tools\", \"Hugginggpt: Solving ai tasks with chatgpt and its friends in huggingface\", \"Prioritizing safeguarding over autonomy: Risks of llm agents for science\", \"Tiobe index\", \"Self-consistency improves chain of thought reasoning in language models\", \"Risks of llm agents for science\"], \"Citation\": []}\n[The End of Assistant's Predicted Answer]\n\n[System]\nWe would like to request your feedback on the performance of the AI assistant in response to the user question displayed above according to the gold answer. Please use the following listed aspects and their descriptions as evaluation criteria:\n    - Accuracy and Hallucinations: The assistant's answer is semantically consistent with the gold answer; The numerical value and order need to be accurate, and there should be no hallucinations.\n    - Completeness: Referring to the reference answers, the assistant's answer should contain all the key points needed to answer the user's question; further elaboration on these key points can be omitted.\nPlease rate whether this answer is suitable for the question. Please note that the gold answer can be considered as a correct answer to the question.\n\nThe assistant receives an overall score on a scale of 1 to 100, where a higher score indicates better overall performance.\nPlease note that if the assistant's answer and the gold answer fully meet the above criteria, its overall rating should be the full marks (100).\nPlease first provide a comprehensive explanation of your evaluation, avoiding any potential bias.\nThen, output a line indicating the score of the Assistant.\n\nPLEASE OUTPUT WITH THE FOLLOWING FORMAT, WHERE THE SCORE IS A SCALE OF 1 TO 100 BY STRICTLY FOLLOWING THIS FORMAT: \"[[score]]\", FOR EXAMPLE \"Rating: [[100]]\":\n<start output>\nEvaluation evidence: your evluation explanation here, no more than 100 words\nRating: [[score]]\n<end output>\n\nNow, start your evaluation:",
        "generate_response": "{\"Reference\": [\"A Survey on Evaluation of Large Language Models\", \"Do as i can and not as i say: Grounding language in robotic affordances\", \"Webgpt: Browser-assisted question-answering with human feedback\", \"Augmented language models: a survey\", \"Measuring massive multitask language understanding\", \"Measuring coding challenge competence with apps\", \"Measuring mathematical problem solving with the math dataset\", \"Composable 3d value maps for robotic manipulation with language models\", \"Llama 2: Open foundation and fine-tuned chat models\", \"Voyager: An openended embodied agent with large language models\", \"Learning to generate from textual interactions\", \"Leti: Learning to generate from textual interactions\", \"Evaluating llms in multi-turn interaction with tools and language feedback\", \"Interactive planning with large language models enables open-world multi-task agents\", \"Chain-of-thought prompting elicits reasoning in large language models\", \"On the tool manipulation capability of open-source large language models\", \"Intercode: Standardizing and benchmarking interactive coding with execution feedback\", \"If llm is the wizard, then code is the wand: A survey on how code empowers large language models to serve as intelligent agents\", \"Towards unified alignment between agents, humans, and environment\", \"Webshop: Towards scalable real-world web interaction with grounded language agents\", \"React: Synergizing reasoning and acting in language models\", \"Tree of thoughts: Deliberate problem solving with large language models\", \"Retroformer: Retrospective large language agents with policy gradient optimization\", \"Customizing llms by creating and retrieving from specialized toolsets\", \"Agenttuning: Enabling generalized agent abilities for llms\", \"Prompt ensemble learning via feedback-reflect-refine\", \"Integrating code generation with execution and refinement\", \"Generally capable agents for open-world enviroments via large language models with text-based knowledge and memory\", \"Executable Python code for the LLM agent's action\", \"Collecting CodeAct-focused multi-turn interaction trajectories CodeActInstruct for instruction tuning\", \"Training language models to follow instructions with human feedback\", \"A code-first agent framework\", \"Tool learning with foundation models\", \"Facilitating large language models to master 16000+ real-world apis\", \"Toolformer: Language models can teach themselves to use tools\", \"Hugginggpt: Solving ai tasks with chatgpt and its friends in huggingface\", \"Prioritizing safeguarding over autonomy: Risks of llm agents for science\", \"Tiobe index\", \"Self-consistency improves chain of thought reasoning in language models\", \"Risks of llm agents for science\"], \"Citation\": []}",
        "eval_response": "Evaluation evidence: The assistant's answer does not match the gold answer in terms of accuracy and completeness. The references listed in the assistant's answer do not align with those in the gold answer, and it incorrectly lists numerous unrelated papers. Additionally, the assistant fails to identify any citations, whereas the gold answer lists one. This indicates both inaccuracies and hallucinations in the assistant's response.\n\nRating: [[10]]",
        "score": 10
    },
    {
        "level": 3,
        "set": 2,
        "length": 62311,
        "type": "paper",
        "language": "en",
        "question": "KVQuant: Towards 10 Million Context Length LLM Inference with KV Cache Quantization",
        "instruction": "We hope you will carefully study the provided papers and determine the citation relationships between them. Please follow the instructions below strictly to complete the task:\n\n#Specific Requirements:\n1. Reference: When a given paper mentions other provided papers, those other papers are considered as \"references\" for the given paper. To summarize in this specific context, references are about what the given paper is using.\n2. Citation: Conversely, when other provided papers mention the given paper in their works, the given paper is being \"cited\" by those other papers. To summarize in this specific context, citations are about who is using the given paper.\n3. Given a paper, you need to determine the citation or reference relationship between this paper and the other papers. Do not consider papers that are not provided.\n3. Please present the paper titles in a json format as follows: {{\"Reference\":[\"Reference Title 1\", \"Reference Title 2\", ..., \"Reference Title n\"], \"Citation\":[\"Citation Title 1\", \"Citation Title 2\", ..., \"Citation Title n\"]}}.\n4. If a paper does not have any references or citations, please leave the corresponding list empty, e.g.{{\"Refernce\":[]}}, {{\"Citation\":[]}}.",
        "prompt_template": "#Papers:\n{docs}\n\n{instruction}\n\n#The paper you need to analyze:\n{question}",
        "doc": [
            "2401.18079.md",
            "2405.06219.md",
            "2405.14591.md"
        ],
        "answer": {
            "Reference": [],
            "Citation": [
                "# SKVQ: Sliding-window Key and Value Cache Quantization for Large Language Models ",
                "# Base of RoPE Bounds Context Length "
            ]
        },
        "shuffle_doc": true,
        "id": "3ababf4b-bf26-405b-8c1f-6a429c7e264d",
        "prompt": "[Question]\n#Papers:\n{docs}\n\nWe hope you will carefully study the provided papers and determine the citation relationships between them. Please follow the instructions below strictly to complete the task:\n\n#Specific Requirements:\n1. Reference: When a given paper mentions other provided papers, those other papers are considered as \"references\" for the given paper. To summarize in this specific context, references are about what the given paper is using.\n2. Citation: Conversely, when other provided papers mention the given paper in their works, the given paper is being \"cited\" by those other papers. To summarize in this specific context, citations are about who is using the given paper.\n3. Given a paper, you need to determine the citation or reference relationship between this paper and the other papers. Do not consider papers that are not provided.\n3. Please present the paper titles in a json format as follows: {{\"Reference\":[\"Reference Title 1\", \"Reference Title 2\", ..., \"Reference Title n\"], \"Citation\":[\"Citation Title 1\", \"Citation Title 2\", ..., \"Citation Title n\"]}}.\n4. If a paper does not have any references or citations, please leave the corresponding list empty, e.g.{{\"Refernce\":[]}}, {{\"Citation\":[]}}.\n\n#The paper you need to analyze:\nKVQuant: Towards 10 Million Context Length LLM Inference with KV Cache Quantization\n\n[Gold Answer]\n{'Reference': [], 'Citation': ['# SKVQ: Sliding-window Key and Value Cache Quantization for Large Language Models ', '# Base of RoPE Bounds Context Length ']}\n\n[The Start of Assistant's Predicted Answer]\n{\"Reference\": [\"Introducing claude 2.1\", \"Understanding and overcoming the challenges of efficient transformer quantization\", \"Extending context window of large language models via positional interpolation\", \"Longlora: Efficient fine-tuning of long-context large language models\", \"Llm.int8 (): 8bit matrix multiplication for transformers at scale\", \"Qlora: Efficient finetuning of quantized llms\", \"Spqr: A sparse-quantized representation for near-lossless llm weight compression\", \"Hawq-v2: Hessian aware trace-weighted quantization of neural networks\", \"Hawq: Hessian aware quantization of neural networks with mixed-precision\", \"Balanced csr sparse matrix-vector product on graphics processors\", \"A framework for few-shot language model evaluation\", \"Model tells you what to discard: Adaptive kv cache compression for llms\", \"Ai and memory wall\", \"Lm-infinite: Simple on-the-fly length generalization for large language models\", \"Measuring massive multitask language understanding\", \"Rethinking channel dimensions to isolate outliers for low-bit weight quantization of large language models\", \"Mistral 7b\", \"Squeezellm: Dense-and-sparse quantization\", \"Full stack optimization of transformer inference: a survey\", \"How long can context length of opensource llms truly promise?\", \"Norm tweaking: Highperformance low-bit quantization of large language models\", \"Awq: Activation-aware weight quantization for llm compression and acceleration\", \"Intactkv: Improving large language model quantization by keeping pivot tokens intact\", \"Llmqat: Data-free quantization aware training for large language models\", \"Scissorhands: Exploiting the persistence of importance hypothesis for llm kv cache compression at test time\", \"Kivi: Plug-and-play 2bit kv cache quantization with streaming asymmetric quantization\", \"Landmark attention: Random-access infinite context length for transformers\", \"Sensitivity and generalization in neural networks: an empirical study\", \"New models and developer products announced at devday 2023\", \"Sparq attention: Bandwidth-efficient llm inference\", \"Omniquant: Omnidirectionally calibrated quantization for large language models\", \"Flexgen: Highthroughput generative inference of large language models with a single gpu\", \"Roformer: Enhanced transformer with rotary position embedding\", \"Llama: Open and efficient foundation language models\", \"Llama 2: Open foundation and fine-tuned chat models\", \"Outlier suppression+: Accurate quantization of large language models by equivalent and optimal shifting and scaling\", \"Outlier suppression: Pushing the limit of low-bit transformer language models\", \"Smoothquant: Accurate and efficient post-training quantization for large language models\", \"Efficient streaming language models with attention sinks\", \"H 2 o Heavy-hitter oracle for efficient generative inference of large language models\", \"Atom: Low-bit quantization for efficient and accurate $1 \\\\mathrm{~lm}$ serving\"], \"Citation\": []}\n[The End of Assistant's Predicted Answer]\n\n[System]\nWe would like to request your feedback on the performance of the AI assistant in response to the user question displayed above according to the gold answer. Please use the following listed aspects and their descriptions as evaluation criteria:\n    - Accuracy and Hallucinations: The assistant's answer is semantically consistent with the gold answer; The numerical value and order need to be accurate, and there should be no hallucinations.\n    - Completeness: Referring to the reference answers, the assistant's answer should contain all the key points needed to answer the user's question; further elaboration on these key points can be omitted.\nPlease rate whether this answer is suitable for the question. Please note that the gold answer can be considered as a correct answer to the question.\n\nThe assistant receives an overall score on a scale of 1 to 100, where a higher score indicates better overall performance.\nPlease note that if the assistant's answer and the gold answer fully meet the above criteria, its overall rating should be the full marks (100).\nPlease first provide a comprehensive explanation of your evaluation, avoiding any potential bias.\nThen, output a line indicating the score of the Assistant.\n\nPLEASE OUTPUT WITH THE FOLLOWING FORMAT, WHERE THE SCORE IS A SCALE OF 1 TO 100 BY STRICTLY FOLLOWING THIS FORMAT: \"[[score]]\", FOR EXAMPLE \"Rating: [[100]]\":\n<start output>\nEvaluation evidence: your evluation explanation here, no more than 100 words\nRating: [[score]]\n<end output>\n\nNow, start your evaluation:",
        "generate_response": "{\"Reference\": [\"Introducing claude 2.1\", \"Understanding and overcoming the challenges of efficient transformer quantization\", \"Extending context window of large language models via positional interpolation\", \"Longlora: Efficient fine-tuning of long-context large language models\", \"Llm.int8 (): 8bit matrix multiplication for transformers at scale\", \"Qlora: Efficient finetuning of quantized llms\", \"Spqr: A sparse-quantized representation for near-lossless llm weight compression\", \"Hawq-v2: Hessian aware trace-weighted quantization of neural networks\", \"Hawq: Hessian aware quantization of neural networks with mixed-precision\", \"Balanced csr sparse matrix-vector product on graphics processors\", \"A framework for few-shot language model evaluation\", \"Model tells you what to discard: Adaptive kv cache compression for llms\", \"Ai and memory wall\", \"Lm-infinite: Simple on-the-fly length generalization for large language models\", \"Measuring massive multitask language understanding\", \"Rethinking channel dimensions to isolate outliers for low-bit weight quantization of large language models\", \"Mistral 7b\", \"Squeezellm: Dense-and-sparse quantization\", \"Full stack optimization of transformer inference: a survey\", \"How long can context length of opensource llms truly promise?\", \"Norm tweaking: Highperformance low-bit quantization of large language models\", \"Awq: Activation-aware weight quantization for llm compression and acceleration\", \"Intactkv: Improving large language model quantization by keeping pivot tokens intact\", \"Llmqat: Data-free quantization aware training for large language models\", \"Scissorhands: Exploiting the persistence of importance hypothesis for llm kv cache compression at test time\", \"Kivi: Plug-and-play 2bit kv cache quantization with streaming asymmetric quantization\", \"Landmark attention: Random-access infinite context length for transformers\", \"Sensitivity and generalization in neural networks: an empirical study\", \"New models and developer products announced at devday 2023\", \"Sparq attention: Bandwidth-efficient llm inference\", \"Omniquant: Omnidirectionally calibrated quantization for large language models\", \"Flexgen: Highthroughput generative inference of large language models with a single gpu\", \"Roformer: Enhanced transformer with rotary position embedding\", \"Llama: Open and efficient foundation language models\", \"Llama 2: Open foundation and fine-tuned chat models\", \"Outlier suppression+: Accurate quantization of large language models by equivalent and optimal shifting and scaling\", \"Outlier suppression: Pushing the limit of low-bit transformer language models\", \"Smoothquant: Accurate and efficient post-training quantization for large language models\", \"Efficient streaming language models with attention sinks\", \"H 2 o Heavy-hitter oracle for efficient generative inference of large language models\", \"Atom: Low-bit quantization for efficient and accurate $1 \\\\mathrm{~lm}$ serving\"], \"Citation\": []}",
        "eval_response": "Evaluation evidence: The assistant's answer does not align with the gold answer. The gold answer specifies two citations for the paper \"KVQuant: Towards 10 Million Context Length LLM Inference with KV Cache Quantization,\" but the assistant's answer incorrectly lists no citations and includes a long list of references not mentioned in the gold answer. This indicates both inaccuracies and hallucinations in the assistant's response.\n\nRating: [[10]]",
        "score": 10
    },
    {
        "level": 3,
        "set": 1,
        "length": 23811,
        "type": "paper",
        "language": "en",
        "question": "LLMs Can't Plan, But Can Help Planning in LLM-Modulo Frameworks",
        "instruction": "We hope you will carefully study the provided papers and determine the citation relationships between them. Please follow the instructions below strictly to complete the task:\n\n#Specific Requirements:\n1. Reference: When a given paper mentions other provided papers, those other papers are considered as \"references\" for the given paper. To summarize in this specific context, references are about what the given paper is using.\n2. Citation: Conversely, when other provided papers mention the given paper in their works, the given paper is being \"cited\" by those other papers. To summarize in this specific context, citations are about who is using the given paper.\n3. Given a paper, you need to determine the citation or reference relationship between this paper and the other papers. Do not consider papers that are not provided.\n3. Please present the paper titles in a json format as follows: {{\"Reference\":[\"Reference Title 1\", \"Reference Title 2\", ..., \"Reference Title n\"], \"Citation\":[\"Citation Title 1\", \"Citation Title 2\", ..., \"Citation Title n\"]}}.\n4. If a paper does not have any references or citations, please leave the corresponding list empty, e.g.{{\"Refernce\":[]}}, {{\"Citation\":[]}}.",
        "prompt_template": "#Papers:\n{docs}\n\n{instruction}\n\n#The paper you need to analyze:\n{question}",
        "doc": [
            "2402.01817.md",
            "2405.05885.md",
            "2310.08118.md"
        ],
        "answer": {
            "Reference": [
                "# Can Large Language Models Really Improve by Self-critiquing Their Own Plans? "
            ],
            "Citation": [
                "# Co-driver: VLM-based Autonomous Driving Assistant with Human-like Behavior and Understanding for Complex Road Scenes "
            ]
        },
        "shuffle_doc": true,
        "id": "74df278b-9320-46dc-8c71-0aea2419aafa",
        "prompt": "[Question]\n#Papers:\n{docs}\n\nWe hope you will carefully study the provided papers and determine the citation relationships between them. Please follow the instructions below strictly to complete the task:\n\n#Specific Requirements:\n1. Reference: When a given paper mentions other provided papers, those other papers are considered as \"references\" for the given paper. To summarize in this specific context, references are about what the given paper is using.\n2. Citation: Conversely, when other provided papers mention the given paper in their works, the given paper is being \"cited\" by those other papers. To summarize in this specific context, citations are about who is using the given paper.\n3. Given a paper, you need to determine the citation or reference relationship between this paper and the other papers. Do not consider papers that are not provided.\n3. Please present the paper titles in a json format as follows: {{\"Reference\":[\"Reference Title 1\", \"Reference Title 2\", ..., \"Reference Title n\"], \"Citation\":[\"Citation Title 1\", \"Citation Title 2\", ..., \"Citation Title n\"]}}.\n4. If a paper does not have any references or citations, please leave the corresponding list empty, e.g.{{\"Refernce\":[]}}, {{\"Citation\":[]}}.\n\n#The paper you need to analyze:\nLLMs Can't Plan, But Can Help Planning in LLM-Modulo Frameworks\n\n[Gold Answer]\n{'Reference': ['# Can Large Language Models Really Improve by Self-critiquing Their Own Plans? '], 'Citation': ['# Co-driver: VLM-based Autonomous Driving Assistant with Human-like Behavior and Understanding for Complex Road Scenes ']}\n\n[The Start of Assistant's Predicted Answer]\n{\"Reference\": [\"A Survey on Evaluation of Large Language Models\", \"Leveraging approximate symbolic models for reinforcement learning via skill diversity\", \"Leveraging pre-trained large language models to construct and utilize world models for model-based task planning\", \"Reasoning with language model is planning with world model\", \"Understanding the capabilities of large language models for automated planning\", \"Logic-lm: Empowering large language models with symbolic solvers for faithful logical reasoning\", \"Saynav: Grounding large language models for dynamic planning to navigation in new environments\", \"Mathematical discoveries from program search with large language models\", \"PDDL planning with pretrained large language models\", \"Gpt-4 doesn't know it's wrong: An analysis of iterative prompting for reasoning problems\", \"Solving olympiad geometry without human demonstrations\", \"Large language models fail on trivial alterations to theory-of-mind tasks\", \"Can large language models really improve by self-critiquing their own plans?\", \"Planbench: An extensible benchmark for evaluating large language models on planning and reasoning about change\", \"On the planning abilities of large language models - a critical investigation\", \"Theory of mind abilities of large language models in human-robot interaction: An illusion?\", \"Voyager: An open-ended embodied agent with large language models\", \"Self-instruct: Aligning language model with self generated instructions\", \"Large language models are better reasoners with self-verification\", \"Translating natural language to planning goals with large-language models\", \"Tree of thoughts: Deliberate problem solving with large language models\", \"React: Synergizing reasoning and acting in language models\"], \"Citation\": []}\n[The End of Assistant's Predicted Answer]\n\n[System]\nWe would like to request your feedback on the performance of the AI assistant in response to the user question displayed above according to the gold answer. Please use the following listed aspects and their descriptions as evaluation criteria:\n    - Accuracy and Hallucinations: The assistant's answer is semantically consistent with the gold answer; The numerical value and order need to be accurate, and there should be no hallucinations.\n    - Completeness: Referring to the reference answers, the assistant's answer should contain all the key points needed to answer the user's question; further elaboration on these key points can be omitted.\nPlease rate whether this answer is suitable for the question. Please note that the gold answer can be considered as a correct answer to the question.\n\nThe assistant receives an overall score on a scale of 1 to 100, where a higher score indicates better overall performance.\nPlease note that if the assistant's answer and the gold answer fully meet the above criteria, its overall rating should be the full marks (100).\nPlease first provide a comprehensive explanation of your evaluation, avoiding any potential bias.\nThen, output a line indicating the score of the Assistant.\n\nPLEASE OUTPUT WITH THE FOLLOWING FORMAT, WHERE THE SCORE IS A SCALE OF 1 TO 100 BY STRICTLY FOLLOWING THIS FORMAT: \"[[score]]\", FOR EXAMPLE \"Rating: [[100]]\":\n<start output>\nEvaluation evidence: your evluation explanation here, no more than 100 words\nRating: [[score]]\n<end output>\n\nNow, start your evaluation:",
        "generate_response": "{\"Reference\": [\"A Survey on Evaluation of Large Language Models\", \"Leveraging approximate symbolic models for reinforcement learning via skill diversity\", \"Leveraging pre-trained large language models to construct and utilize world models for model-based task planning\", \"Reasoning with language model is planning with world model\", \"Understanding the capabilities of large language models for automated planning\", \"Logic-lm: Empowering large language models with symbolic solvers for faithful logical reasoning\", \"Saynav: Grounding large language models for dynamic planning to navigation in new environments\", \"Mathematical discoveries from program search with large language models\", \"PDDL planning with pretrained large language models\", \"Gpt-4 doesn't know it's wrong: An analysis of iterative prompting for reasoning problems\", \"Solving olympiad geometry without human demonstrations\", \"Large language models fail on trivial alterations to theory-of-mind tasks\", \"Can large language models really improve by self-critiquing their own plans?\", \"Planbench: An extensible benchmark for evaluating large language models on planning and reasoning about change\", \"On the planning abilities of large language models - a critical investigation\", \"Theory of mind abilities of large language models in human-robot interaction: An illusion?\", \"Voyager: An open-ended embodied agent with large language models\", \"Self-instruct: Aligning language model with self generated instructions\", \"Large language models are better reasoners with self-verification\", \"Translating natural language to planning goals with large-language models\", \"Tree of thoughts: Deliberate problem solving with large language models\", \"React: Synergizing reasoning and acting in language models\"], \"Citation\": []}",
        "eval_response": "Evaluation evidence: The assistant's answer is inaccurate and contains hallucinations. The provided answer lists numerous references that are not mentioned in the gold answer, indicating a significant deviation from the correct response. The gold answer specifies one reference and one citation, both of which are absent in the assistant's response. This shows a lack of accuracy and introduces incorrect information, failing to meet the requirements of the task.\n\nRating: [[10]]",
        "score": 10
    },
    {
        "level": 3,
        "set": 3,
        "length": 100145,
        "type": "paper",
        "language": "en",
        "question": "KTO: Model Alignment as Prospect Theoretic Optimization",
        "instruction": "We hope you will carefully study the provided papers and determine the citation relationships between them. Please follow the instructions below strictly to complete the task:\n\n#Specific Requirements:\n1. Reference: When a given paper mentions other provided papers, those other papers are considered as \"references\" for the given paper. To summarize in this specific context, references are about what the given paper is using.\n2. Citation: Conversely, when other provided papers mention the given paper in their works, the given paper is being \"cited\" by those other papers. To summarize in this specific context, citations are about who is using the given paper.\n3. Given a paper, you need to determine the citation or reference relationship between this paper and the other papers. Do not consider papers that are not provided.\n3. Please present the paper titles in a json format as follows: {{\"Reference\":[\"Reference Title 1\", \"Reference Title 2\", ..., \"Reference Title n\"], \"Citation\":[\"Citation Title 1\", \"Citation Title 2\", ..., \"Citation Title n\"]}}.\n4. If a paper does not have any references or citations, please leave the corresponding list empty, e.g.{{\"Refernce\":[]}}, {{\"Citation\":[]}}.",
        "prompt_template": "#Papers:\n{docs}\n\n{instruction}\n\n#The paper you need to analyze:\n{question}",
        "doc": [
            "2402.01306.md",
            "2405.20175.md",
            "2405.20830.md",
            "2405.21046.md"
        ],
        "answer": {
            "Reference": [],
            "Citation": [
                "# InstructionCP: A fast approach to transfer Large Language Models into target language ",
                "# Self-Augmented Preference Optimization: Off-Policy Paradigms for Language Model Alignment ",
                "# Exploratory Preference Optimization: "
            ]
        },
        "shuffle_doc": true,
        "id": "8f43c67e-e96d-4c9e-a54a-04e1217cccce",
        "prompt": "[Question]\n#Papers:\n{docs}\n\nWe hope you will carefully study the provided papers and determine the citation relationships between them. Please follow the instructions below strictly to complete the task:\n\n#Specific Requirements:\n1. Reference: When a given paper mentions other provided papers, those other papers are considered as \"references\" for the given paper. To summarize in this specific context, references are about what the given paper is using.\n2. Citation: Conversely, when other provided papers mention the given paper in their works, the given paper is being \"cited\" by those other papers. To summarize in this specific context, citations are about who is using the given paper.\n3. Given a paper, you need to determine the citation or reference relationship between this paper and the other papers. Do not consider papers that are not provided.\n3. Please present the paper titles in a json format as follows: {{\"Reference\":[\"Reference Title 1\", \"Reference Title 2\", ..., \"Reference Title n\"], \"Citation\":[\"Citation Title 1\", \"Citation Title 2\", ..., \"Citation Title n\"]}}.\n4. If a paper does not have any references or citations, please leave the corresponding list empty, e.g.{{\"Refernce\":[]}}, {{\"Citation\":[]}}.\n\n#The paper you need to analyze:\nKTO: Model Alignment as Prospect Theoretic Optimization\n\n[Gold Answer]\n{'Reference': [], 'Citation': ['# InstructionCP: A fast approach to transfer Large Language Models into target language ', '# Self-Augmented Preference Optimization: Off-Policy Paradigms for Language Model Alignment ', '# Exploratory Preference Optimization: ']}\n\n[The Start of Assistant's Predicted Answer]\n{\"Reference\":[\"Pythia: A suite for analyzing large language models across training and scaling\", \"Rank analysis of incomplete block designs: I. the method of paired comparisons\", \"Preference-based reinforcement learning: evolutionary direct policy search using a preference-based racing algorithm\", \"Open problems and fundamental limitations of reinforcement learning from human feedback\", \"Human irrationality: both bad and good for reward inference\", \"Evaluating large language models trained on code\", \"Self-play fine-tuning converts weak language models to strong language models\", \"Deep reinforcement learning from human preferences\", \"Training verifiers to solve math word problems\", \"Ultrafeedback: Boosting language models with high-quality feedback\", \"Understanding dataset difficulty with $\\mathcal{V}$-usable information\", \"Red teaming language models to reduce harms: Methods, scaling behaviors, and lessons learned\", \"Aligning language models with preferences through f-divergence minimization\", \"Decision-making under uncertainty-a field study of cumulative prospect theory\", \"Neural collaborative filtering\", \"Measuring massive multitask language understanding\", \"Constructing stable preferences: A look into dimensions of experience and their impact on preference stability\", \"A simple sequentially rejective multiple test procedure\", \"Learning trajectory preferences for manipulators via iterative improvement\", \"Mistral 7b\", \"Prospect theory: An analysis of decision under risk\", \"Openassistant conversationsdemocratizing large language model alignment\", \"Reliability and learnability of human bandit feedback for sequence-to-sequence reinforcement learning\", \"When humans aren't optimal: Robots that collaborate with risk-aware humans\", \"Alpacaeval: An automatic evaluator of instruction-following models\", \"Training language models to follow instructions with human feedback\", \"Advantage-weighted regression: Simple and scalable off-policy reinforcement learning\", \"Reinforcement learning by reward-weighted regression for operational space control\", \"Direct preference optimization: Your language model is secretly a reward model\", \"Proximal policy optimization algorithms\", \"Beyond the imitation game: Quantifying and extrapolating the capabilities of language models\", \"Learning to summarize with human feedback\", \"Interpretable modelling of driving behaviors in interactive driving scenarios based on cumulative prospect theory\", \"Fine-tuning language models for factuality\", \"Llama: Open and efficient foundation language models\", \"Zephyr: Direct distillation of lm alignment\", \"Advances in prospect theory: Cumulative representation of uncertainty\", \"Transformer reinforcement learning\", \"Neural text generation with unlikelihood training\", \"Self-rewarding language models\", \"Slic-hf: Sequence likelihood calibration with human feedback\", \"Judging llm-as-a-judge with mt-bench and chatbot arena\"], \"Citation\":[]}\n[The End of Assistant's Predicted Answer]\n\n[System]\nWe would like to request your feedback on the performance of the AI assistant in response to the user question displayed above according to the gold answer. Please use the following listed aspects and their descriptions as evaluation criteria:\n    - Accuracy and Hallucinations: The assistant's answer is semantically consistent with the gold answer; The numerical value and order need to be accurate, and there should be no hallucinations.\n    - Completeness: Referring to the reference answers, the assistant's answer should contain all the key points needed to answer the user's question; further elaboration on these key points can be omitted.\nPlease rate whether this answer is suitable for the question. Please note that the gold answer can be considered as a correct answer to the question.\n\nThe assistant receives an overall score on a scale of 1 to 100, where a higher score indicates better overall performance.\nPlease note that if the assistant's answer and the gold answer fully meet the above criteria, its overall rating should be the full marks (100).\nPlease first provide a comprehensive explanation of your evaluation, avoiding any potential bias.\nThen, output a line indicating the score of the Assistant.\n\nPLEASE OUTPUT WITH THE FOLLOWING FORMAT, WHERE THE SCORE IS A SCALE OF 1 TO 100 BY STRICTLY FOLLOWING THIS FORMAT: \"[[score]]\", FOR EXAMPLE \"Rating: [[100]]\":\n<start output>\nEvaluation evidence: your evluation explanation here, no more than 100 words\nRating: [[score]]\n<end output>\n\nNow, start your evaluation:",
        "generate_response": "{\"Reference\":[\"Pythia: A suite for analyzing large language models across training and scaling\", \"Rank analysis of incomplete block designs: I. the method of paired comparisons\", \"Preference-based reinforcement learning: evolutionary direct policy search using a preference-based racing algorithm\", \"Open problems and fundamental limitations of reinforcement learning from human feedback\", \"Human irrationality: both bad and good for reward inference\", \"Evaluating large language models trained on code\", \"Self-play fine-tuning converts weak language models to strong language models\", \"Deep reinforcement learning from human preferences\", \"Training verifiers to solve math word problems\", \"Ultrafeedback: Boosting language models with high-quality feedback\", \"Understanding dataset difficulty with $\\mathcal{V}$-usable information\", \"Red teaming language models to reduce harms: Methods, scaling behaviors, and lessons learned\", \"Aligning language models with preferences through f-divergence minimization\", \"Decision-making under uncertainty-a field study of cumulative prospect theory\", \"Neural collaborative filtering\", \"Measuring massive multitask language understanding\", \"Constructing stable preferences: A look into dimensions of experience and their impact on preference stability\", \"A simple sequentially rejective multiple test procedure\", \"Learning trajectory preferences for manipulators via iterative improvement\", \"Mistral 7b\", \"Prospect theory: An analysis of decision under risk\", \"Openassistant conversationsdemocratizing large language model alignment\", \"Reliability and learnability of human bandit feedback for sequence-to-sequence reinforcement learning\", \"When humans aren't optimal: Robots that collaborate with risk-aware humans\", \"Alpacaeval: An automatic evaluator of instruction-following models\", \"Training language models to follow instructions with human feedback\", \"Advantage-weighted regression: Simple and scalable off-policy reinforcement learning\", \"Reinforcement learning by reward-weighted regression for operational space control\", \"Direct preference optimization: Your language model is secretly a reward model\", \"Proximal policy optimization algorithms\", \"Beyond the imitation game: Quantifying and extrapolating the capabilities of language models\", \"Learning to summarize with human feedback\", \"Interpretable modelling of driving behaviors in interactive driving scenarios based on cumulative prospect theory\", \"Fine-tuning language models for factuality\", \"Llama: Open and efficient foundation language models\", \"Zephyr: Direct distillation of lm alignment\", \"Advances in prospect theory: Cumulative representation of uncertainty\", \"Transformer reinforcement learning\", \"Neural text generation with unlikelihood training\", \"Self-rewarding language models\", \"Slic-hf: Sequence likelihood calibration with human feedback\", \"Judging llm-as-a-judge with mt-bench and chatbot arena\"], \"Citation\":[]}",
        "eval_response": "Evaluation evidence: The assistant's answer is inaccurate and contains hallucinations. The provided answer lists numerous references that are not mentioned in the gold answer, indicating a failure to correctly identify the citation relationships for the paper \"KTO: Model Alignment as Prospect Theoretic Optimization.\" The gold answer specifies that there are no references and lists three citations, none of which appear in the assistant's response. This discrepancy shows a lack of accuracy and the presence of hallucinated content.\n\nRating: [[10]]",
        "score": 10
    },
    {
        "level": 4,
        "set": 3,
        "length": 138138,
        "type": "paper",
        "language": "en",
        "question": "",
        "instruction": "We kindly ask you to thoroughly review the provided papers and construct a citation chain from them. Please adhere to the following instructions strictly while completing the task:\n\n#Task Instructions:\nGiven several papers, you are required to identify and list the longest citation chain, which demonstrates the citation relationship among the provided papers.\n\n#Specific Requirements:\n1.Please present the titles of the papers in the form of a list, as follows: [\"Title of Paper 1\", \"Title of Paper 2\", ..., \"Title of Paper n\"].\n2.Ensure that the citation chain in the list is linear and continuous, meaning that the first paper title in the list (Paper 1) should not cite any other works. Instead, it should be cited by the next paper in the list (Paper 2); subsequently, each paper should then be cited by the next one in the list, continuing up to the last paper (Paper n).\n3.Consider only the citation relationships within the supplied collection of papers, and ensure that the citation chain accurately reflects the sequential citation order among these documents.\n4.Do not take into account any articles not provided, and disregard other non-linear citation relationships.",
        "prompt_template": "{instruction}\n\n#Papers Provided:\n{docs}",
        "doc": [
            "2210.09953.md",
            "2111.14641.md",
            "2210.09953.md"
        ],
        "answer": [
            "# Randomized Cholesky QR factorizations ",
            "# Randomized block Gram-Schmidt process for the solution of linear systems and eigenvalue problems. ",
            "# Randomized Cholesky QR factorizations "
        ],
        "shuffle_doc": true,
        "id": "b07b0e66-826e-4b9e-bc2e-af466d90a378",
        "prompt": "[Question]\nWe kindly ask you to thoroughly review the provided papers and construct a citation chain from them. Please adhere to the following instructions strictly while completing the task:\n\n#Task Instructions:\nGiven several papers, you are required to identify and list the longest citation chain, which demonstrates the citation relationship among the provided papers.\n\n#Specific Requirements:\n1.Please present the titles of the papers in the form of a list, as follows: [\"Title of Paper 1\", \"Title of Paper 2\", ..., \"Title of Paper n\"].\n2.Ensure that the citation chain in the list is linear and continuous, meaning that the first paper title in the list (Paper 1) should not cite any other works. Instead, it should be cited by the next paper in the list (Paper 2); subsequently, each paper should then be cited by the next one in the list, continuing up to the last paper (Paper n).\n3.Consider only the citation relationships within the supplied collection of papers, and ensure that the citation chain accurately reflects the sequential citation order among these documents.\n4.Do not take into account any articles not provided, and disregard other non-linear citation relationships.\n\n#Papers Provided:\n{docs}\n\n[Gold Answer]\n['# Randomized Cholesky QR factorizations ', '# Randomized block Gram-Schmidt process for the solution of linear systems and eigenvalue problems. ', '# Randomized Cholesky QR factorizations ']\n\n[The Start of Assistant's Predicted Answer]\n[\"Randomized Cholesky QR factorizations\", \"A Novel Randomized XR-Based Preconditioned CholeksyQR Algorithm\", \"CholeskyQR2: a simple and communication-avoiding algorithm for computing a tall-skinny QR factorization\", \"Shifted Cholesky QR for computing the QR factorization of ill-conditioned matrices\", \"Mixed-precision Cholesky QR factorization and its case studies on multicore CPU with multiple GPUs\"]\n[The End of Assistant's Predicted Answer]\n\n[System]\nWe would like to request your feedback on the performance of the AI assistant in response to the user question displayed above according to the gold answer. Please use the following listed aspects and their descriptions as evaluation criteria:\n    - Accuracy and Hallucinations: The assistant's answer is semantically consistent with the gold answer; The numerical value and order need to be accurate, and there should be no hallucinations.\n    - Completeness: Referring to the reference answers, the assistant's answer should contain all the key points needed to answer the user's question; further elaboration on these key points can be omitted.\nPlease rate whether this answer is suitable for the question. Please note that the gold answer can be considered as a correct answer to the question.\n\nThe assistant receives an overall score on a scale of 1 to 100, where a higher score indicates better overall performance.\nPlease note that if the assistant's answer and the gold answer fully meet the above criteria, its overall rating should be the full marks (100).\nPlease first provide a comprehensive explanation of your evaluation, avoiding any potential bias.\nThen, output a line indicating the score of the Assistant.\n\nPLEASE OUTPUT WITH THE FOLLOWING FORMAT, WHERE THE SCORE IS A SCALE OF 1 TO 100 BY STRICTLY FOLLOWING THIS FORMAT: \"[[score]]\", FOR EXAMPLE \"Rating: [[100]]\":\n<start output>\nEvaluation evidence: your evluation explanation here, no more than 100 words\nRating: [[score]]\n<end output>\n\nNow, start your evaluation:",
        "generate_response": "[\"Randomized Cholesky QR factorizations\", \"A Novel Randomized XR-Based Preconditioned CholeksyQR Algorithm\", \"CholeskyQR2: a simple and communication-avoiding algorithm for computing a tall-skinny QR factorization\", \"Shifted Cholesky QR for computing the QR factorization of ill-conditioned matrices\", \"Mixed-precision Cholesky QR factorization and its case studies on multicore CPU with multiple GPUs\"]",
        "eval_response": "Evaluation evidence: The assistant's answer does not match the gold answer in terms of the titles and the sequence of the citation chain. The gold answer lists the same paper title twice, indicating a possible error or misunderstanding in the task, while the assistant's answer provides a list of different titles, none of which match the gold answer. This discrepancy suggests a misunderstanding of the task requirements or the provided data.\n\nRating: [[10]]",
        "score": 10
    },
    {
        "level": 3,
        "set": 2,
        "length": 83885,
        "type": "paper",
        "language": "en",
        "question": "Patchscopes: A Unifying Framework for Inspecting Hidden Representations of Language Models",
        "instruction": "We hope you will carefully study the provided papers and determine the citation relationships between them. Please follow the instructions below strictly to complete the task:\n\n#Specific Requirements:\n1. Reference: When a given paper mentions other provided papers, those other papers are considered as \"references\" for the given paper. To summarize in this specific context, references are about what the given paper is using.\n2. Citation: Conversely, when other provided papers mention the given paper in their works, the given paper is being \"cited\" by those other papers. To summarize in this specific context, citations are about who is using the given paper.\n3. Given a paper, you need to determine the citation or reference relationship between this paper and the other papers. Do not consider papers that are not provided.\n3. Please present the paper titles in a json format as follows: {{\"Reference\":[\"Reference Title 1\", \"Reference Title 2\", ..., \"Reference Title n\"], \"Citation\":[\"Citation Title 1\", \"Citation Title 2\", ..., \"Citation Title n\"]}}.\n4. If a paper does not have any references or citations, please leave the corresponding list empty, e.g.{{\"Refernce\":[]}}, {{\"Citation\":[]}}.",
        "prompt_template": "#Papers:\n{docs}\n\n{instruction}\n\n#The paper you need to analyze:\n{question}",
        "doc": [
            "2401.06102.md",
            "2404.05971.md",
            "2310.16570.md",
            "2310.18969.md"
        ],
        "answer": {
            "Reference": [
                "# Give Me the Facts! A Survey on Factual Knowledge Probing in Pre-trained Language Models ",
                "# Analyzing Vision Transformers for Image Classification in Class Embedding Space "
            ],
            "Citation": [
                "# Does Transformer Interpretability Transfer to RNNs? "
            ]
        },
        "shuffle_doc": true,
        "id": "52fe3511-aeea-4c20-b630-dfa9604553d9",
        "prompt": "[Question]\n#Papers:\n{docs}\n\nWe hope you will carefully study the provided papers and determine the citation relationships between them. Please follow the instructions below strictly to complete the task:\n\n#Specific Requirements:\n1. Reference: When a given paper mentions other provided papers, those other papers are considered as \"references\" for the given paper. To summarize in this specific context, references are about what the given paper is using.\n2. Citation: Conversely, when other provided papers mention the given paper in their works, the given paper is being \"cited\" by those other papers. To summarize in this specific context, citations are about who is using the given paper.\n3. Given a paper, you need to determine the citation or reference relationship between this paper and the other papers. Do not consider papers that are not provided.\n3. Please present the paper titles in a json format as follows: {{\"Reference\":[\"Reference Title 1\", \"Reference Title 2\", ..., \"Reference Title n\"], \"Citation\":[\"Citation Title 1\", \"Citation Title 2\", ..., \"Citation Title n\"]}}.\n4. If a paper does not have any references or citations, please leave the corresponding list empty, e.g.{{\"Refernce\":[]}}, {{\"Citation\":[]}}.\n\n#The paper you need to analyze:\nPatchscopes: A Unifying Framework for Inspecting Hidden Representations of Language Models\n\n[Gold Answer]\n{'Reference': ['# Give Me the Facts! A Survey on Factual Knowledge Probing in Pre-trained Language Models ', '# Analyzing Vision Transformers for Image Classification in Class Embedding Space '], 'Citation': ['# Does Transformer Interpretability Transfer to RNNs? ']}\n\n[The Start of Assistant's Predicted Answer]\n{\"Reference\":[\"Analyzing Transformers in Embedding Space\", \"Jump to Conclusions: Short-Cutting Transformers with Linear Transformations\", \"The Pile: An 800GB Dataset of Diverse Text for Language Modeling\", \"Causal Abstractions of Neural Networks\", \"A Survey on Interpreting the Inner Structures of Deep Neural Networks\", \"Vicuna: An Open-Source Chatbot Impressing GPT-4 with $90 \\% *$ ChatGPT quality\", \"Towards Automated Circuit Discovery for Mechanistic Interpretability\", \"Decoderlens: Layerwise Interpretation of Encoder-Decoder Transformers\", \"ROUGE: A Package for Automatic Evaluation of Summaries\", \"Eliciting Latent Predictions from Transformers with the Tuned Lens\", \"Graph of Thoughts: Solving Elaborate Problems with Large Language Models\", \"Interpreting GPT: The Logit Lens\", \"Future Lens: Anticipating Subsequent Tokens from a Single Hidden State\", \"Mapping Language Models to Grounded Conceptual Spaces\", \"Sentence-bert: Sentence Embeddings Using Siamese Bert-Networks\", \"Multimodal Neurons in Pretrained Text-Only Transformers\", \"Explaining Black Box Text Modules in Natural Language with Language Models\", \"A Mechanistic Interpretation of Arithmetic Reasoning in Language Models Using Causal Mediation Analysis\", \"BERTnesia: Investigating the Capture and Forgetting of Knowledge in BERT\", \"In-Context Learning Creates Task Vectors\", \"Mass-Editing Memory in a Transformer\", \"Locating and Editing Factual Associations in GPT\", \"Chain-of-Thought Prompting Elicits Reasoning in Large Language Models\", \"Tree of Thoughts: Deliberate Problem Solving with Large Language Models\", \"Give Me the Facts! A Survey on Factual Knowledge Probing in Pre-trained Language Models\", \"Towards Best Practices of Activation Patching in Language Models: Metrics and Methods\", \"A Mechanism for Solving Relational Tasks in Transformer Language Models\", \"Dissecting Recall of Factual Associations in Auto-regressive Language Models\", \"When Not to Trust Language Models: Investigating Effectiveness of Parametric and Nonparametric Memories\", \"Can LLMs Facilitate Interpretation of Pre-Trained Language Models?\"], \"Citation\":[]}\n[The End of Assistant's Predicted Answer]\n\n[System]\nWe would like to request your feedback on the performance of the AI assistant in response to the user question displayed above according to the gold answer. Please use the following listed aspects and their descriptions as evaluation criteria:\n    - Accuracy and Hallucinations: The assistant's answer is semantically consistent with the gold answer; The numerical value and order need to be accurate, and there should be no hallucinations.\n    - Completeness: Referring to the reference answers, the assistant's answer should contain all the key points needed to answer the user's question; further elaboration on these key points can be omitted.\nPlease rate whether this answer is suitable for the question. Please note that the gold answer can be considered as a correct answer to the question.\n\nThe assistant receives an overall score on a scale of 1 to 100, where a higher score indicates better overall performance.\nPlease note that if the assistant's answer and the gold answer fully meet the above criteria, its overall rating should be the full marks (100).\nPlease first provide a comprehensive explanation of your evaluation, avoiding any potential bias.\nThen, output a line indicating the score of the Assistant.\n\nPLEASE OUTPUT WITH THE FOLLOWING FORMAT, WHERE THE SCORE IS A SCALE OF 1 TO 100 BY STRICTLY FOLLOWING THIS FORMAT: \"[[score]]\", FOR EXAMPLE \"Rating: [[100]]\":\n<start output>\nEvaluation evidence: your evluation explanation here, no more than 100 words\nRating: [[score]]\n<end output>\n\nNow, start your evaluation:",
        "generate_response": "{\"Reference\":[\"Analyzing Transformers in Embedding Space\", \"Jump to Conclusions: Short-Cutting Transformers with Linear Transformations\", \"The Pile: An 800GB Dataset of Diverse Text for Language Modeling\", \"Causal Abstractions of Neural Networks\", \"A Survey on Interpreting the Inner Structures of Deep Neural Networks\", \"Vicuna: An Open-Source Chatbot Impressing GPT-4 with $90 \\% *$ ChatGPT quality\", \"Towards Automated Circuit Discovery for Mechanistic Interpretability\", \"Decoderlens: Layerwise Interpretation of Encoder-Decoder Transformers\", \"ROUGE: A Package for Automatic Evaluation of Summaries\", \"Eliciting Latent Predictions from Transformers with the Tuned Lens\", \"Graph of Thoughts: Solving Elaborate Problems with Large Language Models\", \"Interpreting GPT: The Logit Lens\", \"Future Lens: Anticipating Subsequent Tokens from a Single Hidden State\", \"Mapping Language Models to Grounded Conceptual Spaces\", \"Sentence-bert: Sentence Embeddings Using Siamese Bert-Networks\", \"Multimodal Neurons in Pretrained Text-Only Transformers\", \"Explaining Black Box Text Modules in Natural Language with Language Models\", \"A Mechanistic Interpretation of Arithmetic Reasoning in Language Models Using Causal Mediation Analysis\", \"BERTnesia: Investigating the Capture and Forgetting of Knowledge in BERT\", \"In-Context Learning Creates Task Vectors\", \"Mass-Editing Memory in a Transformer\", \"Locating and Editing Factual Associations in GPT\", \"Chain-of-Thought Prompting Elicits Reasoning in Large Language Models\", \"Tree of Thoughts: Deliberate Problem Solving with Large Language Models\", \"Give Me the Facts! A Survey on Factual Knowledge Probing in Pre-trained Language Models\", \"Towards Best Practices of Activation Patching in Language Models: Metrics and Methods\", \"A Mechanism for Solving Relational Tasks in Transformer Language Models\", \"Dissecting Recall of Factual Associations in Auto-regressive Language Models\", \"When Not to Trust Language Models: Investigating Effectiveness of Parametric and Nonparametric Memories\", \"Can LLMs Facilitate Interpretation of Pre-Trained Language Models?\"], \"Citation\":[]}",
        "eval_response": "Evaluation evidence: The assistant's answer does not match the gold answer in terms of accuracy. The references listed in the assistant's answer do not align with those in the gold answer, and it completely misses the citation mentioned in the gold answer. The assistant's answer includes a long list of references that are not specified in the gold answer, indicating hallucinations. The answer also fails in completeness as it does not correctly identify the citation relationship.\n\nRating: [[20]]",
        "score": 20
    },
    {
        "level": 3,
        "set": 2,
        "length": 51274,
        "type": "paper",
        "language": "en",
        "question": "Soaring from 4K to 400K: Extending LLM's Context with Activation Beacon",
        "instruction": "We hope you will carefully study the provided papers and determine the citation relationships between them. Please follow the instructions below strictly to complete the task:\n\n#Specific Requirements:\n1. Reference: When a given paper mentions other provided papers, those other papers are considered as \"references\" for the given paper. To summarize in this specific context, references are about what the given paper is using.\n2. Citation: Conversely, when other provided papers mention the given paper in their works, the given paper is being \"cited\" by those other papers. To summarize in this specific context, citations are about who is using the given paper.\n3. Given a paper, you need to determine the citation or reference relationship between this paper and the other papers. Do not consider papers that are not provided.\n3. Please present the paper titles in a json format as follows: {{\"Reference\":[\"Reference Title 1\", \"Reference Title 2\", ..., \"Reference Title n\"], \"Citation\":[\"Citation Title 1\", \"Citation Title 2\", ..., \"Citation Title n\"]}}.\n4. If a paper does not have any references or citations, please leave the corresponding list empty, e.g.{{\"Refernce\":[]}}, {{\"Citation\":[]}}.",
        "prompt_template": "#Papers:\n{docs}\n\n{instruction}\n\n#The paper you need to analyze:\n{question}",
        "doc": [
            "2401.03462.md",
            "2405.10480.md",
            "2405.17915.md"
        ],
        "answer": {
            "Reference": [],
            "Citation": [
                "# Lean Attention: Hardware-Aware Scalable Attention Mechanism for the Decode-Phase of Transformers ",
                "# Long Context is Not Long at All: A Prospector of Long-Dependency Data for Large Language Models "
            ]
        },
        "shuffle_doc": true,
        "id": "ec5ec458-a555-43cc-bb84-d2a6741ac150",
        "prompt": "[Question]\n#Papers:\n{docs}\n\nWe hope you will carefully study the provided papers and determine the citation relationships between them. Please follow the instructions below strictly to complete the task:\n\n#Specific Requirements:\n1. Reference: When a given paper mentions other provided papers, those other papers are considered as \"references\" for the given paper. To summarize in this specific context, references are about what the given paper is using.\n2. Citation: Conversely, when other provided papers mention the given paper in their works, the given paper is being \"cited\" by those other papers. To summarize in this specific context, citations are about who is using the given paper.\n3. Given a paper, you need to determine the citation or reference relationship between this paper and the other papers. Do not consider papers that are not provided.\n3. Please present the paper titles in a json format as follows: {{\"Reference\":[\"Reference Title 1\", \"Reference Title 2\", ..., \"Reference Title n\"], \"Citation\":[\"Citation Title 1\", \"Citation Title 2\", ..., \"Citation Title n\"]}}.\n4. If a paper does not have any references or citations, please leave the corresponding list empty, e.g.{{\"Refernce\":[]}}, {{\"Citation\":[]}}.\n\n#The paper you need to analyze:\nSoaring from 4K to 400K: Extending LLM's Context with Activation Beacon\n\n[Gold Answer]\n{'Reference': [], 'Citation': ['# Lean Attention: Hardware-Aware Scalable Attention Mechanism for the Decode-Phase of Transformers ', '# Long Context is Not Long at All: A Prospector of Long-Dependency Data for Large Language Models ']}\n\n[The Start of Assistant's Predicted Answer]\n{\"Reference\": [\"Position Interpolation\", \"NTK-Aware Scaled RoPE\", \"StreamingLLM\", \"AutoCompre.-6K\", \"YaRN-128K\", \"LongChat-32K\", \"LongAlpaca-16K\", \"LongLlama\", \"ALiBi\", \"Ntk-aware scaled rope\", \"Longbench\", \"RMT\", \"Adapting language models to compress contexts\", \"Generating long sequences with sparse transformers\", \"Reformers: The efficient transformer\", \"How long can open-source llms truly promise on context length?\", \"Landmark attention: Random-access infinite context length for transformers\", \"Learning to compress prompts with gist tokens\", \"Efficient context window extension of large language models\", \"Compressive transformers for long-range sequence modelling\", \"Combiner: Full attention transformer with sparse computation cost\", \"Code llama: Open foundation models for code\", \"Long-range language modeling with self-retrieval\", \"A length-extrapolatable transformer\", \"Llama: Open and efficient foundation language models\", \"Llama 2: Open foundation and fine-tuned chat models\", \"Natural language processing with transformers\", \"Focused transformer: Contrastive training for context scaling\", \"Linformer: Self-attention with linear complexity\", \"Augmenting language models with long-term memory\", \"Memorizing transformers\", \"Efficient streaming language models with attention sinks\", \"Retrieval meets long context large language models\", \"Big bird: Transformers for longer sequences\", \"Retrieve anything to augment large language models\", \"Proof-pile\", \"POSE: Efficient context window extension of llms via positional skip-wise training\", \"OPT: Open Pre-Trained Transformer Language Models\"], \"Citation\": []}\n[The End of Assistant's Predicted Answer]\n\n[System]\nWe would like to request your feedback on the performance of the AI assistant in response to the user question displayed above according to the gold answer. Please use the following listed aspects and their descriptions as evaluation criteria:\n    - Accuracy and Hallucinations: The assistant's answer is semantically consistent with the gold answer; The numerical value and order need to be accurate, and there should be no hallucinations.\n    - Completeness: Referring to the reference answers, the assistant's answer should contain all the key points needed to answer the user's question; further elaboration on these key points can be omitted.\nPlease rate whether this answer is suitable for the question. Please note that the gold answer can be considered as a correct answer to the question.\n\nThe assistant receives an overall score on a scale of 1 to 100, where a higher score indicates better overall performance.\nPlease note that if the assistant's answer and the gold answer fully meet the above criteria, its overall rating should be the full marks (100).\nPlease first provide a comprehensive explanation of your evaluation, avoiding any potential bias.\nThen, output a line indicating the score of the Assistant.\n\nPLEASE OUTPUT WITH THE FOLLOWING FORMAT, WHERE THE SCORE IS A SCALE OF 1 TO 100 BY STRICTLY FOLLOWING THIS FORMAT: \"[[score]]\", FOR EXAMPLE \"Rating: [[100]]\":\n<start output>\nEvaluation evidence: your evluation explanation here, no more than 100 words\nRating: [[score]]\n<end output>\n\nNow, start your evaluation:",
        "generate_response": "{\"Reference\": [\"Position Interpolation\", \"NTK-Aware Scaled RoPE\", \"StreamingLLM\", \"AutoCompre.-6K\", \"YaRN-128K\", \"LongChat-32K\", \"LongAlpaca-16K\", \"LongLlama\", \"ALiBi\", \"Ntk-aware scaled rope\", \"Longbench\", \"RMT\", \"Adapting language models to compress contexts\", \"Generating long sequences with sparse transformers\", \"Reformers: The efficient transformer\", \"How long can open-source llms truly promise on context length?\", \"Landmark attention: Random-access infinite context length for transformers\", \"Learning to compress prompts with gist tokens\", \"Efficient context window extension of large language models\", \"Compressive transformers for long-range sequence modelling\", \"Combiner: Full attention transformer with sparse computation cost\", \"Code llama: Open foundation models for code\", \"Long-range language modeling with self-retrieval\", \"A length-extrapolatable transformer\", \"Llama: Open and efficient foundation language models\", \"Llama 2: Open foundation and fine-tuned chat models\", \"Natural language processing with transformers\", \"Focused transformer: Contrastive training for context scaling\", \"Linformer: Self-attention with linear complexity\", \"Augmenting language models with long-term memory\", \"Memorizing transformers\", \"Efficient streaming language models with attention sinks\", \"Retrieval meets long context large language models\", \"Big bird: Transformers for longer sequences\", \"Retrieve anything to augment large language models\", \"Proof-pile\", \"POSE: Efficient context window extension of llms via positional skip-wise training\", \"OPT: Open Pre-Trained Transformer Language Models\"], \"Citation\": []}",
        "eval_response": "Evaluation evidence: The assistant's answer inaccurately lists numerous references that are not mentioned in the gold answer, indicating a significant hallucination issue. The gold answer specifies no references and two citations, which the assistant fails to include, showing a lack of accuracy and completeness. The assistant's response does not align with the gold answer's details, leading to a low score.\n\nRating: [[10]]",
        "score": 10
    },
    {
        "level": 3,
        "set": 4,
        "length": 216619,
        "type": "paper",
        "language": "en",
        "question": "The Power of Noise: Redefining Retrieval for RAG Systems",
        "instruction": "We hope you will carefully study the provided papers and determine the citation relationships between them. Please follow the instructions below strictly to complete the task:\n\n#Specific Requirements:\n1. Reference: When a given paper mentions other provided papers, those other papers are considered as \"references\" for the given paper. To summarize in this specific context, references are about what the given paper is using.\n2. Citation: Conversely, when other provided papers mention the given paper in their works, the given paper is being \"cited\" by those other papers. To summarize in this specific context, citations are about who is using the given paper.\n3. Given a paper, you need to determine the citation or reference relationship between this paper and the other papers. Do not consider papers that are not provided.\n3. Please present the paper titles in a json format as follows: {{\"Reference\":[\"Reference Title 1\", \"Reference Title 2\", ..., \"Reference Title n\"], \"Citation\":[\"Citation Title 1\", \"Citation Title 2\", ..., \"Citation Title n\"]}}.\n4. If a paper does not have any references or citations, please leave the corresponding list empty, e.g.{{\"Refernce\":[]}}, {{\"Citation\":[]}}.",
        "prompt_template": "#Papers:\n{docs}\n\n{instruction}\n\n#The paper you need to analyze:\n{question}",
        "doc": [
            "2401.14887.md",
            "2405.11612.md",
            "2405.18111.md",
            "2405.19010.md",
            "2405.20362.md",
            "2405.20703.md",
            "2309.05463.md",
            "2310.04875.md",
            "2310.11511.md",
            "2310.12956.md",
            "2401.06954.md"
        ],
        "answer": {
            "Reference": [
                "# Textbooks Are All You Need II: phi-1.5 technical report ",
                "# Prompt-to-OS (P2OS): Revolutionizing Operating Systems and Human-Computer Interaction with Integrated AI Generative Models ",
                "# SELF-RAG: LEARNING TO RETRIEVE, GENERATE, AND CRITIQUE THROUGH SELF-REFLECTION ",
                "# EUREKA-MOMENTS IN TRANSFORMERS: MULTI-STEP TASKS REVEAL SOFTMAX INDUCED OPTIMIZATION PROBLEMS ",
                "# Bridging the Preference Gap between Retrievers and LLMs "
            ],
            "Citation": [
                "# SOCIOTECHNICAL IMPLICATIONS OF GENERATIVE ARTIFICIAL INTELLIGENCE FOR INFORMATION ACCESS* ",
                "# ATM: Adversarial Tuning Multi-agent System Makes a Robust Retrieval-Augmented GENERATOR ",
                "# Evaluating the External and Parametric Knowledge Fusion of Large Language Models ",
                "# Hallucination-Free? Assessing the Reliability of Leading AI Legal Research Tools ",
                "# It is Simple Sometimes: A Study On Improving Aspect-Based Sentiment Analysis Performance "
            ]
        },
        "shuffle_doc": true,
        "id": "2f715c2e-d093-4511-9c4e-b2964c403c36",
        "prompt": "[Question]\n#Papers:\n{docs}\n\nWe hope you will carefully study the provided papers and determine the citation relationships between them. Please follow the instructions below strictly to complete the task:\n\n#Specific Requirements:\n1. Reference: When a given paper mentions other provided papers, those other papers are considered as \"references\" for the given paper. To summarize in this specific context, references are about what the given paper is using.\n2. Citation: Conversely, when other provided papers mention the given paper in their works, the given paper is being \"cited\" by those other papers. To summarize in this specific context, citations are about who is using the given paper.\n3. Given a paper, you need to determine the citation or reference relationship between this paper and the other papers. Do not consider papers that are not provided.\n3. Please present the paper titles in a json format as follows: {{\"Reference\":[\"Reference Title 1\", \"Reference Title 2\", ..., \"Reference Title n\"], \"Citation\":[\"Citation Title 1\", \"Citation Title 2\", ..., \"Citation Title n\"]}}.\n4. If a paper does not have any references or citations, please leave the corresponding list empty, e.g.{{\"Refernce\":[]}}, {{\"Citation\":[]}}.\n\n#The paper you need to analyze:\nThe Power of Noise: Redefining Retrieval for RAG Systems\n\n[Gold Answer]\n{'Reference': ['# Textbooks Are All You Need II: phi-1.5 technical report ', '# Prompt-to-OS (P2OS): Revolutionizing Operating Systems and Human-Computer Interaction with Integrated AI Generative Models ', '# SELF-RAG: LEARNING TO RETRIEVE, GENERATE, AND CRITIQUE THROUGH SELF-REFLECTION ', '# EUREKA-MOMENTS IN TRANSFORMERS: MULTI-STEP TASKS REVEAL SOFTMAX INDUCED OPTIMIZATION PROBLEMS ', '# Bridging the Preference Gap between Retrievers and LLMs '], 'Citation': ['# SOCIOTECHNICAL IMPLICATIONS OF GENERATIVE ARTIFICIAL INTELLIGENCE FOR INFORMATION ACCESS* ', '# ATM: Adversarial Tuning Multi-agent System Makes a Robust Retrieval-Augmented GENERATOR ', '# Evaluating the External and Parametric Knowledge Fusion of Large Language Models ', '# Hallucination-Free? Assessing the Reliability of Leading AI Legal Research Tools ', '# It is Simple Sometimes: A Study On Improving Aspect-Based Sentiment Analysis Performance ']}\n\n[The Start of Assistant's Predicted Answer]\n{\"Reference\":[\"Attention is all you need\", \"A Survey on Evaluation of Large Language Models\", \"Vicuna: An Open-Source Chatbot Impressing GPT-4 with $90 \\% *$ ChatGPT Quality\", \"ChatLaw: Open-Source Legal Large Language Model with Integrated External Knowledge Bases\", \"Compositional Semantic Parsing with Large Language Models\", \"Open LLM Leaderboard\", \"How Close is ChatGPT to Human Experts? Comparison Corpus, Evaluation, and Detection\", \"REALM: Retrieval-Augmented Language Model PreTraining\", \"Rethinking with Retrieval: Faithful Large Language Model Inference\", \"Measuring Massive Multitask Language Understanding\", \"Atlas: Few-shot Learning with Retrieval Augmented Language Models\", \"Xcit: Cross-covariance image transformers\", \"Generating long sequences with sparse transformers\", \"Scaling vision transformers to 22 billion parameters\", \"Guiding attention for self-supervised learning with transformers\", \"Intriguing Properties of Transformer Training Instabilities\", \"Gmat: Global memory augmentation for transformers\", \"Escaping the big data paradigm with compact transformers\", \"Improving transformer optimization through better initialization\", \"Normsoftmax: Normalize the input of softmax to accelerate and stabilize training\", \"Bert: Pre-training of deep bidirectional transformers for language understanding\", \"Learning multiple layers of features from tiny images\", \"Mnist handwritten digit database\", \"Roberta: A robustly optimized bert pretraining approach\", \"Towards understanding grokking: An effective theory of representation learning\", \"Omnigrok: Grokking beyond algorithmic data\", \"Decoupled weight decay regularization\", \"Grokking 'grokking'\", \"Pre-training a bert with curriculum learning by increasing block-size of input text\", \"Progress measures for grokking via mechanistic interpretability\", \"Signal propagation in transformers: Theoretical perspectives and the role of rank collapse\", \"In-context learning and induction heads\", \"Grokking: Generalization beyond overfitting on small algorithmic datasets\", \"Are emergent abilities of large language models a mirage?\", \"A study on relu and softmax in transformer\", \"Megatron-lm: Training multi-billion parameter language models using model parallelism\", \"Beyond the imitation game: Quantifying and extrapolating the capabilities of language models\", \"Future $\\mathrm{ml}$ systems will be qualitatively different\", \"The slingshot mechanism: An empirical study of adaptive optimizers and the lemph\\{Grokking Phenomenon\\}\", \"Training data-efficient image transformers \\& distillation through attention\", \"Learning deep transformer models for machine translation\", \"Escaping the gradient vanishing: Periodic alternatives of softmax in attention mechanism\", \"Emergent abilities of large language models\", \"Fashion-mnist: a novel image dataset for benchmarking machine learning algorithms\", \"On layer normalization in the transformer architecture\", \"Stabilizing transformer training by preventing attention entropy collapse\"], \"Citation\":[\"SELF-RAG: LEARNING TO RETRIEVE, GENERATE, AND CRITIQUE THROUGH SELF-REFLECTION\", \"Prompt-to-OS (P2OS): Revolutionizing Operating Systems and Human-Computer Interaction with Integrated AI Generative Models\", \"ATM: Adversarial Tuning Multi-agent System Makes a Robust Retrieval-Augmented GENERATOR\"]}\n[The End of Assistant's Predicted Answer]\n\n[System]\nWe would like to request your feedback on the performance of the AI assistant in response to the user question displayed above according to the gold answer. Please use the following listed aspects and their descriptions as evaluation criteria:\n    - Accuracy and Hallucinations: The assistant's answer is semantically consistent with the gold answer; The numerical value and order need to be accurate, and there should be no hallucinations.\n    - Completeness: Referring to the reference answers, the assistant's answer should contain all the key points needed to answer the user's question; further elaboration on these key points can be omitted.\nPlease rate whether this answer is suitable for the question. Please note that the gold answer can be considered as a correct answer to the question.\n\nThe assistant receives an overall score on a scale of 1 to 100, where a higher score indicates better overall performance.\nPlease note that if the assistant's answer and the gold answer fully meet the above criteria, its overall rating should be the full marks (100).\nPlease first provide a comprehensive explanation of your evaluation, avoiding any potential bias.\nThen, output a line indicating the score of the Assistant.\n\nPLEASE OUTPUT WITH THE FOLLOWING FORMAT, WHERE THE SCORE IS A SCALE OF 1 TO 100 BY STRICTLY FOLLOWING THIS FORMAT: \"[[score]]\", FOR EXAMPLE \"Rating: [[100]]\":\n<start output>\nEvaluation evidence: your evluation explanation here, no more than 100 words\nRating: [[score]]\n<end output>\n\nNow, start your evaluation:",
        "generate_response": "{\"Reference\":[\"Attention is all you need\", \"A Survey on Evaluation of Large Language Models\", \"Vicuna: An Open-Source Chatbot Impressing GPT-4 with $90 \\% *$ ChatGPT Quality\", \"ChatLaw: Open-Source Legal Large Language Model with Integrated External Knowledge Bases\", \"Compositional Semantic Parsing with Large Language Models\", \"Open LLM Leaderboard\", \"How Close is ChatGPT to Human Experts? Comparison Corpus, Evaluation, and Detection\", \"REALM: Retrieval-Augmented Language Model PreTraining\", \"Rethinking with Retrieval: Faithful Large Language Model Inference\", \"Measuring Massive Multitask Language Understanding\", \"Atlas: Few-shot Learning with Retrieval Augmented Language Models\", \"Xcit: Cross-covariance image transformers\", \"Generating long sequences with sparse transformers\", \"Scaling vision transformers to 22 billion parameters\", \"Guiding attention for self-supervised learning with transformers\", \"Intriguing Properties of Transformer Training Instabilities\", \"Gmat: Global memory augmentation for transformers\", \"Escaping the big data paradigm with compact transformers\", \"Improving transformer optimization through better initialization\", \"Normsoftmax: Normalize the input of softmax to accelerate and stabilize training\", \"Bert: Pre-training of deep bidirectional transformers for language understanding\", \"Learning multiple layers of features from tiny images\", \"Mnist handwritten digit database\", \"Roberta: A robustly optimized bert pretraining approach\", \"Towards understanding grokking: An effective theory of representation learning\", \"Omnigrok: Grokking beyond algorithmic data\", \"Decoupled weight decay regularization\", \"Grokking 'grokking'\", \"Pre-training a bert with curriculum learning by increasing block-size of input text\", \"Progress measures for grokking via mechanistic interpretability\", \"Signal propagation in transformers: Theoretical perspectives and the role of rank collapse\", \"In-context learning and induction heads\", \"Grokking: Generalization beyond overfitting on small algorithmic datasets\", \"Are emergent abilities of large language models a mirage?\", \"A study on relu and softmax in transformer\", \"Megatron-lm: Training multi-billion parameter language models using model parallelism\", \"Beyond the imitation game: Quantifying and extrapolating the capabilities of language models\", \"Future $\\mathrm{ml}$ systems will be qualitatively different\", \"The slingshot mechanism: An empirical study of adaptive optimizers and the lemph\\{Grokking Phenomenon\\}\", \"Training data-efficient image transformers \\& distillation through attention\", \"Learning deep transformer models for machine translation\", \"Escaping the gradient vanishing: Periodic alternatives of softmax in attention mechanism\", \"Emergent abilities of large language models\", \"Fashion-mnist: a novel image dataset for benchmarking machine learning algorithms\", \"On layer normalization in the transformer architecture\", \"Stabilizing transformer training by preventing attention entropy collapse\"], \"Citation\":[\"SELF-RAG: LEARNING TO RETRIEVE, GENERATE, AND CRITIQUE THROUGH SELF-REFLECTION\", \"Prompt-to-OS (P2OS): Revolutionizing Operating Systems and Human-Computer Interaction with Integrated AI Generative Models\", \"ATM: Adversarial Tuning Multi-agent System Makes a Robust Retrieval-Augmented GENERATOR\"]}",
        "eval_response": "Evaluation evidence: The assistant's answer contains numerous hallucinations, listing references and citations that do not match the gold answer. The references and citations listed in the assistant's answer are entirely different from those in the gold answer, indicating a significant accuracy issue. The assistant's response fails to accurately reflect the relationships described in the gold answer, leading to a lack of completeness and accuracy.\n\nRating: [[10]]",
        "score": 10
    },
    {
        "level": 3,
        "set": 3,
        "length": 137823,
        "type": "paper",
        "language": "en",
        "question": "LLM4Vuln: A Unified Evaluation Framework for Decoupling and Enhancing LLMs' Vulnerability Reasoning",
        "instruction": "We hope you will carefully study the provided papers and determine the citation relationships between them. Please follow the instructions below strictly to complete the task:\n\n#Specific Requirements:\n1. Reference: When a given paper mentions other provided papers, those other papers are considered as \"references\" for the given paper. To summarize in this specific context, references are about what the given paper is using.\n2. Citation: Conversely, when other provided papers mention the given paper in their works, the given paper is being \"cited\" by those other papers. To summarize in this specific context, citations are about who is using the given paper.\n3. Given a paper, you need to determine the citation or reference relationship between this paper and the other papers. Do not consider papers that are not provided.\n3. Please present the paper titles in a json format as follows: {{\"Reference\":[\"Reference Title 1\", \"Reference Title 2\", ..., \"Reference Title n\"], \"Citation\":[\"Citation Title 1\", \"Citation Title 2\", ..., \"Citation Title n\"]}}.\n4. If a paper does not have any references or citations, please leave the corresponding list empty, e.g.{{\"Refernce\":[]}}, {{\"Citation\":[]}}.",
        "prompt_template": "#Papers:\n{docs}\n\n{instruction}\n\n#The paper you need to analyze:\n{question}",
        "doc": [
            "2401.16185.md",
            "2405.12750.md",
            "2310.01152.md",
            "2311.13721.md"
        ],
        "answer": {
            "Reference": [
                "# Large Language Model-Powered Smart Contract Vulnerability Detection: New Perspectives ",
                "# Nova ${ }^{+}$: Generative Language Models for Binaries "
            ],
            "Citation": [
                "# Generative AI and Large Language Models for Cyber Security: All Insights You Need "
            ]
        },
        "shuffle_doc": true,
        "id": "af5acc59-6efe-4b95-8ea1-79a35678944d",
        "prompt": "[Question]\n#Papers:\n{docs}\n\nWe hope you will carefully study the provided papers and determine the citation relationships between them. Please follow the instructions below strictly to complete the task:\n\n#Specific Requirements:\n1. Reference: When a given paper mentions other provided papers, those other papers are considered as \"references\" for the given paper. To summarize in this specific context, references are about what the given paper is using.\n2. Citation: Conversely, when other provided papers mention the given paper in their works, the given paper is being \"cited\" by those other papers. To summarize in this specific context, citations are about who is using the given paper.\n3. Given a paper, you need to determine the citation or reference relationship between this paper and the other papers. Do not consider papers that are not provided.\n3. Please present the paper titles in a json format as follows: {{\"Reference\":[\"Reference Title 1\", \"Reference Title 2\", ..., \"Reference Title n\"], \"Citation\":[\"Citation Title 1\", \"Citation Title 2\", ..., \"Citation Title n\"]}}.\n4. If a paper does not have any references or citations, please leave the corresponding list empty, e.g.{{\"Refernce\":[]}}, {{\"Citation\":[]}}.\n\n#The paper you need to analyze:\nLLM4Vuln: A Unified Evaluation Framework for Decoupling and Enhancing LLMs' Vulnerability Reasoning\n\n[Gold Answer]\n{'Reference': ['# Large Language Model-Powered Smart Contract Vulnerability Detection: New Perspectives ', '# Nova ${ }^{+}$: Generative Language Models for Binaries '], 'Citation': ['# Generative AI and Large Language Models for Cyber Security: All Insights You Need ']}\n\n[The Start of Assistant's Predicted Answer]\n{\"Reference\":[\"A Survey on Evaluation of Large Language Models\", \"Vicuna: An Open-Source Chatbot Impressing GPT-4 with $90 \\% *$ ChatGPT Quality\", \"ChatLaw: Open-Source Legal Large Language Model with Integrated External Knowledge Bases\", \"Compositional Semantic Parsing with Large Language Models\", \"Open LLM Leaderboard\", \"How Close is ChatGPT to Human Experts? Comparison Corpus, Evaluation, and Detection\", \"REALM: Retrieval-Augmented Language Model PreTraining\", \"Rethinking with Retrieval: Faithful Large Language Model Inference\", \"Measuring Massive Multitask Language Understanding\", \"Atlas: Few-shot Learning with Retrieval Augmented Language Models\", \"Ethereum Whitepaper\", \"Solidity Programming Language\", \"SoulZapV1\", \"Apebond_final_Secure3_Audit_Report.pdf\", \"Code4rena Findings on GitHub\", \"Code4rena Security Audit Reports\", \"codellama-13b-instruct\", \"Function calling - OpenAI API\", \"Llama-2-7b-chat-hf-function-calling-v3\", \"Mistral-7B-Instruct-v0.1-function-calling-v2\", \"Mixtral-8x7B-instruct-v0.1\", \"OpenAI\", \"Low level source code vulnerability detection using advanced bert language model\", \"FlowDroid: precise context, flow, field, object-sensitive and lifecycle-aware taint analysis for Android apps\", \"KLEE: Unassisted and Automatic Generation of High-Coverage Tests for Complex Systems Programs\", \"Deep Learning Based Vulnerability Detection: Are We There Yet?\", \"When ChatGPT Meets Smart Contract Vulnerability Detection: How Far Are We?\", \"Augmenting Decompiler Output with Learned Variable Names and Types\", \"DiverseVul: A New Vulnerable Source Code Dataset for Deep Learning Based Vulnerability Detection\", \"Can Large Language Models Be an Alternative to Human Evaluations?\", \"Why Can GPT Learn In-Context? Language Models Implicitly Perform Gradient Descent as Meta-Optimizers\", \"Do you still need a manual smart contract audit?\", \"Large Language Models Are Zero-Shot Fuzzers: Fuzzing DeepLearning Libraries via Large Language Models\", \"Large Language Models are Edge-Case Generators: Crafting Unusual Programs for Fuzzing Deep Learning Libraries\", \"TRACED: Execution-aware Pre-training for Source Code\", \"Beyond Protected and Private: An Empirical Security Analysis of Custom Function Modifiers in Smart Contracts\", \"AFL++: Combining Incremental Steps of Fuzzing Research\", \"Blockchain Large Language Models\", \"How Far Have We Gone in Vulnerability Detection Using Large Language Models\", \"netFound: Foundation Model for Network Security\", \"Large Language Model-Powered Smart Contract Vulnerability Detection: New Perspectives\", \"Mixtral of Experts\", \"Nova $^{+}$: Generative Language Models for Binaries\", \"Billion-scale similarity search with GPUs\", \"Understanding the Effectiveness of Large Language Models in Detecting Security Vulnerabilities\", \"Large language models are zero-shot reasoners\", \"DIRE: A Neural Approach to Decompiled Identifier Naming\", \"Illustrating Reinforcement Learning from Human Feedback (RLHF)\", \"Retrieval-Augmented Generation for Knowledge-Intensive NLP Tasks\", \"The Hitchhiker's Guide to Program Analysis: A Journey with Large Language Models\", \"PalmTree: Learning an Assembly Language Model for Instruction Embedding\", \"Split and Merge: Aligning Position Biases in Large Language Model based Evaluators\", \"Vuldeepecker: A deep learning-based system for vulnerability detection\", \"Harnessing the Power of LLM to Support Binary Taint Analysis\", \"Large Language Model guided Protocol Fuzzing\", \"Examining Zero-Shot Vulnerability Repair with Large Language Models\", \"Symmetry-Preserving Program Representations for Learning Code Semantics\", \"Code Llama: Open Foundation Models for Code\", \"Toolformer: Language Models Can Teach Themselves to Use Tools\", \"Neuzz: Efficient fuzzing with neural program smoothing\", \"SOK: (State of) The Art of War: Offensive Techniques in Binary Analysis\", \"GPTScan: Detecting Logic Vulnerabilities in Smart Contracts by Combining GPT with Program Analysis\", \"CSGVD: A deep learning approach combining sequence and graph embedding for source code vulnerability detection\", \"Transformer-Based Language Models for Software Vulnerability Detection\", \"Llama 2: Open Foundation and Fine-Tuned Chat Models\", \"Can Large Language Models Identify And Reason About Security Vulnerabilities? Not Yet\", \"Attention Is All You Need\", \"TaintScope: A ChecksumAware Directed Fuzzing Tool for Automatic Software Vulnerability Detection\", \"Chain-of-Thought Prompting Elicits Reasoning in Large Language Models\", \"When Program Analysis Meets Bytecode Search: Targeted and Efficient Inter-procedural Analysis of Modern Android Apps in BackDroid\", \"Fuzz4All: Universal Fuzzing with Large Language Models\", \"Neural Network-based Graph Embedding for Cross-Platform Binary Code Similarity Detection\", \"BlockScope: Detecting and Investigating Propagated Vulnerabilities in Forked Blockchain Projects\", \"Devign: Effective Vulnerability Identification by Learning Comprehensive Program Semantics via Graph Neural Networks\", \"kTrans: Knowledge-Aware Transformer for Binary Code Embedding\"], \"Citation\":[]}\n[The End of Assistant's Predicted Answer]\n\n[System]\nWe would like to request your feedback on the performance of the AI assistant in response to the user question displayed above according to the gold answer. Please use the following listed aspects and their descriptions as evaluation criteria:\n    - Accuracy and Hallucinations: The assistant's answer is semantically consistent with the gold answer; The numerical value and order need to be accurate, and there should be no hallucinations.\n    - Completeness: Referring to the reference answers, the assistant's answer should contain all the key points needed to answer the user's question; further elaboration on these key points can be omitted.\nPlease rate whether this answer is suitable for the question. Please note that the gold answer can be considered as a correct answer to the question.\n\nThe assistant receives an overall score on a scale of 1 to 100, where a higher score indicates better overall performance.\nPlease note that if the assistant's answer and the gold answer fully meet the above criteria, its overall rating should be the full marks (100).\nPlease first provide a comprehensive explanation of your evaluation, avoiding any potential bias.\nThen, output a line indicating the score of the Assistant.\n\nPLEASE OUTPUT WITH THE FOLLOWING FORMAT, WHERE THE SCORE IS A SCALE OF 1 TO 100 BY STRICTLY FOLLOWING THIS FORMAT: \"[[score]]\", FOR EXAMPLE \"Rating: [[100]]\":\n<start output>\nEvaluation evidence: your evluation explanation here, no more than 100 words\nRating: [[score]]\n<end output>\n\nNow, start your evaluation:",
        "generate_response": "{\"Reference\":[\"A Survey on Evaluation of Large Language Models\", \"Vicuna: An Open-Source Chatbot Impressing GPT-4 with $90 \\% *$ ChatGPT Quality\", \"ChatLaw: Open-Source Legal Large Language Model with Integrated External Knowledge Bases\", \"Compositional Semantic Parsing with Large Language Models\", \"Open LLM Leaderboard\", \"How Close is ChatGPT to Human Experts? Comparison Corpus, Evaluation, and Detection\", \"REALM: Retrieval-Augmented Language Model PreTraining\", \"Rethinking with Retrieval: Faithful Large Language Model Inference\", \"Measuring Massive Multitask Language Understanding\", \"Atlas: Few-shot Learning with Retrieval Augmented Language Models\", \"Ethereum Whitepaper\", \"Solidity Programming Language\", \"SoulZapV1\", \"Apebond_final_Secure3_Audit_Report.pdf\", \"Code4rena Findings on GitHub\", \"Code4rena Security Audit Reports\", \"codellama-13b-instruct\", \"Function calling - OpenAI API\", \"Llama-2-7b-chat-hf-function-calling-v3\", \"Mistral-7B-Instruct-v0.1-function-calling-v2\", \"Mixtral-8x7B-instruct-v0.1\", \"OpenAI\", \"Low level source code vulnerability detection using advanced bert language model\", \"FlowDroid: precise context, flow, field, object-sensitive and lifecycle-aware taint analysis for Android apps\", \"KLEE: Unassisted and Automatic Generation of High-Coverage Tests for Complex Systems Programs\", \"Deep Learning Based Vulnerability Detection: Are We There Yet?\", \"When ChatGPT Meets Smart Contract Vulnerability Detection: How Far Are We?\", \"Augmenting Decompiler Output with Learned Variable Names and Types\", \"DiverseVul: A New Vulnerable Source Code Dataset for Deep Learning Based Vulnerability Detection\", \"Can Large Language Models Be an Alternative to Human Evaluations?\", \"Why Can GPT Learn In-Context? Language Models Implicitly Perform Gradient Descent as Meta-Optimizers\", \"Do you still need a manual smart contract audit?\", \"Large Language Models Are Zero-Shot Fuzzers: Fuzzing DeepLearning Libraries via Large Language Models\", \"Large Language Models are Edge-Case Generators: Crafting Unusual Programs for Fuzzing Deep Learning Libraries\", \"TRACED: Execution-aware Pre-training for Source Code\", \"Beyond Protected and Private: An Empirical Security Analysis of Custom Function Modifiers in Smart Contracts\", \"AFL++: Combining Incremental Steps of Fuzzing Research\", \"Blockchain Large Language Models\", \"How Far Have We Gone in Vulnerability Detection Using Large Language Models\", \"netFound: Foundation Model for Network Security\", \"Large Language Model-Powered Smart Contract Vulnerability Detection: New Perspectives\", \"Mixtral of Experts\", \"Nova $^{+}$: Generative Language Models for Binaries\", \"Billion-scale similarity search with GPUs\", \"Understanding the Effectiveness of Large Language Models in Detecting Security Vulnerabilities\", \"Large language models are zero-shot reasoners\", \"DIRE: A Neural Approach to Decompiled Identifier Naming\", \"Illustrating Reinforcement Learning from Human Feedback (RLHF)\", \"Retrieval-Augmented Generation for Knowledge-Intensive NLP Tasks\", \"The Hitchhiker's Guide to Program Analysis: A Journey with Large Language Models\", \"PalmTree: Learning an Assembly Language Model for Instruction Embedding\", \"Split and Merge: Aligning Position Biases in Large Language Model based Evaluators\", \"Vuldeepecker: A deep learning-based system for vulnerability detection\", \"Harnessing the Power of LLM to Support Binary Taint Analysis\", \"Large Language Model guided Protocol Fuzzing\", \"Examining Zero-Shot Vulnerability Repair with Large Language Models\", \"Symmetry-Preserving Program Representations for Learning Code Semantics\", \"Code Llama: Open Foundation Models for Code\", \"Toolformer: Language Models Can Teach Themselves to Use Tools\", \"Neuzz: Efficient fuzzing with neural program smoothing\", \"SOK: (State of) The Art of War: Offensive Techniques in Binary Analysis\", \"GPTScan: Detecting Logic Vulnerabilities in Smart Contracts by Combining GPT with Program Analysis\", \"CSGVD: A deep learning approach combining sequence and graph embedding for source code vulnerability detection\", \"Transformer-Based Language Models for Software Vulnerability Detection\", \"Llama 2: Open Foundation and Fine-Tuned Chat Models\", \"Can Large Language Models Identify And Reason About Security Vulnerabilities? Not Yet\", \"Attention Is All You Need\", \"TaintScope: A ChecksumAware Directed Fuzzing Tool for Automatic Software Vulnerability Detection\", \"Chain-of-Thought Prompting Elicits Reasoning in Large Language Models\", \"When Program Analysis Meets Bytecode Search: Targeted and Efficient Inter-procedural Analysis of Modern Android Apps in BackDroid\", \"Fuzz4All: Universal Fuzzing with Large Language Models\", \"Neural Network-based Graph Embedding for Cross-Platform Binary Code Similarity Detection\", \"BlockScope: Detecting and Investigating Propagated Vulnerabilities in Forked Blockchain Projects\", \"Devign: Effective Vulnerability Identification by Learning Comprehensive Program Semantics via Graph Neural Networks\", \"kTrans: Knowledge-Aware Transformer for Binary Code Embedding\"], \"Citation\":[]}",
        "eval_response": "Evaluation evidence: The assistant's answer is inaccurate and contains hallucinations. The provided answer lists numerous references and citations that are not mentioned in the gold answer, indicating a failure to correctly identify the relationships between the provided papers. The gold answer specifies two references and one citation, none of which are correctly identified in the assistant's response. This demonstrates a lack of accuracy and the presence of hallucinations in the assistant's output.\n\nRating: [[10]]",
        "score": 10
    },
    {
        "level": 4,
        "set": 2,
        "length": 87612,
        "type": "paper",
        "language": "en",
        "question": "",
        "instruction": "We kindly ask you to thoroughly review the provided papers and construct a citation chain from them. Please adhere to the following instructions strictly while completing the task:\n\n#Task Instructions:\nGiven several papers, you are required to identify and list the longest citation chain, which demonstrates the citation relationship among the provided papers.\n\n#Specific Requirements:\n1.Please present the titles of the papers in the form of a list, as follows: [\"Title of Paper 1\", \"Title of Paper 2\", ..., \"Title of Paper n\"].\n2.Ensure that the citation chain in the list is linear and continuous, meaning that the first paper title in the list (Paper 1) should not cite any other works. Instead, it should be cited by the next paper in the list (Paper 2); subsequently, each paper should then be cited by the next one in the list, continuing up to the last paper (Paper n).\n3.Consider only the citation relationships within the supplied collection of papers, and ensure that the citation chain accurately reflects the sequential citation order among these documents.\n4.Do not take into account any articles not provided, and disregard other non-linear citation relationships.",
        "prompt_template": "{instruction}\n\n#Papers Provided:\n{docs}",
        "doc": [
            "2103.11955.md",
            "2109.09193.md",
            "2112.04359.md"
        ],
        "answer": [
            "# Improving and Simplifying Pattern Exploiting Training ",
            "# Towards Zero-Label Language Learning ",
            "# Ethical and social risks of harm from Language Models "
        ],
        "shuffle_doc": true,
        "id": "31277bf2-ba09-464c-a780-b507746a1f6e",
        "prompt": "[Question]\nWe kindly ask you to thoroughly review the provided papers and construct a citation chain from them. Please adhere to the following instructions strictly while completing the task:\n\n#Task Instructions:\nGiven several papers, you are required to identify and list the longest citation chain, which demonstrates the citation relationship among the provided papers.\n\n#Specific Requirements:\n1.Please present the titles of the papers in the form of a list, as follows: [\"Title of Paper 1\", \"Title of Paper 2\", ..., \"Title of Paper n\"].\n2.Ensure that the citation chain in the list is linear and continuous, meaning that the first paper title in the list (Paper 1) should not cite any other works. Instead, it should be cited by the next paper in the list (Paper 2); subsequently, each paper should then be cited by the next one in the list, continuing up to the last paper (Paper n).\n3.Consider only the citation relationships within the supplied collection of papers, and ensure that the citation chain accurately reflects the sequential citation order among these documents.\n4.Do not take into account any articles not provided, and disregard other non-linear citation relationships.\n\n#Papers Provided:\n{docs}\n\n[Gold Answer]\n['# Improving and Simplifying Pattern Exploiting Training ', '# Towards Zero-Label Language Learning ', '# Ethical and social risks of harm from Language Models ']\n\n[The Start of Assistant's Predicted Answer]\n[\"A Densely-supervised Approach to Pattern Exploiting Training\", \"A lite bert for self-supervised learning of language representations\", \"Language models are few-shot learners\", \"MixText: Linguistically-informed interpolation of hidden space for semi-supervised text classification\", \"Don't stop pretraining: Adapt language models to domains and tasks\", \"WiC: the word-in-context dataset for evaluating context-sensitive meaning representations\", \"Improving language understanding by generative pre-training\", \"Exploring the limits of transfer learning with a unified text-totext transformer\", \"RECORD: Bridging the gap between human and machine commonsense reading comprehension\", \"Pytorch: An imperative style, high-performance deep learning library\", \"Superglue: A stickier benchmark for general-purpose language understanding systems\", \"The pascal recognising textual entailment challenge\", \"The commitmentbank: Investigating projection in naturally occurring discourse\", \"Looking beyond the surface: A challenge set for reading comprehension over multiple sentences\", \"Cloze procedure: A new tool for measuring readability\", \"Deep inside convolutional networks: Visualising image classification models and saliency maps\", \"Transformers: State-of-the-art natural language processing\"]\n[The End of Assistant's Predicted Answer]\n\n[System]\nWe would like to request your feedback on the performance of the AI assistant in response to the user question displayed above according to the gold answer. Please use the following listed aspects and their descriptions as evaluation criteria:\n    - Accuracy and Hallucinations: The assistant's answer is semantically consistent with the gold answer; The numerical value and order need to be accurate, and there should be no hallucinations.\n    - Completeness: Referring to the reference answers, the assistant's answer should contain all the key points needed to answer the user's question; further elaboration on these key points can be omitted.\nPlease rate whether this answer is suitable for the question. Please note that the gold answer can be considered as a correct answer to the question.\n\nThe assistant receives an overall score on a scale of 1 to 100, where a higher score indicates better overall performance.\nPlease note that if the assistant's answer and the gold answer fully meet the above criteria, its overall rating should be the full marks (100).\nPlease first provide a comprehensive explanation of your evaluation, avoiding any potential bias.\nThen, output a line indicating the score of the Assistant.\n\nPLEASE OUTPUT WITH THE FOLLOWING FORMAT, WHERE THE SCORE IS A SCALE OF 1 TO 100 BY STRICTLY FOLLOWING THIS FORMAT: \"[[score]]\", FOR EXAMPLE \"Rating: [[100]]\":\n<start output>\nEvaluation evidence: your evluation explanation here, no more than 100 words\nRating: [[score]]\n<end output>\n\nNow, start your evaluation:",
        "generate_response": "[\"A Densely-supervised Approach to Pattern Exploiting Training\", \"A lite bert for self-supervised learning of language representations\", \"Language models are few-shot learners\", \"MixText: Linguistically-informed interpolation of hidden space for semi-supervised text classification\", \"Don't stop pretraining: Adapt language models to domains and tasks\", \"WiC: the word-in-context dataset for evaluating context-sensitive meaning representations\", \"Improving language understanding by generative pre-training\", \"Exploring the limits of transfer learning with a unified text-totext transformer\", \"RECORD: Bridging the gap between human and machine commonsense reading comprehension\", \"Pytorch: An imperative style, high-performance deep learning library\", \"Superglue: A stickier benchmark for general-purpose language understanding systems\", \"The pascal recognising textual entailment challenge\", \"The commitmentbank: Investigating projection in naturally occurring discourse\", \"Looking beyond the surface: A challenge set for reading comprehension over multiple sentences\", \"Cloze procedure: A new tool for measuring readability\", \"Deep inside convolutional networks: Visualising image classification models and saliency maps\", \"Transformers: State-of-the-art natural language processing\"]",
        "eval_response": "Evaluation evidence: The assistant's response does not align with the gold answer and fails to follow the task instructions. The provided answer lists a series of paper titles that do not form a citation chain, as required by the task. The titles mentioned do not reflect a linear and continuous citation relationship. The response includes multiple titles that are not related in a citation sequence, thus not meeting the task's specific requirement of constructing a citation chain from the provided papers.\n\nRating: [[10]]",
        "score": 10
    },
    {
        "level": 3,
        "set": 3,
        "length": 103579,
        "type": "paper",
        "language": "en",
        "question": "KVQuant: Towards 10 Million Context Length LLM Inference with KV Cache Quantization",
        "instruction": "We hope you will carefully study the provided papers and determine the citation relationships between them. Please follow the instructions below strictly to complete the task:\n\n#Specific Requirements:\n1. Reference: When a given paper mentions other provided papers, those other papers are considered as \"references\" for the given paper. To summarize in this specific context, references are about what the given paper is using.\n2. Citation: Conversely, when other provided papers mention the given paper in their works, the given paper is being \"cited\" by those other papers. To summarize in this specific context, citations are about who is using the given paper.\n3. Given a paper, you need to determine the citation or reference relationship between this paper and the other papers. Do not consider papers that are not provided.\n3. Please present the paper titles in a json format as follows: {{\"Reference\":[\"Reference Title 1\", \"Reference Title 2\", ..., \"Reference Title n\"], \"Citation\":[\"Citation Title 1\", \"Citation Title 2\", ..., \"Citation Title n\"]}}.\n4. If a paper does not have any references or citations, please leave the corresponding list empty, e.g.{{\"Refernce\":[]}}, {{\"Citation\":[]}}.",
        "prompt_template": "#Papers:\n{docs}\n\n{instruction}\n\n#The paper you need to analyze:\n{question}",
        "doc": [
            "2401.18079.md",
            "2405.04434.md",
            "2405.06219.md",
            "2405.14591.md"
        ],
        "answer": {
            "Reference": [],
            "Citation": [
                "# DeepSeek-V2: A Strong, Economical, and Efficient Mixture-of-Experts Language Model ",
                "# SKVQ: Sliding-window Key and Value Cache Quantization for Large Language Models ",
                "# Base of RoPE Bounds Context Length "
            ]
        },
        "shuffle_doc": true,
        "id": "29c9920f-3ea3-41e7-b334-fd95a23e6731",
        "prompt": "[Question]\n#Papers:\n{docs}\n\nWe hope you will carefully study the provided papers and determine the citation relationships between them. Please follow the instructions below strictly to complete the task:\n\n#Specific Requirements:\n1. Reference: When a given paper mentions other provided papers, those other papers are considered as \"references\" for the given paper. To summarize in this specific context, references are about what the given paper is using.\n2. Citation: Conversely, when other provided papers mention the given paper in their works, the given paper is being \"cited\" by those other papers. To summarize in this specific context, citations are about who is using the given paper.\n3. Given a paper, you need to determine the citation or reference relationship between this paper and the other papers. Do not consider papers that are not provided.\n3. Please present the paper titles in a json format as follows: {{\"Reference\":[\"Reference Title 1\", \"Reference Title 2\", ..., \"Reference Title n\"], \"Citation\":[\"Citation Title 1\", \"Citation Title 2\", ..., \"Citation Title n\"]}}.\n4. If a paper does not have any references or citations, please leave the corresponding list empty, e.g.{{\"Refernce\":[]}}, {{\"Citation\":[]}}.\n\n#The paper you need to analyze:\nKVQuant: Towards 10 Million Context Length LLM Inference with KV Cache Quantization\n\n[Gold Answer]\n{'Reference': [], 'Citation': ['# DeepSeek-V2: A Strong, Economical, and Efficient Mixture-of-Experts Language Model ', '# SKVQ: Sliding-window Key and Value Cache Quantization for Large Language Models ', '# Base of RoPE Bounds Context Length ']}\n\n[The Start of Assistant's Predicted Answer]\n{\"Reference\":[\"Introducing claude 2.1\", \"Understanding and overcoming the challenges of efficient transformer quantization\", \"Extending context window of large language models via positional interpolation\", \"Longlora: Efficient fine-tuning of long-context large language models\", \"Llm.int8 (): 8bit matrix multiplication for transformers at scale\", \"Qlora: Efficient finetuning of quantized llms\", \"Spqr: A sparse-quantized representation for near-lossless llm weight compression\", \"Hawq-v2: Hessian aware trace-weighted quantization of neural networks\", \"Hawq: Hessian aware quantization of neural networks with mixed-precision\", \"Balanced csr sparse matrix-vector product on graphics processors\", \"A framework for few-shot language model evaluation\", \"Model tells you what to discard: Adaptive kv cache compression for llms\", \"Ai and memory wall\", \"Lm-infinite: Simple on-the-fly length generalization for large language models\", \"Measuring massive multitask language understanding\", \"Rethinking channel dimensions to isolate outliers for low-bit weight quantization of large language models\", \"Mistral 7b\", \"Squeezellm: Dense-and-sparse quantization\", \"Full stack optimization of transformer inference: a survey\", \"How long can context length of opensource llms truly promise?\", \"Norm tweaking: High-performance low-bit quantization of large language models\", \"Awq: Activation-aware weight quantization for llm compression and acceleration\", \"Intactkv: Improving large language model quantization by keeping pivot tokens intact\", \"Llmqat: Data-free quantization aware training for large language models\", \"Scissorhands: Exploiting the persistence of importance hypothesis for llm kv cache compression at test time\", \"Kivi: Plug-and-play 2bit kv cache quantization with streaming asymmetric quantization\", \"Landmark attention: Random-access infinite context length for transformers\", \"Sensitivity and generalization in neural networks: an empirical study\", \"New models and developer products announced at devday 2023\", \"Sparq attention: Bandwidth-efficient llm inference\", \"Omniquant: Omnidirectionally calibrated quantization for large language models\", \"Flexgen: Highthroughput generative inference of large language models with a single gpu\", \"Roformer: Enhanced transformer with rotary position embedding\", \"Llama: Open and efficient foundation language models\", \"Llama 2: Open foundation and fine-tuned chat models\", \"Outlier suppression+: Accurate quantization of large language models by equivalent and optimal shifting and scaling\", \"Outlier suppression: Pushing the limit of low-bit transformer language models\", \"Smoothquant: Accurate and efficient post-training quantization for large language models\", \"Efficient streaming language models with attention sinks\", \"H 2 o Heavy-hitter oracle for efficient generative inference of large language models\", \"Atom: Low-bit quantization for efficient and accurate $1 \\mathrm{~lm}$ serving\", \"Pose: Efficient context window extension of llms via positional skipwise training\"], \"Citation\":[\"SKVQ: Sliding-window Key and Value Cache Quantization for Large Language Models\"]}\n[The End of Assistant's Predicted Answer]\n\n[System]\nWe would like to request your feedback on the performance of the AI assistant in response to the user question displayed above according to the gold answer. Please use the following listed aspects and their descriptions as evaluation criteria:\n    - Accuracy and Hallucinations: The assistant's answer is semantically consistent with the gold answer; The numerical value and order need to be accurate, and there should be no hallucinations.\n    - Completeness: Referring to the reference answers, the assistant's answer should contain all the key points needed to answer the user's question; further elaboration on these key points can be omitted.\nPlease rate whether this answer is suitable for the question. Please note that the gold answer can be considered as a correct answer to the question.\n\nThe assistant receives an overall score on a scale of 1 to 100, where a higher score indicates better overall performance.\nPlease note that if the assistant's answer and the gold answer fully meet the above criteria, its overall rating should be the full marks (100).\nPlease first provide a comprehensive explanation of your evaluation, avoiding any potential bias.\nThen, output a line indicating the score of the Assistant.\n\nPLEASE OUTPUT WITH THE FOLLOWING FORMAT, WHERE THE SCORE IS A SCALE OF 1 TO 100 BY STRICTLY FOLLOWING THIS FORMAT: \"[[score]]\", FOR EXAMPLE \"Rating: [[100]]\":\n<start output>\nEvaluation evidence: your evluation explanation here, no more than 100 words\nRating: [[score]]\n<end output>\n\nNow, start your evaluation:",
        "generate_response": "{\"Reference\":[\"Introducing claude 2.1\", \"Understanding and overcoming the challenges of efficient transformer quantization\", \"Extending context window of large language models via positional interpolation\", \"Longlora: Efficient fine-tuning of long-context large language models\", \"Llm.int8 (): 8bit matrix multiplication for transformers at scale\", \"Qlora: Efficient finetuning of quantized llms\", \"Spqr: A sparse-quantized representation for near-lossless llm weight compression\", \"Hawq-v2: Hessian aware trace-weighted quantization of neural networks\", \"Hawq: Hessian aware quantization of neural networks with mixed-precision\", \"Balanced csr sparse matrix-vector product on graphics processors\", \"A framework for few-shot language model evaluation\", \"Model tells you what to discard: Adaptive kv cache compression for llms\", \"Ai and memory wall\", \"Lm-infinite: Simple on-the-fly length generalization for large language models\", \"Measuring massive multitask language understanding\", \"Rethinking channel dimensions to isolate outliers for low-bit weight quantization of large language models\", \"Mistral 7b\", \"Squeezellm: Dense-and-sparse quantization\", \"Full stack optimization of transformer inference: a survey\", \"How long can context length of opensource llms truly promise?\", \"Norm tweaking: High-performance low-bit quantization of large language models\", \"Awq: Activation-aware weight quantization for llm compression and acceleration\", \"Intactkv: Improving large language model quantization by keeping pivot tokens intact\", \"Llmqat: Data-free quantization aware training for large language models\", \"Scissorhands: Exploiting the persistence of importance hypothesis for llm kv cache compression at test time\", \"Kivi: Plug-and-play 2bit kv cache quantization with streaming asymmetric quantization\", \"Landmark attention: Random-access infinite context length for transformers\", \"Sensitivity and generalization in neural networks: an empirical study\", \"New models and developer products announced at devday 2023\", \"Sparq attention: Bandwidth-efficient llm inference\", \"Omniquant: Omnidirectionally calibrated quantization for large language models\", \"Flexgen: Highthroughput generative inference of large language models with a single gpu\", \"Roformer: Enhanced transformer with rotary position embedding\", \"Llama: Open and efficient foundation language models\", \"Llama 2: Open foundation and fine-tuned chat models\", \"Outlier suppression+: Accurate quantization of large language models by equivalent and optimal shifting and scaling\", \"Outlier suppression: Pushing the limit of low-bit transformer language models\", \"Smoothquant: Accurate and efficient post-training quantization for large language models\", \"Efficient streaming language models with attention sinks\", \"H 2 o Heavy-hitter oracle for efficient generative inference of large language models\", \"Atom: Low-bit quantization for efficient and accurate $1 \\mathrm{~lm}$ serving\", \"Pose: Efficient context window extension of llms via positional skipwise training\"], \"Citation\":[\"SKVQ: Sliding-window Key and Value Cache Quantization for Large Language Models\"]}",
        "eval_response": "Evaluation evidence: The assistant's answer contains significant hallucinations, listing numerous papers that are not mentioned in the gold answer. The gold answer specifies three papers that cite \"KVQuant,\" but the assistant only correctly identifies one of these (\"SKVQ: Sliding-window Key and Value Cache Quantization for Large Language Models\") and misses the other two. Additionally, the assistant incorrectly lists a large number of references which are not required or mentioned in the gold answer. This results in both inaccuracies and incompleteness in addressing the task requirements.\n\nRating: [[20]]",
        "score": 20
    },
    {
        "level": 3,
        "set": 2,
        "length": 84705,
        "type": "paper",
        "language": "en",
        "question": "Adapting Large Language Models for Document-Level Machine Translation",
        "instruction": "We hope you will carefully study the provided papers and determine the citation relationships between them. Please follow the instructions below strictly to complete the task:\n\n#Specific Requirements:\n1. Reference: When a given paper mentions other provided papers, those other papers are considered as \"references\" for the given paper. To summarize in this specific context, references are about what the given paper is using.\n2. Citation: Conversely, when other provided papers mention the given paper in their works, the given paper is being \"cited\" by those other papers. To summarize in this specific context, citations are about who is using the given paper.\n3. Given a paper, you need to determine the citation or reference relationship between this paper and the other papers. Do not consider papers that are not provided.\n3. Please present the paper titles in a json format as follows: {{\"Reference\":[\"Reference Title 1\", \"Reference Title 2\", ..., \"Reference Title n\"], \"Citation\":[\"Citation Title 1\", \"Citation Title 2\", ..., \"Citation Title n\"]}}.\n4. If a paper does not have any references or citations, please leave the corresponding list empty, e.g.{{\"Refernce\":[]}}, {{\"Citation\":[]}}.",
        "prompt_template": "#Papers:\n{docs}\n\n{instruction}\n\n#The paper you need to analyze:\n{question}",
        "doc": [
            "2401.06468.md",
            "2405.12819.md",
            "2309.09400.md",
            "2401.15360.md"
        ],
        "answer": {
            "Reference": [
                "# CulturaX: A Cleaned, Enormous, and Multilingual Dataset for Large Language Models in 167 Languages ",
                "# Importance-Aware Data Augmentation for Document-Level Neural Machine Translation "
            ],
            "Citation": [
                "# Large Language Models Meet NLP: A Survey "
            ]
        },
        "shuffle_doc": true,
        "id": "2bd1d4ed-cd22-4d11-8551-68bcd95b5071",
        "prompt": "[Question]\n#Papers:\n{docs}\n\nWe hope you will carefully study the provided papers and determine the citation relationships between them. Please follow the instructions below strictly to complete the task:\n\n#Specific Requirements:\n1. Reference: When a given paper mentions other provided papers, those other papers are considered as \"references\" for the given paper. To summarize in this specific context, references are about what the given paper is using.\n2. Citation: Conversely, when other provided papers mention the given paper in their works, the given paper is being \"cited\" by those other papers. To summarize in this specific context, citations are about who is using the given paper.\n3. Given a paper, you need to determine the citation or reference relationship between this paper and the other papers. Do not consider papers that are not provided.\n3. Please present the paper titles in a json format as follows: {{\"Reference\":[\"Reference Title 1\", \"Reference Title 2\", ..., \"Reference Title n\"], \"Citation\":[\"Citation Title 1\", \"Citation Title 2\", ..., \"Citation Title n\"]}}.\n4. If a paper does not have any references or citations, please leave the corresponding list empty, e.g.{{\"Refernce\":[]}}, {{\"Citation\":[]}}.\n\n#The paper you need to analyze:\nAdapting Large Language Models for Document-Level Machine Translation\n\n[Gold Answer]\n{'Reference': ['# CulturaX: A Cleaned, Enormous, and Multilingual Dataset for Large Language Models in 167 Languages ', '# Importance-Aware Data Augmentation for Document-Level Neural Machine Translation '], 'Citation': ['# Large Language Models Meet NLP: A Survey ']}\n\n[The Start of Assistant's Predicted Answer]\n{\"Reference\":[\"Multidimensional Quality Metrics: a flexible system for assessing translation quality\", \"Overview of the IWSLT 2017 evaluation campaign\", \"Monolingual or multilingual instruction tuning: Which makes a better alpaca\", \"Vicuna: An Open-Source Chatbot Impressing GPT-4 with $90 \\% *$ ChatGPT Quality\", \"Chain-of-dictionary prompting elicits translation in large language models\", \"Wizardcoder: Empowering code large language models with evolinstruct\", \"A simple and effective unified encoder for documentlevel machine translation\", \"Using whole document context in neural machine translation\", \"Adaptive machine translation with large language models\", \"A large-scale test set for the evaluation of context-aware pronoun translation in neural machine translation\", \"Culturax: A cleaned, enormous, and multilingual dataset for large language models in 167 languages\", \"GPT-4 technical report\", \"Training language models to follow instructions with human feedback\", \"BLOOM: A 176b-parameter open-access multilingual language model\", \"Llama: Open and efficient foundation language models\", \"Llama 2: Open foundation and fine-tuned chat models\", \"Exploiting cross-sentence context for neural machine translation\", \"Chain-of-thought prompting elicits reasoning in large language models\", \"mT5: A massively multilingual pre-trained text-to-text transformer\", \"Bigtrans: Augmenting large language models with multilingual translation capability over 100 languages\", \"Bridging cross-lingual alignment and instruction following through interactive translation for large language models\", \"Multilingual machine translation with large language models: Empirical results and analysis\"], \"Citation\":[]}\n[The End of Assistant's Predicted Answer]\n\n[System]\nWe would like to request your feedback on the performance of the AI assistant in response to the user question displayed above according to the gold answer. Please use the following listed aspects and their descriptions as evaluation criteria:\n    - Accuracy and Hallucinations: The assistant's answer is semantically consistent with the gold answer; The numerical value and order need to be accurate, and there should be no hallucinations.\n    - Completeness: Referring to the reference answers, the assistant's answer should contain all the key points needed to answer the user's question; further elaboration on these key points can be omitted.\nPlease rate whether this answer is suitable for the question. Please note that the gold answer can be considered as a correct answer to the question.\n\nThe assistant receives an overall score on a scale of 1 to 100, where a higher score indicates better overall performance.\nPlease note that if the assistant's answer and the gold answer fully meet the above criteria, its overall rating should be the full marks (100).\nPlease first provide a comprehensive explanation of your evaluation, avoiding any potential bias.\nThen, output a line indicating the score of the Assistant.\n\nPLEASE OUTPUT WITH THE FOLLOWING FORMAT, WHERE THE SCORE IS A SCALE OF 1 TO 100 BY STRICTLY FOLLOWING THIS FORMAT: \"[[score]]\", FOR EXAMPLE \"Rating: [[100]]\":\n<start output>\nEvaluation evidence: your evluation explanation here, no more than 100 words\nRating: [[score]]\n<end output>\n\nNow, start your evaluation:",
        "generate_response": "{\"Reference\":[\"Multidimensional Quality Metrics: a flexible system for assessing translation quality\", \"Overview of the IWSLT 2017 evaluation campaign\", \"Monolingual or multilingual instruction tuning: Which makes a better alpaca\", \"Vicuna: An Open-Source Chatbot Impressing GPT-4 with $90 \\% *$ ChatGPT Quality\", \"Chain-of-dictionary prompting elicits translation in large language models\", \"Wizardcoder: Empowering code large language models with evolinstruct\", \"A simple and effective unified encoder for documentlevel machine translation\", \"Using whole document context in neural machine translation\", \"Adaptive machine translation with large language models\", \"A large-scale test set for the evaluation of context-aware pronoun translation in neural machine translation\", \"Culturax: A cleaned, enormous, and multilingual dataset for large language models in 167 languages\", \"GPT-4 technical report\", \"Training language models to follow instructions with human feedback\", \"BLOOM: A 176b-parameter open-access multilingual language model\", \"Llama: Open and efficient foundation language models\", \"Llama 2: Open foundation and fine-tuned chat models\", \"Exploiting cross-sentence context for neural machine translation\", \"Chain-of-thought prompting elicits reasoning in large language models\", \"mT5: A massively multilingual pre-trained text-to-text transformer\", \"Bigtrans: Augmenting large language models with multilingual translation capability over 100 languages\", \"Bridging cross-lingual alignment and instruction following through interactive translation for large language models\", \"Multilingual machine translation with large language models: Empirical results and analysis\"], \"Citation\":[]}",
        "eval_response": "Evaluation evidence: The assistant's answer does not match the gold answer in terms of accuracy and completeness. The assistant listed numerous papers as references which are not mentioned in the gold answer, indicating a significant hallucination issue. Additionally, the assistant failed to identify any citations, whereas the gold answer lists one. This discrepancy shows a lack of accuracy in identifying the correct citation relationships and an overgeneration of references that are not verified by the gold answer.\n\nRating: [[20]]",
        "score": 20
    },
    {
        "level": 3,
        "set": 2,
        "length": 88368,
        "type": "paper",
        "language": "en",
        "question": "Mastering Text-to-Image Diffusion: Recaptioning, Planning, and Generating with Multimodal LLMs",
        "instruction": "We hope you will carefully study the provided papers and determine the citation relationships between them. Please follow the instructions below strictly to complete the task:\n\n#Specific Requirements:\n1. Reference: When a given paper mentions other provided papers, those other papers are considered as \"references\" for the given paper. To summarize in this specific context, references are about what the given paper is using.\n2. Citation: Conversely, when other provided papers mention the given paper in their works, the given paper is being \"cited\" by those other papers. To summarize in this specific context, citations are about who is using the given paper.\n3. Given a paper, you need to determine the citation or reference relationship between this paper and the other papers. Do not consider papers that are not provided.\n3. Please present the paper titles in a json format as follows: {{\"Reference\":[\"Reference Title 1\", \"Reference Title 2\", ..., \"Reference Title n\"], \"Citation\":[\"Citation Title 1\", \"Citation Title 2\", ..., \"Citation Title n\"]}}.\n4. If a paper does not have any references or citations, please leave the corresponding list empty, e.g.{{\"Refernce\":[]}}, {{\"Citation\":[]}}.",
        "prompt_template": "#Papers:\n{docs}\n\n{instruction}\n\n#The paper you need to analyze:\n{question}",
        "doc": [
            "2401.11708.md",
            "2405.08748.md",
            "2405.15287.md",
            "2311.17946.md",
            "2402.17563.md"
        ],
        "answer": {
            "Reference": [
                "# DreamSync: Aligning Text-to-Image Generation with Image Understanding Feedback ",
                "# Structure-Guided Adversarial Training of Diffusion Models "
            ],
            "Citation": [
                "# Hunyuan-DiT : A Powerful Multi-Resolution Diffusion Transformer with Fine-Grained Chinese Understanding ",
                "# StyleMaster: Towards Flexible Stylized Image Generation with Diffusion Models "
            ]
        },
        "shuffle_doc": true,
        "id": "ee2e0576-eccc-4b4f-8319-90f2a0a5ebc2",
        "prompt": "[Question]\n#Papers:\n{docs}\n\nWe hope you will carefully study the provided papers and determine the citation relationships between them. Please follow the instructions below strictly to complete the task:\n\n#Specific Requirements:\n1. Reference: When a given paper mentions other provided papers, those other papers are considered as \"references\" for the given paper. To summarize in this specific context, references are about what the given paper is using.\n2. Citation: Conversely, when other provided papers mention the given paper in their works, the given paper is being \"cited\" by those other papers. To summarize in this specific context, citations are about who is using the given paper.\n3. Given a paper, you need to determine the citation or reference relationship between this paper and the other papers. Do not consider papers that are not provided.\n3. Please present the paper titles in a json format as follows: {{\"Reference\":[\"Reference Title 1\", \"Reference Title 2\", ..., \"Reference Title n\"], \"Citation\":[\"Citation Title 1\", \"Citation Title 2\", ..., \"Citation Title n\"]}}.\n4. If a paper does not have any references or citations, please leave the corresponding list empty, e.g.{{\"Refernce\":[]}}, {{\"Citation\":[]}}.\n\n#The paper you need to analyze:\nMastering Text-to-Image Diffusion: Recaptioning, Planning, and Generating with Multimodal LLMs\n\n[Gold Answer]\n{'Reference': ['# DreamSync: Aligning Text-to-Image Generation with Image Understanding Feedback ', '# Structure-Guided Adversarial Training of Diffusion Models '], 'Citation': ['# Hunyuan-DiT : A Powerful Multi-Resolution Diffusion Transformer with Fine-Grained Chinese Understanding ', '# StyleMaster: Towards Flexible Stylized Image Generation with Diffusion Models ']}\n\n[The Start of Assistant's Predicted Answer]\n{\"Reference\":[\"Spatext: Spatio-textual representation for controllable image generation\", \"Multidiffusion: Fusing diffusion paths for controlled image generation\", \"Improving image generation with better captions\", \"Instructpix2pix: Learning to follow image editing instructions\", \"Masactrl: Tuning-free mutual self-attention control for consistent image synthesis and editing\", \"Introducing chatgpt\", \"Attend-and-excite: Attention-based semantic guidance for text-to-image diffusion models\", \"Pixart-alpha: Fast training of diffusion transformer for photorealistic text-to-image synthesis\", \"Training-free layout control with cross-attention guidance\", \"Llava-interactive: An all-in-one demo for image chat, segmentation, generation and editing\", \"Vicuna: An open-source chatbot impressing gpt-4 with $90 \\% *$ chatgpt quality\", \"Palm: Scaling language modeling with pathways\", \"Scaling instruction-finetuned language models\", \"Generative adversarial networks: An overview\", \"Diffusion models beat gans on image synthesis\", \"Dreamllm: Synergistic multimodal comprehension and creation\", \"Boosting text-to-image diffusion models with fine-grained semantic rewards\", \"Training-free structured diffusion guidance for compositional text-to-image synthesis\", \"Layoutgpt: Compositional visual planning and generation with large language models\", \"Guiding instruction-based image editing via multimodal large language models\", \"Visual programming: Compositional visual reasoning without training\", \"Learning to summarize with human feedback\", \"Dreamsync: Aligning text-to-image generation with image understanding feedback\", \"Generative pretraining in multimodality\", \"Vipergpt: Visual inference via python execution for reasoning\", \"Galactica: A large language model for science\", \"Llama: Open and efficient foundation language models\", \"Llama 2: Open foundation and finetuned chat models\", \"Compositional text-to-image synthesis with attention map control of diffusion models\", \"Boxdiff: Text-to-image synthesis with training-free box-constrained diffusion\", \"Improving diffusion-based image synthesis with context prediction\", \"Diffusion models: A comprehensive survey of methods and applications\", \"Structureguided adversarial training of diffusion models\", \"Cross-modal contextualized diffusion models for text-guided visual generation and editing\", \"Mm-react: Prompting chatgpt for multimodal reasoning and action\", \"Reco: Regioncontrolled text-to-image generation\", \"Scaling autoregressive multi-modal models: Pretraining and instruction tuning\", \"Glm-130b: An open bilingual pre-trained model\", \"Adding conditional control to text-to-image diffusion models\", \"Opt: Open pre-trained transformer language models\", \"Controllable text-to-image generation with gpt-4\", \"Enhanced visual instruction tuning for textrich image understanding\", \"Multimodal chain-of-thought reasoning in language models\", \"Minigpt-4: Enhancing vision-language understanding with advanced large language models\", \"Generalized decoding for pixel, image, and language\"], \"Citation\":[\"Composable Diffusion\", \"GLIDE\", \"Imagen\", \"Latent Diffusion Models\", \"SDXL\", \"ContextDiff\", \"DALL-E 3\", \"GLIGEN\", \"ReCo\", \"T2I-Adapter\", \"ControlNet\", \"Composer\", \"GILL\", \"Stablellava\", \"RPG\", \"LMD\", \"GORS\", \"Compositional Visual Generation with Composable Diffusion Models\", \"LayoutLLMt2i\", \"BoxDiff\", \"Compositional Text-to-Image Synthesis with Attention Map Control of Diffusion Models\", \"Baichuan 2\", \"ConPreDiff\", \"StructureGuided Adversarial Training of Diffusion Models\", \"Cross-Modal Contextualized Diffusion Models for Text-Guided Visual Generation and Editing\", \"MM-REACT\", \"ReCO\", \"Scaling Autoregressive Multi-Modal Models: Pretraining and Instruction Tuning\", \"GLM-130B\", \"Adding Conditional Control to Text-to-Image Diffusion Models\", \"Opt\", \"Controllable Text-to-Image Generation with GPT-4\", \"Enhanced Visual Instruction Tuning for Text-Rich Image Understanding\", \"Multimodal Chain-of-Thought Reasoning in Language Models\", \"MiniGPT-4\", \"Generalized Decoding for Pixel, Image, and Language\"]}\n[The End of Assistant's Predicted Answer]\n\n[System]\nWe would like to request your feedback on the performance of the AI assistant in response to the user question displayed above according to the gold answer. Please use the following listed aspects and their descriptions as evaluation criteria:\n    - Accuracy and Hallucinations: The assistant's answer is semantically consistent with the gold answer; The numerical value and order need to be accurate, and there should be no hallucinations.\n    - Completeness: Referring to the reference answers, the assistant's answer should contain all the key points needed to answer the user's question; further elaboration on these key points can be omitted.\nPlease rate whether this answer is suitable for the question. Please note that the gold answer can be considered as a correct answer to the question.\n\nThe assistant receives an overall score on a scale of 1 to 100, where a higher score indicates better overall performance.\nPlease note that if the assistant's answer and the gold answer fully meet the above criteria, its overall rating should be the full marks (100).\nPlease first provide a comprehensive explanation of your evaluation, avoiding any potential bias.\nThen, output a line indicating the score of the Assistant.\n\nPLEASE OUTPUT WITH THE FOLLOWING FORMAT, WHERE THE SCORE IS A SCALE OF 1 TO 100 BY STRICTLY FOLLOWING THIS FORMAT: \"[[score]]\", FOR EXAMPLE \"Rating: [[100]]\":\n<start output>\nEvaluation evidence: your evluation explanation here, no more than 100 words\nRating: [[score]]\n<end output>\n\nNow, start your evaluation:",
        "generate_response": "{\"Reference\":[\"Spatext: Spatio-textual representation for controllable image generation\", \"Multidiffusion: Fusing diffusion paths for controlled image generation\", \"Improving image generation with better captions\", \"Instructpix2pix: Learning to follow image editing instructions\", \"Masactrl: Tuning-free mutual self-attention control for consistent image synthesis and editing\", \"Introducing chatgpt\", \"Attend-and-excite: Attention-based semantic guidance for text-to-image diffusion models\", \"Pixart-alpha: Fast training of diffusion transformer for photorealistic text-to-image synthesis\", \"Training-free layout control with cross-attention guidance\", \"Llava-interactive: An all-in-one demo for image chat, segmentation, generation and editing\", \"Vicuna: An open-source chatbot impressing gpt-4 with $90 \\% *$ chatgpt quality\", \"Palm: Scaling language modeling with pathways\", \"Scaling instruction-finetuned language models\", \"Generative adversarial networks: An overview\", \"Diffusion models beat gans on image synthesis\", \"Dreamllm: Synergistic multimodal comprehension and creation\", \"Boosting text-to-image diffusion models with fine-grained semantic rewards\", \"Training-free structured diffusion guidance for compositional text-to-image synthesis\", \"Layoutgpt: Compositional visual planning and generation with large language models\", \"Guiding instruction-based image editing via multimodal large language models\", \"Visual programming: Compositional visual reasoning without training\", \"Learning to summarize with human feedback\", \"Dreamsync: Aligning text-to-image generation with image understanding feedback\", \"Generative pretraining in multimodality\", \"Vipergpt: Visual inference via python execution for reasoning\", \"Galactica: A large language model for science\", \"Llama: Open and efficient foundation language models\", \"Llama 2: Open foundation and finetuned chat models\", \"Compositional text-to-image synthesis with attention map control of diffusion models\", \"Boxdiff: Text-to-image synthesis with training-free box-constrained diffusion\", \"Improving diffusion-based image synthesis with context prediction\", \"Diffusion models: A comprehensive survey of methods and applications\", \"Structureguided adversarial training of diffusion models\", \"Cross-modal contextualized diffusion models for text-guided visual generation and editing\", \"Mm-react: Prompting chatgpt for multimodal reasoning and action\", \"Reco: Regioncontrolled text-to-image generation\", \"Scaling autoregressive multi-modal models: Pretraining and instruction tuning\", \"Glm-130b: An open bilingual pre-trained model\", \"Adding conditional control to text-to-image diffusion models\", \"Opt: Open pre-trained transformer language models\", \"Controllable text-to-image generation with gpt-4\", \"Enhanced visual instruction tuning for textrich image understanding\", \"Multimodal chain-of-thought reasoning in language models\", \"Minigpt-4: Enhancing vision-language understanding with advanced large language models\", \"Generalized decoding for pixel, image, and language\"], \"Citation\":[\"Composable Diffusion\", \"GLIDE\", \"Imagen\", \"Latent Diffusion Models\", \"SDXL\", \"ContextDiff\", \"DALL-E 3\", \"GLIGEN\", \"ReCo\", \"T2I-Adapter\", \"ControlNet\", \"Composer\", \"GILL\", \"Stablellava\", \"RPG\", \"LMD\", \"GORS\", \"Compositional Visual Generation with Composable Diffusion Models\", \"LayoutLLMt2i\", \"BoxDiff\", \"Compositional Text-to-Image Synthesis with Attention Map Control of Diffusion Models\", \"Baichuan 2\", \"ConPreDiff\", \"StructureGuided Adversarial Training of Diffusion Models\", \"Cross-Modal Contextualized Diffusion Models for Text-Guided Visual Generation and Editing\", \"MM-REACT\", \"ReCO\", \"Scaling Autoregressive Multi-Modal Models: Pretraining and Instruction Tuning\", \"GLM-130B\", \"Adding Conditional Control to Text-to-Image Diffusion Models\", \"Opt\", \"Controllable Text-to-Image Generation with GPT-4\", \"Enhanced Visual Instruction Tuning for Text-Rich Image Understanding\", \"Multimodal Chain-of-Thought Reasoning in Language Models\", \"MiniGPT-4\", \"Generalized Decoding for Pixel, Image, and Language\"]}",
        "eval_response": "Evaluation evidence: The assistant's answer is not accurate and contains hallucinations. The provided references and citations do not match those in the gold answer, indicating a misunderstanding of the task. The assistant listed numerous unrelated titles, showing a lack of focus on the specific papers provided. This results in a failure to meet the accuracy and completeness criteria.\n\nRating: [[10]]",
        "score": 10
    },
    {
        "level": 3,
        "set": 4,
        "length": 201072,
        "type": "paper",
        "language": "en",
        "question": "Evolution of Heuristics: Towards Efficient Automatic Algorithm Design Using Large Language Model",
        "instruction": "We hope you will carefully study the provided papers and determine the citation relationships between them. Please follow the instructions below strictly to complete the task:\n\n#Specific Requirements:\n1. Reference: When a given paper mentions other provided papers, those other papers are considered as \"references\" for the given paper. To summarize in this specific context, references are about what the given paper is using.\n2. Citation: Conversely, when other provided papers mention the given paper in their works, the given paper is being \"cited\" by those other papers. To summarize in this specific context, citations are about who is using the given paper.\n3. Given a paper, you need to determine the citation or reference relationship between this paper and the other papers. Do not consider papers that are not provided.\n3. Please present the paper titles in a json format as follows: {{\"Reference\":[\"Reference Title 1\", \"Reference Title 2\", ..., \"Reference Title n\"], \"Citation\":[\"Citation Title 1\", \"Citation Title 2\", ..., \"Citation Title n\"]}}.\n4. If a paper does not have any references or citations, please leave the corresponding list empty, e.g.{{\"Refernce\":[]}}, {{\"Citation\":[]}}.",
        "prompt_template": "#Papers:\n{docs}\n\n{instruction}\n\n#The paper you need to analyze:\n{question}",
        "doc": [
            "2401.02051.md",
            "2402.01145.md",
            "2402.10705.md",
            "2403.07559.md",
            "2404.16906.md",
            "2405.17950.md",
            "2310.12541.md",
            "2311.15249.md",
            "2312.01797.md",
            "2401.07102.md",
            "2401.10034.md"
        ],
        "answer": {
            "Reference": [
                "# Large Language Model for Multi-objective Evolutionary Optimization ",
                "# Algorithm Evolution Using Large Language Model ",
                "# LLM A*: Human in the Loop Large Language Models Enabled A* Search for Robotics ",
                "# Evolving Code with A Large Language Model "
            ],
            "Citation": [
                "# Large Language Models as Hyper-Heuristics for Combinatorial Optimization ",
                "# AutoSAT: Automatically Optimize SAT Solvers via Large Language Models ",
                "# Ensembling Prioritized Hybrid Policies for Multi-agent Pathfinding ",
                "# Evolve Cost-aware Acquisition Functions Using Large Language Models ",
                "# Self-Guiding Exploration for Combinatorial Problems "
            ]
        },
        "shuffle_doc": true,
        "id": "afb93dcd-b82c-41de-82de-115325b3323e",
        "prompt": "[Question]\n#Papers:\n{docs}\n\nWe hope you will carefully study the provided papers and determine the citation relationships between them. Please follow the instructions below strictly to complete the task:\n\n#Specific Requirements:\n1. Reference: When a given paper mentions other provided papers, those other papers are considered as \"references\" for the given paper. To summarize in this specific context, references are about what the given paper is using.\n2. Citation: Conversely, when other provided papers mention the given paper in their works, the given paper is being \"cited\" by those other papers. To summarize in this specific context, citations are about who is using the given paper.\n3. Given a paper, you need to determine the citation or reference relationship between this paper and the other papers. Do not consider papers that are not provided.\n3. Please present the paper titles in a json format as follows: {{\"Reference\":[\"Reference Title 1\", \"Reference Title 2\", ..., \"Reference Title n\"], \"Citation\":[\"Citation Title 1\", \"Citation Title 2\", ..., \"Citation Title n\"]}}.\n4. If a paper does not have any references or citations, please leave the corresponding list empty, e.g.{{\"Refernce\":[]}}, {{\"Citation\":[]}}.\n\n#The paper you need to analyze:\nEvolution of Heuristics: Towards Efficient Automatic Algorithm Design Using Large Language Model\n\n[Gold Answer]\n{'Reference': ['# Large Language Model for Multi-objective Evolutionary Optimization ', '# Algorithm Evolution Using Large Language Model ', '# LLM A*: Human in the Loop Large Language Models Enabled A* Search for Robotics ', '# Evolving Code with A Large Language Model '], 'Citation': ['# Large Language Models as Hyper-Heuristics for Combinatorial Optimization ', '# AutoSAT: Automatically Optimize SAT Solvers via Large Language Models ', '# Ensembling Prioritized Hybrid Policies for Multi-agent Pathfinding ', '# Evolve Cost-aware Acquisition Functions Using Large Language Models ', '# Self-Guiding Exploration for Combinatorial Problems ']}\n\n[The Start of Assistant's Predicted Answer]\n{\"Reference\": [\"Optuna: A next-generation hyperparameter optimization framework\", \"Guided local search.\", \"Online bin packing with predictions\", \"Concorde tsp solver\", \"Knowledge-guided local search for the vehicle routing problem\", \"Program synthesis with large language models\", \"Handbook of evolutionary computation\", \"Neural combinatorial optimization with reinforcement learning\", \"Machine learning for combinatorial optimization: a methodological tour d'horizon\", \"Mo-paramils: A multi-objective automatic algorithm configuration framework\", \"Survey on evolutionary deep learning: Principles, algorithms, applications, and open issues\", \"Starcoder: may the source be with you!\", \"Large language model for multi-objective evolutionary optimization\", \"Algorithm evolution using large language model\", \"Fully autonomous programming with large language models.\", \"Large language model guided tree-of-thought\", \"The irace package: Iterated racing for automatic algorithm configuration\", \"Iterated local search\", \"Neural combinatorial optimization with heavy decoder: Toward large scale generalization\", \"Eureka: Human-level reward design via coding large language models\", \"Dissociating language and thought in large language models: a cognitive perspective\", \"Handbook of collective intelligence\", \"Handbook of heuristics\", \"Lower bounds and reduction procedures for the bin packing problem\", \"Traveling salesman problem: an overview of applications, formulations, and solution approaches\", \"Explainable artificial intelligence by genetic programming: A survey\", \"Language model crossover: Variation through few-shot prompting\", \"Llmatic: Neural architecture search via large language models and quality-diversity optimization\", \"A comprehensive overview of large language models\", \"An analysis of several heuristics for the traveling salesman problem\", \"On the online bin packing problem\", \"Algorithm of thoughts: Enhancing exploration of ideas in large language models\", \"Navigation with large language models: Semantic guesswork as a heuristic for planning\", \"Eb-gls: an improved guided local search based on the big valley structure\", \"Applying iterated local search to the permutation flow shop problem\", \"Automated design of metaheuristic algorithms\", \"Neuralgls: learning to guide local search with graph convolutional network for the traveling salesman problem\", \"Benchmarks for basic scheduling problems\", \"Simulated annealing\", \"Pointer networks\", \"Chain-of-thought prompting elicits reasoning in large language models\", \"As-llm: When algorithm selection meets large language model\", \"Evolutionary computation in the era of large language model: Survey and roadmap\", \"Llm a*: Human in the loop large language models enabled a* search for robotics\", \"Step-wise deep learning models for solving routing problems\", \"Wizardlm: Empowering large language models to follow complex instructions\", \"Reprompting: Automated chain-of-thought prompt inference through gibbs sampling\", \"Large language models as optimizers\", \"Beyond chain-of-thought, effective graph-of-thought reasoning in large language models\", \"Survey on genetic programming and machine learning techniques for heuristic design in job shop scheduling\", \"Large language models as commonsense knowledge for large-scale task planning\"], \"Citation\": []}\n[The End of Assistant's Predicted Answer]\n\n[System]\nWe would like to request your feedback on the performance of the AI assistant in response to the user question displayed above according to the gold answer. Please use the following listed aspects and their descriptions as evaluation criteria:\n    - Accuracy and Hallucinations: The assistant's answer is semantically consistent with the gold answer; The numerical value and order need to be accurate, and there should be no hallucinations.\n    - Completeness: Referring to the reference answers, the assistant's answer should contain all the key points needed to answer the user's question; further elaboration on these key points can be omitted.\nPlease rate whether this answer is suitable for the question. Please note that the gold answer can be considered as a correct answer to the question.\n\nThe assistant receives an overall score on a scale of 1 to 100, where a higher score indicates better overall performance.\nPlease note that if the assistant's answer and the gold answer fully meet the above criteria, its overall rating should be the full marks (100).\nPlease first provide a comprehensive explanation of your evaluation, avoiding any potential bias.\nThen, output a line indicating the score of the Assistant.\n\nPLEASE OUTPUT WITH THE FOLLOWING FORMAT, WHERE THE SCORE IS A SCALE OF 1 TO 100 BY STRICTLY FOLLOWING THIS FORMAT: \"[[score]]\", FOR EXAMPLE \"Rating: [[100]]\":\n<start output>\nEvaluation evidence: your evluation explanation here, no more than 100 words\nRating: [[score]]\n<end output>\n\nNow, start your evaluation:",
        "generate_response": "{\"Reference\": [\"Optuna: A next-generation hyperparameter optimization framework\", \"Guided local search.\", \"Online bin packing with predictions\", \"Concorde tsp solver\", \"Knowledge-guided local search for the vehicle routing problem\", \"Program synthesis with large language models\", \"Handbook of evolutionary computation\", \"Neural combinatorial optimization with reinforcement learning\", \"Machine learning for combinatorial optimization: a methodological tour d'horizon\", \"Mo-paramils: A multi-objective automatic algorithm configuration framework\", \"Survey on evolutionary deep learning: Principles, algorithms, applications, and open issues\", \"Starcoder: may the source be with you!\", \"Large language model for multi-objective evolutionary optimization\", \"Algorithm evolution using large language model\", \"Fully autonomous programming with large language models.\", \"Large language model guided tree-of-thought\", \"The irace package: Iterated racing for automatic algorithm configuration\", \"Iterated local search\", \"Neural combinatorial optimization with heavy decoder: Toward large scale generalization\", \"Eureka: Human-level reward design via coding large language models\", \"Dissociating language and thought in large language models: a cognitive perspective\", \"Handbook of collective intelligence\", \"Handbook of heuristics\", \"Lower bounds and reduction procedures for the bin packing problem\", \"Traveling salesman problem: an overview of applications, formulations, and solution approaches\", \"Explainable artificial intelligence by genetic programming: A survey\", \"Language model crossover: Variation through few-shot prompting\", \"Llmatic: Neural architecture search via large language models and quality-diversity optimization\", \"A comprehensive overview of large language models\", \"An analysis of several heuristics for the traveling salesman problem\", \"On the online bin packing problem\", \"Algorithm of thoughts: Enhancing exploration of ideas in large language models\", \"Navigation with large language models: Semantic guesswork as a heuristic for planning\", \"Eb-gls: an improved guided local search based on the big valley structure\", \"Applying iterated local search to the permutation flow shop problem\", \"Automated design of metaheuristic algorithms\", \"Neuralgls: learning to guide local search with graph convolutional network for the traveling salesman problem\", \"Benchmarks for basic scheduling problems\", \"Simulated annealing\", \"Pointer networks\", \"Chain-of-thought prompting elicits reasoning in large language models\", \"As-llm: When algorithm selection meets large language model\", \"Evolutionary computation in the era of large language model: Survey and roadmap\", \"Llm a*: Human in the loop large language models enabled a* search for robotics\", \"Step-wise deep learning models for solving routing problems\", \"Wizardlm: Empowering large language models to follow complex instructions\", \"Reprompting: Automated chain-of-thought prompt inference through gibbs sampling\", \"Large language models as optimizers\", \"Beyond chain-of-thought, effective graph-of-thought reasoning in large language models\", \"Survey on genetic programming and machine learning techniques for heuristic design in job shop scheduling\", \"Large language models as commonsense knowledge for large-scale task planning\"], \"Citation\": []}",
        "eval_response": "Evaluation evidence: The assistant's answer does not match the gold answer in terms of both references and citations. The references listed in the assistant's answer are entirely different from those in the gold answer, indicating a significant hallucination issue. Additionally, the assistant failed to list any citations, whereas the gold answer provided several. This shows a lack of completeness and accuracy in the assistant's response.\n\nRating: [[10]]",
        "score": 10
    },
    {
        "level": 3,
        "set": 3,
        "length": 111111,
        "type": "paper",
        "language": "en",
        "question": "Multilingual Instruction Tuning With Just a Pinch of Multilinguality",
        "instruction": "We hope you will carefully study the provided papers and determine the citation relationships between them. Please follow the instructions below strictly to complete the task:\n\n#Specific Requirements:\n1. Reference: When a given paper mentions other provided papers, those other papers are considered as \"references\" for the given paper. To summarize in this specific context, references are about what the given paper is using.\n2. Citation: Conversely, when other provided papers mention the given paper in their works, the given paper is being \"cited\" by those other papers. To summarize in this specific context, citations are about who is using the given paper.\n3. Given a paper, you need to determine the citation or reference relationship between this paper and the other papers. Do not consider papers that are not provided.\n3. Please present the paper titles in a json format as follows: {{\"Reference\":[\"Reference Title 1\", \"Reference Title 2\", ..., \"Reference Title n\"], \"Citation\":[\"Citation Title 1\", \"Citation Title 2\", ..., \"Citation Title n\"]}}.\n4. If a paper does not have any references or citations, please leave the corresponding list empty, e.g.{{\"Refernce\":[]}}, {{\"Citation\":[]}}.",
        "prompt_template": "#Papers:\n{docs}\n\n{instruction}\n\n#The paper you need to analyze:\n{question}",
        "doc": [
            "2401.01854.md",
            "2404.04850.md",
            "2405.15604.md",
            "2306.05685.md",
            "2312.12683.md"
        ],
        "answer": {
            "Reference": [
                "# Judging LLM-as-a-Judge with MT-Bench and Chatbot Arena ",
                "# Turning English-centric LLMs Into Polyglots: How Much Multilinguality Is Needed? "
            ],
            "Citation": [
                "# Lucky 52: How Many Languages Are Needed to Instruction Fine-Tune Large Language Models? ",
                "# Text Generation: A Systematic Literature Review of Tasks, Evaluation, and Challenges "
            ]
        },
        "shuffle_doc": true,
        "id": "c2ebfee0-9748-418e-8c3f-528ea2a2d19a",
        "prompt": "[Question]\n#Papers:\n{docs}\n\nWe hope you will carefully study the provided papers and determine the citation relationships between them. Please follow the instructions below strictly to complete the task:\n\n#Specific Requirements:\n1. Reference: When a given paper mentions other provided papers, those other papers are considered as \"references\" for the given paper. To summarize in this specific context, references are about what the given paper is using.\n2. Citation: Conversely, when other provided papers mention the given paper in their works, the given paper is being \"cited\" by those other papers. To summarize in this specific context, citations are about who is using the given paper.\n3. Given a paper, you need to determine the citation or reference relationship between this paper and the other papers. Do not consider papers that are not provided.\n3. Please present the paper titles in a json format as follows: {{\"Reference\":[\"Reference Title 1\", \"Reference Title 2\", ..., \"Reference Title n\"], \"Citation\":[\"Citation Title 1\", \"Citation Title 2\", ..., \"Citation Title n\"]}}.\n4. If a paper does not have any references or citations, please leave the corresponding list empty, e.g.{{\"Refernce\":[]}}, {{\"Citation\":[]}}.\n\n#The paper you need to analyze:\nMultilingual Instruction Tuning With Just a Pinch of Multilinguality\n\n[Gold Answer]\n{'Reference': ['# Judging LLM-as-a-Judge with MT-Bench and Chatbot Arena ', '# Turning English-centric LLMs Into Polyglots: How Much Multilinguality Is Needed? '], 'Citation': ['# Lucky 52: How Many Languages Are Needed to Instruction Fine-Tune Large Language Models? ', '# Text Generation: A Systematic Literature Review of Tasks, Evaluation, and Challenges ']}\n\n[The Start of Assistant's Predicted Answer]\n{\"Reference\":[\"A Survey on Evaluation of Large Language Models\", \"Vicuna: An Open-Source Chatbot Impressing GPT-4 with $90 \\% *$ ChatGPT Quality\", \"Atlas: Few-shot Learning with Retrieval Augmented Language Models\", \"Causes and Cures for Interference in Multilingual Translation\", \"Stanford Alpaca: An Instruction-Following Llama Model\", \"Llama: Open and Efficient Foundation Language Models\", \"Llama 2: Open Foundation and Fine-Tuned Chat Models\", \"Attention is all you need\", \"Self-instruct: Aligning Language Models with Self-Generated Instructions\", \"Finetuned Language Models are Zero-shot Learners\", \"mT5: A Massively Multilingual Pre-trained Text-to-text Transformer\", \"Language Versatilists vs. Specialists: An Empirical Revisiting on Multilingual Transfer Ability\", \"Plug: Leveraging Pivot Language in Crosslingual Instruction Tuning\", \"Judging LLM-as-a-Judge with MT-Bench and Chatbot Arena\", \"Less is More for Alignment\"], \"Citation\":[\"Exploring the Limits of Transfer Learning with a Unified Text-to-text Transformer\", \"Massively Multilingual Sentence Embeddings for Zero-shot Cross-lingual Transfer and Beyond\", \"Training a Helpful and Harmless Assistant with Reinforcement Learning from Human Feedback\", \"Sparks of Artificial General Intelligence: Early Experiments with GPT-4\", \"Monolingual or Multilingual Instruction Tuning: Which Makes a Better Alpaca\", \"How do Languages Influence Each Other? Studying Cross-lingual Data Sharing During LM Fine-tuning\", \"Unsupervised Cross-lingual Representation Learning at Scale\", \"Emerging Cross-lingual Structure in Pretrained Language Models\", \"QLoRA: Efficient Fine-tuning of Quantized LLMs\", \"BERT: Pre-training of Deep Bidirectional Transformers for Language Understanding\", \"Alpacafarm: A Simulation Framework for Methods that Learn from Human Feedback\", \"Scaling Laws for Multilingual Neural Machine Translation\", \"Koala: A Dialogue Model for Academic Research\", \"The False Promise of Imitating Proprietary LLMs\", \"XTREME: A Massively Multilingual MultiTask Benchmark for Evaluating Cross-lingual Generalisation\", \"Okapi: Instruction-Tuned Large Language Models in Multiple Languages with Reinforcement Learning from Human Feedback\", \"Self-alignment with Instruction Backtranslation\", \"ROUGE: A Package for Automatic Evaluation of Summaries\", \"A Balanced Data Approach for Evaluating Cross-Lingual Transfer: Mapping the Linguistic Blood Bank\", \"Cross-task Generalization via Natural Language Crowdsourcing Instructions\", \"Crosslingual Generalization through Multitask Fine-tuning\", \"Multitask Prompted Training Enables Zero-shot Task Generalization\", \"How Multilingual is Multilingual BERT?\", \"MAD-X: An Adapter-Based Framework for Multi-Task Cross-Lingual Transfer\", \"English Intermediate-Task Training Improves Zero-shot Cross-lingual Transfer Too\", \"Causes and Cures for Interference in Multilingual Translation\", \"Stanford Alpaca: An Instruction-Following Llama Model\", \"Llama: Open and Efficient Foundation Language Models\", \"Llama 2: Open Foundation and Fine-Tuned Chat Models\", \"Less is More for Alignment\"]}\n[The End of Assistant's Predicted Answer]\n\n[System]\nWe would like to request your feedback on the performance of the AI assistant in response to the user question displayed above according to the gold answer. Please use the following listed aspects and their descriptions as evaluation criteria:\n    - Accuracy and Hallucinations: The assistant's answer is semantically consistent with the gold answer; The numerical value and order need to be accurate, and there should be no hallucinations.\n    - Completeness: Referring to the reference answers, the assistant's answer should contain all the key points needed to answer the user's question; further elaboration on these key points can be omitted.\nPlease rate whether this answer is suitable for the question. Please note that the gold answer can be considered as a correct answer to the question.\n\nThe assistant receives an overall score on a scale of 1 to 100, where a higher score indicates better overall performance.\nPlease note that if the assistant's answer and the gold answer fully meet the above criteria, its overall rating should be the full marks (100).\nPlease first provide a comprehensive explanation of your evaluation, avoiding any potential bias.\nThen, output a line indicating the score of the Assistant.\n\nPLEASE OUTPUT WITH THE FOLLOWING FORMAT, WHERE THE SCORE IS A SCALE OF 1 TO 100 BY STRICTLY FOLLOWING THIS FORMAT: \"[[score]]\", FOR EXAMPLE \"Rating: [[100]]\":\n<start output>\nEvaluation evidence: your evluation explanation here, no more than 100 words\nRating: [[score]]\n<end output>\n\nNow, start your evaluation:",
        "generate_response": "{\"Reference\":[\"A Survey on Evaluation of Large Language Models\", \"Vicuna: An Open-Source Chatbot Impressing GPT-4 with $90 \\% *$ ChatGPT Quality\", \"Atlas: Few-shot Learning with Retrieval Augmented Language Models\", \"Causes and Cures for Interference in Multilingual Translation\", \"Stanford Alpaca: An Instruction-Following Llama Model\", \"Llama: Open and Efficient Foundation Language Models\", \"Llama 2: Open Foundation and Fine-Tuned Chat Models\", \"Attention is all you need\", \"Self-instruct: Aligning Language Models with Self-Generated Instructions\", \"Finetuned Language Models are Zero-shot Learners\", \"mT5: A Massively Multilingual Pre-trained Text-to-text Transformer\", \"Language Versatilists vs. Specialists: An Empirical Revisiting on Multilingual Transfer Ability\", \"Plug: Leveraging Pivot Language in Crosslingual Instruction Tuning\", \"Judging LLM-as-a-Judge with MT-Bench and Chatbot Arena\", \"Less is More for Alignment\"], \"Citation\":[\"Exploring the Limits of Transfer Learning with a Unified Text-to-text Transformer\", \"Massively Multilingual Sentence Embeddings for Zero-shot Cross-lingual Transfer and Beyond\", \"Training a Helpful and Harmless Assistant with Reinforcement Learning from Human Feedback\", \"Sparks of Artificial General Intelligence: Early Experiments with GPT-4\", \"Monolingual or Multilingual Instruction Tuning: Which Makes a Better Alpaca\", \"How do Languages Influence Each Other? Studying Cross-lingual Data Sharing During LM Fine-tuning\", \"Unsupervised Cross-lingual Representation Learning at Scale\", \"Emerging Cross-lingual Structure in Pretrained Language Models\", \"QLoRA: Efficient Fine-tuning of Quantized LLMs\", \"BERT: Pre-training of Deep Bidirectional Transformers for Language Understanding\", \"Alpacafarm: A Simulation Framework for Methods that Learn from Human Feedback\", \"Scaling Laws for Multilingual Neural Machine Translation\", \"Koala: A Dialogue Model for Academic Research\", \"The False Promise of Imitating Proprietary LLMs\", \"XTREME: A Massively Multilingual MultiTask Benchmark for Evaluating Cross-lingual Generalisation\", \"Okapi: Instruction-Tuned Large Language Models in Multiple Languages with Reinforcement Learning from Human Feedback\", \"Self-alignment with Instruction Backtranslation\", \"ROUGE: A Package for Automatic Evaluation of Summaries\", \"A Balanced Data Approach for Evaluating Cross-Lingual Transfer: Mapping the Linguistic Blood Bank\", \"Cross-task Generalization via Natural Language Crowdsourcing Instructions\", \"Crosslingual Generalization through Multitask Fine-tuning\", \"Multitask Prompted Training Enables Zero-shot Task Generalization\", \"How Multilingual is Multilingual BERT?\", \"MAD-X: An Adapter-Based Framework for Multi-Task Cross-Lingual Transfer\", \"English Intermediate-Task Training Improves Zero-shot Cross-lingual Transfer Too\", \"Causes and Cures for Interference in Multilingual Translation\", \"Stanford Alpaca: An Instruction-Following Llama Model\", \"Llama: Open and Efficient Foundation Language Models\", \"Llama 2: Open Foundation and Fine-Tuned Chat Models\", \"Less is More for Alignment\"]}",
        "eval_response": "Evaluation evidence: The assistant's answer does not match the gold answer in terms of accuracy and completeness. The references and citations listed in the assistant's response do not align with those in the gold answer. The assistant incorrectly lists multiple papers that are not mentioned in the gold answer, indicating hallucinations and inaccuracies. The correct references and citations from the gold answer are missing in the assistant's response, leading to a lack of completeness.\n\nRating: [[10]]",
        "score": 10
    },
    {
        "level": 3,
        "set": 3,
        "length": 137277,
        "type": "paper",
        "language": "en",
        "question": "Patchscopes: A Unifying Framework for Inspecting Hidden Representations of Language Models",
        "instruction": "We hope you will carefully study the provided papers and determine the citation relationships between them. Please follow the instructions below strictly to complete the task:\n\n#Specific Requirements:\n1. Reference: When a given paper mentions other provided papers, those other papers are considered as \"references\" for the given paper. To summarize in this specific context, references are about what the given paper is using.\n2. Citation: Conversely, when other provided papers mention the given paper in their works, the given paper is being \"cited\" by those other papers. To summarize in this specific context, citations are about who is using the given paper.\n3. Given a paper, you need to determine the citation or reference relationship between this paper and the other papers. Do not consider papers that are not provided.\n3. Please present the paper titles in a json format as follows: {{\"Reference\":[\"Reference Title 1\", \"Reference Title 2\", ..., \"Reference Title n\"], \"Citation\":[\"Citation Title 1\", \"Citation Title 2\", ..., \"Citation Title n\"]}}.\n4. If a paper does not have any references or citations, please leave the corresponding list empty, e.g.{{\"Refernce\":[]}}, {{\"Citation\":[]}}.",
        "prompt_template": "#Papers:\n{docs}\n\n{instruction}\n\n#The paper you need to analyze:\n{question}",
        "doc": [
            "2401.06102.md",
            "2404.14082.md",
            "2405.17653.md",
            "2405.20778.md"
        ],
        "answer": {
            "Reference": [],
            "Citation": [
                "# Mechanistic Interpretability for AI Safety A Review ",
                "# InversionView: A General-Purpose Method for Reading Information from Neural Activations ",
                "# Improved Generation of Adversarial Examples Against Safety-aligned LLMs "
            ]
        },
        "shuffle_doc": true,
        "id": "2941580d-7018-4c0f-aa7b-97aaa2d73dcb",
        "prompt": "[Question]\n#Papers:\n{docs}\n\nWe hope you will carefully study the provided papers and determine the citation relationships between them. Please follow the instructions below strictly to complete the task:\n\n#Specific Requirements:\n1. Reference: When a given paper mentions other provided papers, those other papers are considered as \"references\" for the given paper. To summarize in this specific context, references are about what the given paper is using.\n2. Citation: Conversely, when other provided papers mention the given paper in their works, the given paper is being \"cited\" by those other papers. To summarize in this specific context, citations are about who is using the given paper.\n3. Given a paper, you need to determine the citation or reference relationship between this paper and the other papers. Do not consider papers that are not provided.\n3. Please present the paper titles in a json format as follows: {{\"Reference\":[\"Reference Title 1\", \"Reference Title 2\", ..., \"Reference Title n\"], \"Citation\":[\"Citation Title 1\", \"Citation Title 2\", ..., \"Citation Title n\"]}}.\n4. If a paper does not have any references or citations, please leave the corresponding list empty, e.g.{{\"Refernce\":[]}}, {{\"Citation\":[]}}.\n\n#The paper you need to analyze:\nPatchscopes: A Unifying Framework for Inspecting Hidden Representations of Language Models\n\n[Gold Answer]\n{'Reference': [], 'Citation': ['# Mechanistic Interpretability for AI Safety A Review ', '# InversionView: A General-Purpose Method for Reading Information from Neural Activations ', '# Improved Generation of Adversarial Examples Against Safety-aligned LLMs ']}\n\n[The Start of Assistant's Predicted Answer]\n{\"Reference\": [\"Understanding intermediate layers using linear classifier probes\", \"Revisiting model stitching to compare neural representations\", \"Probing classifiers: Promises, shortcomings, and advances\", \"Analysis methods in neural language processing: A survey\", \"A Unifying Framework for Interpreting Deep Neural Networks\", \"The expressive power of transformers with chain of thought\", \"Linearly mapping from image to text space\", \"A mechanism for solving relational tasks in transformer language models\", \"Can llms facilitate interpretation of pre-trained language models?\", \"The curious case of hallucinatory (un)answerability: Finding truths in the hidden states of over-confident large language models\", \"A mechanistic interpretation of arithmetic reasoning in language models using causal mediation analysis\", \"Investigating gender bias in language models using causal mediation analysis\", \"GPT-J-6B: A 6 Billion Parameter Autoregressive Language Model\", \"Chain-of-thought prompting elicits reasoning in large language models\", \"Tree of thoughts: Deliberate problem solving with large language models\", \"Give me the facts! a survey on factual knowledge probing in pre-trained language models\", \"Towards best practices of activation patching in language models: Metrics and methods\", \"Interpreting deep visual representations via network dissection\"], \"Citation\": [\"Understanding intermediate layers using linear classifier probes\", \"Revisiting model stitching to compare neural representations\", \"Probing classifiers: Promises, shortcomings, and advances\", \"Analysis methods in neural language processing: A survey\", \"A Unifying Framework for Interpreting Deep Neural Networks\", \"The expressive power of transformers with chain of thought\", \"Linearly mapping from image to text space\", \"A mechanism for solving relational tasks in transformer language models\", \"Can llms facilitate interpretation of pre-trained language models?\", \"The curious case of hallucinatory (un)answerability: Finding truths in the hidden states of over-confident large language models\", \"A mechanistic interpretation of arithmetic reasoning in language models using causal mediation analysis\", \"Investigating gender bias in language models using causal mediation analysis\", \"GPT-J-6B: A 6 Billion Parameter Autoregressive Language Model\", \"Chain-of-thought prompting elicits reasoning in large language models\", \"Tree of thoughts: Deliberate problem solving with large language models\", \"Give me the facts! a survey on factual knowledge probing in pre-trained language models\", \"Towards best practices of activation patching in language models: Metrics and methods\", \"Interpreting deep visual representations via network dissection\"]}\n[The End of Assistant's Predicted Answer]\n\n[System]\nWe would like to request your feedback on the performance of the AI assistant in response to the user question displayed above according to the gold answer. Please use the following listed aspects and their descriptions as evaluation criteria:\n    - Accuracy and Hallucinations: The assistant's answer is semantically consistent with the gold answer; The numerical value and order need to be accurate, and there should be no hallucinations.\n    - Completeness: Referring to the reference answers, the assistant's answer should contain all the key points needed to answer the user's question; further elaboration on these key points can be omitted.\nPlease rate whether this answer is suitable for the question. Please note that the gold answer can be considered as a correct answer to the question.\n\nThe assistant receives an overall score on a scale of 1 to 100, where a higher score indicates better overall performance.\nPlease note that if the assistant's answer and the gold answer fully meet the above criteria, its overall rating should be the full marks (100).\nPlease first provide a comprehensive explanation of your evaluation, avoiding any potential bias.\nThen, output a line indicating the score of the Assistant.\n\nPLEASE OUTPUT WITH THE FOLLOWING FORMAT, WHERE THE SCORE IS A SCALE OF 1 TO 100 BY STRICTLY FOLLOWING THIS FORMAT: \"[[score]]\", FOR EXAMPLE \"Rating: [[100]]\":\n<start output>\nEvaluation evidence: your evluation explanation here, no more than 100 words\nRating: [[score]]\n<end output>\n\nNow, start your evaluation:",
        "generate_response": "{\"Reference\": [\"Understanding intermediate layers using linear classifier probes\", \"Revisiting model stitching to compare neural representations\", \"Probing classifiers: Promises, shortcomings, and advances\", \"Analysis methods in neural language processing: A survey\", \"A Unifying Framework for Interpreting Deep Neural Networks\", \"The expressive power of transformers with chain of thought\", \"Linearly mapping from image to text space\", \"A mechanism for solving relational tasks in transformer language models\", \"Can llms facilitate interpretation of pre-trained language models?\", \"The curious case of hallucinatory (un)answerability: Finding truths in the hidden states of over-confident large language models\", \"A mechanistic interpretation of arithmetic reasoning in language models using causal mediation analysis\", \"Investigating gender bias in language models using causal mediation analysis\", \"GPT-J-6B: A 6 Billion Parameter Autoregressive Language Model\", \"Chain-of-thought prompting elicits reasoning in large language models\", \"Tree of thoughts: Deliberate problem solving with large language models\", \"Give me the facts! a survey on factual knowledge probing in pre-trained language models\", \"Towards best practices of activation patching in language models: Metrics and methods\", \"Interpreting deep visual representations via network dissection\"], \"Citation\": [\"Understanding intermediate layers using linear classifier probes\", \"Revisiting model stitching to compare neural representations\", \"Probing classifiers: Promises, shortcomings, and advances\", \"Analysis methods in neural language processing: A survey\", \"A Unifying Framework for Interpreting Deep Neural Networks\", \"The expressive power of transformers with chain of thought\", \"Linearly mapping from image to text space\", \"A mechanism for solving relational tasks in transformer language models\", \"Can llms facilitate interpretation of pre-trained language models?\", \"The curious case of hallucinatory (un)answerability: Finding truths in the hidden states of over-confident large language models\", \"A mechanistic interpretation of arithmetic reasoning in language models using causal mediation analysis\", \"Investigating gender bias in language models using causal mediation analysis\", \"GPT-J-6B: A 6 Billion Parameter Autoregressive Language Model\", \"Chain-of-thought prompting elicits reasoning in large language models\", \"Tree of thoughts: Deliberate problem solving with large language models\", \"Give me the facts! a survey on factual knowledge probing in pre-trained language models\", \"Towards best practices of activation patching in language models: Metrics and methods\", \"Interpreting deep visual representations via network dissection\"]}",
        "eval_response": "Evaluation evidence: The assistant's answer is incorrect and contains multiple hallucinations. The provided answer lists numerous references and citations that are not mentioned in the gold answer, indicating a misunderstanding of the task. The gold answer specifies that there are no references and lists three citations, none of which appear in the assistant's response. This discrepancy shows a lack of accuracy and completeness in addressing the user's question.\n\nRating: [[10]]",
        "score": 10
    },
    {
        "level": 3,
        "set": 2,
        "length": 88704,
        "type": "paper",
        "language": "en",
        "question": "Scalable Pre-training of Large Autoregressive Image Models",
        "instruction": "We hope you will carefully study the provided papers and determine the citation relationships between them. Please follow the instructions below strictly to complete the task:\n\n#Specific Requirements:\n1. Reference: When a given paper mentions other provided papers, those other papers are considered as \"references\" for the given paper. To summarize in this specific context, references are about what the given paper is using.\n2. Citation: Conversely, when other provided papers mention the given paper in their works, the given paper is being \"cited\" by those other papers. To summarize in this specific context, citations are about who is using the given paper.\n3. Given a paper, you need to determine the citation or reference relationship between this paper and the other papers. Do not consider papers that are not provided.\n3. Please present the paper titles in a json format as follows: {{\"Reference\":[\"Reference Title 1\", \"Reference Title 2\", ..., \"Reference Title n\"], \"Citation\":[\"Citation Title 1\", \"Citation Title 2\", ..., \"Citation Title n\"]}}.\n4. If a paper does not have any references or citations, please leave the corresponding list empty, e.g.{{\"Refernce\":[]}}, {{\"Citation\":[]}}.",
        "prompt_template": "#Papers:\n{docs}\n\n{instruction}\n\n#The paper you need to analyze:\n{question}",
        "doc": [
            "2401.08541.md",
            "2405.15160.md",
            "2405.16847.md",
            "2405.18320.md"
        ],
        "answer": {
            "Reference": [],
            "Citation": [
                "# ARVideo: Autoregressive Pretraining for Self-Supervised Video Representation Learning ",
                "# TokenUnify: Scalable Autoregressive Visual Pre-training with Mixture Token Prediction ",
                "# Self-Supervised Learning Based Handwriting Verification "
            ]
        },
        "shuffle_doc": true,
        "id": "9ad72b7e-08da-46d4-a1fb-26943be50f6f",
        "prompt": "[Question]\n#Papers:\n{docs}\n\nWe hope you will carefully study the provided papers and determine the citation relationships between them. Please follow the instructions below strictly to complete the task:\n\n#Specific Requirements:\n1. Reference: When a given paper mentions other provided papers, those other papers are considered as \"references\" for the given paper. To summarize in this specific context, references are about what the given paper is using.\n2. Citation: Conversely, when other provided papers mention the given paper in their works, the given paper is being \"cited\" by those other papers. To summarize in this specific context, citations are about who is using the given paper.\n3. Given a paper, you need to determine the citation or reference relationship between this paper and the other papers. Do not consider papers that are not provided.\n3. Please present the paper titles in a json format as follows: {{\"Reference\":[\"Reference Title 1\", \"Reference Title 2\", ..., \"Reference Title n\"], \"Citation\":[\"Citation Title 1\", \"Citation Title 2\", ..., \"Citation Title n\"]}}.\n4. If a paper does not have any references or citations, please leave the corresponding list empty, e.g.{{\"Refernce\":[]}}, {{\"Citation\":[]}}.\n\n#The paper you need to analyze:\nScalable Pre-training of Large Autoregressive Image Models\n\n[Gold Answer]\n{'Reference': [], 'Citation': ['# ARVideo: Autoregressive Pretraining for Self-Supervised Video Representation Learning ', '# TokenUnify: Scalable Autoregressive Visual Pre-training with Mixture Token Prediction ', '# Self-Supervised Learning Based Handwriting Verification ']}\n\n[The Start of Assistant's Predicted Answer]\n{\"Reference\":[\"V-JEPA: Latent video prediction for visual representation learning\", \"Self-supervised learning from images with a joint-embedding predictive architecture\", \"Sequential modeling enables scalable learning for large vision models\", \"Vicreg: Variance-invariance-covariance regularization for selfsupervised learning\", \"Vicuna: An Open-Source Chatbot Impressing GPT-4 with $90 \\% *$ ChatGPT Quality\", \"Large scale gan training for high fidelity natural image synthesis\", \"Language models are few-shot learners\", \"Deep clustering for unsupervised learning of visual features\", \"Unsupervised pre-training of image features on non-curated data\", \"Unsupervised learning of visual features by contrasting cluster assignments\", \"Emerging properties in self-supervised vision transformers\", \"Training compute-optimal large language models\", \"Momentum contrast for unsupervised visual representation learning\", \"Masked autoencoders are scalable vision learners\", \"Set transformer: A framework for attention-based permutation-invariant neural networks\", \"Dinov2: Learning robust visual features without supervision\", \"The effectiveness of mae pre-pretraining for billionscale pretraining\", \"Going deeper with image transformers\", \"Llama: Open and efficient foundation language models\", \"Diffusion models as masked autoencoders\", \"Coca: Contrastive captioners are image-text foundation models\", \"Mask R-CNN\", \"Attention is all you need\", \"Lora: Low-rank adaptation of large language models\", \"Deep networks with stochastic depth\", \"BEiT: Bert pretraining of image transformers\", \"Self-supervised learning from images with a joint-embedding predictive architecture\", \"Datacomp: In search of the next generation of multimodal datasets\", \"Data filtering networks\", \"Are large-scale datasets necessary for self-supervised pre-training?\", \"Image transformer\", \"Context encoders: Feature learning by inpainting\", \"Generative pretraining from pixels\", \"Unsupervised visual representation learning by predicting image rotations\", \"Unsupervised representation learning with contrastive predictive coding\", \"Pixelcnn++: Improving the pixelcnn with discretized logistic mixture likelihood and other modifications\", \"Generative adversarial nets\", \"Divide and contrast: Self-supervised learning from uncurated data\", \"Zero-shot text-to-image generation\", \"Scaling vision transformers to 22 billion parameters\", \"Training data-efficient image transformers & distillation through attention\", \"Generative adversarial nets\", \"An image is worth $16 \\times 16$ words: Transformers for image recognition at scale\", \"The neural autoregressive distribution estimator\", \"Unsupervised pre-training of image features on non-curated data\", \"Unsupervised learning of visual representations by solving jigsaw puzzles\", \"Unsupervised learning by predicting image rotations\", \"Self-supervised learning of pretext-invariant representations\", \"Supervised and self-supervised learning for vision and language\", \"Masked language modeling with visual context\", \"Self-supervised learning of visual features by solving jigsaw puzzles\", \"Unsupervised and self-supervised learning for computer vision\", \"Self-supervised learning of visual features by solving jigsaw puzzles\", \"Unsupervised and self-supervised learning for computer vision\", \"Self-supervised learning of visual features by solving jigsaw puzzles\", \"Unsupervised and self-supervised learning for computer vision\", \"Self-supervised learning of visual features by solving jigsaw puzzles\", \"Unsupervised and self-supervised learning for computer vision\", \"Self-supervised learning of visual features by solving jigsaw puzzles\", \"Unsupervised and self-supervised learning for computer vision\", \"Self-supervised learning of visual features by solving jigsaw puzzles\", \"Unsupervised and self-supervised learning for computer vision\", \"Self-supervised learning of visual features by solving jigsaw puzzles\", \"Supervised and self-supervised learning for vision and language\", \"Unsupervised and self-supervised learning for computer vision\", \"Self-supervised learning of visual features by solving jigsaw puzzles\", \"Supervised and self-supervised learning for vision and language\", \"Unsupervised and self-supervised learning for computer vision\", \"Self-supervised learning of visual features by solving jigsaw puzzles\", \"Supervised and self-supervised learning for vision and language\", \"Unsupervised and self-supervised learning for computer vision\", \"Self-supervised learning of visual features by solving jigsaw puzzles\", \"Supervised and self-supervised learning for vision and language\", \"Unsupervised and self-supervised learning for computer vision\", \"Self-supervised learning of visual features by solving jigsaw puzzles\", \"Supervised and self-supervised learning for vision and language\", \"Unsupervised and self-supervised learning for computer vision\", \"Self-supervised learning of visual features by solving jigsaw puzzles\", \"Supervised and self-supervised learning for vision and language\", \"Unsupervised and self-supervised learning for computer vision\", \"Self-supervised learning of visual features by solving jigsaw puzzles\", \"Supervised and self-supervised learning for vision and language\", \"Unsupervised and self-supervised learning for computer vision\", \"Self-supervised learning of visual features by solving jigsaw puzzles\", \"Supervised and self-supervised learning for vision and language\", \"Unsupervised and self-supervised learning for computer vision\", \"Self-supervised learning of visual features by solving jigsaw puzzles\", \"Supervised and self-supervised learning for vision and language\", \"Unsupervised and self-supervised learning for computer vision\", \"Self-supervised learning of visual features by solving jigsaw puzzles\", \"Supervised and self-supervised learning for vision and language\", \"Unsupervised and self-supervised learning for computer vision\", \"Self-supervised learning of visual features by solving jigsaw puzzles\", \"Language models are unsupervised multitask learners\"], \"Citation\":[]}\n[The End of Assistant's Predicted Answer]\n\n[System]\nWe would like to request your feedback on the performance of the AI assistant in response to the user question displayed above according to the gold answer. Please use the following listed aspects and their descriptions as evaluation criteria:\n    - Accuracy and Hallucinations: The assistant's answer is semantically consistent with the gold answer; The numerical value and order need to be accurate, and there should be no hallucinations.\n    - Completeness: Referring to the reference answers, the assistant's answer should contain all the key points needed to answer the user's question; further elaboration on these key points can be omitted.\nPlease rate whether this answer is suitable for the question. Please note that the gold answer can be considered as a correct answer to the question.\n\nThe assistant receives an overall score on a scale of 1 to 100, where a higher score indicates better overall performance.\nPlease note that if the assistant's answer and the gold answer fully meet the above criteria, its overall rating should be the full marks (100).\nPlease first provide a comprehensive explanation of your evaluation, avoiding any potential bias.\nThen, output a line indicating the score of the Assistant.\n\nPLEASE OUTPUT WITH THE FOLLOWING FORMAT, WHERE THE SCORE IS A SCALE OF 1 TO 100 BY STRICTLY FOLLOWING THIS FORMAT: \"[[score]]\", FOR EXAMPLE \"Rating: [[100]]\":\n<start output>\nEvaluation evidence: your evluation explanation here, no more than 100 words\nRating: [[score]]\n<end output>\n\nNow, start your evaluation:",
        "generate_response": "{\"Reference\":[\"V-JEPA: Latent video prediction for visual representation learning\", \"Self-supervised learning from images with a joint-embedding predictive architecture\", \"Sequential modeling enables scalable learning for large vision models\", \"Vicreg: Variance-invariance-covariance regularization for selfsupervised learning\", \"Vicuna: An Open-Source Chatbot Impressing GPT-4 with $90 \\% *$ ChatGPT Quality\", \"Large scale gan training for high fidelity natural image synthesis\", \"Language models are few-shot learners\", \"Deep clustering for unsupervised learning of visual features\", \"Unsupervised pre-training of image features on non-curated data\", \"Unsupervised learning of visual features by contrasting cluster assignments\", \"Emerging properties in self-supervised vision transformers\", \"Training compute-optimal large language models\", \"Momentum contrast for unsupervised visual representation learning\", \"Masked autoencoders are scalable vision learners\", \"Set transformer: A framework for attention-based permutation-invariant neural networks\", \"Dinov2: Learning robust visual features without supervision\", \"The effectiveness of mae pre-pretraining for billionscale pretraining\", \"Going deeper with image transformers\", \"Llama: Open and efficient foundation language models\", \"Diffusion models as masked autoencoders\", \"Coca: Contrastive captioners are image-text foundation models\", \"Mask R-CNN\", \"Attention is all you need\", \"Lora: Low-rank adaptation of large language models\", \"Deep networks with stochastic depth\", \"BEiT: Bert pretraining of image transformers\", \"Self-supervised learning from images with a joint-embedding predictive architecture\", \"Datacomp: In search of the next generation of multimodal datasets\", \"Data filtering networks\", \"Are large-scale datasets necessary for self-supervised pre-training?\", \"Image transformer\", \"Context encoders: Feature learning by inpainting\", \"Generative pretraining from pixels\", \"Unsupervised visual representation learning by predicting image rotations\", \"Unsupervised representation learning with contrastive predictive coding\", \"Pixelcnn++: Improving the pixelcnn with discretized logistic mixture likelihood and other modifications\", \"Generative adversarial nets\", \"Divide and contrast: Self-supervised learning from uncurated data\", \"Zero-shot text-to-image generation\", \"Scaling vision transformers to 22 billion parameters\", \"Training data-efficient image transformers & distillation through attention\", \"Generative adversarial nets\", \"An image is worth $16 \\times 16$ words: Transformers for image recognition at scale\", \"The neural autoregressive distribution estimator\", \"Unsupervised pre-training of image features on non-curated data\", \"Unsupervised learning of visual representations by solving jigsaw puzzles\", \"Unsupervised learning by predicting image rotations\", \"Self-supervised learning of pretext-invariant representations\", \"Supervised and self-supervised learning for vision and language\", \"Masked language modeling with visual context\", \"Self-supervised learning of visual features by solving jigsaw puzzles\", \"Unsupervised and self-supervised learning for computer vision\", \"Self-supervised learning of visual features by solving jigsaw puzzles\", \"Unsupervised and self-supervised learning for computer vision\", \"Self-supervised learning of visual features by solving jigsaw puzzles\", \"Unsupervised and self-supervised learning for computer vision\", \"Self-supervised learning of visual features by solving jigsaw puzzles\", \"Unsupervised and self-supervised learning for computer vision\", \"Self-supervised learning of visual features by solving jigsaw puzzles\", \"Unsupervised and self-supervised learning for computer vision\", \"Self-supervised learning of visual features by solving jigsaw puzzles\", \"Unsupervised and self-supervised learning for computer vision\", \"Self-supervised learning of visual features by solving jigsaw puzzles\", \"Supervised and self-supervised learning for vision and language\", \"Unsupervised and self-supervised learning for computer vision\", \"Self-supervised learning of visual features by solving jigsaw puzzles\", \"Supervised and self-supervised learning for vision and language\", \"Unsupervised and self-supervised learning for computer vision\", \"Self-supervised learning of visual features by solving jigsaw puzzles\", \"Supervised and self-supervised learning for vision and language\", \"Unsupervised and self-supervised learning for computer vision\", \"Self-supervised learning of visual features by solving jigsaw puzzles\", \"Supervised and self-supervised learning for vision and language\", \"Unsupervised and self-supervised learning for computer vision\", \"Self-supervised learning of visual features by solving jigsaw puzzles\", \"Supervised and self-supervised learning for vision and language\", \"Unsupervised and self-supervised learning for computer vision\", \"Self-supervised learning of visual features by solving jigsaw puzzles\", \"Supervised and self-supervised learning for vision and language\", \"Unsupervised and self-supervised learning for computer vision\", \"Self-supervised learning of visual features by solving jigsaw puzzles\", \"Supervised and self-supervised learning for vision and language\", \"Unsupervised and self-supervised learning for computer vision\", \"Self-supervised learning of visual features by solving jigsaw puzzles\", \"Supervised and self-supervised learning for vision and language\", \"Unsupervised and self-supervised learning for computer vision\", \"Self-supervised learning of visual features by solving jigsaw puzzles\", \"Supervised and self-supervised learning for vision and language\", \"Unsupervised and self-supervised learning for computer vision\", \"Self-supervised learning of visual features by solving jigsaw puzzles\", \"Supervised and self-supervised learning for vision and language\", \"Unsupervised and self-supervised learning for computer vision\", \"Self-supervised learning of visual features by solving jigsaw puzzles\", \"Language models are unsupervised multitask learners\"], \"Citation\":[]}",
        "eval_response": "Evaluation evidence: The assistant's answer is inaccurate and contains hallucinations. The provided answer lists numerous references that are not mentioned in the gold answer, indicating a misunderstanding of the task. The gold answer specifies that there are no references and lists three citations, which are completely omitted in the assistant's response. This shows a failure in both accuracy and completeness.\n\nRating: [[10]]",
        "score": 10
    },
    {
        "level": 4,
        "set": 1,
        "length": 39121,
        "type": "paper",
        "language": "en",
        "question": "",
        "instruction": "We kindly ask you to thoroughly review the provided papers and construct a citation chain from them. Please adhere to the following instructions strictly while completing the task:\n\n#Task Instructions:\nGiven several papers, you are required to identify and list the longest citation chain, which demonstrates the citation relationship among the provided papers.\n\n#Specific Requirements:\n1.Please present the titles of the papers in the form of a list, as follows: [\"Title of Paper 1\", \"Title of Paper 2\", ..., \"Title of Paper n\"].\n2.Ensure that the citation chain in the list is linear and continuous, meaning that the first paper title in the list (Paper 1) should not cite any other works. Instead, it should be cited by the next paper in the list (Paper 2); subsequently, each paper should then be cited by the next one in the list, continuing up to the last paper (Paper n).\n3.Consider only the citation relationships within the supplied collection of papers, and ensure that the citation chain accurately reflects the sequential citation order among these documents.\n4.Do not take into account any articles not provided, and disregard other non-linear citation relationships.",
        "prompt_template": "{instruction}\n\n#Papers Provided:\n{docs}",
        "doc": [
            "2402.09178.md",
            "2403.09746.md",
            "2405.08555.md"
        ],
        "answer": [
            "# Generalized Portrait Quality Assessment ",
            "# PICNIQ: Pairwise Comparisons for Natural Image Quality Assessment ",
            "# Dual-Branch Network for Portrait Image Quality Assessment "
        ],
        "shuffle_doc": true,
        "id": "4efcb525-bf7a-442e-b767-d3fc30aa1305",
        "prompt": "[Question]\nWe kindly ask you to thoroughly review the provided papers and construct a citation chain from them. Please adhere to the following instructions strictly while completing the task:\n\n#Task Instructions:\nGiven several papers, you are required to identify and list the longest citation chain, which demonstrates the citation relationship among the provided papers.\n\n#Specific Requirements:\n1.Please present the titles of the papers in the form of a list, as follows: [\"Title of Paper 1\", \"Title of Paper 2\", ..., \"Title of Paper n\"].\n2.Ensure that the citation chain in the list is linear and continuous, meaning that the first paper title in the list (Paper 1) should not cite any other works. Instead, it should be cited by the next paper in the list (Paper 2); subsequently, each paper should then be cited by the next one in the list, continuing up to the last paper (Paper n).\n3.Consider only the citation relationships within the supplied collection of papers, and ensure that the citation chain accurately reflects the sequential citation order among these documents.\n4.Do not take into account any articles not provided, and disregard other non-linear citation relationships.\n\n#Papers Provided:\n{docs}\n\n[Gold Answer]\n['# Generalized Portrait Quality Assessment ', '# PICNIQ: Pairwise Comparisons for Natural Image Quality Assessment ', '# Dual-Branch Network for Portrait Image Quality Assessment ']\n\n[The Start of Assistant's Predicted Answer]\n[\"Generalized Portrait Quality Assessment\", \"An Image Quality Assessment Dataset for Portraits\", \"Perceptual quality assessment of smartphone photography\"]\n[The End of Assistant's Predicted Answer]\n\n[System]\nWe would like to request your feedback on the performance of the AI assistant in response to the user question displayed above according to the gold answer. Please use the following listed aspects and their descriptions as evaluation criteria:\n    - Accuracy and Hallucinations: The assistant's answer is semantically consistent with the gold answer; The numerical value and order need to be accurate, and there should be no hallucinations.\n    - Completeness: Referring to the reference answers, the assistant's answer should contain all the key points needed to answer the user's question; further elaboration on these key points can be omitted.\nPlease rate whether this answer is suitable for the question. Please note that the gold answer can be considered as a correct answer to the question.\n\nThe assistant receives an overall score on a scale of 1 to 100, where a higher score indicates better overall performance.\nPlease note that if the assistant's answer and the gold answer fully meet the above criteria, its overall rating should be the full marks (100).\nPlease first provide a comprehensive explanation of your evaluation, avoiding any potential bias.\nThen, output a line indicating the score of the Assistant.\n\nPLEASE OUTPUT WITH THE FOLLOWING FORMAT, WHERE THE SCORE IS A SCALE OF 1 TO 100 BY STRICTLY FOLLOWING THIS FORMAT: \"[[score]]\", FOR EXAMPLE \"Rating: [[100]]\":\n<start output>\nEvaluation evidence: your evluation explanation here, no more than 100 words\nRating: [[score]]\n<end output>\n\nNow, start your evaluation:",
        "generate_response": "[\"Generalized Portrait Quality Assessment\", \"An Image Quality Assessment Dataset for Portraits\", \"Perceptual quality assessment of smartphone photography\"]",
        "eval_response": "Evaluation evidence: The assistant's answer does not match the gold answer in terms of the titles of the papers listed. The titles in the assistant's answer do not correspond to those in the gold answer, indicating a significant deviation in accuracy. Additionally, the assistant's answer introduces titles not present in the gold answer, which can be considered as hallucinations. This discrepancy suggests that the assistant did not correctly identify the longest citation chain among the provided papers, failing to meet the task's requirements.\n\nRating: [[20]]",
        "score": 20
    },
    {
        "level": 3,
        "set": 3,
        "length": 130531,
        "type": "paper",
        "language": "en",
        "question": "WARM: On the Benefits of Weight Averaged Reward Models",
        "instruction": "We hope you will carefully study the provided papers and determine the citation relationships between them. Please follow the instructions below strictly to complete the task:\n\n#Specific Requirements:\n1. Reference: When a given paper mentions other provided papers, those other papers are considered as \"references\" for the given paper. To summarize in this specific context, references are about what the given paper is using.\n2. Citation: Conversely, when other provided papers mention the given paper in their works, the given paper is being \"cited\" by those other papers. To summarize in this specific context, citations are about who is using the given paper.\n3. Given a paper, you need to determine the citation or reference relationship between this paper and the other papers. Do not consider papers that are not provided.\n3. Please present the paper titles in a json format as follows: {{\"Reference\":[\"Reference Title 1\", \"Reference Title 2\", ..., \"Reference Title n\"], \"Citation\":[\"Citation Title 1\", \"Citation Title 2\", ..., \"Citation Title n\"]}}.\n4. If a paper does not have any references or citations, please leave the corresponding list empty, e.g.{{\"Refernce\":[]}}, {{\"Citation\":[]}}.",
        "prompt_template": "#Papers:\n{docs}\n\n{instruction}\n\n#The paper you need to analyze:\n{question}",
        "doc": [
            "2401.12187.md",
            "2405.04781.md",
            "2405.16276.md",
            "2312.07551.md",
            "2312.09244.md"
        ],
        "answer": {
            "Reference": [
                "# Language Model Alignment with Elastic Reset ",
                "# Helping or Herding?  "
            ],
            "Citation": [
                "# COURSEGPT-ZH: AN EDUCATIONAL LARGE LANGUAGE MODEL BASED ON KNOWLEDGE DISTILLATION INCORPORATING PROMPT OPTIMIZATION * ",
                "# MECHANISM DESIGN FOR LLM FINE-TUNING WITH MULTIPLE REWARD MODELS "
            ]
        },
        "shuffle_doc": true,
        "id": "6c91556d-05d3-4e70-8005-d82165f5fc27",
        "prompt": "[Question]\n#Papers:\n{docs}\n\nWe hope you will carefully study the provided papers and determine the citation relationships between them. Please follow the instructions below strictly to complete the task:\n\n#Specific Requirements:\n1. Reference: When a given paper mentions other provided papers, those other papers are considered as \"references\" for the given paper. To summarize in this specific context, references are about what the given paper is using.\n2. Citation: Conversely, when other provided papers mention the given paper in their works, the given paper is being \"cited\" by those other papers. To summarize in this specific context, citations are about who is using the given paper.\n3. Given a paper, you need to determine the citation or reference relationship between this paper and the other papers. Do not consider papers that are not provided.\n3. Please present the paper titles in a json format as follows: {{\"Reference\":[\"Reference Title 1\", \"Reference Title 2\", ..., \"Reference Title n\"], \"Citation\":[\"Citation Title 1\", \"Citation Title 2\", ..., \"Citation Title n\"]}}.\n4. If a paper does not have any references or citations, please leave the corresponding list empty, e.g.{{\"Refernce\":[]}}, {{\"Citation\":[]}}.\n\n#The paper you need to analyze:\nWARM: On the Benefits of Weight Averaged Reward Models\n\n[Gold Answer]\n{'Reference': ['# Language Model Alignment with Elastic Reset ', '# Helping or Herding?  '], 'Citation': ['# COURSEGPT-ZH: AN EDUCATIONAL LARGE LANGUAGE MODEL BASED ON KNOWLEDGE DISTILLATION INCORPORATING PROMPT OPTIMIZATION * ', '# MECHANISM DESIGN FOR LLM FINE-TUNING WITH MULTIPLE REWARD MODELS ']}\n\n[The Start of Assistant's Predicted Answer]\n{\"Reference\":[\"Gemini: A family of highly capable multimodal models\", \"Gpt-4 technical report\", \"Sparks of artificial general intelligence: Early experiments with gpt-4\", \"Improving language understanding by generative pre-training\", \"Bert: Pre-training of deep bidirectional transformers for language understanding\", \"Language models are few-shot learners\", \"Finetuned language models are zero-shot learners\", \"Super-NaturalInstructions: Generalization via declarative instructions on 1600+ NLP tasks\", \"Stanford Alpaca: An instruction-following LLaMA model\", \"Factually consistent summarization via reinforcement learning with textual entailment feedback\", \"Bias plus variance decomposition for zero-one loss functions\"], \"Citation\":[]}\n[The End of Assistant's Predicted Answer]\n\n[System]\nWe would like to request your feedback on the performance of the AI assistant in response to the user question displayed above according to the gold answer. Please use the following listed aspects and their descriptions as evaluation criteria:\n    - Accuracy and Hallucinations: The assistant's answer is semantically consistent with the gold answer; The numerical value and order need to be accurate, and there should be no hallucinations.\n    - Completeness: Referring to the reference answers, the assistant's answer should contain all the key points needed to answer the user's question; further elaboration on these key points can be omitted.\nPlease rate whether this answer is suitable for the question. Please note that the gold answer can be considered as a correct answer to the question.\n\nThe assistant receives an overall score on a scale of 1 to 100, where a higher score indicates better overall performance.\nPlease note that if the assistant's answer and the gold answer fully meet the above criteria, its overall rating should be the full marks (100).\nPlease first provide a comprehensive explanation of your evaluation, avoiding any potential bias.\nThen, output a line indicating the score of the Assistant.\n\nPLEASE OUTPUT WITH THE FOLLOWING FORMAT, WHERE THE SCORE IS A SCALE OF 1 TO 100 BY STRICTLY FOLLOWING THIS FORMAT: \"[[score]]\", FOR EXAMPLE \"Rating: [[100]]\":\n<start output>\nEvaluation evidence: your evluation explanation here, no more than 100 words\nRating: [[score]]\n<end output>\n\nNow, start your evaluation:",
        "generate_response": "{\"Reference\":[\"Gemini: A family of highly capable multimodal models\", \"Gpt-4 technical report\", \"Sparks of artificial general intelligence: Early experiments with gpt-4\", \"Improving language understanding by generative pre-training\", \"Bert: Pre-training of deep bidirectional transformers for language understanding\", \"Language models are few-shot learners\", \"Finetuned language models are zero-shot learners\", \"Super-NaturalInstructions: Generalization via declarative instructions on 1600+ NLP tasks\", \"Stanford Alpaca: An instruction-following LLaMA model\", \"Factually consistent summarization via reinforcement learning with textual entailment feedback\", \"Bias plus variance decomposition for zero-one loss functions\"], \"Citation\":[]}",
        "eval_response": "Evaluation evidence: The assistant's answer does not match the gold answer in terms of accuracy and completeness. The references and citations listed by the assistant are entirely different from those in the gold answer, indicating a significant error or hallucination. The assistant failed to identify the correct references and citations for the paper \"WARM: On the Benefits of Weight Averaged Reward Models,\" which leads to a complete mismatch with the provided gold answer.\n\nRating: [[1]]",
        "score": 1
    },
    {
        "level": 3,
        "set": 4,
        "length": 214992,
        "type": "paper",
        "language": "en",
        "question": "MedusA: Simple LLM Inference Acceleration Framework with Multiple Decoding Heads",
        "instruction": "We hope you will carefully study the provided papers and determine the citation relationships between them. Please follow the instructions below strictly to complete the task:\n\n#Specific Requirements:\n1. Reference: When a given paper mentions other provided papers, those other papers are considered as \"references\" for the given paper. To summarize in this specific context, references are about what the given paper is using.\n2. Citation: Conversely, when other provided papers mention the given paper in their works, the given paper is being \"cited\" by those other papers. To summarize in this specific context, citations are about who is using the given paper.\n3. Given a paper, you need to determine the citation or reference relationship between this paper and the other papers. Do not consider papers that are not provided.\n3. Please present the paper titles in a json format as follows: {{\"Reference\":[\"Reference Title 1\", \"Reference Title 2\", ..., \"Reference Title n\"], \"Citation\":[\"Citation Title 1\", \"Citation Title 2\", ..., \"Citation Title n\"]}}.\n4. If a paper does not have any references or citations, please leave the corresponding list empty, e.g.{{\"Refernce\":[]}}, {{\"Citation\":[]}}.",
        "prompt_template": "#Papers:\n{docs}\n\n{instruction}\n\n#The paper you need to analyze:\n{question}",
        "doc": [
            "2401.10774.md",
            "2405.17741.md",
            "2405.18628.md",
            "2405.19261.md",
            "2405.19715.md",
            "2405.20314.md",
            "2309.06180.md",
            "2310.07177.md",
            "2310.08461.md",
            "2311.08252.md",
            "2402.02057.md"
        ],
        "answer": {
            "Reference": [
                "# Efficient Memory Management for Large Language Model Serving with PagedAttention ",
                "# ONLINE SPECULATIVE DECODING ",
                "# DISTILLSPEC: IMPROVING SPECULATIVE DECODING VIA KNOWLEDGE DISTILLATION ",
                "# REST: Retrieval-Based Speculative Decoding ",
                "# Break the Sequential Dependency of LLM Inference Using LOOKAHEAD DECODING "
            ],
            "Citation": [
                "# LoRA-Switch: Boosting the Efficiency of Dynamic LLM Adapters via System-Algorithm Co-design ",
                "# Hardware-Aware Parallel Prompt Decoding for Memory-Efficient Acceleration of LLM Inference ",
                "# Faster Cascades via Speculative Decoding ",
                "# SpecDec++: Boosting Speculative Decoding via Adaptive Candidate Lengths ",
                "# S3D: A Simple and Cost-Effective Self-Speculative Decoding Scheme for Low-Memory GPUs "
            ]
        },
        "shuffle_doc": true,
        "id": "270f5cb8-e745-488c-b580-97c7ea75710f",
        "prompt": "[Question]\n#Papers:\n{docs}\n\nWe hope you will carefully study the provided papers and determine the citation relationships between them. Please follow the instructions below strictly to complete the task:\n\n#Specific Requirements:\n1. Reference: When a given paper mentions other provided papers, those other papers are considered as \"references\" for the given paper. To summarize in this specific context, references are about what the given paper is using.\n2. Citation: Conversely, when other provided papers mention the given paper in their works, the given paper is being \"cited\" by those other papers. To summarize in this specific context, citations are about who is using the given paper.\n3. Given a paper, you need to determine the citation or reference relationship between this paper and the other papers. Do not consider papers that are not provided.\n3. Please present the paper titles in a json format as follows: {{\"Reference\":[\"Reference Title 1\", \"Reference Title 2\", ..., \"Reference Title n\"], \"Citation\":[\"Citation Title 1\", \"Citation Title 2\", ..., \"Citation Title n\"]}}.\n4. If a paper does not have any references or citations, please leave the corresponding list empty, e.g.{{\"Refernce\":[]}}, {{\"Citation\":[]}}.\n\n#The paper you need to analyze:\nMedusA: Simple LLM Inference Acceleration Framework with Multiple Decoding Heads\n\n[Gold Answer]\n{'Reference': ['# Efficient Memory Management for Large Language Model Serving with PagedAttention ', '# ONLINE SPECULATIVE DECODING ', '# DISTILLSPEC: IMPROVING SPECULATIVE DECODING VIA KNOWLEDGE DISTILLATION ', '# REST: Retrieval-Based Speculative Decoding ', '# Break the Sequential Dependency of LLM Inference Using LOOKAHEAD DECODING '], 'Citation': ['# LoRA-Switch: Boosting the Efficiency of Dynamic LLM Adapters via System-Algorithm Co-design ', '# Hardware-Aware Parallel Prompt Decoding for Memory-Efficient Acceleration of LLM Inference ', '# Faster Cascades via Speculative Decoding ', '# SpecDec++: Boosting Speculative Decoding via Adaptive Candidate Lengths ', '# S3D: A Simple and Cost-Effective Self-Speculative Decoding Scheme for Low-Memory GPUs ']}\n\n[The Start of Assistant's Predicted Answer]\n{\"Reference\":[\"A Survey on Evaluation of Large Language Models\", \"Vicuna: An Open-Source Chatbot Impressing GPT-4 with $90 \\% *$ ChatGPT Quality\", \"Compositional Semantic Parsing with Large Language Models\", \"Open LLM Leaderboard\", \"How Close is ChatGPT to Human Experts? Comparison Corpus, Evaluation, and Detection\", \"Rethinking with Retrieval: Faithful Large Language Model Inference\", \"Measuring Massive Multitask Language Understanding\", \"DeepSpeed Inference: Enabling Efficient Inference of Transformer Models at Unprecedented Scale\", \"Palm: Scaling Language Modeling with Pathways\", \"TurboTransformers: An Efficient GPU Serving System for Transformer Models\", \"FlashAttention: Fast and Memory-Efficient Exact Attention with IO-Awareness\", \"DVABatch: Diversity-Aware Multi-Entry Multi-Exit Batching for Efficient Processing of DNN Services on GPUs\", \"Checkmate: Breaking the Memory Wall with Optimal Tensor Rematerialization\", \"POET: Training Neural Networks on Tiny Devices with Integrated Rematerialization and Paging\", \"Efficiently Scaling Transformer Inference\", \"Superneurons: Dynamic GPU Memory Management for Training Deep Neural Networks\", \"LightSeq: A High Performance Inference Library for Transformers\", \"Self-Instruct: Aligning Language Model with Self Generated Instructions\", \"Orca: A Distributed Serving System for Transformer-Based Generative Models\", \"Shepherd: Serving DNNs in the Wild\", \"Opt: Open Pre-Trained Transformer Language Models\", \"Alpa: Automating Inter- and Intra-Operator Parallelism for Distributed Deep Learning\", \"PetS: A Unified Framework for Parameter-Efficient Transformers Serving\"], \"Citation\":[\"Faster Cascades via Speculative Decoding\", \"Hardware-Aware Parallel Prompt Decoding for Memory-Efficient Acceleration of LLM Inference\", \"REST: Retrieval-Based Speculative Decoding\", \"S3D: A Simple and Cost-Effective Self-Speculative Decoding Scheme for Low-Memory GPUs\"]}\n[The End of Assistant's Predicted Answer]\n\n[System]\nWe would like to request your feedback on the performance of the AI assistant in response to the user question displayed above according to the gold answer. Please use the following listed aspects and their descriptions as evaluation criteria:\n    - Accuracy and Hallucinations: The assistant's answer is semantically consistent with the gold answer; The numerical value and order need to be accurate, and there should be no hallucinations.\n    - Completeness: Referring to the reference answers, the assistant's answer should contain all the key points needed to answer the user's question; further elaboration on these key points can be omitted.\nPlease rate whether this answer is suitable for the question. Please note that the gold answer can be considered as a correct answer to the question.\n\nThe assistant receives an overall score on a scale of 1 to 100, where a higher score indicates better overall performance.\nPlease note that if the assistant's answer and the gold answer fully meet the above criteria, its overall rating should be the full marks (100).\nPlease first provide a comprehensive explanation of your evaluation, avoiding any potential bias.\nThen, output a line indicating the score of the Assistant.\n\nPLEASE OUTPUT WITH THE FOLLOWING FORMAT, WHERE THE SCORE IS A SCALE OF 1 TO 100 BY STRICTLY FOLLOWING THIS FORMAT: \"[[score]]\", FOR EXAMPLE \"Rating: [[100]]\":\n<start output>\nEvaluation evidence: your evluation explanation here, no more than 100 words\nRating: [[score]]\n<end output>\n\nNow, start your evaluation:",
        "generate_response": "{\"Reference\":[\"A Survey on Evaluation of Large Language Models\", \"Vicuna: An Open-Source Chatbot Impressing GPT-4 with $90 \\% *$ ChatGPT Quality\", \"Compositional Semantic Parsing with Large Language Models\", \"Open LLM Leaderboard\", \"How Close is ChatGPT to Human Experts? Comparison Corpus, Evaluation, and Detection\", \"Rethinking with Retrieval: Faithful Large Language Model Inference\", \"Measuring Massive Multitask Language Understanding\", \"DeepSpeed Inference: Enabling Efficient Inference of Transformer Models at Unprecedented Scale\", \"Palm: Scaling Language Modeling with Pathways\", \"TurboTransformers: An Efficient GPU Serving System for Transformer Models\", \"FlashAttention: Fast and Memory-Efficient Exact Attention with IO-Awareness\", \"DVABatch: Diversity-Aware Multi-Entry Multi-Exit Batching for Efficient Processing of DNN Services on GPUs\", \"Checkmate: Breaking the Memory Wall with Optimal Tensor Rematerialization\", \"POET: Training Neural Networks on Tiny Devices with Integrated Rematerialization and Paging\", \"Efficiently Scaling Transformer Inference\", \"Superneurons: Dynamic GPU Memory Management for Training Deep Neural Networks\", \"LightSeq: A High Performance Inference Library for Transformers\", \"Self-Instruct: Aligning Language Model with Self Generated Instructions\", \"Orca: A Distributed Serving System for Transformer-Based Generative Models\", \"Shepherd: Serving DNNs in the Wild\", \"Opt: Open Pre-Trained Transformer Language Models\", \"Alpa: Automating Inter- and Intra-Operator Parallelism for Distributed Deep Learning\", \"PetS: A Unified Framework for Parameter-Efficient Transformers Serving\"], \"Citation\":[\"Faster Cascades via Speculative Decoding\", \"Hardware-Aware Parallel Prompt Decoding for Memory-Efficient Acceleration of LLM Inference\", \"REST: Retrieval-Based Speculative Decoding\", \"S3D: A Simple and Cost-Effective Self-Speculative Decoding Scheme for Low-Memory GPUs\"]}",
        "eval_response": "Evaluation evidence: The assistant's answer does not match the gold answer in terms of accuracy and completeness. The references and citations listed in the assistant's answer are entirely different from those in the gold answer, indicating a significant deviation and hallucination. The assistant's answer fails to identify any of the correct references or citations as per the gold answer, which suggests a misunderstanding or misinterpretation of the task requirements.\n\nRating: [[10]]",
        "score": 10
    },
    {
        "level": 3,
        "set": 2,
        "length": 82366,
        "type": "paper",
        "language": "en",
        "question": "Soaring from 4K to 400K: Extending LLM's Context with Activation Beacon",
        "instruction": "We hope you will carefully study the provided papers and determine the citation relationships between them. Please follow the instructions below strictly to complete the task:\n\n#Specific Requirements:\n1. Reference: When a given paper mentions other provided papers, those other papers are considered as \"references\" for the given paper. To summarize in this specific context, references are about what the given paper is using.\n2. Citation: Conversely, when other provided papers mention the given paper in their works, the given paper is being \"cited\" by those other papers. To summarize in this specific context, citations are about who is using the given paper.\n3. Given a paper, you need to determine the citation or reference relationship between this paper and the other papers. Do not consider papers that are not provided.\n3. Please present the paper titles in a json format as follows: {{\"Reference\":[\"Reference Title 1\", \"Reference Title 2\", ..., \"Reference Title n\"], \"Citation\":[\"Citation Title 1\", \"Citation Title 2\", ..., \"Citation Title n\"]}}.\n4. If a paper does not have any references or citations, please leave the corresponding list empty, e.g.{{\"Refernce\":[]}}, {{\"Citation\":[]}}.",
        "prompt_template": "#Papers:\n{docs}\n\n{instruction}\n\n#The paper you need to analyze:\n{question}",
        "doc": [
            "2401.03462.md",
            "2405.17915.md",
            "2405.18573.md",
            "2309.10400.md",
            "2310.03025.md"
        ],
        "answer": {
            "Reference": [
                "# PoSE: EFFICIENT CONTEXT WINDOW EXTENSION OF LLMS VIA POSITIONAL SKIP-WISE TRAINING ",
                "# RETRIEVAL MEETS LONG CONTEXT LARGE LANGUAGE MODELS "
            ],
            "Citation": [
                "# Long Context is Not Long at All: A Prospector of Long-Dependency Data for Large Language Models ",
                "# Programmer Visual Attention During Context-Aware Code Summarization "
            ]
        },
        "shuffle_doc": true,
        "id": "4298a085-8d39-48fb-807f-b518e5d119d0",
        "prompt": "[Question]\n#Papers:\n{docs}\n\nWe hope you will carefully study the provided papers and determine the citation relationships between them. Please follow the instructions below strictly to complete the task:\n\n#Specific Requirements:\n1. Reference: When a given paper mentions other provided papers, those other papers are considered as \"references\" for the given paper. To summarize in this specific context, references are about what the given paper is using.\n2. Citation: Conversely, when other provided papers mention the given paper in their works, the given paper is being \"cited\" by those other papers. To summarize in this specific context, citations are about who is using the given paper.\n3. Given a paper, you need to determine the citation or reference relationship between this paper and the other papers. Do not consider papers that are not provided.\n3. Please present the paper titles in a json format as follows: {{\"Reference\":[\"Reference Title 1\", \"Reference Title 2\", ..., \"Reference Title n\"], \"Citation\":[\"Citation Title 1\", \"Citation Title 2\", ..., \"Citation Title n\"]}}.\n4. If a paper does not have any references or citations, please leave the corresponding list empty, e.g.{{\"Refernce\":[]}}, {{\"Citation\":[]}}.\n\n#The paper you need to analyze:\nSoaring from 4K to 400K: Extending LLM's Context with Activation Beacon\n\n[Gold Answer]\n{'Reference': ['# PoSE: EFFICIENT CONTEXT WINDOW EXTENSION OF LLMS VIA POSITIONAL SKIP-WISE TRAINING ', '# RETRIEVAL MEETS LONG CONTEXT LARGE LANGUAGE MODELS '], 'Citation': ['# Long Context is Not Long at All: A Prospector of Long-Dependency Data for Large Language Models ', '# Programmer Visual Attention During Context-Aware Code Summarization ']}\n\n[The Start of Assistant's Predicted Answer]\n{\"Reference\": [\"Position Interpolation\", \"NTK-Aware Scaled RoPE\", \"StreamingLLM\", \"AutoCompre.-6K\", \"YaRN-128K\", \"LongChat-32K\", \"LongAlpaca-16K\", \"LongLlama\", \"ALiBi\", \"RMT\", \"Longformer\", \"Scaling transformer to $1 \\mathrm{~m}$ tokens and beyond with RMT\", \"Generating long sequences with sparse transformers\", \"Reformer: The efficient transformer\", \"Train short, test long: Attention with linear biases enables input length extrapolation\", \"Compressive transformers for long-range sequence modelling\", \"Combiner: Full attention transformer with sparse computation cost\", \"Code llama: Open foundation models for code\", \"Roformer: Enhanced transformer with rotary position embedding\", \"A length-extrapolatable transformer\", \"Llama: Open and efficient foundation language models\", \"Llama 2: Open foundation and fine-tuned chat models\", \"Natural language processing with transformers\", \"Focused transformer: Contrastive training for context scaling\", \"Linformer: Self-attention with linear complexity\", \"Augmenting language models with long-term memory\", \"Memorizing transformers\", \"Efficient streaming language models with attention sinks\", \"Retrieval meets long context large language models\", \"Big bird: Transformers for longer sequences\", \"Retrieve anything to augment large language models\", \"Proof-pile\", \"Pose: Efficient context window extension of llms via positional skip-wise training\"], \"Citation\": []}\n[The End of Assistant's Predicted Answer]\n\n[System]\nWe would like to request your feedback on the performance of the AI assistant in response to the user question displayed above according to the gold answer. Please use the following listed aspects and their descriptions as evaluation criteria:\n    - Accuracy and Hallucinations: The assistant's answer is semantically consistent with the gold answer; The numerical value and order need to be accurate, and there should be no hallucinations.\n    - Completeness: Referring to the reference answers, the assistant's answer should contain all the key points needed to answer the user's question; further elaboration on these key points can be omitted.\nPlease rate whether this answer is suitable for the question. Please note that the gold answer can be considered as a correct answer to the question.\n\nThe assistant receives an overall score on a scale of 1 to 100, where a higher score indicates better overall performance.\nPlease note that if the assistant's answer and the gold answer fully meet the above criteria, its overall rating should be the full marks (100).\nPlease first provide a comprehensive explanation of your evaluation, avoiding any potential bias.\nThen, output a line indicating the score of the Assistant.\n\nPLEASE OUTPUT WITH THE FOLLOWING FORMAT, WHERE THE SCORE IS A SCALE OF 1 TO 100 BY STRICTLY FOLLOWING THIS FORMAT: \"[[score]]\", FOR EXAMPLE \"Rating: [[100]]\":\n<start output>\nEvaluation evidence: your evluation explanation here, no more than 100 words\nRating: [[score]]\n<end output>\n\nNow, start your evaluation:",
        "generate_response": "{\"Reference\": [\"Position Interpolation\", \"NTK-Aware Scaled RoPE\", \"StreamingLLM\", \"AutoCompre.-6K\", \"YaRN-128K\", \"LongChat-32K\", \"LongAlpaca-16K\", \"LongLlama\", \"ALiBi\", \"RMT\", \"Longformer\", \"Scaling transformer to $1 \\mathrm{~m}$ tokens and beyond with RMT\", \"Generating long sequences with sparse transformers\", \"Reformer: The efficient transformer\", \"Train short, test long: Attention with linear biases enables input length extrapolation\", \"Compressive transformers for long-range sequence modelling\", \"Combiner: Full attention transformer with sparse computation cost\", \"Code llama: Open foundation models for code\", \"Roformer: Enhanced transformer with rotary position embedding\", \"A length-extrapolatable transformer\", \"Llama: Open and efficient foundation language models\", \"Llama 2: Open foundation and fine-tuned chat models\", \"Natural language processing with transformers\", \"Focused transformer: Contrastive training for context scaling\", \"Linformer: Self-attention with linear complexity\", \"Augmenting language models with long-term memory\", \"Memorizing transformers\", \"Efficient streaming language models with attention sinks\", \"Retrieval meets long context large language models\", \"Big bird: Transformers for longer sequences\", \"Retrieve anything to augment large language models\", \"Proof-pile\", \"Pose: Efficient context window extension of llms via positional skip-wise training\"], \"Citation\": []}",
        "eval_response": "Evaluation evidence: The assistant's answer contains a significant number of hallucinations, listing numerous paper titles that are not mentioned in the gold answer. The correct references and citations from the gold answer are missing in the assistant's response. This indicates a lack of accuracy and completeness, as the assistant fails to identify the correct relationships and includes irrelevant information.\n\nRating: [[10]]",
        "score": 10
    },
    {
        "level": 4,
        "set": 2,
        "length": 92508,
        "type": "paper",
        "language": "en",
        "question": "",
        "instruction": "We kindly ask you to thoroughly review the provided papers and construct a citation chain from them. Please adhere to the following instructions strictly while completing the task:\n\n#Task Instructions:\nGiven several papers, you are required to identify and list the longest citation chain, which demonstrates the citation relationship among the provided papers.\n\n#Specific Requirements:\n1.Please present the titles of the papers in the form of a list, as follows: [\"Title of Paper 1\", \"Title of Paper 2\", ..., \"Title of Paper n\"].\n2.Ensure that the citation chain in the list is linear and continuous, meaning that the first paper title in the list (Paper 1) should not cite any other works. Instead, it should be cited by the next paper in the list (Paper 2); subsequently, each paper should then be cited by the next one in the list, continuing up to the last paper (Paper n).\n3.Consider only the citation relationships within the supplied collection of papers, and ensure that the citation chain accurately reflects the sequential citation order among these documents.\n4.Do not take into account any articles not provided, and disregard other non-linear citation relationships.",
        "prompt_template": "{instruction}\n\n#Papers Provided:\n{docs}",
        "doc": [
            "2109.00666.md",
            "2209.08648.md",
            "2211.05617.md",
            "2310.12560.md",
            "2405.09341.md"
        ],
        "answer": [
            "# TabFairGAN: Fair Tabular Data Generation with Generative Adversarial Networks ",
            "",
            "# Debiasing Methods for Fairer Neural Models in Vision and Language Research: A Survey ",
            "# Fast Model Debias with Machine Unlearning ",
            "# LARGE LANGUAGE Model Bias MitiGATION FROM THE PERSPECTIVE OF KNOWLEDGE EDITING "
        ],
        "shuffle_doc": true,
        "id": "ca0fa197-a774-44db-8f7d-a5a0de9d52a4",
        "prompt": "[Question]\nWe kindly ask you to thoroughly review the provided papers and construct a citation chain from them. Please adhere to the following instructions strictly while completing the task:\n\n#Task Instructions:\nGiven several papers, you are required to identify and list the longest citation chain, which demonstrates the citation relationship among the provided papers.\n\n#Specific Requirements:\n1.Please present the titles of the papers in the form of a list, as follows: [\"Title of Paper 1\", \"Title of Paper 2\", ..., \"Title of Paper n\"].\n2.Ensure that the citation chain in the list is linear and continuous, meaning that the first paper title in the list (Paper 1) should not cite any other works. Instead, it should be cited by the next paper in the list (Paper 2); subsequently, each paper should then be cited by the next one in the list, continuing up to the last paper (Paper n).\n3.Consider only the citation relationships within the supplied collection of papers, and ensure that the citation chain accurately reflects the sequential citation order among these documents.\n4.Do not take into account any articles not provided, and disregard other non-linear citation relationships.\n\n#Papers Provided:\n{docs}\n\n[Gold Answer]\n['# TabFairGAN: Fair Tabular Data Generation with Generative Adversarial Networks ', '', '# Debiasing Methods for Fairer Neural Models in Vision and Language Research: A Survey ', '# Fast Model Debias with Machine Unlearning ', '# LARGE LANGUAGE Model Bias MitiGATION FROM THE PERSPECTIVE OF KNOWLEDGE EDITING ']\n\n[The Start of Assistant's Predicted Answer]\n[\"TabFairGAN: Fair Tabular Data Generation with Generative Adversarial Networks\", \"Generative adversarial nets\", \"Data preprocessing techniques for classification without discrimination\", \"Certifying and removing disparate impact\", \"Fair prediction with disparate impact: A study of bias in recidivism prediction instruments\"]\n[The End of Assistant's Predicted Answer]\n\n[System]\nWe would like to request your feedback on the performance of the AI assistant in response to the user question displayed above according to the gold answer. Please use the following listed aspects and their descriptions as evaluation criteria:\n    - Accuracy and Hallucinations: The assistant's answer is semantically consistent with the gold answer; The numerical value and order need to be accurate, and there should be no hallucinations.\n    - Completeness: Referring to the reference answers, the assistant's answer should contain all the key points needed to answer the user's question; further elaboration on these key points can be omitted.\nPlease rate whether this answer is suitable for the question. Please note that the gold answer can be considered as a correct answer to the question.\n\nThe assistant receives an overall score on a scale of 1 to 100, where a higher score indicates better overall performance.\nPlease note that if the assistant's answer and the gold answer fully meet the above criteria, its overall rating should be the full marks (100).\nPlease first provide a comprehensive explanation of your evaluation, avoiding any potential bias.\nThen, output a line indicating the score of the Assistant.\n\nPLEASE OUTPUT WITH THE FOLLOWING FORMAT, WHERE THE SCORE IS A SCALE OF 1 TO 100 BY STRICTLY FOLLOWING THIS FORMAT: \"[[score]]\", FOR EXAMPLE \"Rating: [[100]]\":\n<start output>\nEvaluation evidence: your evluation explanation here, no more than 100 words\nRating: [[score]]\n<end output>\n\nNow, start your evaluation:",
        "generate_response": "[\"TabFairGAN: Fair Tabular Data Generation with Generative Adversarial Networks\", \"Generative adversarial nets\", \"Data preprocessing techniques for classification without discrimination\", \"Certifying and removing disparate impact\", \"Fair prediction with disparate impact: A study of bias in recidivism prediction instruments\"]",
        "eval_response": "Evaluation evidence: The assistant's answer does not match the gold answer in terms of the titles of the papers and their citation chain. The titles listed by the assistant are entirely different from those in the gold answer, indicating a significant accuracy issue and hallucination of content not provided in the task. The citation chain concept was understood, but the execution was incorrect due to the use of unrelated paper titles.\n\nRating: [[10]]",
        "score": 10
    },
    {
        "level": 4,
        "set": 2,
        "length": 99268,
        "type": "paper",
        "language": "en",
        "question": "",
        "instruction": "We kindly ask you to thoroughly review the provided papers and construct a citation chain from them. Please adhere to the following instructions strictly while completing the task:\n\n#Task Instructions:\nGiven several papers, you are required to identify and list the longest citation chain, which demonstrates the citation relationship among the provided papers.\n\n#Specific Requirements:\n1.Please present the titles of the papers in the form of a list, as follows: [\"Title of Paper 1\", \"Title of Paper 2\", ..., \"Title of Paper n\"].\n2.Ensure that the citation chain in the list is linear and continuous, meaning that the first paper title in the list (Paper 1) should not cite any other works. Instead, it should be cited by the next paper in the list (Paper 2); subsequently, each paper should then be cited by the next one in the list, continuing up to the last paper (Paper n).\n3.Consider only the citation relationships within the supplied collection of papers, and ensure that the citation chain accurately reflects the sequential citation order among these documents.\n4.Do not take into account any articles not provided, and disregard other non-linear citation relationships.",
        "prompt_template": "{instruction}\n\n#Papers Provided:\n{docs}",
        "doc": [
            "2307.10719.md",
            "2308.12950.md",
            "2310.06825.md",
            "2405.07518.md"
        ],
        "answer": [
            "# LLM CensorSHiP: A MACHINE LEARNing CHALLENGE OR A COMPUTER SECURITY PROBLEM? ",
            "# Code Llama: Open Foundation Models for Code ",
            "# Mistral 7B ",
            "# SambaNova SN40L: Scaling the AI Memory Wall with Dataflow and Composition of Experts "
        ],
        "shuffle_doc": true,
        "id": "b3dabd37-087d-44c1-97ab-be4acb16264f",
        "prompt": "[Question]\nWe kindly ask you to thoroughly review the provided papers and construct a citation chain from them. Please adhere to the following instructions strictly while completing the task:\n\n#Task Instructions:\nGiven several papers, you are required to identify and list the longest citation chain, which demonstrates the citation relationship among the provided papers.\n\n#Specific Requirements:\n1.Please present the titles of the papers in the form of a list, as follows: [\"Title of Paper 1\", \"Title of Paper 2\", ..., \"Title of Paper n\"].\n2.Ensure that the citation chain in the list is linear and continuous, meaning that the first paper title in the list (Paper 1) should not cite any other works. Instead, it should be cited by the next paper in the list (Paper 2); subsequently, each paper should then be cited by the next one in the list, continuing up to the last paper (Paper n).\n3.Consider only the citation relationships within the supplied collection of papers, and ensure that the citation chain accurately reflects the sequential citation order among these documents.\n4.Do not take into account any articles not provided, and disregard other non-linear citation relationships.\n\n#Papers Provided:\n{docs}\n\n[Gold Answer]\n['# LLM CensorSHiP: A MACHINE LEARNing CHALLENGE OR A COMPUTER SECURITY PROBLEM? ', '# Code Llama: Open Foundation Models for Code ', '# Mistral 7B ', '# SambaNova SN40L: Scaling the AI Memory Wall with Dataflow and Composition of Experts ']\n\n[The Start of Assistant's Predicted Answer]\n[\"Llama 2: Open foundation and fine-tuned chat models\", \"Llama: Open and efficient foundation language models\", \"Mistral 7B\"]\n[The End of Assistant's Predicted Answer]\n\n[System]\nWe would like to request your feedback on the performance of the AI assistant in response to the user question displayed above according to the gold answer. Please use the following listed aspects and their descriptions as evaluation criteria:\n    - Accuracy and Hallucinations: The assistant's answer is semantically consistent with the gold answer; The numerical value and order need to be accurate, and there should be no hallucinations.\n    - Completeness: Referring to the reference answers, the assistant's answer should contain all the key points needed to answer the user's question; further elaboration on these key points can be omitted.\nPlease rate whether this answer is suitable for the question. Please note that the gold answer can be considered as a correct answer to the question.\n\nThe assistant receives an overall score on a scale of 1 to 100, where a higher score indicates better overall performance.\nPlease note that if the assistant's answer and the gold answer fully meet the above criteria, its overall rating should be the full marks (100).\nPlease first provide a comprehensive explanation of your evaluation, avoiding any potential bias.\nThen, output a line indicating the score of the Assistant.\n\nPLEASE OUTPUT WITH THE FOLLOWING FORMAT, WHERE THE SCORE IS A SCALE OF 1 TO 100 BY STRICTLY FOLLOWING THIS FORMAT: \"[[score]]\", FOR EXAMPLE \"Rating: [[100]]\":\n<start output>\nEvaluation evidence: your evluation explanation here, no more than 100 words\nRating: [[score]]\n<end output>\n\nNow, start your evaluation:",
        "generate_response": "[\"Llama 2: Open foundation and fine-tuned chat models\", \"Llama: Open and efficient foundation language models\", \"Mistral 7B\"]",
        "eval_response": "Evaluation evidence: The assistant's answer does not match the gold answer in terms of the titles of the papers and their order. The titles in the assistant's answer do not appear in the gold answer, indicating a significant deviation and inaccuracies. This results in both hallucinations and a lack of accuracy. The citation chain provided by the assistant does not reflect a correct sequential citation order from the provided papers, leading to an incomplete response.\n\nRating: [[10]]",
        "score": 10
    },
    {
        "level": 3,
        "set": 4,
        "length": 222175,
        "type": "paper",
        "language": "en",
        "question": "Contrastive Preference Optimization: Pushing the Boundaries of LLM Performance in Machine Translation",
        "instruction": "We hope you will carefully study the provided papers and determine the citation relationships between them. Please follow the instructions below strictly to complete the task:\n\n#Specific Requirements:\n1. Reference: When a given paper mentions other provided papers, those other papers are considered as \"references\" for the given paper. To summarize in this specific context, references are about what the given paper is using.\n2. Citation: Conversely, when other provided papers mention the given paper in their works, the given paper is being \"cited\" by those other papers. To summarize in this specific context, citations are about who is using the given paper.\n3. Given a paper, you need to determine the citation or reference relationship between this paper and the other papers. Do not consider papers that are not provided.\n3. Please present the paper titles in a json format as follows: {{\"Reference\":[\"Reference Title 1\", \"Reference Title 2\", ..., \"Reference Title n\"], \"Citation\":[\"Citation Title 1\", \"Citation Title 2\", ..., \"Citation Title n\"]}}.\n4. If a paper does not have any references or citations, please leave the corresponding list empty, e.g.{{\"Refernce\":[]}}, {{\"Citation\":[]}}.",
        "prompt_template": "#Papers:\n{docs}\n\n{instruction}\n\n#The paper you need to analyze:\n{question}",
        "doc": [
            "2401.08417.md",
            "2405.14734.md",
            "2405.16681.md",
            "2405.18348.md",
            "2405.19534.md",
            "2405.20512.md",
            "2309.11925.md",
            "2310.06825.md",
            "2310.10482.md",
            "2310.13639.md",
            "2401.06760.md"
        ],
        "answer": {
            "Reference": [
                "# Scaling up CoMETKIWI: Unbabel-IST 2023 Submission for the Quality Estimation Shared Task ",
                "# Mistral 7B ",
                "# xCOMET: Transparent Machine Translation Evaluation through Fine-grained Error Detection ",
                "# ContRASTIVE PREFERENCE LEARNING: LEARNING FROM HUMAN FEEDBACK WITHOUT RL ",
                "# Navigating the Metrics Maze: Reconciling Score Magnitudes and Accuracies "
            ],
            "Citation": [
                "# SimPO: Simple Preference Optimization with a Reference-Free Reward ",
                "# Triple Preference Optimization: Achieving Better Alignment with Less Data in a Single Step Optimization ",
                "# Can Automatic Metrics Assess High-Quality Translations? ",
                "# Preference Learning Algorithms Do Not Learn Preference Rankings ",
                "# How Multilingual Are Large Language Models Fine-Tuned for Translation? "
            ]
        },
        "shuffle_doc": true,
        "id": "e67695d1-63e5-4de2-822b-92797ab9113f",
        "prompt": "[Question]\n#Papers:\n{docs}\n\nWe hope you will carefully study the provided papers and determine the citation relationships between them. Please follow the instructions below strictly to complete the task:\n\n#Specific Requirements:\n1. Reference: When a given paper mentions other provided papers, those other papers are considered as \"references\" for the given paper. To summarize in this specific context, references are about what the given paper is using.\n2. Citation: Conversely, when other provided papers mention the given paper in their works, the given paper is being \"cited\" by those other papers. To summarize in this specific context, citations are about who is using the given paper.\n3. Given a paper, you need to determine the citation or reference relationship between this paper and the other papers. Do not consider papers that are not provided.\n3. Please present the paper titles in a json format as follows: {{\"Reference\":[\"Reference Title 1\", \"Reference Title 2\", ..., \"Reference Title n\"], \"Citation\":[\"Citation Title 1\", \"Citation Title 2\", ..., \"Citation Title n\"]}}.\n4. If a paper does not have any references or citations, please leave the corresponding list empty, e.g.{{\"Refernce\":[]}}, {{\"Citation\":[]}}.\n\n#The paper you need to analyze:\nContrastive Preference Optimization: Pushing the Boundaries of LLM Performance in Machine Translation\n\n[Gold Answer]\n{'Reference': ['# Scaling up CoMETKIWI: Unbabel-IST 2023 Submission for the Quality Estimation Shared Task ', '# Mistral 7B ', '# xCOMET: Transparent Machine Translation Evaluation through Fine-grained Error Detection ', '# ContRASTIVE PREFERENCE LEARNING: LEARNING FROM HUMAN FEEDBACK WITHOUT RL ', '# Navigating the Metrics Maze: Reconciling Score Magnitudes and Accuracies '], 'Citation': ['# SimPO: Simple Preference Optimization with a Reference-Free Reward ', '# Triple Preference Optimization: Achieving Better Alignment with Less Data in a Single Step Optimization ', '# Can Automatic Metrics Assess High-Quality Translations? ', '# Preference Learning Algorithms Do Not Learn Preference Rankings ', '# How Multilingual Are Large Language Models Fine-Tuned for Translation? ']}\n\n[The Start of Assistant's Predicted Answer]\n{\"Reference\":[\"A Survey on Evaluation of Large Language Models\", \"Vicuna: An Open-Source Chatbot Impressing GPT-4 with $90 \\% *$ ChatGPT Quality\", \"ChatLaw: Open-Source Legal Large Language Model with Integrated External Knowledge Bases\", \"Compositional Semantic Parsing with Large Language Models\", \"Open LLM Leaderboard\", \"How Close is ChatGPT to Human Experts? Comparison Corpus, Evaluation, and Detection\", \"REALM: Retrieval-Augmented Language Model PreTraining\", \"Rethinking with Retrieval: Faithful Large Language Model Inference\", \"Measuring Massive Multitask Language Understanding\", \"Atlas: Few-shot Learning with Retrieval Augmented Language Models\", \"A general language assistant as a laboratory for alignment\", \"A general theoretical paradigm to understand learning from human preferences\", \"Training a helpful and harmless assistant with reinforcement learning from human feedback\", \"Open llm leaderboard\", \"Deep reinforcement learning from human preferences\", \"Language models are few-shot learners\", \"Open problems and fundamental limitations of reinforcement learning from human feedback\", \"Alpagasus: Training a better alpaca with fewer data\", \"Disentangled reward mitigates hacking in rlhf\", \"Deep reinforcement learning from human preferences\", \"Thinking you have solved question answering? try arc, the ai 2 reasoning challenge\", \"Training verifiers to solve math word problems\", \"Free dolly: Introducing the world's first truly open instruction-tuned llm\", \"Support-vector networks\", \"Rank analysis of incomplete block designs: I. the method of paired comparisons\", \"Language models are unsupervised multitask learners\", \"Direct preference optimization: Your language model is secretly a reward model\", \"Direct nash optimization: Teaching language models to self-improve with general preferences\", \"Understanding the performance gap between online and offline alignment algorithms\", \"Generalized preference optimization: A unified approach to offline alignment\", \"Stanford alpaca: An instruction-following llama model\", \"Iterative DPO alignment\", \"Zephyr: Direct distillation of LM alignment\", \"DecodingTrust: A comprehensive assessment of trustworthiness in gpt models\", \"OpenChat: Advancing open-source language models with mixed-quality data\", \"How far can camels go? exploring the state of instruction tuning on open resources\", \"LESS: Selecting influential data for targeted instruction tuning\", \"Contrastive preference optimization: Pushing the boundaries of llm performance in machine translation\", \"Some things are more cringe than others: Preference optimization with the pairwise cringe loss\", \"Is DPO superior to PPO for llm alignment? a comprehensive study\", \"RRHF: Rank responses to align language models with human feedback\", \"Self-rewarding language models\", \"HellaSwag: Can a machine really finish your sentence?\", \"Wildchat: $1 \\mathrm{~m}$ chatGPT interaction logs in the wild\", \"SLiC-HF: Sequence likelihood calibration with human feedback\", \"Judging llm-as-a-judge with mt-bench and chatbot arena\", \"Secrets of RLHF in large language models part I: PPO\", \"LIMA: Less is more for alignment\", \"Fine-tuning language models from human preferences\"], \"Citation\":[]}\n[The End of Assistant's Predicted Answer]\n\n[System]\nWe would like to request your feedback on the performance of the AI assistant in response to the user question displayed above according to the gold answer. Please use the following listed aspects and their descriptions as evaluation criteria:\n    - Accuracy and Hallucinations: The assistant's answer is semantically consistent with the gold answer; The numerical value and order need to be accurate, and there should be no hallucinations.\n    - Completeness: Referring to the reference answers, the assistant's answer should contain all the key points needed to answer the user's question; further elaboration on these key points can be omitted.\nPlease rate whether this answer is suitable for the question. Please note that the gold answer can be considered as a correct answer to the question.\n\nThe assistant receives an overall score on a scale of 1 to 100, where a higher score indicates better overall performance.\nPlease note that if the assistant's answer and the gold answer fully meet the above criteria, its overall rating should be the full marks (100).\nPlease first provide a comprehensive explanation of your evaluation, avoiding any potential bias.\nThen, output a line indicating the score of the Assistant.\n\nPLEASE OUTPUT WITH THE FOLLOWING FORMAT, WHERE THE SCORE IS A SCALE OF 1 TO 100 BY STRICTLY FOLLOWING THIS FORMAT: \"[[score]]\", FOR EXAMPLE \"Rating: [[100]]\":\n<start output>\nEvaluation evidence: your evluation explanation here, no more than 100 words\nRating: [[score]]\n<end output>\n\nNow, start your evaluation:",
        "generate_response": "{\"Reference\":[\"A Survey on Evaluation of Large Language Models\", \"Vicuna: An Open-Source Chatbot Impressing GPT-4 with $90 \\% *$ ChatGPT Quality\", \"ChatLaw: Open-Source Legal Large Language Model with Integrated External Knowledge Bases\", \"Compositional Semantic Parsing with Large Language Models\", \"Open LLM Leaderboard\", \"How Close is ChatGPT to Human Experts? Comparison Corpus, Evaluation, and Detection\", \"REALM: Retrieval-Augmented Language Model PreTraining\", \"Rethinking with Retrieval: Faithful Large Language Model Inference\", \"Measuring Massive Multitask Language Understanding\", \"Atlas: Few-shot Learning with Retrieval Augmented Language Models\", \"A general language assistant as a laboratory for alignment\", \"A general theoretical paradigm to understand learning from human preferences\", \"Training a helpful and harmless assistant with reinforcement learning from human feedback\", \"Open llm leaderboard\", \"Deep reinforcement learning from human preferences\", \"Language models are few-shot learners\", \"Open problems and fundamental limitations of reinforcement learning from human feedback\", \"Alpagasus: Training a better alpaca with fewer data\", \"Disentangled reward mitigates hacking in rlhf\", \"Deep reinforcement learning from human preferences\", \"Thinking you have solved question answering? try arc, the ai 2 reasoning challenge\", \"Training verifiers to solve math word problems\", \"Free dolly: Introducing the world's first truly open instruction-tuned llm\", \"Support-vector networks\", \"Rank analysis of incomplete block designs: I. the method of paired comparisons\", \"Language models are unsupervised multitask learners\", \"Direct preference optimization: Your language model is secretly a reward model\", \"Direct nash optimization: Teaching language models to self-improve with general preferences\", \"Understanding the performance gap between online and offline alignment algorithms\", \"Generalized preference optimization: A unified approach to offline alignment\", \"Stanford alpaca: An instruction-following llama model\", \"Iterative DPO alignment\", \"Zephyr: Direct distillation of LM alignment\", \"DecodingTrust: A comprehensive assessment of trustworthiness in gpt models\", \"OpenChat: Advancing open-source language models with mixed-quality data\", \"How far can camels go? exploring the state of instruction tuning on open resources\", \"LESS: Selecting influential data for targeted instruction tuning\", \"Contrastive preference optimization: Pushing the boundaries of llm performance in machine translation\", \"Some things are more cringe than others: Preference optimization with the pairwise cringe loss\", \"Is DPO superior to PPO for llm alignment? a comprehensive study\", \"RRHF: Rank responses to align language models with human feedback\", \"Self-rewarding language models\", \"HellaSwag: Can a machine really finish your sentence?\", \"Wildchat: $1 \\mathrm{~m}$ chatGPT interaction logs in the wild\", \"SLiC-HF: Sequence likelihood calibration with human feedback\", \"Judging llm-as-a-judge with mt-bench and chatbot arena\", \"Secrets of RLHF in large language models part I: PPO\", \"LIMA: Less is more for alignment\", \"Fine-tuning language models from human preferences\"], \"Citation\":[]}",
        "eval_response": "Evaluation evidence: The assistant's answer does not match the gold answer in terms of accuracy and completeness. The references and citations listed in the assistant's answer are entirely different from those in the gold answer, indicating a significant deviation and hallucination. The assistant's answer fails to identify any correct references or citations as per the gold answer, showing a lack of understanding of the task requirements.\n\nRating: [[1]]",
        "score": 1
    },
    {
        "level": 3,
        "set": 2,
        "length": 78116,
        "type": "paper",
        "language": "en",
        "question": "WebVoyager : Building an End-to-End Web Agent with Large Multimodal Models",
        "instruction": "We hope you will carefully study the provided papers and determine the citation relationships between them. Please follow the instructions below strictly to complete the task:\n\n#Specific Requirements:\n1. Reference: When a given paper mentions other provided papers, those other papers are considered as \"references\" for the given paper. To summarize in this specific context, references are about what the given paper is using.\n2. Citation: Conversely, when other provided papers mention the given paper in their works, the given paper is being \"cited\" by those other papers. To summarize in this specific context, citations are about who is using the given paper.\n3. Given a paper, you need to determine the citation or reference relationship between this paper and the other papers. Do not consider papers that are not provided.\n3. Please present the paper titles in a json format as follows: {{\"Reference\":[\"Reference Title 1\", \"Reference Title 2\", ..., \"Reference Title n\"], \"Citation\":[\"Citation Title 1\", \"Citation Title 2\", ..., \"Citation Title n\"]}}.\n4. If a paper does not have any references or citations, please leave the corresponding list empty, e.g.{{\"Refernce\":[]}}, {{\"Citation\":[]}}.",
        "prompt_template": "#Papers:\n{docs}\n\n{instruction}\n\n#The paper you need to analyze:\n{question}",
        "doc": [
            "2401.13919.md",
            "2404.05955.md",
            "2311.12983.md",
            "2401.13649.md"
        ],
        "answer": {
            "Reference": [
                "# GAIA: ",
                "# VisualWebArena: EvalUATING MULTIMODAL AGENTS ON REALISTIC VISUAL WEB TASKS "
            ],
            "Citation": [
                "# VisualWebBench: How Far Have Multimodal LLMs Evolved in Web Page Understanding and Grounding? "
            ]
        },
        "shuffle_doc": true,
        "id": "c942407b-cafc-4af8-9dbf-6442687b679b",
        "prompt": "[Question]\n#Papers:\n{docs}\n\nWe hope you will carefully study the provided papers and determine the citation relationships between them. Please follow the instructions below strictly to complete the task:\n\n#Specific Requirements:\n1. Reference: When a given paper mentions other provided papers, those other papers are considered as \"references\" for the given paper. To summarize in this specific context, references are about what the given paper is using.\n2. Citation: Conversely, when other provided papers mention the given paper in their works, the given paper is being \"cited\" by those other papers. To summarize in this specific context, citations are about who is using the given paper.\n3. Given a paper, you need to determine the citation or reference relationship between this paper and the other papers. Do not consider papers that are not provided.\n3. Please present the paper titles in a json format as follows: {{\"Reference\":[\"Reference Title 1\", \"Reference Title 2\", ..., \"Reference Title n\"], \"Citation\":[\"Citation Title 1\", \"Citation Title 2\", ..., \"Citation Title n\"]}}.\n4. If a paper does not have any references or citations, please leave the corresponding list empty, e.g.{{\"Refernce\":[]}}, {{\"Citation\":[]}}.\n\n#The paper you need to analyze:\nWebVoyager : Building an End-to-End Web Agent with Large Multimodal Models\n\n[Gold Answer]\n{'Reference': ['# GAIA: ', '# VisualWebArena: EvalUATING MULTIMODAL AGENTS ON REALISTIC VISUAL WEB TASKS '], 'Citation': ['# VisualWebBench: How Far Have Multimodal LLMs Evolved in Web Page Understanding and Grounding? ']}\n\n[The Start of Assistant's Predicted Answer]\n{\"Reference\": [\"Cm3: A causal masked multimodal model of the internet\", \"AutoGPT\", \"Language models are few-shot learners\", \"Evaluating large language models trained on code\", \"Webvln: Vision-and-language navigation on websites\", \"Seeclick: Harnessing gui grounding for advanced visual gui agents\", \"Palm: Scaling language modeling with pathways\", \"A coefficient of agreement for nominal scales\", \"Mind2web: Towards a generalist agent for the web\", \"A survey for in-context learning\", \"An image is worth $16 \\\\times 16$ words: Transformers for image recognition at scale\", \"Measuring nominal scale agreement among many raters\", \"Multimodal web navigation with instruction-finetuned foundation models\", \"Making the v in vqa matter: Elevating the role of image understanding in visual question answering\", \"A real-world webagent with planning, long context understanding, and program synthesis\", \"The abduction of sherlock holmes: A dataset for visual abductive reasoning\", \"Visualwebarena: Evaluating multimodal agents on realistic visual web tasks\", \"Pix2struct: Screenshot parsing as pretraining for visual language understanding\", \"Visualbert: A simple and performant baseline for vision and language\", \"Learn to explain: Multimodal reasoning via thought chains for science question answering\", \"Chameleon: Plug-and-play compositional reasoning with large language models\", \"Laser: Llm agent with state-space exploration for web navigation\", \"Gaia: a benchmark for general ai assistants\", \"Webgpt: Browser-assisted questionanswering with human feedback\", \"Gpt-4 technical report\", \"Training language models to follow instructions with human feedback\", \"Toolllm: Facilitating large language models to master 16000+ real-world apis\", \"Exploring the limits of transfer learning with a unified text-to-text transformer\", \"Toolformer: Language models can teach themselves to use tools\", \"From pixels to ui actions: Learning to follow instructions via graphical user interfaces\", \"World of bits: An open-domain platform for web-based agents\", \"Reflexion: Language agents with verbal reinforcement learning\", \"Gemini: a family of highly capable multimodal models\", \"Self-instruct: Aligning language model with self generated instructions\", \"Simvlm: Simple visual language model pretraining with weak supervision\", \"Llm-powered autonomous agents\", \"Gpt-4v in wonderland: Large multimodal models for zero-shot smartphone gui navigation\", \"Set-of-mark prompting unleashes extraordinary visual grounding in gpt-4v\", \"The dawn of $1 \\\\mathrm{mms}$ : Preliminary explorations with gpt-4v (ision)\", \"Webshop: Towards scalable real-world web interaction with grounded language agents\", \"React: Synergizing reasoning and acting in language models\", \"Lumos: Learning agents with unified data, modular design, and open-source llms\", \"From recognition to cognition: Visual commonsense reasoning\", \"Appagent: Multimodal agents as smartphone users\", \"Gpt-4v (ision) is a generalist web agent, if grounded\", \"Webarena: A realistic web environment for building autonomous agents\", \"Object detection in 20 years: A survey\"], \"Citation\": []}\n[The End of Assistant's Predicted Answer]\n\n[System]\nWe would like to request your feedback on the performance of the AI assistant in response to the user question displayed above according to the gold answer. Please use the following listed aspects and their descriptions as evaluation criteria:\n    - Accuracy and Hallucinations: The assistant's answer is semantically consistent with the gold answer; The numerical value and order need to be accurate, and there should be no hallucinations.\n    - Completeness: Referring to the reference answers, the assistant's answer should contain all the key points needed to answer the user's question; further elaboration on these key points can be omitted.\nPlease rate whether this answer is suitable for the question. Please note that the gold answer can be considered as a correct answer to the question.\n\nThe assistant receives an overall score on a scale of 1 to 100, where a higher score indicates better overall performance.\nPlease note that if the assistant's answer and the gold answer fully meet the above criteria, its overall rating should be the full marks (100).\nPlease first provide a comprehensive explanation of your evaluation, avoiding any potential bias.\nThen, output a line indicating the score of the Assistant.\n\nPLEASE OUTPUT WITH THE FOLLOWING FORMAT, WHERE THE SCORE IS A SCALE OF 1 TO 100 BY STRICTLY FOLLOWING THIS FORMAT: \"[[score]]\", FOR EXAMPLE \"Rating: [[100]]\":\n<start output>\nEvaluation evidence: your evluation explanation here, no more than 100 words\nRating: [[score]]\n<end output>\n\nNow, start your evaluation:",
        "generate_response": "{\"Reference\": [\"Cm3: A causal masked multimodal model of the internet\", \"AutoGPT\", \"Language models are few-shot learners\", \"Evaluating large language models trained on code\", \"Webvln: Vision-and-language navigation on websites\", \"Seeclick: Harnessing gui grounding for advanced visual gui agents\", \"Palm: Scaling language modeling with pathways\", \"A coefficient of agreement for nominal scales\", \"Mind2web: Towards a generalist agent for the web\", \"A survey for in-context learning\", \"An image is worth $16 \\\\times 16$ words: Transformers for image recognition at scale\", \"Measuring nominal scale agreement among many raters\", \"Multimodal web navigation with instruction-finetuned foundation models\", \"Making the v in vqa matter: Elevating the role of image understanding in visual question answering\", \"A real-world webagent with planning, long context understanding, and program synthesis\", \"The abduction of sherlock holmes: A dataset for visual abductive reasoning\", \"Visualwebarena: Evaluating multimodal agents on realistic visual web tasks\", \"Pix2struct: Screenshot parsing as pretraining for visual language understanding\", \"Visualbert: A simple and performant baseline for vision and language\", \"Learn to explain: Multimodal reasoning via thought chains for science question answering\", \"Chameleon: Plug-and-play compositional reasoning with large language models\", \"Laser: Llm agent with state-space exploration for web navigation\", \"Gaia: a benchmark for general ai assistants\", \"Webgpt: Browser-assisted questionanswering with human feedback\", \"Gpt-4 technical report\", \"Training language models to follow instructions with human feedback\", \"Toolllm: Facilitating large language models to master 16000+ real-world apis\", \"Exploring the limits of transfer learning with a unified text-to-text transformer\", \"Toolformer: Language models can teach themselves to use tools\", \"From pixels to ui actions: Learning to follow instructions via graphical user interfaces\", \"World of bits: An open-domain platform for web-based agents\", \"Reflexion: Language agents with verbal reinforcement learning\", \"Gemini: a family of highly capable multimodal models\", \"Self-instruct: Aligning language model with self generated instructions\", \"Simvlm: Simple visual language model pretraining with weak supervision\", \"Llm-powered autonomous agents\", \"Gpt-4v in wonderland: Large multimodal models for zero-shot smartphone gui navigation\", \"Set-of-mark prompting unleashes extraordinary visual grounding in gpt-4v\", \"The dawn of $1 \\\\mathrm{mms}$ : Preliminary explorations with gpt-4v (ision)\", \"Webshop: Towards scalable real-world web interaction with grounded language agents\", \"React: Synergizing reasoning and acting in language models\", \"Lumos: Learning agents with unified data, modular design, and open-source llms\", \"From recognition to cognition: Visual commonsense reasoning\", \"Appagent: Multimodal agents as smartphone users\", \"Gpt-4v (ision) is a generalist web agent, if grounded\", \"Webarena: A realistic web environment for building autonomous agents\", \"Object detection in 20 years: A survey\"], \"Citation\": []}",
        "eval_response": "Evaluation evidence: The assistant's answer does not match the gold answer in terms of accuracy and completeness. The assistant listed numerous references that are not mentioned in the gold answer, indicating a significant hallucination issue. Additionally, it failed to include the correct citation from the gold answer, further impacting its accuracy. The response does not align with the provided instructions and gold answer, leading to a low score.\n\nRating: [[10]]",
        "score": 10
    },
    {
        "level": 3,
        "set": 3,
        "length": 178200,
        "type": "paper",
        "language": "en",
        "question": "Aligner: Achieving Efficient Alignment through Weak-to-Strong Correction",
        "instruction": "We hope you will carefully study the provided papers and determine the citation relationships between them. Please follow the instructions below strictly to complete the task:\n\n#Specific Requirements:\n1. Reference: When a given paper mentions other provided papers, those other papers are considered as \"references\" for the given paper. To summarize in this specific context, references are about what the given paper is using.\n2. Citation: Conversely, when other provided papers mention the given paper in their works, the given paper is being \"cited\" by those other papers. To summarize in this specific context, citations are about who is using the given paper.\n3. Given a paper, you need to determine the citation or reference relationship between this paper and the other papers. Do not consider papers that are not provided.\n3. Please present the paper titles in a json format as follows: {{\"Reference\":[\"Reference Title 1\", \"Reference Title 2\", ..., \"Reference Title n\"], \"Citation\":[\"Citation Title 1\", \"Citation Title 2\", ..., \"Citation Title n\"]}}.\n4. If a paper does not have any references or citations, please leave the corresponding list empty, e.g.{{\"Refernce\":[]}}, {{\"Citation\":[]}}.",
        "prompt_template": "#Papers:\n{docs}\n\n{instruction}\n\n#The paper you need to analyze:\n{question}",
        "doc": [
            "2402.02416.md",
            "2405.18718.md",
            "2405.21040.md",
            "2311.17035.md",
            "2312.09390.md"
        ],
        "answer": {
            "Reference": [
                "# Scalable Extraction of Training Data from (Production) Language Models ",
                "## WEAK-TO-STRONG GENERALIZATION: ELICITING STRONG CAPABILITIES WITH WEAK SUPERVISION"
            ],
            "Citation": [
                "# Efficient Model-agnostic Alignment via Bayesian Persuasion ",
                "# Direct Alignment of Language Models via Quality-Aware Self-Refinement "
            ]
        },
        "shuffle_doc": true,
        "id": "28c91201-a732-4e6e-9cc1-cc0d2f74a518",
        "prompt": "[Question]\n#Papers:\n{docs}\n\nWe hope you will carefully study the provided papers and determine the citation relationships between them. Please follow the instructions below strictly to complete the task:\n\n#Specific Requirements:\n1. Reference: When a given paper mentions other provided papers, those other papers are considered as \"references\" for the given paper. To summarize in this specific context, references are about what the given paper is using.\n2. Citation: Conversely, when other provided papers mention the given paper in their works, the given paper is being \"cited\" by those other papers. To summarize in this specific context, citations are about who is using the given paper.\n3. Given a paper, you need to determine the citation or reference relationship between this paper and the other papers. Do not consider papers that are not provided.\n3. Please present the paper titles in a json format as follows: {{\"Reference\":[\"Reference Title 1\", \"Reference Title 2\", ..., \"Reference Title n\"], \"Citation\":[\"Citation Title 1\", \"Citation Title 2\", ..., \"Citation Title n\"]}}.\n4. If a paper does not have any references or citations, please leave the corresponding list empty, e.g.{{\"Refernce\":[]}}, {{\"Citation\":[]}}.\n\n#The paper you need to analyze:\nAligner: Achieving Efficient Alignment through Weak-to-Strong Correction\n\n[Gold Answer]\n{'Reference': ['# Scalable Extraction of Training Data from (Production) Language Models ', '## WEAK-TO-STRONG GENERALIZATION: ELICITING STRONG CAPABILITIES WITH WEAK SUPERVISION'], 'Citation': ['# Efficient Model-agnostic Alignment via Bayesian Persuasion ', '# Direct Alignment of Language Models via Quality-Aware Self-Refinement ']}\n\n[The Start of Assistant's Predicted Answer]\n{\"Citation\": [\"Gpt-4 technical report\", \"Concrete problems in ai safety and governance\", \"Palm 2 technical report\", \"Claude 2\", \"A general language assistant as a laboratory for alignment\", \"Training a helpful and harmless assistant with reinforcement learning from human feedback\", \"Constitutional ai: Harmlessness from ai feedback\", \"Red-teaming large language models using chain of utterances for safety-alignment\", \"Measuring progress on scalable oversight for large language models\", \"Weak-to-strong generalization: Eliciting strong capabilities with weak supervision\", \"Open problems and fundamental limitations of reinforcement learning from human feedback\", \"Teaching large language models to self-debug\", \"Seq2sick: Evaluating the robustness of sequence-tosequence models with adversarial examples\", \"Vicuna: An open-source chatbot impressing gpt-4 with $90 \\% *$ chatgpt quality\", \"Reinforced self-training (rest) for language modeling\", \"Deep residual learning for image recognition\", \"Large language models can self-improve\", \"Beavertails: Towards improved safety alignment of LLM via a human-preference dataset\", \"Ai alignment: A comprehensive survey\", \"Survey of hallucination in natural language generation\", \"Moderation API\", \"Deep reinforcement learning for sequence-to-sequence models\", \"Efficient memory management for large language model serving with pagedattention\", \"Rlaif: Scaling reinforcement learning from human feedback with ai feedback\", \"Multi-step jailbreaking privacy attacks on chatGPT\", \"Self-refine: Iterative refinement with self-feedback\", \"A self-refinement strategy for noise reduction in grammatical error correction\", \"Scalable extraction of training data from (production) language models\", \"The alignment problem from a deep learning perspective\", \"Introducing Superalignment\", \"Training language models to follow instructions with human feedback\", \"Direct preference optimization: Your language model is secretly a reward model\", \"Towards empathetic open-domain conversation models: A new benchmark and dataset\", \"Learning to model editing processes\", \"Self-critiquing models for assisting human evaluators\", \"Proximal policy optimization algorithms\", \"Reflexion: Language agents with verbal reinforcement learning\", \"A minimaximalist approach to reinforcement learning from human feedback\", \"Stanford alpaca: An instruction-following llama model\", \"Gemini: a family of highly capable multimodal models\", \"Llama 2: Open foundation and finetuned chat models\", \"Small language models improve giants by rewriting their outputs\", \"Open problems and fundamental limitations of reinforcement learning from human feedback\", \"Finegrained human feedback gives better rewards for language model training\", \"Baichuan 2: Open large-scale language models\", \"Shadow alignment: The ease of subverting safely-aligned language models\", \"Idea2img: Iterative self-refinement with gpt-4v (ision) for automatic image design and generation\", \"Deepspeed-chat: Easy, fast and affordable rlhf training of chatgpt-like models at all scales\", \"RRHF: Rank responses to align language models with human feedback\", \"A constrained sequence-to-sequence neural model for sentence simplification\", \"A survey of large language models\", \"Fine-tuning language models from human preferences\"], \"Reference\": []}\n[The End of Assistant's Predicted Answer]\n\n[System]\nWe would like to request your feedback on the performance of the AI assistant in response to the user question displayed above according to the gold answer. Please use the following listed aspects and their descriptions as evaluation criteria:\n    - Accuracy and Hallucinations: The assistant's answer is semantically consistent with the gold answer; The numerical value and order need to be accurate, and there should be no hallucinations.\n    - Completeness: Referring to the reference answers, the assistant's answer should contain all the key points needed to answer the user's question; further elaboration on these key points can be omitted.\nPlease rate whether this answer is suitable for the question. Please note that the gold answer can be considered as a correct answer to the question.\n\nThe assistant receives an overall score on a scale of 1 to 100, where a higher score indicates better overall performance.\nPlease note that if the assistant's answer and the gold answer fully meet the above criteria, its overall rating should be the full marks (100).\nPlease first provide a comprehensive explanation of your evaluation, avoiding any potential bias.\nThen, output a line indicating the score of the Assistant.\n\nPLEASE OUTPUT WITH THE FOLLOWING FORMAT, WHERE THE SCORE IS A SCALE OF 1 TO 100 BY STRICTLY FOLLOWING THIS FORMAT: \"[[score]]\", FOR EXAMPLE \"Rating: [[100]]\":\n<start output>\nEvaluation evidence: your evluation explanation here, no more than 100 words\nRating: [[score]]\n<end output>\n\nNow, start your evaluation:",
        "generate_response": "{\"Citation\": [\"Gpt-4 technical report\", \"Concrete problems in ai safety and governance\", \"Palm 2 technical report\", \"Claude 2\", \"A general language assistant as a laboratory for alignment\", \"Training a helpful and harmless assistant with reinforcement learning from human feedback\", \"Constitutional ai: Harmlessness from ai feedback\", \"Red-teaming large language models using chain of utterances for safety-alignment\", \"Measuring progress on scalable oversight for large language models\", \"Weak-to-strong generalization: Eliciting strong capabilities with weak supervision\", \"Open problems and fundamental limitations of reinforcement learning from human feedback\", \"Teaching large language models to self-debug\", \"Seq2sick: Evaluating the robustness of sequence-tosequence models with adversarial examples\", \"Vicuna: An open-source chatbot impressing gpt-4 with $90 \\% *$ chatgpt quality\", \"Reinforced self-training (rest) for language modeling\", \"Deep residual learning for image recognition\", \"Large language models can self-improve\", \"Beavertails: Towards improved safety alignment of LLM via a human-preference dataset\", \"Ai alignment: A comprehensive survey\", \"Survey of hallucination in natural language generation\", \"Moderation API\", \"Deep reinforcement learning for sequence-to-sequence models\", \"Efficient memory management for large language model serving with pagedattention\", \"Rlaif: Scaling reinforcement learning from human feedback with ai feedback\", \"Multi-step jailbreaking privacy attacks on chatGPT\", \"Self-refine: Iterative refinement with self-feedback\", \"A self-refinement strategy for noise reduction in grammatical error correction\", \"Scalable extraction of training data from (production) language models\", \"The alignment problem from a deep learning perspective\", \"Introducing Superalignment\", \"Training language models to follow instructions with human feedback\", \"Direct preference optimization: Your language model is secretly a reward model\", \"Towards empathetic open-domain conversation models: A new benchmark and dataset\", \"Learning to model editing processes\", \"Self-critiquing models for assisting human evaluators\", \"Proximal policy optimization algorithms\", \"Reflexion: Language agents with verbal reinforcement learning\", \"A minimaximalist approach to reinforcement learning from human feedback\", \"Stanford alpaca: An instruction-following llama model\", \"Gemini: a family of highly capable multimodal models\", \"Llama 2: Open foundation and finetuned chat models\", \"Small language models improve giants by rewriting their outputs\", \"Open problems and fundamental limitations of reinforcement learning from human feedback\", \"Finegrained human feedback gives better rewards for language model training\", \"Baichuan 2: Open large-scale language models\", \"Shadow alignment: The ease of subverting safely-aligned language models\", \"Idea2img: Iterative self-refinement with gpt-4v (ision) for automatic image design and generation\", \"Deepspeed-chat: Easy, fast and affordable rlhf training of chatgpt-like models at all scales\", \"RRHF: Rank responses to align language models with human feedback\", \"A constrained sequence-to-sequence neural model for sentence simplification\", \"A survey of large language models\", \"Fine-tuning language models from human preferences\"], \"Reference\": []}",
        "eval_response": "Evaluation evidence: The assistant's response does not match the gold answer in terms of accuracy and completeness. The assistant's answer lists numerous titles under \"Citation\" that are not mentioned in the gold answer, indicating hallucinations. Additionally, the \"Reference\" section is empty, which contradicts the gold answer that lists two references. This shows a lack of accuracy and completeness in the assistant's response.\n\nRating: [[10]]",
        "score": 10
    },
    {
        "level": 3,
        "set": 1,
        "length": 47238,
        "type": "paper",
        "language": "en",
        "question": "Mastering Text-to-Image Diffusion: Recaptioning, Planning, and Generating with Multimodal LLMs",
        "instruction": "We hope you will carefully study the provided papers and determine the citation relationships between them. Please follow the instructions below strictly to complete the task:\n\n#Specific Requirements:\n1. Reference: When a given paper mentions other provided papers, those other papers are considered as \"references\" for the given paper. To summarize in this specific context, references are about what the given paper is using.\n2. Citation: Conversely, when other provided papers mention the given paper in their works, the given paper is being \"cited\" by those other papers. To summarize in this specific context, citations are about who is using the given paper.\n3. Given a paper, you need to determine the citation or reference relationship between this paper and the other papers. Do not consider papers that are not provided.\n3. Please present the paper titles in a json format as follows: {{\"Reference\":[\"Reference Title 1\", \"Reference Title 2\", ..., \"Reference Title n\"], \"Citation\":[\"Citation Title 1\", \"Citation Title 2\", ..., \"Citation Title n\"]}}.\n4. If a paper does not have any references or citations, please leave the corresponding list empty, e.g.{{\"Refernce\":[]}}, {{\"Citation\":[]}}.",
        "prompt_template": "#Papers:\n{docs}\n\n{instruction}\n\n#The paper you need to analyze:\n{question}",
        "doc": [
            "2401.11708.md",
            "2405.14828.md",
            "2311.16090.md"
        ],
        "answer": {
            "Reference": [
                "# Self-correcting LLM-controlled Diffusion Models "
            ],
            "Citation": [
                "# Good Seed Makes a Good Crop: Discovering Secret Seeds in Text-to-Image Diffusion Models "
            ]
        },
        "shuffle_doc": true,
        "id": "81b1a92b-20c5-41af-9080-372d64ae9414",
        "prompt": "[Question]\n#Papers:\n{docs}\n\nWe hope you will carefully study the provided papers and determine the citation relationships between them. Please follow the instructions below strictly to complete the task:\n\n#Specific Requirements:\n1. Reference: When a given paper mentions other provided papers, those other papers are considered as \"references\" for the given paper. To summarize in this specific context, references are about what the given paper is using.\n2. Citation: Conversely, when other provided papers mention the given paper in their works, the given paper is being \"cited\" by those other papers. To summarize in this specific context, citations are about who is using the given paper.\n3. Given a paper, you need to determine the citation or reference relationship between this paper and the other papers. Do not consider papers that are not provided.\n3. Please present the paper titles in a json format as follows: {{\"Reference\":[\"Reference Title 1\", \"Reference Title 2\", ..., \"Reference Title n\"], \"Citation\":[\"Citation Title 1\", \"Citation Title 2\", ..., \"Citation Title n\"]}}.\n4. If a paper does not have any references or citations, please leave the corresponding list empty, e.g.{{\"Refernce\":[]}}, {{\"Citation\":[]}}.\n\n#The paper you need to analyze:\nMastering Text-to-Image Diffusion: Recaptioning, Planning, and Generating with Multimodal LLMs\n\n[Gold Answer]\n{'Reference': ['# Self-correcting LLM-controlled Diffusion Models '], 'Citation': ['# Good Seed Makes a Good Crop: Discovering Secret Seeds in Text-to-Image Diffusion Models ']}\n\n[The Start of Assistant's Predicted Answer]\n{\"Reference\":[\"Composable Diffusion\", \"GLIDE\", \"Imagen\", \"Latent Diffusion Models (LDMs)\", \"SDXL\", \"ContextDiff\", \"DALL-E 3\", \"StructureDiffusion\", \"GLIGEN\", \"ReCo\", \"T2I-Adapter\", \"ControlNet\", \"GORS\", \"Attend-and-Excite: Attention-based Semantic Guidance for Text-to-Image Diffusion Models\", \"Pixart-alpha: Fast Training of Diffusion Transformer for Photorealistic Text-to-Image Synthesis\", \"Training-free Layout Control with Cross-Attention Guidance\", \"Llava-interactive: An All-in-One Demo for Image Chat, Segmentation, Generation and Editing\", \"Vicuna: An Open-source Chatbot Impressing GPT-4 with $90 \\% *$ ChatGPT Quality\", \"Palm: Scaling Language Modeling with Pathways\", \"Scaling Instruction-finetuned Language Models\", \"Generative Adversarial Networks: An Overview\", \"Diffusion Models Beat GANs on Image Synthesis\", \"Dreamllm: Synergistic Multimodal Comprehension and Creation\", \"Boosting Text-to-Image Diffusion Models with Fine-grained Semantic Rewards\", \"Training-free Structured Diffusion Guidance for Compositional Text-to-Image Synthesis\", \"LayoutGPT: Compositional Visual Planning and Generation with Large Language Models\", \"Guiding Instruction-based Image Editing via Multimodal Large Language Models\", \"Visual Programming: Compositional Visual Reasoning without Training\", \"Prompt-to-Prompt Image Editing with Cross Attention Control\", \"High-resolution Image Synthesis with Latent Diffusion Models\", \"Photorealistic Text-to-Image Diffusion Models with Deep Language Understanding\", \"Score-based Generative Modeling through Stochastic Differential Equations\", \"Learning to Summarize with Human Feedback\", \"Dreamsync: Aligning Text-to-Image Generation with Image Understanding Feedback\", \"Generative Pretraining in Multimodality\", \"Bloom: A 176B-Parameter Open-Access Multilingual Language Model\", \"Visual ChatGPT: Talking, Drawing and Editing with Visual Foundation Models\", \"Self-correcting LLM-Controlled Diffusion Models\", \"BoxDiff: Text-to-Image Synthesis with Training-free Box-constrained Diffusion\", \"ImageReward: Learning and Evaluating Human Preferences for Text-to-Image Generation\", \"Baichuan 2: Open Large-scale Language Models\", \"Improving Diffusion-based Image Synthesis with Context Prediction\", \"Diffusion Models: A Comprehensive Survey of Methods and Applications\", \"Structure-guided Adversarial Training of Diffusion Models\", \"Cross-modal Contextualized Diffusion Models for Text-guided Visual Generation and Editing\", \"Prompting ChatGPT for Multimodal Reasoning and Action\", \"Region-controlled Text-to-Image Generation\", \"Scaling Autoregressive Multi-modal Models: Pretraining and Instruction Tuning\", \"GLM-130B: An Open Bilingual Pre-trained Model\", \"Adding Conditional Control to Text-to-Image Diffusion Models\", \"OPT: Open Pre-trained Transformer Language Models\", \"Controllable Text-to-Image Generation with GPT-4\", \"Enhanced Visual Instruction Tuning for Text-rich Image Understanding\", \"Multimodal Chain-of-thought Reasoning in Language Models\", \"Generalized Decoding for Pixel, Image, and Language\"], \"Citation\":[\"Good Seed Makes a Good Crop: Discovering Secret Seeds in Text-to-Image Diffusion Models\", \"Self-correcting LLM-controlled Diffusion Models\"]}\n[The End of Assistant's Predicted Answer]\n\n[System]\nWe would like to request your feedback on the performance of the AI assistant in response to the user question displayed above according to the gold answer. Please use the following listed aspects and their descriptions as evaluation criteria:\n    - Accuracy and Hallucinations: The assistant's answer is semantically consistent with the gold answer; The numerical value and order need to be accurate, and there should be no hallucinations.\n    - Completeness: Referring to the reference answers, the assistant's answer should contain all the key points needed to answer the user's question; further elaboration on these key points can be omitted.\nPlease rate whether this answer is suitable for the question. Please note that the gold answer can be considered as a correct answer to the question.\n\nThe assistant receives an overall score on a scale of 1 to 100, where a higher score indicates better overall performance.\nPlease note that if the assistant's answer and the gold answer fully meet the above criteria, its overall rating should be the full marks (100).\nPlease first provide a comprehensive explanation of your evaluation, avoiding any potential bias.\nThen, output a line indicating the score of the Assistant.\n\nPLEASE OUTPUT WITH THE FOLLOWING FORMAT, WHERE THE SCORE IS A SCALE OF 1 TO 100 BY STRICTLY FOLLOWING THIS FORMAT: \"[[score]]\", FOR EXAMPLE \"Rating: [[100]]\":\n<start output>\nEvaluation evidence: your evluation explanation here, no more than 100 words\nRating: [[score]]\n<end output>\n\nNow, start your evaluation:",
        "generate_response": "{\"Reference\":[\"Composable Diffusion\", \"GLIDE\", \"Imagen\", \"Latent Diffusion Models (LDMs)\", \"SDXL\", \"ContextDiff\", \"DALL-E 3\", \"StructureDiffusion\", \"GLIGEN\", \"ReCo\", \"T2I-Adapter\", \"ControlNet\", \"GORS\", \"Attend-and-Excite: Attention-based Semantic Guidance for Text-to-Image Diffusion Models\", \"Pixart-alpha: Fast Training of Diffusion Transformer for Photorealistic Text-to-Image Synthesis\", \"Training-free Layout Control with Cross-Attention Guidance\", \"Llava-interactive: An All-in-One Demo for Image Chat, Segmentation, Generation and Editing\", \"Vicuna: An Open-source Chatbot Impressing GPT-4 with $90 \\% *$ ChatGPT Quality\", \"Palm: Scaling Language Modeling with Pathways\", \"Scaling Instruction-finetuned Language Models\", \"Generative Adversarial Networks: An Overview\", \"Diffusion Models Beat GANs on Image Synthesis\", \"Dreamllm: Synergistic Multimodal Comprehension and Creation\", \"Boosting Text-to-Image Diffusion Models with Fine-grained Semantic Rewards\", \"Training-free Structured Diffusion Guidance for Compositional Text-to-Image Synthesis\", \"LayoutGPT: Compositional Visual Planning and Generation with Large Language Models\", \"Guiding Instruction-based Image Editing via Multimodal Large Language Models\", \"Visual Programming: Compositional Visual Reasoning without Training\", \"Prompt-to-Prompt Image Editing with Cross Attention Control\", \"High-resolution Image Synthesis with Latent Diffusion Models\", \"Photorealistic Text-to-Image Diffusion Models with Deep Language Understanding\", \"Score-based Generative Modeling through Stochastic Differential Equations\", \"Learning to Summarize with Human Feedback\", \"Dreamsync: Aligning Text-to-Image Generation with Image Understanding Feedback\", \"Generative Pretraining in Multimodality\", \"Bloom: A 176B-Parameter Open-Access Multilingual Language Model\", \"Visual ChatGPT: Talking, Drawing and Editing with Visual Foundation Models\", \"Self-correcting LLM-Controlled Diffusion Models\", \"BoxDiff: Text-to-Image Synthesis with Training-free Box-constrained Diffusion\", \"ImageReward: Learning and Evaluating Human Preferences for Text-to-Image Generation\", \"Baichuan 2: Open Large-scale Language Models\", \"Improving Diffusion-based Image Synthesis with Context Prediction\", \"Diffusion Models: A Comprehensive Survey of Methods and Applications\", \"Structure-guided Adversarial Training of Diffusion Models\", \"Cross-modal Contextualized Diffusion Models for Text-guided Visual Generation and Editing\", \"Prompting ChatGPT for Multimodal Reasoning and Action\", \"Region-controlled Text-to-Image Generation\", \"Scaling Autoregressive Multi-modal Models: Pretraining and Instruction Tuning\", \"GLM-130B: An Open Bilingual Pre-trained Model\", \"Adding Conditional Control to Text-to-Image Diffusion Models\", \"OPT: Open Pre-trained Transformer Language Models\", \"Controllable Text-to-Image Generation with GPT-4\", \"Enhanced Visual Instruction Tuning for Text-rich Image Understanding\", \"Multimodal Chain-of-thought Reasoning in Language Models\", \"Generalized Decoding for Pixel, Image, and Language\"], \"Citation\":[\"Good Seed Makes a Good Crop: Discovering Secret Seeds in Text-to-Image Diffusion Models\", \"Self-correcting LLM-controlled Diffusion Models\"]}",
        "eval_response": "Evaluation evidence: The assistant's answer includes a significant number of hallucinated references and citations that are not mentioned in the gold answer. The gold answer specifies one reference and one citation, while the assistant's answer lists numerous unrelated titles. This indicates a lack of accuracy and introduces hallucinations, which significantly deviates from the gold answer's content.\n\nRating: [[20]]",
        "score": 20
    },
    {
        "level": 3,
        "set": 2,
        "length": 95282,
        "type": "paper",
        "language": "en",
        "question": "Large Language Models for Mathematical Reasoning: Progresses and Challenges",
        "instruction": "We hope you will carefully study the provided papers and determine the citation relationships between them. Please follow the instructions below strictly to complete the task:\n\n#Specific Requirements:\n1. Reference: When a given paper mentions other provided papers, those other papers are considered as \"references\" for the given paper. To summarize in this specific context, references are about what the given paper is using.\n2. Citation: Conversely, when other provided papers mention the given paper in their works, the given paper is being \"cited\" by those other papers. To summarize in this specific context, citations are about who is using the given paper.\n3. Given a paper, you need to determine the citation or reference relationship between this paper and the other papers. Do not consider papers that are not provided.\n3. Please present the paper titles in a json format as follows: {{\"Reference\":[\"Reference Title 1\", \"Reference Title 2\", ..., \"Reference Title n\"], \"Citation\":[\"Citation Title 1\", \"Citation Title 2\", ..., \"Citation Title n\"]}}.\n4. If a paper does not have any references or citations, please leave the corresponding list empty, e.g.{{\"Refernce\":[]}}, {{\"Citation\":[]}}.",
        "prompt_template": "#Papers:\n{docs}\n\n{instruction}\n\n#The paper you need to analyze:\n{question}",
        "doc": [
            "2402.00157.md",
            "2405.16510.md",
            "2405.18272.md",
            "2310.10047.md",
            "2310.13615.md"
        ],
        "answer": {
            "Reference": [
                "# IMPROVING LARGE LANGUAGE MODEL FINE-TUNING FOR SOLVING MATH PROBLEMS ",
                "# Three Questions Concerning the Use of Large Language Models to Facilitate Mathematics Learning "
            ],
            "Citation": [
                "# Meta-Task Planning for Language Agents ",
                "# Metaheuristics and Large Language Models Join Forces: Towards an Integrated Optimization Approach "
            ]
        },
        "shuffle_doc": true,
        "id": "2d7cf44e-94a1-4f0b-9f4c-8e9c4b72d63b",
        "prompt": "[Question]\n#Papers:\n{docs}\n\nWe hope you will carefully study the provided papers and determine the citation relationships between them. Please follow the instructions below strictly to complete the task:\n\n#Specific Requirements:\n1. Reference: When a given paper mentions other provided papers, those other papers are considered as \"references\" for the given paper. To summarize in this specific context, references are about what the given paper is using.\n2. Citation: Conversely, when other provided papers mention the given paper in their works, the given paper is being \"cited\" by those other papers. To summarize in this specific context, citations are about who is using the given paper.\n3. Given a paper, you need to determine the citation or reference relationship between this paper and the other papers. Do not consider papers that are not provided.\n3. Please present the paper titles in a json format as follows: {{\"Reference\":[\"Reference Title 1\", \"Reference Title 2\", ..., \"Reference Title n\"], \"Citation\":[\"Citation Title 1\", \"Citation Title 2\", ..., \"Citation Title n\"]}}.\n4. If a paper does not have any references or citations, please leave the corresponding list empty, e.g.{{\"Refernce\":[]}}, {{\"Citation\":[]}}.\n\n#The paper you need to analyze:\nLarge Language Models for Mathematical Reasoning: Progresses and Challenges\n\n[Gold Answer]\n{'Reference': ['# IMPROVING LARGE LANGUAGE MODEL FINE-TUNING FOR SOLVING MATH PROBLEMS ', '# Three Questions Concerning the Use of Large Language Models to Facilitate Mathematics Learning '], 'Citation': ['# Meta-Task Planning for Language Agents ', '# Metaheuristics and Large Language Models Join Forces: Towards an Integrated Optimization Approach ']}\n\n[The Start of Assistant's Predicted Answer]\n{\"Reference\":[\"A neural network solves, explains, and generates university math problems by program synthesis and fewshot learning at human level\", \"Large language models for mathematicians\", \"Mathematical capabilities of chatgpt\", \"Learning to solve arithmetic word problems with verb categorization\", \"Solving geometry problems: Combining text and diagram interpretation\", \"Can neural networks do arithmetic? A survey on the elementary numerical skills of state-of-the-art deep learning models\", \"Llama: Open and efficient foundation language models\", \"Llama 2: Open foundation and fine-tuned chat models\", \"Solving olympiad geometry without human demonstrations\", \"Annotating derivations: A new evaluation strategy and dataset for algebra word problems\", \"Deep neural solver for math word problems\", \"Math word problem generation with mathematical consistency and problem context constraints\", \"Chain-of-thought prompting elicits reasoning in large language models\", \"CMATH: can your language model pass chinese elementary school math test?\", \"LPML: llm-prompting markup language for mathematical reasoning\", \"Learning to prove theorems via interacting with proof assistants\", \"How well do large language models perform in arithmetic tasks?\", \"Mammoth: Building math generalist models through hybrid instruction tuning\", \"GLM-130B: an open bilingual pre-trained model\", \"Evaluating and improving tool-augmented computation-intensive math reasoning\", \"Interpretable math word problem solution generation via step-by-step planning\", \"Ape210k: A large-scale and template-rich dataset of math word problems\", \"Minif2f: a cross-system benchmark for formal olympiad-level mathematics\", \"AgiEval: A human-centric benchmark for evaluating foundation models\", \"Solving challenging math word problems using GPT-4 code interpreter with code-based self-verification\", \"Mathattack: Attacking large language models towards math solving ability\", \"Rewriting math word problems with large language models\", \"Synthesis of solutions for shaded area geometry problems\", \"MathQA: Towards interpretable math word problem solving with operation-based formalisms\", \"Does ChatGPT comprehend the place value in numbers when solving math word problems?\", \"PALM 2 technical report\", \"Training a helpful and harmless assistant with reinforcement learning from human feedback\", \"Holist: An environment for machine learning of higher-order theorem proving\", \"Solving math word problems with reexamination\", \"GPT-Neo: Large scale autoregressive language modeling with Mesh-TensorFlow\", \"A survey on evaluation of large language models\", \"UniGeo: Unifying geometry logical reasoning via reformulating mathematical expression\", \"Dynamic prompt learning via policy gradient for semi-structured mathematical reasoning\", \"A survey of deep learning for mathematical reasoning\", \"WizardMath: Empowering mathematical reasoning for large language models via reinforced evol-instruct\", \"ChartQA: A benchmark for question answering about charts with visual and logical reasoning\", \"Learning mathematics with large language models: A comparative study with computer algebra systems and other tools\", \"A diverse corpus for evaluating and developing English math word problem solvers\", \"LILA: A unified benchmark for mathematical reasoning\", \"Rewriting math word problems with large language models\", \"Show your work: Scratchpads for intermediate computation with language models\", \"Training language models to follow instructions with human feedback\", \"Are NLP models really able to solve simple math word problems?\", \"Neural-symbolic solver for math word problems with auxiliary tasks\", \"Language models are unsupervised multitask learners\", \"Math word problem solving by generating linguistic variants of problem statements\", \"Enhancing mathematical capabilities through ChatGPT and similar generative artificial intelligence: Roles and challenges in solving mathematical problems\", \"Mathematical discoveries from program search with large language models\", \"From textbooks to knowledge: A case study in harvesting axiomatic knowledge from textbooks to solve geometry problems\", \"Learning to solve geometry problems from natural language demonstrations in textbooks\", \"An independent evaluation of ChatGPT on mathematical word problems (MWP)\", \"A causal framework to quantify the robustness of mathematical reasoning with language models\", \"Galactica: A large language model for science\", \"Solving math word problems via cooperative reasoning induced language models\", \"Solving math word problems concerning systems of equations with GPT-3\"], \"Citation\":[]}\n[The End of Assistant's Predicted Answer]\n\n[System]\nWe would like to request your feedback on the performance of the AI assistant in response to the user question displayed above according to the gold answer. Please use the following listed aspects and their descriptions as evaluation criteria:\n    - Accuracy and Hallucinations: The assistant's answer is semantically consistent with the gold answer; The numerical value and order need to be accurate, and there should be no hallucinations.\n    - Completeness: Referring to the reference answers, the assistant's answer should contain all the key points needed to answer the user's question; further elaboration on these key points can be omitted.\nPlease rate whether this answer is suitable for the question. Please note that the gold answer can be considered as a correct answer to the question.\n\nThe assistant receives an overall score on a scale of 1 to 100, where a higher score indicates better overall performance.\nPlease note that if the assistant's answer and the gold answer fully meet the above criteria, its overall rating should be the full marks (100).\nPlease first provide a comprehensive explanation of your evaluation, avoiding any potential bias.\nThen, output a line indicating the score of the Assistant.\n\nPLEASE OUTPUT WITH THE FOLLOWING FORMAT, WHERE THE SCORE IS A SCALE OF 1 TO 100 BY STRICTLY FOLLOWING THIS FORMAT: \"[[score]]\", FOR EXAMPLE \"Rating: [[100]]\":\n<start output>\nEvaluation evidence: your evluation explanation here, no more than 100 words\nRating: [[score]]\n<end output>\n\nNow, start your evaluation:",
        "generate_response": "{\"Reference\":[\"A neural network solves, explains, and generates university math problems by program synthesis and fewshot learning at human level\", \"Large language models for mathematicians\", \"Mathematical capabilities of chatgpt\", \"Learning to solve arithmetic word problems with verb categorization\", \"Solving geometry problems: Combining text and diagram interpretation\", \"Can neural networks do arithmetic? A survey on the elementary numerical skills of state-of-the-art deep learning models\", \"Llama: Open and efficient foundation language models\", \"Llama 2: Open foundation and fine-tuned chat models\", \"Solving olympiad geometry without human demonstrations\", \"Annotating derivations: A new evaluation strategy and dataset for algebra word problems\", \"Deep neural solver for math word problems\", \"Math word problem generation with mathematical consistency and problem context constraints\", \"Chain-of-thought prompting elicits reasoning in large language models\", \"CMATH: can your language model pass chinese elementary school math test?\", \"LPML: llm-prompting markup language for mathematical reasoning\", \"Learning to prove theorems via interacting with proof assistants\", \"How well do large language models perform in arithmetic tasks?\", \"Mammoth: Building math generalist models through hybrid instruction tuning\", \"GLM-130B: an open bilingual pre-trained model\", \"Evaluating and improving tool-augmented computation-intensive math reasoning\", \"Interpretable math word problem solution generation via step-by-step planning\", \"Ape210k: A large-scale and template-rich dataset of math word problems\", \"Minif2f: a cross-system benchmark for formal olympiad-level mathematics\", \"AgiEval: A human-centric benchmark for evaluating foundation models\", \"Solving challenging math word problems using GPT-4 code interpreter with code-based self-verification\", \"Mathattack: Attacking large language models towards math solving ability\", \"Rewriting math word problems with large language models\", \"Synthesis of solutions for shaded area geometry problems\", \"MathQA: Towards interpretable math word problem solving with operation-based formalisms\", \"Does ChatGPT comprehend the place value in numbers when solving math word problems?\", \"PALM 2 technical report\", \"Training a helpful and harmless assistant with reinforcement learning from human feedback\", \"Holist: An environment for machine learning of higher-order theorem proving\", \"Solving math word problems with reexamination\", \"GPT-Neo: Large scale autoregressive language modeling with Mesh-TensorFlow\", \"A survey on evaluation of large language models\", \"UniGeo: Unifying geometry logical reasoning via reformulating mathematical expression\", \"Dynamic prompt learning via policy gradient for semi-structured mathematical reasoning\", \"A survey of deep learning for mathematical reasoning\", \"WizardMath: Empowering mathematical reasoning for large language models via reinforced evol-instruct\", \"ChartQA: A benchmark for question answering about charts with visual and logical reasoning\", \"Learning mathematics with large language models: A comparative study with computer algebra systems and other tools\", \"A diverse corpus for evaluating and developing English math word problem solvers\", \"LILA: A unified benchmark for mathematical reasoning\", \"Rewriting math word problems with large language models\", \"Show your work: Scratchpads for intermediate computation with language models\", \"Training language models to follow instructions with human feedback\", \"Are NLP models really able to solve simple math word problems?\", \"Neural-symbolic solver for math word problems with auxiliary tasks\", \"Language models are unsupervised multitask learners\", \"Math word problem solving by generating linguistic variants of problem statements\", \"Enhancing mathematical capabilities through ChatGPT and similar generative artificial intelligence: Roles and challenges in solving mathematical problems\", \"Mathematical discoveries from program search with large language models\", \"From textbooks to knowledge: A case study in harvesting axiomatic knowledge from textbooks to solve geometry problems\", \"Learning to solve geometry problems from natural language demonstrations in textbooks\", \"An independent evaluation of ChatGPT on mathematical word problems (MWP)\", \"A causal framework to quantify the robustness of mathematical reasoning with language models\", \"Galactica: A large language model for science\", \"Solving math word problems via cooperative reasoning induced language models\", \"Solving math word problems concerning systems of equations with GPT-3\"], \"Citation\":[]}",
        "eval_response": "Evaluation evidence: The assistant's answer does not match the gold answer in terms of accuracy and completeness. The references and citations listed in the assistant's response do not align with those in the gold answer, indicating a significant deviation and hallucination of data. The assistant's response includes a long list of unrelated papers, which are not mentioned in the gold answer, thus failing to meet the required accuracy and completeness.\n\nRating: [[10]]",
        "score": 10
    },
    {
        "level": 3,
        "set": 2,
        "length": 81582,
        "type": "paper",
        "language": "en",
        "question": "RAG VS Fine-TUNing: PiPELINES, TRADEOFFS, AND A CASE STUDY ON AGRICULTURE",
        "instruction": "We hope you will carefully study the provided papers and determine the citation relationships between them. Please follow the instructions below strictly to complete the task:\n\n#Specific Requirements:\n1. Reference: When a given paper mentions other provided papers, those other papers are considered as \"references\" for the given paper. To summarize in this specific context, references are about what the given paper is using.\n2. Citation: Conversely, when other provided papers mention the given paper in their works, the given paper is being \"cited\" by those other papers. To summarize in this specific context, citations are about who is using the given paper.\n3. Given a paper, you need to determine the citation or reference relationship between this paper and the other papers. Do not consider papers that are not provided.\n3. Please present the paper titles in a json format as follows: {{\"Reference\":[\"Reference Title 1\", \"Reference Title 2\", ..., \"Reference Title n\"], \"Citation\":[\"Citation Title 1\", \"Citation Title 2\", ..., \"Citation Title n\"]}}.\n4. If a paper does not have any references or citations, please leave the corresponding list empty, e.g.{{\"Refernce\":[]}}, {{\"Citation\":[]}}.",
        "prompt_template": "#Papers:\n{docs}\n\n{instruction}\n\n#The paper you need to analyze:\n{question}",
        "doc": [
            "2401.08406.md",
            "2405.17147.md",
            "2306.05685.md",
            "2307.16877.md"
        ],
        "answer": {
            "Reference": [
                "# Judging LLM-as-a-Judge with MT-Bench and Chatbot Arena ",
                "# Evaluating Correctness and Faithfulness of Instruction-Following Models for Question Answering "
            ],
            "Citation": [
                "# Large Language Models (LLMs): Deployment, Tokenomics and Sustainability "
            ]
        },
        "shuffle_doc": true,
        "id": "cd165305-71c8-4aed-9316-e5b9f9e88e3a",
        "prompt": "[Question]\n#Papers:\n{docs}\n\nWe hope you will carefully study the provided papers and determine the citation relationships between them. Please follow the instructions below strictly to complete the task:\n\n#Specific Requirements:\n1. Reference: When a given paper mentions other provided papers, those other papers are considered as \"references\" for the given paper. To summarize in this specific context, references are about what the given paper is using.\n2. Citation: Conversely, when other provided papers mention the given paper in their works, the given paper is being \"cited\" by those other papers. To summarize in this specific context, citations are about who is using the given paper.\n3. Given a paper, you need to determine the citation or reference relationship between this paper and the other papers. Do not consider papers that are not provided.\n3. Please present the paper titles in a json format as follows: {{\"Reference\":[\"Reference Title 1\", \"Reference Title 2\", ..., \"Reference Title n\"], \"Citation\":[\"Citation Title 1\", \"Citation Title 2\", ..., \"Citation Title n\"]}}.\n4. If a paper does not have any references or citations, please leave the corresponding list empty, e.g.{{\"Refernce\":[]}}, {{\"Citation\":[]}}.\n\n#The paper you need to analyze:\nRAG VS Fine-TUNing: PiPELINES, TRADEOFFS, AND A CASE STUDY ON AGRICULTURE\n\n[Gold Answer]\n{'Reference': ['# Judging LLM-as-a-Judge with MT-Bench and Chatbot Arena ', '# Evaluating Correctness and Faithfulness of Instruction-Following Models for Question Answering '], 'Citation': ['# Large Language Models (LLMs): Deployment, Tokenomics and Sustainability ']}\n\n[The Start of Assistant's Predicted Answer]\n{\"Reference\":[\"A Comparison of Statistical Downscaling Methods Suited for Wildfire Applications\", \"500 Questions 500 Answers - Embrapa/SCT\", \"Krishi Vigyan Kendra (KVK) Q&A portal\", \"Vikaspedia\", \"Accurate, Large Minibatch SGD: Training ImageNet in 1 Hour\", \"Lora: Low-rank adaptation of large language models\", \"From word embeddings to document distances\", \"Retrieval-augmented generation for knowledge-intensive nlp tasks\", \"Large language models for supply chain optimization\", \"Pdf2text\", \"Pypdf\", \"ZeRO: Memory Optimizations Toward Training Trillion Parameter Models\", \"Sentence-BERT: Sentence Embeddings using Siamese BERT-Networks\", \"Knowledge guided representation learning and causal structure learning in soil science\", \"GPT-4 as an agronomist assistant? answering agriculture exams using large language models\", \"How LLM applications are revolutionizing the manufacturing industry\", \"Seeing through clouds in satellite images\", \"Judging LLM-as-a-Judge with MT-Bench and Chatbot Arena\", \"JudgeLM: Fine-tuned Large Language Models are Scalable Judges\", \"Scrapy\"], \"Citation\":[\"A Comparison of Statistical Downscaling Methods Suited for Wildfire Applications\", \"500 Questions 500 Answers - Embrapa/SCT\", \"Krishi Vigyan Kendra (KVK) Q&A portal\", \"Vikaspedia\", \"Accurate, Large Minibatch SGD: Training ImageNet in 1 Hour\", \"Lora: Low-rank adaptation of large language models\", \"From word embeddings to document distances\", \"Retrieval-augmented generation for knowledge-intensive nlp tasks\", \"Large language models for supply chain optimization\", \"Pdf2text\", \"Pypdf\", \"ZeRO: Memory Optimizations Toward Training Trillion Parameter Models\", \"Sentence-BERT: Sentence Embeddings using Siamese BERT-Networks\", \"Knowledge guided representation learning and causal structure learning in soil science\", \"GPT-4 as an agronomist assistant? answering agriculture exams using large language models\", \"How LLM applications are revolutionizing the manufacturing industry\", \"Seeing through clouds in satellite images\", \"Judging LLM-as-a-Judge with MT-Bench and Chatbot Arena\", \"JudgeLM: Fine-tuned Large Language Models are Scalable Judges\", \"Scrapy\"]}\n[The End of Assistant's Predicted Answer]\n\n[System]\nWe would like to request your feedback on the performance of the AI assistant in response to the user question displayed above according to the gold answer. Please use the following listed aspects and their descriptions as evaluation criteria:\n    - Accuracy and Hallucinations: The assistant's answer is semantically consistent with the gold answer; The numerical value and order need to be accurate, and there should be no hallucinations.\n    - Completeness: Referring to the reference answers, the assistant's answer should contain all the key points needed to answer the user's question; further elaboration on these key points can be omitted.\nPlease rate whether this answer is suitable for the question. Please note that the gold answer can be considered as a correct answer to the question.\n\nThe assistant receives an overall score on a scale of 1 to 100, where a higher score indicates better overall performance.\nPlease note that if the assistant's answer and the gold answer fully meet the above criteria, its overall rating should be the full marks (100).\nPlease first provide a comprehensive explanation of your evaluation, avoiding any potential bias.\nThen, output a line indicating the score of the Assistant.\n\nPLEASE OUTPUT WITH THE FOLLOWING FORMAT, WHERE THE SCORE IS A SCALE OF 1 TO 100 BY STRICTLY FOLLOWING THIS FORMAT: \"[[score]]\", FOR EXAMPLE \"Rating: [[100]]\":\n<start output>\nEvaluation evidence: your evluation explanation here, no more than 100 words\nRating: [[score]]\n<end output>\n\nNow, start your evaluation:",
        "generate_response": "{\"Reference\":[\"A Comparison of Statistical Downscaling Methods Suited for Wildfire Applications\", \"500 Questions 500 Answers - Embrapa/SCT\", \"Krishi Vigyan Kendra (KVK) Q&A portal\", \"Vikaspedia\", \"Accurate, Large Minibatch SGD: Training ImageNet in 1 Hour\", \"Lora: Low-rank adaptation of large language models\", \"From word embeddings to document distances\", \"Retrieval-augmented generation for knowledge-intensive nlp tasks\", \"Large language models for supply chain optimization\", \"Pdf2text\", \"Pypdf\", \"ZeRO: Memory Optimizations Toward Training Trillion Parameter Models\", \"Sentence-BERT: Sentence Embeddings using Siamese BERT-Networks\", \"Knowledge guided representation learning and causal structure learning in soil science\", \"GPT-4 as an agronomist assistant? answering agriculture exams using large language models\", \"How LLM applications are revolutionizing the manufacturing industry\", \"Seeing through clouds in satellite images\", \"Judging LLM-as-a-Judge with MT-Bench and Chatbot Arena\", \"JudgeLM: Fine-tuned Large Language Models are Scalable Judges\", \"Scrapy\"], \"Citation\":[\"A Comparison of Statistical Downscaling Methods Suited for Wildfire Applications\", \"500 Questions 500 Answers - Embrapa/SCT\", \"Krishi Vigyan Kendra (KVK) Q&A portal\", \"Vikaspedia\", \"Accurate, Large Minibatch SGD: Training ImageNet in 1 Hour\", \"Lora: Low-rank adaptation of large language models\", \"From word embeddings to document distances\", \"Retrieval-augmented generation for knowledge-intensive nlp tasks\", \"Large language models for supply chain optimization\", \"Pdf2text\", \"Pypdf\", \"ZeRO: Memory Optimizations Toward Training Trillion Parameter Models\", \"Sentence-BERT: Sentence Embeddings using Siamese BERT-Networks\", \"Knowledge guided representation learning and causal structure learning in soil science\", \"GPT-4 as an agronomist assistant? answering agriculture exams using large language models\", \"How LLM applications are revolutionizing the manufacturing industry\", \"Seeing through clouds in satellite images\", \"Judging LLM-as-a-Judge with MT-Bench and Chatbot Arena\", \"JudgeLM: Fine-tuned Large Language Models are Scalable Judges\", \"Scrapy\"]}",
        "eval_response": "Evaluation evidence: The assistant's response is incorrect and contains numerous hallucinations. The provided answer lists a large number of references and citations that are not mentioned in the gold answer, indicating a misunderstanding of the task. The gold answer specifies only three papers in total, two as references and one as a citation, which the assistant failed to accurately identify or list. This shows a lack of accuracy and completeness in the response.\n\nRating: [[10]]",
        "score": 10
    },
    {
        "level": 4,
        "set": 2,
        "length": 55389,
        "type": "paper",
        "language": "en",
        "question": "",
        "instruction": "We kindly ask you to thoroughly review the provided papers and construct a citation chain from them. Please adhere to the following instructions strictly while completing the task:\n\n#Task Instructions:\nGiven several papers, you are required to identify and list the longest citation chain, which demonstrates the citation relationship among the provided papers.\n\n#Specific Requirements:\n1.Please present the titles of the papers in the form of a list, as follows: [\"Title of Paper 1\", \"Title of Paper 2\", ..., \"Title of Paper n\"].\n2.Ensure that the citation chain in the list is linear and continuous, meaning that the first paper title in the list (Paper 1) should not cite any other works. Instead, it should be cited by the next paper in the list (Paper 2); subsequently, each paper should then be cited by the next one in the list, continuing up to the last paper (Paper n).\n3.Consider only the citation relationships within the supplied collection of papers, and ensure that the citation chain accurately reflects the sequential citation order among these documents.\n4.Do not take into account any articles not provided, and disregard other non-linear citation relationships.",
        "prompt_template": "{instruction}\n\n#Papers Provided:\n{docs}",
        "doc": [
            "2309.08532.md",
            "2310.05204.md",
            "2310.12541.md"
        ],
        "answer": [
            "# ConneCTING LARGE LANGUAGE MODELS WITH EVOLUTIONARY ALGORITHMS YIELDS POWERFUL PROMPT OPTIMIZERS ",
            "# Towards Optimizing with Large Language Model ",
            "# Large Language Model for Multi-objective Evolutionary Optimization "
        ],
        "shuffle_doc": true,
        "id": "f7a89936-1270-4bfd-ae5c-5fabaea6b694",
        "prompt": "[Question]\nWe kindly ask you to thoroughly review the provided papers and construct a citation chain from them. Please adhere to the following instructions strictly while completing the task:\n\n#Task Instructions:\nGiven several papers, you are required to identify and list the longest citation chain, which demonstrates the citation relationship among the provided papers.\n\n#Specific Requirements:\n1.Please present the titles of the papers in the form of a list, as follows: [\"Title of Paper 1\", \"Title of Paper 2\", ..., \"Title of Paper n\"].\n2.Ensure that the citation chain in the list is linear and continuous, meaning that the first paper title in the list (Paper 1) should not cite any other works. Instead, it should be cited by the next paper in the list (Paper 2); subsequently, each paper should then be cited by the next one in the list, continuing up to the last paper (Paper n).\n3.Consider only the citation relationships within the supplied collection of papers, and ensure that the citation chain accurately reflects the sequential citation order among these documents.\n4.Do not take into account any articles not provided, and disregard other non-linear citation relationships.\n\n#Papers Provided:\n{docs}\n\n[Gold Answer]\n['# ConneCTING LARGE LANGUAGE MODELS WITH EVOLUTIONARY ALGORITHMS YIELDS POWERFUL PROMPT OPTIMIZERS ', '# Towards Optimizing with Large Language Model ', '# Large Language Model for Multi-objective Evolutionary Optimization ']\n\n[The Start of Assistant's Predicted Answer]\n[\"ConneCTING LARGE LANGUAGE MODELS WITH EVOLUTIONARY ALGORITHMS YIELDS POWERFUL PROMPT OPTIMIZERS\", \"Language models are few-shot learners\", \"Evoprompting: Language models for code-level neural architecture search\", \"Differential evolution: A survey of the state-of-the-art\", \"A Survey on Evaluation of Large Language Models\", \"Vicuna: An Open-Source Chatbot Impressing GPT-4 with $90 \\% *$ ChatGPT Quality\", \"ChatLaw: Open-Source Legal Large Language Model with Integrated External Knowledge Bases\", \"Compositional Semantic Parsing with Large Language Models\", \"Open LLM Leaderboard\", \"How Close is ChatGPT to Human Experts? Comparison Corpus, Evaluation, and Detection\", \"REALM: Retrieval-Augmented Language Model PreTraining\", \"Rethinking with Retrieval: Faithful Large Language Model Inference\", \"Measuring Massive Multitask Language Understanding\", \"Atlas: Few-shot Learning with Retrieval Augmented Language Models\", \"Particle swarm optimization\", \"Ant colony system: a cooperative learning approach to the traveling salesman problem\", \"Is chatgpt a general-purpose natural language processing task solver?\", \"Llama: Open and efficient foundation language models\", \"Opt-iml: Scaling language model instruction meta learning through the lens of generalization\", \"Pre-train, prompt, and predict: A systematic survey of prompting methods in natural language processing\", \"P-tuning v2: Prompt tuning can be comparable to fine-tuning universally across scales and tasks\", \"Gpt understands, too\", \"A sentimental education: Sentiment analysis using subjectivity summarization based on minimum cuts\", \"Autoprompt: Eliciting knowledge from language models with automatically generated prompts\", \"Opt: Open pre-trained transformer language models\", \"Differentiable prompt makes pre-trained language models better few-shot learners\", \"Recursive deep models for semantic compositionality over a sentiment treebank\", \"Sentiment analysis in the era of large language models: A reality check\", \"Multi-task instruction tuning of llama for specific scenarios: A preliminary study on writing assistance\", \"Can gpt-4 perform neural architecture search?\", \"Large language models are human-level prompt engineers\", \"Promptbench: Towards evaluating the robustness of large language models on adversarial prompts\", \"Toward human readable prompt tuning: Kubrick's the shining is a good movie, and a good prompt too?\", \"A thorough examination of decoding methods in the era of llms\"]\n[The End of Assistant's Predicted Answer]\n\n[System]\nWe would like to request your feedback on the performance of the AI assistant in response to the user question displayed above according to the gold answer. Please use the following listed aspects and their descriptions as evaluation criteria:\n    - Accuracy and Hallucinations: The assistant's answer is semantically consistent with the gold answer; The numerical value and order need to be accurate, and there should be no hallucinations.\n    - Completeness: Referring to the reference answers, the assistant's answer should contain all the key points needed to answer the user's question; further elaboration on these key points can be omitted.\nPlease rate whether this answer is suitable for the question. Please note that the gold answer can be considered as a correct answer to the question.\n\nThe assistant receives an overall score on a scale of 1 to 100, where a higher score indicates better overall performance.\nPlease note that if the assistant's answer and the gold answer fully meet the above criteria, its overall rating should be the full marks (100).\nPlease first provide a comprehensive explanation of your evaluation, avoiding any potential bias.\nThen, output a line indicating the score of the Assistant.\n\nPLEASE OUTPUT WITH THE FOLLOWING FORMAT, WHERE THE SCORE IS A SCALE OF 1 TO 100 BY STRICTLY FOLLOWING THIS FORMAT: \"[[score]]\", FOR EXAMPLE \"Rating: [[100]]\":\n<start output>\nEvaluation evidence: your evluation explanation here, no more than 100 words\nRating: [[score]]\n<end output>\n\nNow, start your evaluation:",
        "generate_response": "[\"ConneCTING LARGE LANGUAGE MODELS WITH EVOLUTIONARY ALGORITHMS YIELDS POWERFUL PROMPT OPTIMIZERS\", \"Language models are few-shot learners\", \"Evoprompting: Language models for code-level neural architecture search\", \"Differential evolution: A survey of the state-of-the-art\", \"A Survey on Evaluation of Large Language Models\", \"Vicuna: An Open-Source Chatbot Impressing GPT-4 with $90 \\% *$ ChatGPT Quality\", \"ChatLaw: Open-Source Legal Large Language Model with Integrated External Knowledge Bases\", \"Compositional Semantic Parsing with Large Language Models\", \"Open LLM Leaderboard\", \"How Close is ChatGPT to Human Experts? Comparison Corpus, Evaluation, and Detection\", \"REALM: Retrieval-Augmented Language Model PreTraining\", \"Rethinking with Retrieval: Faithful Large Language Model Inference\", \"Measuring Massive Multitask Language Understanding\", \"Atlas: Few-shot Learning with Retrieval Augmented Language Models\", \"Particle swarm optimization\", \"Ant colony system: a cooperative learning approach to the traveling salesman problem\", \"Is chatgpt a general-purpose natural language processing task solver?\", \"Llama: Open and efficient foundation language models\", \"Opt-iml: Scaling language model instruction meta learning through the lens of generalization\", \"Pre-train, prompt, and predict: A systematic survey of prompting methods in natural language processing\", \"P-tuning v2: Prompt tuning can be comparable to fine-tuning universally across scales and tasks\", \"Gpt understands, too\", \"A sentimental education: Sentiment analysis using subjectivity summarization based on minimum cuts\", \"Autoprompt: Eliciting knowledge from language models with automatically generated prompts\", \"Opt: Open pre-trained transformer language models\", \"Differentiable prompt makes pre-trained language models better few-shot learners\", \"Recursive deep models for semantic compositionality over a sentiment treebank\", \"Sentiment analysis in the era of large language models: A reality check\", \"Multi-task instruction tuning of llama for specific scenarios: A preliminary study on writing assistance\", \"Can gpt-4 perform neural architecture search?\", \"Large language models are human-level prompt engineers\", \"Promptbench: Towards evaluating the robustness of large language models on adversarial prompts\", \"Toward human readable prompt tuning: Kubrick's the shining is a good movie, and a good prompt too?\", \"A thorough examination of decoding methods in the era of llms\"]",
        "eval_response": "Evaluation evidence: The assistant's response does not align with the gold answer. The gold answer lists a specific citation chain of three papers, while the assistant's response includes a long list of unrelated paper titles that do not form a citation chain. The assistant's response fails to meet the task's requirement of identifying and listing the longest citation chain among the provided papers. There are also hallucinations in the form of titles not relevant to the task. Therefore, the response is inaccurate and incomplete concerning the task requirements.\n\nRating: [[10]]",
        "score": 10
    },
    {
        "level": 3,
        "set": 2,
        "length": 73124,
        "type": "paper",
        "language": "en",
        "question": "LLMs Can't Plan, But Can Help Planning in LLM-Modulo Frameworks",
        "instruction": "We hope you will carefully study the provided papers and determine the citation relationships between them. Please follow the instructions below strictly to complete the task:\n\n#Specific Requirements:\n1. Reference: When a given paper mentions other provided papers, those other papers are considered as \"references\" for the given paper. To summarize in this specific context, references are about what the given paper is using.\n2. Citation: Conversely, when other provided papers mention the given paper in their works, the given paper is being \"cited\" by those other papers. To summarize in this specific context, citations are about who is using the given paper.\n3. Given a paper, you need to determine the citation or reference relationship between this paper and the other papers. Do not consider papers that are not provided.\n3. Please present the paper titles in a json format as follows: {{\"Reference\":[\"Reference Title 1\", \"Reference Title 2\", ..., \"Reference Title n\"], \"Citation\":[\"Citation Title 1\", \"Citation Title 2\", ..., \"Citation Title n\"]}}.\n4. If a paper does not have any references or citations, please leave the corresponding list empty, e.g.{{\"Refernce\":[]}}, {{\"Citation\":[]}}.",
        "prompt_template": "#Papers:\n{docs}\n\n{instruction}\n\n#The paper you need to analyze:\n{question}",
        "doc": [
            "2402.01817.md",
            "2404.18311.md",
            "2405.05885.md",
            "2405.15194.md"
        ],
        "answer": {
            "Reference": [],
            "Citation": [
                "# Towards Incremental Learning in Large Language Models: A Critical Review ",
                "# Co-driver: VLM-based Autonomous Driving Assistant with Human-like Behavior and Understanding for Complex Road Scenes ",
                "# Efficient Reinforcement Learning via Large Language Model-based Search "
            ]
        },
        "shuffle_doc": true,
        "id": "a0a0d04f-3020-4de1-ba28-aa2b17962672",
        "prompt": "[Question]\n#Papers:\n{docs}\n\nWe hope you will carefully study the provided papers and determine the citation relationships between them. Please follow the instructions below strictly to complete the task:\n\n#Specific Requirements:\n1. Reference: When a given paper mentions other provided papers, those other papers are considered as \"references\" for the given paper. To summarize in this specific context, references are about what the given paper is using.\n2. Citation: Conversely, when other provided papers mention the given paper in their works, the given paper is being \"cited\" by those other papers. To summarize in this specific context, citations are about who is using the given paper.\n3. Given a paper, you need to determine the citation or reference relationship between this paper and the other papers. Do not consider papers that are not provided.\n3. Please present the paper titles in a json format as follows: {{\"Reference\":[\"Reference Title 1\", \"Reference Title 2\", ..., \"Reference Title n\"], \"Citation\":[\"Citation Title 1\", \"Citation Title 2\", ..., \"Citation Title n\"]}}.\n4. If a paper does not have any references or citations, please leave the corresponding list empty, e.g.{{\"Refernce\":[]}}, {{\"Citation\":[]}}.\n\n#The paper you need to analyze:\nLLMs Can't Plan, But Can Help Planning in LLM-Modulo Frameworks\n\n[Gold Answer]\n{'Reference': [], 'Citation': ['# Towards Incremental Learning in Large Language Models: A Critical Review ', '# Co-driver: VLM-based Autonomous Driving Assistant with Human-like Behavior and Understanding for Complex Road Scenes ', '# Efficient Reinforcement Learning via Large Language Model-based Search ']}\n\n[The Start of Assistant's Predicted Answer]\n{\"Reference\": [\"Clever Hans\", \"Gipo: an integrated graphical tool to support knowledge engineering in ai planning\", \"Do as i can, not as i say: Grounding language in robotic affordances\", \"Language models are few-shot learners\", \"Sparks of artificial general intelligence: Early experiments with gpt-4\", \"Two theses of knowledge representation: Language restrictions, taxonomic classification, and the utility of representation services\", \"Faith and fate: Limits of transformers on compositionality\", \"Large language models are not abstract reasoners\", \"Automated Planning: theory and practice\", \"Leveraging approximate symbolic models for reinforcement learning via skill diversity\", \"Leveraging pre-trained large language models to construct and utilize world models for model-based task planning\", \"Reasoning with language model is planning with world model\", \"The FF planning system: fast plan generation through heuristic search\", \"VAL: Automatic plan validation, continuous effects and mixed initiative planning using PDDL\", \"Large language models cannot self-correct reasoning yet\", \"Large language models can self-improve\", \"Inner monologue: Embodied reasoning through planning with language models\", \"A new frontier for travel scammers: A.i.-generated guidebooks\", \"Reward design with language models\", \"Code as policies: Language model programs for embodied control\", \"Understanding the capabilities of large language models for automated planning\", \"Logic-lm: Empowering large language models with symbolic solvers for faithful logical reasoning\", \"Saynav: Grounding large language models for dynamic planning to navigation in new environments\", \"Mathematical discoveries from program search with large language models\", \"Artificial intelligence a modern approach\", \"Reflexion: Language agents with verbal reinforcement learning\", \"ALFWorld: Aligning Text and Embodied Environments for Interactive Learning\", \"PDDL planning with pretrained large language models\", \"Gpt-4 doesn't know it's wrong: An analysis of iterative prompting for reasoning problems\", \"Solving olympiad geometry without human demonstrations\", \"Large language models fail on trivial alterations to theory-of-mind tasks\", \"Can large language models really improve by self-critiquing their own plans?\", \"Planbench: An extensible benchmark for evaluating large language models on planning and reasoning about change\", \"On the planning abilities of large language models - a critical investigation\", \"Theory of mind abilities of large language models in human-robot interaction: An illusion?\", \"Voyager: An open-ended embodied agent with large language models\", \"Self-instruct: Aligning language model with self generated instructions\", \"Large language models are better reasoners with self-verification\", \"Translating natural language to planning goals with large-language models\", \"Tree of thoughts: Deliberate problem solving with large language models\", \"React: Synergizing reasoning and acting in language models\"], \"Citation\": [\"Towards Incremental Learning in Large Language Models: A Critical Review\"]}\n[The End of Assistant's Predicted Answer]\n\n[System]\nWe would like to request your feedback on the performance of the AI assistant in response to the user question displayed above according to the gold answer. Please use the following listed aspects and their descriptions as evaluation criteria:\n    - Accuracy and Hallucinations: The assistant's answer is semantically consistent with the gold answer; The numerical value and order need to be accurate, and there should be no hallucinations.\n    - Completeness: Referring to the reference answers, the assistant's answer should contain all the key points needed to answer the user's question; further elaboration on these key points can be omitted.\nPlease rate whether this answer is suitable for the question. Please note that the gold answer can be considered as a correct answer to the question.\n\nThe assistant receives an overall score on a scale of 1 to 100, where a higher score indicates better overall performance.\nPlease note that if the assistant's answer and the gold answer fully meet the above criteria, its overall rating should be the full marks (100).\nPlease first provide a comprehensive explanation of your evaluation, avoiding any potential bias.\nThen, output a line indicating the score of the Assistant.\n\nPLEASE OUTPUT WITH THE FOLLOWING FORMAT, WHERE THE SCORE IS A SCALE OF 1 TO 100 BY STRICTLY FOLLOWING THIS FORMAT: \"[[score]]\", FOR EXAMPLE \"Rating: [[100]]\":\n<start output>\nEvaluation evidence: your evluation explanation here, no more than 100 words\nRating: [[score]]\n<end output>\n\nNow, start your evaluation:",
        "generate_response": "{\"Reference\": [\"Clever Hans\", \"Gipo: an integrated graphical tool to support knowledge engineering in ai planning\", \"Do as i can, not as i say: Grounding language in robotic affordances\", \"Language models are few-shot learners\", \"Sparks of artificial general intelligence: Early experiments with gpt-4\", \"Two theses of knowledge representation: Language restrictions, taxonomic classification, and the utility of representation services\", \"Faith and fate: Limits of transformers on compositionality\", \"Large language models are not abstract reasoners\", \"Automated Planning: theory and practice\", \"Leveraging approximate symbolic models for reinforcement learning via skill diversity\", \"Leveraging pre-trained large language models to construct and utilize world models for model-based task planning\", \"Reasoning with language model is planning with world model\", \"The FF planning system: fast plan generation through heuristic search\", \"VAL: Automatic plan validation, continuous effects and mixed initiative planning using PDDL\", \"Large language models cannot self-correct reasoning yet\", \"Large language models can self-improve\", \"Inner monologue: Embodied reasoning through planning with language models\", \"A new frontier for travel scammers: A.i.-generated guidebooks\", \"Reward design with language models\", \"Code as policies: Language model programs for embodied control\", \"Understanding the capabilities of large language models for automated planning\", \"Logic-lm: Empowering large language models with symbolic solvers for faithful logical reasoning\", \"Saynav: Grounding large language models for dynamic planning to navigation in new environments\", \"Mathematical discoveries from program search with large language models\", \"Artificial intelligence a modern approach\", \"Reflexion: Language agents with verbal reinforcement learning\", \"ALFWorld: Aligning Text and Embodied Environments for Interactive Learning\", \"PDDL planning with pretrained large language models\", \"Gpt-4 doesn't know it's wrong: An analysis of iterative prompting for reasoning problems\", \"Solving olympiad geometry without human demonstrations\", \"Large language models fail on trivial alterations to theory-of-mind tasks\", \"Can large language models really improve by self-critiquing their own plans?\", \"Planbench: An extensible benchmark for evaluating large language models on planning and reasoning about change\", \"On the planning abilities of large language models - a critical investigation\", \"Theory of mind abilities of large language models in human-robot interaction: An illusion?\", \"Voyager: An open-ended embodied agent with large language models\", \"Self-instruct: Aligning language model with self generated instructions\", \"Large language models are better reasoners with self-verification\", \"Translating natural language to planning goals with large-language models\", \"Tree of thoughts: Deliberate problem solving with large language models\", \"React: Synergizing reasoning and acting in language models\"], \"Citation\": [\"Towards Incremental Learning in Large Language Models: A Critical Review\"]}",
        "eval_response": "Evaluation evidence: The assistant's answer partially matches the gold answer. It correctly identifies one citation from the gold answer but misses two others and includes numerous references that are not requested or relevant to the task. This results in both inaccuracies and hallucinations, as well as incompleteness in addressing the citations correctly.\n\nRating: [[30]]",
        "score": 30
    },
    {
        "level": 3,
        "set": 1,
        "length": 36149,
        "type": "paper",
        "language": "en",
        "question": "Multilingual Instruction Tuning With Just a Pinch of Multilinguality",
        "instruction": "We hope you will carefully study the provided papers and determine the citation relationships between them. Please follow the instructions below strictly to complete the task:\n\n#Specific Requirements:\n1. Reference: When a given paper mentions other provided papers, those other papers are considered as \"references\" for the given paper. To summarize in this specific context, references are about what the given paper is using.\n2. Citation: Conversely, when other provided papers mention the given paper in their works, the given paper is being \"cited\" by those other papers. To summarize in this specific context, citations are about who is using the given paper.\n3. Given a paper, you need to determine the citation or reference relationship between this paper and the other papers. Do not consider papers that are not provided.\n3. Please present the paper titles in a json format as follows: {{\"Reference\":[\"Reference Title 1\", \"Reference Title 2\", ..., \"Reference Title n\"], \"Citation\":[\"Citation Title 1\", \"Citation Title 2\", ..., \"Citation Title n\"]}}.\n4. If a paper does not have any references or citations, please leave the corresponding list empty, e.g.{{\"Refernce\":[]}}, {{\"Citation\":[]}}.",
        "prompt_template": "#Papers:\n{docs}\n\n{instruction}\n\n#The paper you need to analyze:\n{question}",
        "doc": [
            "2401.01854.md",
            "2404.04850.md",
            "2309.08958.md"
        ],
        "answer": {
            "Reference": [
                "# Monolingual or Multilingual Instruction Tuning: Which Makes a Better Alpaca "
            ],
            "Citation": [
                "# Lucky 52: How Many Languages Are Needed to Instruction Fine-Tune Large Language Models? "
            ]
        },
        "shuffle_doc": true,
        "id": "da57097c-d4fa-494d-90b8-a86d6578904b",
        "prompt": "[Question]\n#Papers:\n{docs}\n\nWe hope you will carefully study the provided papers and determine the citation relationships between them. Please follow the instructions below strictly to complete the task:\n\n#Specific Requirements:\n1. Reference: When a given paper mentions other provided papers, those other papers are considered as \"references\" for the given paper. To summarize in this specific context, references are about what the given paper is using.\n2. Citation: Conversely, when other provided papers mention the given paper in their works, the given paper is being \"cited\" by those other papers. To summarize in this specific context, citations are about who is using the given paper.\n3. Given a paper, you need to determine the citation or reference relationship between this paper and the other papers. Do not consider papers that are not provided.\n3. Please present the paper titles in a json format as follows: {{\"Reference\":[\"Reference Title 1\", \"Reference Title 2\", ..., \"Reference Title n\"], \"Citation\":[\"Citation Title 1\", \"Citation Title 2\", ..., \"Citation Title n\"]}}.\n4. If a paper does not have any references or citations, please leave the corresponding list empty, e.g.{{\"Refernce\":[]}}, {{\"Citation\":[]}}.\n\n#The paper you need to analyze:\nMultilingual Instruction Tuning With Just a Pinch of Multilinguality\n\n[Gold Answer]\n{'Reference': ['# Monolingual or Multilingual Instruction Tuning: Which Makes a Better Alpaca '], 'Citation': ['# Lucky 52: How Many Languages Are Needed to Instruction Fine-Tune Large Language Models? ']}\n\n[The Start of Assistant's Predicted Answer]\n{{\"Reference\":[\"Instruction tuning is a fundamental aspect of building modern general-purpose large language models (LLMs)\", \"We investigate the effect of multilingual data on instruction-following across languages using an LLM pre-trained on hundreds of languages (Anil et al., 2023)\", \"high-quality, open-ended instructions and responses (Zhou et al., 2023; Kpf et al., 2023) translated into 11 languages\", \"We use datasets of high-quality open-ended instructions and responses, rather than classic taskspecific datasets\", \"We find that replacing even just 40 English training examples with multilingual examples, significantly improves instructionfollowing in those languages\", \"Inspired by this result, we turn to ask how much multilingual data is required to improve multilingual instruction-following, while preserving English performance\", \"We find that tuning using a few languages enables better performance for languages unseen during tuning, compared to monolingual tuning with the same number of examples\", \"We experiment with different numbers of languages in the tuning set and indeed observe that the transfer to languages only seen during pre-training improves from the very first additional languages\", \"We do not find a signal indicating that any of the aspects above is correlated with cross-lingual transfer for this setting\", \"We find a weak Pearson correlation of 0.22 between the average cross-lingual score of each language and the number of documents in that language in pre-training corpus (Table 21 in Anil et al. (2023))\", \"In contrast to most cross-lingual transfer literature that is focused on task-specific fine-tuning, we explore trends of cross-lingual generalization for general-purpose instruction-following LLMs\"], \"Citation\":[]}}\n[The End of Assistant's Predicted Answer]\n\n[System]\nWe would like to request your feedback on the performance of the AI assistant in response to the user question displayed above according to the gold answer. Please use the following listed aspects and their descriptions as evaluation criteria:\n    - Accuracy and Hallucinations: The assistant's answer is semantically consistent with the gold answer; The numerical value and order need to be accurate, and there should be no hallucinations.\n    - Completeness: Referring to the reference answers, the assistant's answer should contain all the key points needed to answer the user's question; further elaboration on these key points can be omitted.\nPlease rate whether this answer is suitable for the question. Please note that the gold answer can be considered as a correct answer to the question.\n\nThe assistant receives an overall score on a scale of 1 to 100, where a higher score indicates better overall performance.\nPlease note that if the assistant's answer and the gold answer fully meet the above criteria, its overall rating should be the full marks (100).\nPlease first provide a comprehensive explanation of your evaluation, avoiding any potential bias.\nThen, output a line indicating the score of the Assistant.\n\nPLEASE OUTPUT WITH THE FOLLOWING FORMAT, WHERE THE SCORE IS A SCALE OF 1 TO 100 BY STRICTLY FOLLOWING THIS FORMAT: \"[[score]]\", FOR EXAMPLE \"Rating: [[100]]\":\n<start output>\nEvaluation evidence: your evluation explanation here, no more than 100 words\nRating: [[score]]\n<end output>\n\nNow, start your evaluation:",
        "generate_response": "{{\"Reference\":[\"Instruction tuning is a fundamental aspect of building modern general-purpose large language models (LLMs)\", \"We investigate the effect of multilingual data on instruction-following across languages using an LLM pre-trained on hundreds of languages (Anil et al., 2023)\", \"high-quality, open-ended instructions and responses (Zhou et al., 2023; Kpf et al., 2023) translated into 11 languages\", \"We use datasets of high-quality open-ended instructions and responses, rather than classic taskspecific datasets\", \"We find that replacing even just 40 English training examples with multilingual examples, significantly improves instructionfollowing in those languages\", \"Inspired by this result, we turn to ask how much multilingual data is required to improve multilingual instruction-following, while preserving English performance\", \"We find that tuning using a few languages enables better performance for languages unseen during tuning, compared to monolingual tuning with the same number of examples\", \"We experiment with different numbers of languages in the tuning set and indeed observe that the transfer to languages only seen during pre-training improves from the very first additional languages\", \"We do not find a signal indicating that any of the aspects above is correlated with cross-lingual transfer for this setting\", \"We find a weak Pearson correlation of 0.22 between the average cross-lingual score of each language and the number of documents in that language in pre-training corpus (Table 21 in Anil et al. (2023))\", \"In contrast to most cross-lingual transfer literature that is focused on task-specific fine-tuning, we explore trends of cross-lingual generalization for general-purpose instruction-following LLMs\"], \"Citation\":[]}}",
        "eval_response": "Evaluation evidence: The assistant's answer does not align with the gold answer. The assistant provided a list of general statements about instruction tuning and multilingual data, which are not specific paper titles. The gold answer specifies exact titles for references and citations, which the assistant failed to mention. This indicates a significant deviation in accuracy and completeness, as the assistant neither correctly identified the reference nor cited the correct papers.\n\nRating: [[10]]",
        "score": 10
    },
    {
        "level": 4,
        "set": 1,
        "length": 43838,
        "type": "paper",
        "language": "en",
        "question": "",
        "instruction": "We kindly ask you to thoroughly review the provided papers and construct a citation chain from them. Please adhere to the following instructions strictly while completing the task:\n\n#Task Instructions:\nGiven several papers, you are required to identify and list the longest citation chain, which demonstrates the citation relationship among the provided papers.\n\n#Specific Requirements:\n1.Please present the titles of the papers in the form of a list, as follows: [\"Title of Paper 1\", \"Title of Paper 2\", ..., \"Title of Paper n\"].\n2.Ensure that the citation chain in the list is linear and continuous, meaning that the first paper title in the list (Paper 1) should not cite any other works. Instead, it should be cited by the next paper in the list (Paper 2); subsequently, each paper should then be cited by the next one in the list, continuing up to the last paper (Paper n).\n3.Consider only the citation relationships within the supplied collection of papers, and ensure that the citation chain accurately reflects the sequential citation order among these documents.\n4.Do not take into account any articles not provided, and disregard other non-linear citation relationships.",
        "prompt_template": "{instruction}\n\n#Papers Provided:\n{docs}",
        "doc": [
            "2008.07772.md",
            "2004.08249.md",
            "2208.12615.md"
        ],
        "answer": [
            "# Very Deep Transformers for Neural Machine Translation ",
            "# Understanding the Difficulty of Training Transformers ",
            "# MonaCoBERT: Monotonic attention based ConvBERT for Knowledge Tracing "
        ],
        "shuffle_doc": true,
        "id": "ecded47a-3741-4458-ba61-c04271c33651",
        "prompt": "[Question]\nWe kindly ask you to thoroughly review the provided papers and construct a citation chain from them. Please adhere to the following instructions strictly while completing the task:\n\n#Task Instructions:\nGiven several papers, you are required to identify and list the longest citation chain, which demonstrates the citation relationship among the provided papers.\n\n#Specific Requirements:\n1.Please present the titles of the papers in the form of a list, as follows: [\"Title of Paper 1\", \"Title of Paper 2\", ..., \"Title of Paper n\"].\n2.Ensure that the citation chain in the list is linear and continuous, meaning that the first paper title in the list (Paper 1) should not cite any other works. Instead, it should be cited by the next paper in the list (Paper 2); subsequently, each paper should then be cited by the next one in the list, continuing up to the last paper (Paper n).\n3.Consider only the citation relationships within the supplied collection of papers, and ensure that the citation chain accurately reflects the sequential citation order among these documents.\n4.Do not take into account any articles not provided, and disregard other non-linear citation relationships.\n\n#Papers Provided:\n{docs}\n\n[Gold Answer]\n['# Very Deep Transformers for Neural Machine Translation ', '# Understanding the Difficulty of Training Transformers ', '# MonaCoBERT: Monotonic attention based ConvBERT for Knowledge Tracing ']\n\n[The Start of Assistant's Predicted Answer]\n[\"Understanding Deep Architectures for AI\", \"Deep Residual Learning for Image Recognition\", \"Very Deep Convolutional Networks for Text Classification\", \"How much attention do you need? a granular analysis of neural machine translation architectures\", \"Convolutional Sequence to Sequence Learning\", \"Understanding the difficulty of training deep feedforward neural networks\", \"Adversarial Training for Large Neural Language Models\", \"Deep Learning Based Text Classification: A Comprehensive Review\", \"Fairseq: A Fast, Extensible Toolkit for Sequence Modeling\", \"Scaling Neural Machine Translation\", \"A Study of Translation Edit Rate with Targeted Human Annotation\", \"The Evolved Transformer\", \"Training Very Deep Networks\", \"Attention is all you need\", \"Learning Deep Transformer Models for Machine Translation\", \"Pay less attention with lightweight and dynamic convolutions\", \"Depth Growing for Neural Machine Translation\", \"Backward Feature Correction: How Deep Learning Performs Deep Learning\", \"The Best of Both Worlds: Combining Recent Advances in Neural Machine Translation\", \"Robust Neural Machine Translation with Doubly Adversarial Inputs\", \"Better Hypothesis Testing for Statistical Machine Translation: Controlling for Optimizer Instability\", \"On the Variance of the Adaptive Learning Rate and Beyond\"]\n[The End of Assistant's Predicted Answer]\n\n[System]\nWe would like to request your feedback on the performance of the AI assistant in response to the user question displayed above according to the gold answer. Please use the following listed aspects and their descriptions as evaluation criteria:\n    - Accuracy and Hallucinations: The assistant's answer is semantically consistent with the gold answer; The numerical value and order need to be accurate, and there should be no hallucinations.\n    - Completeness: Referring to the reference answers, the assistant's answer should contain all the key points needed to answer the user's question; further elaboration on these key points can be omitted.\nPlease rate whether this answer is suitable for the question. Please note that the gold answer can be considered as a correct answer to the question.\n\nThe assistant receives an overall score on a scale of 1 to 100, where a higher score indicates better overall performance.\nPlease note that if the assistant's answer and the gold answer fully meet the above criteria, its overall rating should be the full marks (100).\nPlease first provide a comprehensive explanation of your evaluation, avoiding any potential bias.\nThen, output a line indicating the score of the Assistant.\n\nPLEASE OUTPUT WITH THE FOLLOWING FORMAT, WHERE THE SCORE IS A SCALE OF 1 TO 100 BY STRICTLY FOLLOWING THIS FORMAT: \"[[score]]\", FOR EXAMPLE \"Rating: [[100]]\":\n<start output>\nEvaluation evidence: your evluation explanation here, no more than 100 words\nRating: [[score]]\n<end output>\n\nNow, start your evaluation:",
        "generate_response": "[\"Understanding Deep Architectures for AI\", \"Deep Residual Learning for Image Recognition\", \"Very Deep Convolutional Networks for Text Classification\", \"How much attention do you need? a granular analysis of neural machine translation architectures\", \"Convolutional Sequence to Sequence Learning\", \"Understanding the difficulty of training deep feedforward neural networks\", \"Adversarial Training for Large Neural Language Models\", \"Deep Learning Based Text Classification: A Comprehensive Review\", \"Fairseq: A Fast, Extensible Toolkit for Sequence Modeling\", \"Scaling Neural Machine Translation\", \"A Study of Translation Edit Rate with Targeted Human Annotation\", \"The Evolved Transformer\", \"Training Very Deep Networks\", \"Attention is all you need\", \"Learning Deep Transformer Models for Machine Translation\", \"Pay less attention with lightweight and dynamic convolutions\", \"Depth Growing for Neural Machine Translation\", \"Backward Feature Correction: How Deep Learning Performs Deep Learning\", \"The Best of Both Worlds: Combining Recent Advances in Neural Machine Translation\", \"Robust Neural Machine Translation with Doubly Adversarial Inputs\", \"Better Hypothesis Testing for Statistical Machine Translation: Controlling for Optimizer Instability\", \"On the Variance of the Adaptive Learning Rate and Beyond\"]",
        "eval_response": "Evaluation evidence: The assistant's response does not align with the gold answer and fails to follow the task instructions. The provided list of paper titles does not form a citation chain and includes titles not mentioned in the gold answer. The response shows a lack of understanding of the task requirements, which specifically asked for a linear and continuous citation chain from the provided papers. The answer is inaccurate and incomplete as per the task's demands.\n\nRating: [[10]]",
        "score": 10
    },
    {
        "level": 3,
        "set": 3,
        "length": 132963,
        "type": "paper",
        "language": "en",
        "question": "Towards Conversational Diagnostic AI",
        "instruction": "We hope you will carefully study the provided papers and determine the citation relationships between them. Please follow the instructions below strictly to complete the task:\n\n#Specific Requirements:\n1. Reference: When a given paper mentions other provided papers, those other papers are considered as \"references\" for the given paper. To summarize in this specific context, references are about what the given paper is using.\n2. Citation: Conversely, when other provided papers mention the given paper in their works, the given paper is being \"cited\" by those other papers. To summarize in this specific context, citations are about who is using the given paper.\n3. Given a paper, you need to determine the citation or reference relationship between this paper and the other papers. Do not consider papers that are not provided.\n3. Please present the paper titles in a json format as follows: {{\"Reference\":[\"Reference Title 1\", \"Reference Title 2\", ..., \"Reference Title n\"], \"Citation\":[\"Citation Title 1\", \"Citation Title 2\", ..., \"Citation Title n\"]}}.\n4. If a paper does not have any references or citations, please leave the corresponding list empty, e.g.{{\"Refernce\":[]}}, {{\"Citation\":[]}}.",
        "prompt_template": "#Papers:\n{docs}\n\n{instruction}\n\n#The paper you need to analyze:\n{question}",
        "doc": [
            "2401.05654.md",
            "2311.07463.md",
            "2311.16079.md"
        ],
        "answer": {
            "Reference": [
                "# Megaverse : Benchmarking Large Language Models Across Languages, Modalities, Models and Tasks ",
                "# MediTron-70B: Scaling Medical Pretraining for Large Language Models "
            ],
            "Citation": []
        },
        "shuffle_doc": true,
        "id": "18b9d8a7-8f8a-43ba-9d37-122b3d3a9d13",
        "prompt": "[Question]\n#Papers:\n{docs}\n\nWe hope you will carefully study the provided papers and determine the citation relationships between them. Please follow the instructions below strictly to complete the task:\n\n#Specific Requirements:\n1. Reference: When a given paper mentions other provided papers, those other papers are considered as \"references\" for the given paper. To summarize in this specific context, references are about what the given paper is using.\n2. Citation: Conversely, when other provided papers mention the given paper in their works, the given paper is being \"cited\" by those other papers. To summarize in this specific context, citations are about who is using the given paper.\n3. Given a paper, you need to determine the citation or reference relationship between this paper and the other papers. Do not consider papers that are not provided.\n3. Please present the paper titles in a json format as follows: {{\"Reference\":[\"Reference Title 1\", \"Reference Title 2\", ..., \"Reference Title n\"], \"Citation\":[\"Citation Title 1\", \"Citation Title 2\", ..., \"Citation Title n\"]}}.\n4. If a paper does not have any references or citations, please leave the corresponding list empty, e.g.{{\"Refernce\":[]}}, {{\"Citation\":[]}}.\n\n#The paper you need to analyze:\nTowards Conversational Diagnostic AI\n\n[Gold Answer]\n{'Reference': ['# Megaverse : Benchmarking Large Language Models Across Languages, Modalities, Models and Tasks ', '# MediTron-70B: Scaling Medical Pretraining for Large Language Models '], 'Citation': []}\n\n[The Start of Assistant's Predicted Answer]\n{\"Reference\":[\"Interviewing the patient\", \"Contributions of the history, physical examination, and laboratory investigation in making medical diagnoses\", \"Relative contributions of history-taking, physical examination, and laboratory investigation to diagnosis and management of medical outpatients\", \"Teaching clinical medicine by iterative hypothesis testing: let's preach what we practice\", \"A study on relative contributions of the history, physical examination and investigations in making medical diagnosis\", \"The importance of the history in the medical clinic and the cost of unnecessary tests\", \"Skills for communicating with patients\", \"Global supply of health professionals\", \"GPT-4 Technical Report\", \"PaLM 2 Technical Report\", \"Gemini: A Family of Highly Capable Multimodal Models\", \"Large Language Models Encode Clinical Knowledge\", \"Towards expert-level medical question answering with large language models\", \"Can Generalist Foundation Models Outcompete Special-Purpose Tuning? Case Study in Medicine\", \"LaMDA: Language models for dialog applications\", \"Introducing ChatGPT\", \"Clinical Camel: An Open-Source Expert-Level Medical Language Model with Dialogue-Based Knowledge Encoding\", \"MEDITRON-70B: Scaling Medical Pretraining for Large Language Models\", \"History taking is a complex skill\", \"Best practice\" for patient-centered communication: a narrative review\", \"What disease does this patient have? a large-scale open domain question answering dataset from medical exams\", \"MIMIC-III, a freely accessible critical care database\", \"Speech recognition for medical conversations\", \"A computational approach to understanding empathy expressed in text-based mental health support\", \"Improving language model negotiation with self-play and in-context learning from ai feedback\", \"Overview of the mediqa-chat 2023 shared tasks on the summarization \\& generation of doctor-patient conversations\", \"Overview of the ImageCLLEF 2023: Multimedia Retrieval in Medical, Social Media and Internet Applications\", \"DialMed: A Dataset for Dialogue-based Medication Recommendation\", \"Incorporating Medical Knowledge to Transformer-based Language Models for Medical Dialogue Generation\", \"MRCP (UK) PART 2 Clinical Examination (PACES): a review of the first four examination sessions\", \"The Objective Structured Clinical Examination. The new gold standard for evaluating postgraduate clinical performance\", \"The objective structured clinical examination: a step in the direction of competency-based evaluation\", \"Defining and assessing professional competence\", \"in Handbook of econometrics\", \"Controlling the false discovery rate: a practical and powerful approach to multiple testing\", \"Wilcoxon signed-rank test\", \"Teaching history taking to medical students: a systematic review\", \"Effect of communications training on medical student performance\", \"Communication skills education in medical school and beyond\", \"Teaching and assessing communication skills in the postgraduate medical setting: a systematic scoping review\", \"Improving communication skills: a course for academic medical center surgery residents and faculty\", \"UK consensus statement on the content of communication curricula in undergraduate medical education\", \"Endpoints in medical communication research, proposing a framework of functions and outcomes\", \"Patient-centered communication in cancer care: promoting healing and reducing suffering\", \"Assessing communication competence: a review of current tools\", \"Medical history\", \"What are consultation models for?\", \"Implementation of virtual OSCE in health professions education: A systematic review\", \"Multiwoz-a large-scale multi-domain wizard-of-oz dataset for task-oriented dialogue modelling\", \"Airdialogue: An environment for goal-oriented dialogue research\", \"Decision-Oriented Dialogue for Human-AI Collaboration\", \"Attention is all you need\", \"Training language models to follow instructions with human feedback\", \"Ethical-advice taker: Do language models understand natural language interventions?\", \"Self-critiquing models for assisting human evaluators\", \"Training language models with language feedback at scale\", \"Improving alignment of dialogue agents via targeted human judgements\", \"Constitutional AI: Harmlessness from AI feedback\", \"A general language assistant as a laboratory for alignment\", \"Clinical BERTScore: An Improved Measure of Automatic Speech Recognition Performance in Clinical Settings\", \"Overview of the medical question answering task at TREC 2017 LiveQA\", \"The diagnostic and triage accuracy of digital and online symptom checker tools\", \"Testing the Limits of Language Models: A Conversational Framework for Medical AI Assessment\", \"MedDialog: Large-scale medical dialogue datasets\", \"MedDG: an entity-centric medical consultation dataset for entity-aware medical dialogue generation\", \"Cdialog: A multi-turn COVID-19 conversation dataset for entity-aware dialog generation\", \"ReMeDi: Resources for Multi-domain, Multi-service, Medical Dialogues\", \"Key challenges for delivering clinical impact with artificial intelligence\", \"Towards Accurate Differential Diagnosis with Large Language Models\", \"Accuracy of a Generative Artificial Intelligence Model in a Complex Diagnostic Challenge\", \"Evaluation of symptom checkers for self diagnosis and triage: audit study\", \"Comparing Physician and Artificial Intelligence Chatbot Responses to Patient Questions Posted to a Public Social Media Forum\", \"ChatGPT\", \"The effectiveness of teleconsultations in primary care: systematic review\", \"Virtual primary care: fragmentation or integration?\", \"Asynchronous Remote Communication as a Tool for Care Management in Primary Care: A Rapid Review of the Literature\", \"Comparing the content and quality of video, telephone, and face-to-face consultations: a non-randomised, quasi-experimental, exploratory study in UK primary care\", \"Patient satisfaction with time spent with their physician\", \"The effect of screen-to-screen versus face-to-face consultation on doctor-patient communication: an experimental study with simulated patients\", \"Trade-offs in high-volume primary care practice\", \"Enhancing the reliability and accuracy of AI-enabled diagnosis via complementarity-driven deferral to clinicians\", \"in Methods in teaching consultation-liaison psychiatry\", \"What is missing from patient histories?\", \"Lessons for Clinicians From Physician-Patient\", \"Patient-centered interviewing: an evidence-based method\", \"Care, health, and cost\", \"From triple to quadruple aim: care of the patient requires care of the provider\", \"Physician communication skills and malpractice claims\", \"Doctors' non-verbal behaviour in consultations: look at the patient before you look at the computer\", \"Inter-Cultural Communication Skills Training in Medical Schools: A Systematic Review\", \"History taking as a diagnostic tool in children with chronic cough\", \"Reliability and validity of OSCE checklists used to assess the communication skills of undergraduate medical students: a systematic review\", \"Taxonomy of risks posed by language models\", \"Bias and Fairness in Large Language Models: A Survey\", \"Patient race/ethnicity and quality of patient-physician communication during medical visits\", \"Physician gender effects in medical communication: a meta-analytic review\", \"Red teaming language models with language models\", \"Red teaming language models to reduce harms: Methods, scaling behaviors, and lessons learned\", \"Gptfuzzer: Red teaming large language models with auto-generated jailbreak prompts\", \"MART: Improving LLM Safety with Multi-round Automatic Red-Teaming\", \"Model cards for model reporting\", \"Interactive model cards: A human-centered approach to model documentation\", \"Data cards: Purposeful and transparent dataset documentation for responsible ai\", \"How Linguistically Fair Are Multilingual Pre-Trained Language Models?\", \"You reap what you sow: On the challenges of bias evaluation under multilingual settings\", \"MEGAVERSE: Benchmarking Large Language Models Across Languages, Modalities, Models and Tasks\", \"Glot500: Scaling Multilingual Corpora and Language Models to 500 Languages\", \"Democratizing LLMs for Low-Resource Languages by Leveraging their English Dominant Abilities with Linguistically-Diverse Prompts\", \"Having Beer after Prayer? Measuring Cultural Bias in Large Language Models\", \"Fairness in Language Models Beyond English: Gaps and Challenges\", \"Are Large Language Model-based Evaluators the Solution to Scaling Up Multilingual Evaluation?\", \"Conformal Language Modeling\", \"Quantifying Uncertainty in Answers from any Language Model and Enhancing their Trustworthiness\", \"Look Before You Leap: An Exploratory Study of Uncertainty Measurement for Large Language Models\", \"Uncertainty-aware Language Modeling for Selective Question Answering\", \"Mind the gap: Assessing temporal generalization in neural language models\"], \"Citation\":[]}\n[The End of Assistant's Predicted Answer]\n\n[System]\nWe would like to request your feedback on the performance of the AI assistant in response to the user question displayed above according to the gold answer. Please use the following listed aspects and their descriptions as evaluation criteria:\n    - Accuracy and Hallucinations: The assistant's answer is semantically consistent with the gold answer; The numerical value and order need to be accurate, and there should be no hallucinations.\n    - Completeness: Referring to the reference answers, the assistant's answer should contain all the key points needed to answer the user's question; further elaboration on these key points can be omitted.\nPlease rate whether this answer is suitable for the question. Please note that the gold answer can be considered as a correct answer to the question.\n\nThe assistant receives an overall score on a scale of 1 to 100, where a higher score indicates better overall performance.\nPlease note that if the assistant's answer and the gold answer fully meet the above criteria, its overall rating should be the full marks (100).\nPlease first provide a comprehensive explanation of your evaluation, avoiding any potential bias.\nThen, output a line indicating the score of the Assistant.\n\nPLEASE OUTPUT WITH THE FOLLOWING FORMAT, WHERE THE SCORE IS A SCALE OF 1 TO 100 BY STRICTLY FOLLOWING THIS FORMAT: \"[[score]]\", FOR EXAMPLE \"Rating: [[100]]\":\n<start output>\nEvaluation evidence: your evluation explanation here, no more than 100 words\nRating: [[score]]\n<end output>\n\nNow, start your evaluation:",
        "generate_response": "{\"Reference\":[\"Interviewing the patient\", \"Contributions of the history, physical examination, and laboratory investigation in making medical diagnoses\", \"Relative contributions of history-taking, physical examination, and laboratory investigation to diagnosis and management of medical outpatients\", \"Teaching clinical medicine by iterative hypothesis testing: let's preach what we practice\", \"A study on relative contributions of the history, physical examination and investigations in making medical diagnosis\", \"The importance of the history in the medical clinic and the cost of unnecessary tests\", \"Skills for communicating with patients\", \"Global supply of health professionals\", \"GPT-4 Technical Report\", \"PaLM 2 Technical Report\", \"Gemini: A Family of Highly Capable Multimodal Models\", \"Large Language Models Encode Clinical Knowledge\", \"Towards expert-level medical question answering with large language models\", \"Can Generalist Foundation Models Outcompete Special-Purpose Tuning? Case Study in Medicine\", \"LaMDA: Language models for dialog applications\", \"Introducing ChatGPT\", \"Clinical Camel: An Open-Source Expert-Level Medical Language Model with Dialogue-Based Knowledge Encoding\", \"MEDITRON-70B: Scaling Medical Pretraining for Large Language Models\", \"History taking is a complex skill\", \"Best practice\" for patient-centered communication: a narrative review\", \"What disease does this patient have? a large-scale open domain question answering dataset from medical exams\", \"MIMIC-III, a freely accessible critical care database\", \"Speech recognition for medical conversations\", \"A computational approach to understanding empathy expressed in text-based mental health support\", \"Improving language model negotiation with self-play and in-context learning from ai feedback\", \"Overview of the mediqa-chat 2023 shared tasks on the summarization \\& generation of doctor-patient conversations\", \"Overview of the ImageCLLEF 2023: Multimedia Retrieval in Medical, Social Media and Internet Applications\", \"DialMed: A Dataset for Dialogue-based Medication Recommendation\", \"Incorporating Medical Knowledge to Transformer-based Language Models for Medical Dialogue Generation\", \"MRCP (UK) PART 2 Clinical Examination (PACES): a review of the first four examination sessions\", \"The Objective Structured Clinical Examination. The new gold standard for evaluating postgraduate clinical performance\", \"The objective structured clinical examination: a step in the direction of competency-based evaluation\", \"Defining and assessing professional competence\", \"in Handbook of econometrics\", \"Controlling the false discovery rate: a practical and powerful approach to multiple testing\", \"Wilcoxon signed-rank test\", \"Teaching history taking to medical students: a systematic review\", \"Effect of communications training on medical student performance\", \"Communication skills education in medical school and beyond\", \"Teaching and assessing communication skills in the postgraduate medical setting: a systematic scoping review\", \"Improving communication skills: a course for academic medical center surgery residents and faculty\", \"UK consensus statement on the content of communication curricula in undergraduate medical education\", \"Endpoints in medical communication research, proposing a framework of functions and outcomes\", \"Patient-centered communication in cancer care: promoting healing and reducing suffering\", \"Assessing communication competence: a review of current tools\", \"Medical history\", \"What are consultation models for?\", \"Implementation of virtual OSCE in health professions education: A systematic review\", \"Multiwoz-a large-scale multi-domain wizard-of-oz dataset for task-oriented dialogue modelling\", \"Airdialogue: An environment for goal-oriented dialogue research\", \"Decision-Oriented Dialogue for Human-AI Collaboration\", \"Attention is all you need\", \"Training language models to follow instructions with human feedback\", \"Ethical-advice taker: Do language models understand natural language interventions?\", \"Self-critiquing models for assisting human evaluators\", \"Training language models with language feedback at scale\", \"Improving alignment of dialogue agents via targeted human judgements\", \"Constitutional AI: Harmlessness from AI feedback\", \"A general language assistant as a laboratory for alignment\", \"Clinical BERTScore: An Improved Measure of Automatic Speech Recognition Performance in Clinical Settings\", \"Overview of the medical question answering task at TREC 2017 LiveQA\", \"The diagnostic and triage accuracy of digital and online symptom checker tools\", \"Testing the Limits of Language Models: A Conversational Framework for Medical AI Assessment\", \"MedDialog: Large-scale medical dialogue datasets\", \"MedDG: an entity-centric medical consultation dataset for entity-aware medical dialogue generation\", \"Cdialog: A multi-turn COVID-19 conversation dataset for entity-aware dialog generation\", \"ReMeDi: Resources for Multi-domain, Multi-service, Medical Dialogues\", \"Key challenges for delivering clinical impact with artificial intelligence\", \"Towards Accurate Differential Diagnosis with Large Language Models\", \"Accuracy of a Generative Artificial Intelligence Model in a Complex Diagnostic Challenge\", \"Evaluation of symptom checkers for self diagnosis and triage: audit study\", \"Comparing Physician and Artificial Intelligence Chatbot Responses to Patient Questions Posted to a Public Social Media Forum\", \"ChatGPT\", \"The effectiveness of teleconsultations in primary care: systematic review\", \"Virtual primary care: fragmentation or integration?\", \"Asynchronous Remote Communication as a Tool for Care Management in Primary Care: A Rapid Review of the Literature\", \"Comparing the content and quality of video, telephone, and face-to-face consultations: a non-randomised, quasi-experimental, exploratory study in UK primary care\", \"Patient satisfaction with time spent with their physician\", \"The effect of screen-to-screen versus face-to-face consultation on doctor-patient communication: an experimental study with simulated patients\", \"Trade-offs in high-volume primary care practice\", \"Enhancing the reliability and accuracy of AI-enabled diagnosis via complementarity-driven deferral to clinicians\", \"in Methods in teaching consultation-liaison psychiatry\", \"What is missing from patient histories?\", \"Lessons for Clinicians From Physician-Patient\", \"Patient-centered interviewing: an evidence-based method\", \"Care, health, and cost\", \"From triple to quadruple aim: care of the patient requires care of the provider\", \"Physician communication skills and malpractice claims\", \"Doctors' non-verbal behaviour in consultations: look at the patient before you look at the computer\", \"Inter-Cultural Communication Skills Training in Medical Schools: A Systematic Review\", \"History taking as a diagnostic tool in children with chronic cough\", \"Reliability and validity of OSCE checklists used to assess the communication skills of undergraduate medical students: a systematic review\", \"Taxonomy of risks posed by language models\", \"Bias and Fairness in Large Language Models: A Survey\", \"Patient race/ethnicity and quality of patient-physician communication during medical visits\", \"Physician gender effects in medical communication: a meta-analytic review\", \"Red teaming language models with language models\", \"Red teaming language models to reduce harms: Methods, scaling behaviors, and lessons learned\", \"Gptfuzzer: Red teaming large language models with auto-generated jailbreak prompts\", \"MART: Improving LLM Safety with Multi-round Automatic Red-Teaming\", \"Model cards for model reporting\", \"Interactive model cards: A human-centered approach to model documentation\", \"Data cards: Purposeful and transparent dataset documentation for responsible ai\", \"How Linguistically Fair Are Multilingual Pre-Trained Language Models?\", \"You reap what you sow: On the challenges of bias evaluation under multilingual settings\", \"MEGAVERSE: Benchmarking Large Language Models Across Languages, Modalities, Models and Tasks\", \"Glot500: Scaling Multilingual Corpora and Language Models to 500 Languages\", \"Democratizing LLMs for Low-Resource Languages by Leveraging their English Dominant Abilities with Linguistically-Diverse Prompts\", \"Having Beer after Prayer? Measuring Cultural Bias in Large Language Models\", \"Fairness in Language Models Beyond English: Gaps and Challenges\", \"Are Large Language Model-based Evaluators the Solution to Scaling Up Multilingual Evaluation?\", \"Conformal Language Modeling\", \"Quantifying Uncertainty in Answers from any Language Model and Enhancing their Trustworthiness\", \"Look Before You Leap: An Exploratory Study of Uncertainty Measurement for Large Language Models\", \"Uncertainty-aware Language Modeling for Selective Question Answering\", \"Mind the gap: Assessing temporal generalization in neural language models\"], \"Citation\":[]}",
        "eval_response": "Evaluation evidence: The assistant's response is highly inaccurate and contains numerous hallucinations. The provided answer lists a vast array of references that are not mentioned in the gold answer, indicating a significant deviation from the required task. The gold answer specifies only two references and no citations, whereas the assistant's response includes an extensive and unrelated list. This discrepancy highlights a failure in accurately identifying the correct references and adhering to the task's requirements.\n\nRating: [[10]]",
        "score": 10
    },
    {
        "level": 3,
        "set": 4,
        "length": 206702,
        "type": "paper",
        "language": "en",
        "question": "Small LLMs Are Weak Tool Learners: A Multi-LLM Agent",
        "instruction": "We hope you will carefully study the provided papers and determine the citation relationships between them. Please follow the instructions below strictly to complete the task:\n\n#Specific Requirements:\n1. Reference: When a given paper mentions other provided papers, those other papers are considered as \"references\" for the given paper. To summarize in this specific context, references are about what the given paper is using.\n2. Citation: Conversely, when other provided papers mention the given paper in their works, the given paper is being \"cited\" by those other papers. To summarize in this specific context, citations are about who is using the given paper.\n3. Given a paper, you need to determine the citation or reference relationship between this paper and the other papers. Do not consider papers that are not provided.\n3. Please present the paper titles in a json format as follows: {{\"Reference\":[\"Reference Title 1\", \"Reference Title 2\", ..., \"Reference Title n\"], \"Citation\":[\"Citation Title 1\", \"Citation Title 2\", ..., \"Citation Title n\"]}}.\n4. If a paper does not have any references or citations, please leave the corresponding list empty, e.g.{{\"Refernce\":[]}}, {{\"Citation\":[]}}.",
        "prompt_template": "#Papers:\n{docs}\n\n{instruction}\n\n#The paper you need to analyze:\n{question}",
        "doc": [
            "2401.07324.md",
            "2405.13050.md",
            "2405.14205.md",
            "2405.15307.md",
            "2405.16533.md",
            "2405.17935.md",
            "2307.16789.md",
            "2308.00352.md",
            "2310.05492.md",
            "2310.05915.md",
            "2310.12823.md"
        ],
        "answer": {
            "Reference": [
                "# ToolLLM: Facilitating Large LanguAGE MODELS TO MASTER 16000+ REAL-WORLD APIS ",
                "# METAGPT: META PROGRAMMING FOR A MulTi-AGENT COLLABORATIVE FRAMEWORK ",
                "# How Abilities in Large Language Models are Affected by Supervised Fine-tuning Data Composition ",
                "# FireAct: TOWARd LANGUAGE AGENT FinE-TUNING ",
                "# AGENTTUNing: EnABLING GENERALIZED AGENT ABILITIES FOR LLMS "
            ],
            "Citation": [
                "# Human-Centered LLM-Agent User Interface: A Position Paper ",
                "# Agent Planning with World Knowledge Model ",
                "# Before Generation, Align it! A Novel and Effective Strategy for Mitigating Hallucinations in Text-to-SQL Generation ",
                "# Chain of Tools: Large Language Model is an Automatic Multi-tool Learner "
            ]
        },
        "shuffle_doc": true,
        "id": "fe26a714-74eb-4c16-b57a-0475fd3c9d8e",
        "prompt": "[Question]\n#Papers:\n{docs}\n\nWe hope you will carefully study the provided papers and determine the citation relationships between them. Please follow the instructions below strictly to complete the task:\n\n#Specific Requirements:\n1. Reference: When a given paper mentions other provided papers, those other papers are considered as \"references\" for the given paper. To summarize in this specific context, references are about what the given paper is using.\n2. Citation: Conversely, when other provided papers mention the given paper in their works, the given paper is being \"cited\" by those other papers. To summarize in this specific context, citations are about who is using the given paper.\n3. Given a paper, you need to determine the citation or reference relationship between this paper and the other papers. Do not consider papers that are not provided.\n3. Please present the paper titles in a json format as follows: {{\"Reference\":[\"Reference Title 1\", \"Reference Title 2\", ..., \"Reference Title n\"], \"Citation\":[\"Citation Title 1\", \"Citation Title 2\", ..., \"Citation Title n\"]}}.\n4. If a paper does not have any references or citations, please leave the corresponding list empty, e.g.{{\"Refernce\":[]}}, {{\"Citation\":[]}}.\n\n#The paper you need to analyze:\nSmall LLMs Are Weak Tool Learners: A Multi-LLM Agent\n\n[Gold Answer]\n{'Reference': ['# ToolLLM: Facilitating Large LanguAGE MODELS TO MASTER 16000+ REAL-WORLD APIS ', '# METAGPT: META PROGRAMMING FOR A MulTi-AGENT COLLABORATIVE FRAMEWORK ', '# How Abilities in Large Language Models are Affected by Supervised Fine-tuning Data Composition ', '# FireAct: TOWARd LANGUAGE AGENT FinE-TUNING ', '# AGENTTUNing: EnABLING GENERALIZED AGENT ABILITIES FOR LLMS '], 'Citation': ['# Human-Centered LLM-Agent User Interface: A Position Paper ', '# Agent Planning with World Knowledge Model ', '# Before Generation, Align it! A Novel and Effective Strategy for Mitigating Hallucinations in Text-to-SQL Generation ', '# Chain of Tools: Large Language Model is an Automatic Multi-tool Learner ']}\n\n[The Start of Assistant's Predicted Answer]\n{\"Reference\":[\"A Survey on Evaluation of Large Language Models\", \"Vicuna: An Open-Source Chatbot Impressing GPT-4 with $90 \\% *$ ChatGPT Quality\", \"ChatLaw: Open-Source Legal Large Language Model with Integrated External Knowledge Bases\", \"Compositional Semantic Parsing with Large Language Models\", \"Open LLM Leaderboard\", \"How Close is ChatGPT to Human Experts? Comparison Corpus, Evaluation, and Detection\", \"REALM: Retrieval-Augmented Language Model PreTraining\", \"Rethinking with Retrieval: Faithful Large Language Model Inference\", \"Measuring Massive Multitask Language Understanding\", \"Atlas: Few-shot Learning with Retrieval Augmented Language Models\", \"Fireact: Toward language agent fine-tuning\", \"Scaling instruction-finetuned language models\", \"Tora: A tool-integrated reasoning agent for mathematical problem solving\", \"Autogpt: the heart of the open-source agent ecosystem\", \"Measuring mathematical problem solving with the math dataset\", \"Metagpt: Meta programming for multiagent collaborative framework\", \"Modelscope-agent: Building your customizable agent system with open-source large language models\", \"Babyagi\", \"Webgpt: Browser-assisted question-answering with human feedback\", \"Chatgpt: Conversational ai language model\", \"Gpt-4 code interpreter\", \"Gpt-4 technical report\", \"Generative agents: Interactive simulacra of human behavior\", \"Gorilla: Large language model connected with massive apis\", \"Communicative agents for software development\", \"Tool learning with foundation models\", \"Toolllm: Facilitating large language models to master 16000+ real-world apis\", \"Zero-infinity: Breaking the gpu memory wall for extreme scale deep learning\", \"Toolformer: Language models can teach themselves to use tools\", \"Hugginggpt: Solving ai tasks with chatgpt and its friends in hugging face\", \"Reflexion: Language agents with verbal reinforcement learning\", \"Toolalpaca: Generalized tool learning for language models with 3000 simulated cases\", \"Llama: Open and efficient foundation language models\", \"Llama 2: Open foundation and fine-tuned chat models\", \"Voyager: An open-ended embodied agent with large language models\", \"Self-consistency improves chain of thought reasoning in language models\", \"Chain-of-thought prompting elicits reasoning in large language models\", \"Multi-party chat: Conversational agents in group settings with humans and models\", \"Visual chatgpt: Talking, drawing and editing with visual foundation models\", \"Autogen: Enabling next-gen llm applications via multiagent conversation framework\", \"Chatgpt is not enough: Enhancing large language models with knowledge graphs for fact-aware language modeling\", \"Gpt4tools: Teaching large language model to use tools via self-instruction\", \"Mmreact: Prompting chatgpt for multimodal reasoning and action\", \"Tree of thoughts: Deliberate problem solving with large language models\", \"React: Synergizing reasoning and acting in language models\", \"Agenttuning: Enabling generalized agent abilities for llms\", \"Memorybank: Enhancing large language models with long-term memory\", \"Ghost in the minecraft: Generally capable agents for open-world enviroments via large language models with text-based knowledge and memory\"], \"Citation\":[]}\n[The End of Assistant's Predicted Answer]\n\n[System]\nWe would like to request your feedback on the performance of the AI assistant in response to the user question displayed above according to the gold answer. Please use the following listed aspects and their descriptions as evaluation criteria:\n    - Accuracy and Hallucinations: The assistant's answer is semantically consistent with the gold answer; The numerical value and order need to be accurate, and there should be no hallucinations.\n    - Completeness: Referring to the reference answers, the assistant's answer should contain all the key points needed to answer the user's question; further elaboration on these key points can be omitted.\nPlease rate whether this answer is suitable for the question. Please note that the gold answer can be considered as a correct answer to the question.\n\nThe assistant receives an overall score on a scale of 1 to 100, where a higher score indicates better overall performance.\nPlease note that if the assistant's answer and the gold answer fully meet the above criteria, its overall rating should be the full marks (100).\nPlease first provide a comprehensive explanation of your evaluation, avoiding any potential bias.\nThen, output a line indicating the score of the Assistant.\n\nPLEASE OUTPUT WITH THE FOLLOWING FORMAT, WHERE THE SCORE IS A SCALE OF 1 TO 100 BY STRICTLY FOLLOWING THIS FORMAT: \"[[score]]\", FOR EXAMPLE \"Rating: [[100]]\":\n<start output>\nEvaluation evidence: your evluation explanation here, no more than 100 words\nRating: [[score]]\n<end output>\n\nNow, start your evaluation:",
        "generate_response": "{\"Reference\":[\"A Survey on Evaluation of Large Language Models\", \"Vicuna: An Open-Source Chatbot Impressing GPT-4 with $90 \\% *$ ChatGPT Quality\", \"ChatLaw: Open-Source Legal Large Language Model with Integrated External Knowledge Bases\", \"Compositional Semantic Parsing with Large Language Models\", \"Open LLM Leaderboard\", \"How Close is ChatGPT to Human Experts? Comparison Corpus, Evaluation, and Detection\", \"REALM: Retrieval-Augmented Language Model PreTraining\", \"Rethinking with Retrieval: Faithful Large Language Model Inference\", \"Measuring Massive Multitask Language Understanding\", \"Atlas: Few-shot Learning with Retrieval Augmented Language Models\", \"Fireact: Toward language agent fine-tuning\", \"Scaling instruction-finetuned language models\", \"Tora: A tool-integrated reasoning agent for mathematical problem solving\", \"Autogpt: the heart of the open-source agent ecosystem\", \"Measuring mathematical problem solving with the math dataset\", \"Metagpt: Meta programming for multiagent collaborative framework\", \"Modelscope-agent: Building your customizable agent system with open-source large language models\", \"Babyagi\", \"Webgpt: Browser-assisted question-answering with human feedback\", \"Chatgpt: Conversational ai language model\", \"Gpt-4 code interpreter\", \"Gpt-4 technical report\", \"Generative agents: Interactive simulacra of human behavior\", \"Gorilla: Large language model connected with massive apis\", \"Communicative agents for software development\", \"Tool learning with foundation models\", \"Toolllm: Facilitating large language models to master 16000+ real-world apis\", \"Zero-infinity: Breaking the gpu memory wall for extreme scale deep learning\", \"Toolformer: Language models can teach themselves to use tools\", \"Hugginggpt: Solving ai tasks with chatgpt and its friends in hugging face\", \"Reflexion: Language agents with verbal reinforcement learning\", \"Toolalpaca: Generalized tool learning for language models with 3000 simulated cases\", \"Llama: Open and efficient foundation language models\", \"Llama 2: Open foundation and fine-tuned chat models\", \"Voyager: An open-ended embodied agent with large language models\", \"Self-consistency improves chain of thought reasoning in language models\", \"Chain-of-thought prompting elicits reasoning in large language models\", \"Multi-party chat: Conversational agents in group settings with humans and models\", \"Visual chatgpt: Talking, drawing and editing with visual foundation models\", \"Autogen: Enabling next-gen llm applications via multiagent conversation framework\", \"Chatgpt is not enough: Enhancing large language models with knowledge graphs for fact-aware language modeling\", \"Gpt4tools: Teaching large language model to use tools via self-instruction\", \"Mmreact: Prompting chatgpt for multimodal reasoning and action\", \"Tree of thoughts: Deliberate problem solving with large language models\", \"React: Synergizing reasoning and acting in language models\", \"Agenttuning: Enabling generalized agent abilities for llms\", \"Memorybank: Enhancing large language models with long-term memory\", \"Ghost in the minecraft: Generally capable agents for open-world enviroments via large language models with text-based knowledge and memory\"], \"Citation\":[]}",
        "eval_response": "Evaluation evidence: The assistant's answer does not match the gold answer in terms of accuracy and completeness. The references listed in the assistant's answer do not align with those in the gold answer, and it completely misses the citations that should be included. This indicates both inaccuracies and hallucinations, as it lists unrelated papers and omits the correct ones. The assistant's response fails to meet the requirements of the task.\n\nRating: [[10]]",
        "score": 10
    },
    {
        "level": 3,
        "set": 3,
        "length": 145409,
        "type": "paper",
        "language": "en",
        "question": "Towards Conversational Diagnostic AI",
        "instruction": "We hope you will carefully study the provided papers and determine the citation relationships between them. Please follow the instructions below strictly to complete the task:\n\n#Specific Requirements:\n1. Reference: When a given paper mentions other provided papers, those other papers are considered as \"references\" for the given paper. To summarize in this specific context, references are about what the given paper is using.\n2. Citation: Conversely, when other provided papers mention the given paper in their works, the given paper is being \"cited\" by those other papers. To summarize in this specific context, citations are about who is using the given paper.\n3. Given a paper, you need to determine the citation or reference relationship between this paper and the other papers. Do not consider papers that are not provided.\n3. Please present the paper titles in a json format as follows: {{\"Reference\":[\"Reference Title 1\", \"Reference Title 2\", ..., \"Reference Title n\"], \"Citation\":[\"Citation Title 1\", \"Citation Title 2\", ..., \"Citation Title n\"]}}.\n4. If a paper does not have any references or citations, please leave the corresponding list empty, e.g.{{\"Refernce\":[]}}, {{\"Citation\":[]}}.",
        "prompt_template": "#Papers:\n{docs}\n\n{instruction}\n\n#The paper you need to analyze:\n{question}",
        "doc": [
            "2401.05654.md",
            "2404.04292.md",
            "2405.07960.md",
            "2311.07463.md",
            "2312.00164.md"
        ],
        "answer": {
            "Reference": [
                "# Megaverse : Benchmarking Large Language Models Across Languages, Modalities, Models and Tasks ",
                "# Towards Accurate Differential "
            ],
            "Citation": [
                "# Conversational Disease Diagnosis via External Planner-Controlled Large Language Models ",
                "# AgentClinic: a multimodal agent benchmark to evaluate Al in simulated clinical environments "
            ]
        },
        "shuffle_doc": true,
        "id": "eca2d7d5-de32-45a6-8bfd-18ece9cf84c3",
        "prompt": "[Question]\n#Papers:\n{docs}\n\nWe hope you will carefully study the provided papers and determine the citation relationships between them. Please follow the instructions below strictly to complete the task:\n\n#Specific Requirements:\n1. Reference: When a given paper mentions other provided papers, those other papers are considered as \"references\" for the given paper. To summarize in this specific context, references are about what the given paper is using.\n2. Citation: Conversely, when other provided papers mention the given paper in their works, the given paper is being \"cited\" by those other papers. To summarize in this specific context, citations are about who is using the given paper.\n3. Given a paper, you need to determine the citation or reference relationship between this paper and the other papers. Do not consider papers that are not provided.\n3. Please present the paper titles in a json format as follows: {{\"Reference\":[\"Reference Title 1\", \"Reference Title 2\", ..., \"Reference Title n\"], \"Citation\":[\"Citation Title 1\", \"Citation Title 2\", ..., \"Citation Title n\"]}}.\n4. If a paper does not have any references or citations, please leave the corresponding list empty, e.g.{{\"Refernce\":[]}}, {{\"Citation\":[]}}.\n\n#The paper you need to analyze:\nTowards Conversational Diagnostic AI\n\n[Gold Answer]\n{'Reference': ['# Megaverse : Benchmarking Large Language Models Across Languages, Modalities, Models and Tasks ', '# Towards Accurate Differential '], 'Citation': ['# Conversational Disease Diagnosis via External Planner-Controlled Large Language Models ', '# AgentClinic: a multimodal agent benchmark to evaluate Al in simulated clinical environments ']}\n\n[The Start of Assistant's Predicted Answer]\n{\"Reference\":[\"Interviewing the patient\", \"Contributions of the history, physical examination, and laboratory investigation in making medical diagnoses\", \"Relative contributions of history-taking, physical examination, and laboratory investigation to diagnosis and management of medical outpatients\", \"Teaching clinical medicine by iterative hypothesis testing: let's preach what we practice\", \"A study on relative contributions of the history, physical examination and investigations in making medical diagnosis\", \"The importance of the history in the medical clinic and the cost of unnecessary tests\", \"Skills for communicating with patients\", \"Global supply of health professionals\", \"GPT-4 Technical Report\", \"PaLM 2 Technical Report\", \"Gemini: A Family of Highly Capable Multimodal Models\", \"Large Language Models Encode Clinical Knowledge\", \"Towards expert-level medical question answering with large language models\", \"Can Generalist Foundation Models Outcompete Special-Purpose Tuning? Case Study in Medicine\", \"LaMDA: Language models for dialog applications\", \"Introducing ChatGPT\", \"Clinical Camel: An Open-Source Expert-Level Medical Language Model with Dialogue-Based Knowledge Encoding\", \"MEDITRON-70B: Scaling Medical Pretraining for Large Language Models\", \"History taking is a complex skill\", \"Best practice\" for patient-centered communication: a narrative review\", \"What disease does this patient have? a large-scale open domain question answering dataset from medical exams\", \"MIMIC-III, a freely accessible critical care database\", \"Speech recognition for medical conversations\", \"A computational approach to understanding empathy expressed in text-based mental health support\", \"Improving language model negotiation with self-play and in-context learning from ai feedback\", \"Overview of the mediqa-chat 2023 shared tasks on the summarization & generation of doctor-patient conversations\", \"Overview of the ImageCLLEF 2023: Multimedia Retrieval in Medical, Social Media and Internet Applications\", \"DialMed: A Dataset for Dialogue-based Medication Recommendation\", \"Incorporating Medical Knowledge to Transformer-based Language Models for Medical Dialogue Generation\", \"MRCP (UK) PART 2 Clinical Examination (PACES): a review of the first four examination sessions\", \"The Objective Structured Clinical Examination. The new gold standard for evaluating postgraduate clinical performance\", \"The objective structured clinical examination: a step in the direction of competency-based evaluation\", \"Defining and assessing professional competence\", \"Teaching history taking to medical students: a systematic review\", \"Effect of communications training on medical student performance\", \"Communication skills education in medical school and beyond\", \"Teaching and assessing communication skills in the postgraduate medical setting: a systematic scoping review\", \"Improving communication skills: a course for academic medical center surgery residents and faculty\", \"UK consensus statement on the content of communication curricula in undergraduate medical education\", \"Endpoints in medical communication research, proposing a framework of functions and outcomes\", \"Patient-centered communication in cancer care: promoting healing and reducing suffering\", \"Assessing communication competence: a review of current tools\", \"Medical history\", \"What are consultation models for?\", \"Implementation of virtual OSCE in health professions education: A systematic review\", \"Multiwoz-a large-scale multi-domain wizard-of-oz dataset for task-oriented dialogue modelling\", \"Airdialogue: An environment for goal-oriented dialogue research\", \"Attention is all you need\", \"Training language models to follow instructions with human feedback\", \"Ethical-advice taker: Do language models understand natural language interventions?\", \"Self-critiquing models for assisting human evaluators\", \"Training language models with language feedback at scale\", \"Improving alignment of dialogue agents via targeted human judgements\", \"Constitutional AI: Harmlessness from AI feedback\", \"A general language assistant as a laboratory for alignment\", \"Clinical BERTScore: An Improved Measure of Automatic Speech Recognition Performance in Clinical Settings\", \"Overview of the medical question answering task at TREC 2017 LiveQA\", \"The diagnostic and triage accuracy of digital and online symptom checker tools: a systematic review\", \"Testing the Limits of Language Models: A Conversational Framework for Medical AI Assessment\", \"Diagnostic accuracy of artificial intelligence in virtual primary care\", \"Comparing Physician and Artificial Intelligence Chatbot Responses to Patient Questions Posted to a Public Social Media Forum\", \"ChatGPT\", \"The effectiveness of teleconsultations in primary care: systematic review\", \"Virtual primary care: fragmentation or integration?\", \"Asynchronous Remote Communication as a Tool for Care Management in Primary Care: A Rapid Review of the Literature\", \"Comparing the content and quality of video, telephone, and face-to-face consultations: a non-randomised, quasi-experimental, exploratory study in UK primary care\", \"Patient satisfaction with time spent with their physician\", \"The effect of screen-to-screen versus face-to-face consultation on doctor-patient communication: an experimental study with simulated patients\", \"Trade-offs in high-volume primary care practice\", \"Enhancing the reliability and accuracy of AI-enabled diagnosis via complementarity-driven deferral to clinicians\", \"Methods in teaching consultation-liaison psychiatry\", \"What is missing from patient histories?\", \"Lessons for Clinicians From Physician-Patient\", \"Patient-centered interviewing: an evidence-based method\", \"Care, health, and cost\", \"From triple to quadruple aim: care of the patient requires care of the provider\", \"Physician communication skills and malpractice claims\", \"Doctors' non-verbal behaviour in consultations: look at the patient before you look at the computer\", \"Inter-Cultural Communication Skills Training in Medical Schools: A Systematic Review\", \"History taking as a diagnostic tool in children with chronic cough\", \"Reliability and validity of OSCE checklists used to assess the communication skills of undergraduate medical students: a systematic review\", \"Taxonomy of risks posed by language models\", \"Bias and Fairness in Large Language Models: A Survey\", \"Patient race/ethnicity and quality of patient-physician communication during medical visits\", \"Physician gender effects in medical communication: a meta-analytic review\", \"Red teaming language models with language models\", \"Red teaming language models to reduce harms: Methods, scaling behaviors, and lessons learned\", \"Gptfuzzer: Red teaming large language models with auto-generated jailbreak prompts\", \"MART: Improving LLM Safety with Multi-round Automatic Red-Teaming\", \"Model cards for model reporting\", \"Interactive model cards: A human-centered approach to model documentation\", \"Data cards: Purposeful and transparent dataset documentation for responsible ai\", \"How Linguistically Fair Are Multilingual Pre-Trained Language Models?\", \"You reap what you sow: On the challenges of bias evaluation under multilingual settings\", \"MEGAVERSE: Benchmarking Large Language Models Across Languages, Modalities, Models and Tasks\", \"Glot500: Scaling Multilingual Corpora and Language Models to 500 Languages\", \"Democratizing LLMs for Low-Resource Languages by Leveraging their English Dominant Abilities with Linguistically-Diverse Prompts\", \"Having Beer after Prayer? Measuring Cultural Bias in Large Language Models\", \"Fairness in Language Models Beyond English: Gaps and Challenges\", \"Are Large Language Model-based Evaluators the Solution to Scaling Up Multilingual Evaluation?\", \"Conformal Language Modeling\", \"Quantifying Uncertainty in Answers from any Language Model and Enhancing their Trustworthiness\", \"Look Before You Leap: An Exploratory Study of Uncertainty Measurement for Large Language Models\", \"Uncertainty-aware Language Modeling for Selective Question Answering\", \"Mind the gap: Assessing temporal generalization in neural language models\"], \"Citation\":[\"AgentClinic: a multimodal agent benchmark to evaluate Al in simulated clinical environments\"]}\n[The End of Assistant's Predicted Answer]\n\n[System]\nWe would like to request your feedback on the performance of the AI assistant in response to the user question displayed above according to the gold answer. Please use the following listed aspects and their descriptions as evaluation criteria:\n    - Accuracy and Hallucinations: The assistant's answer is semantically consistent with the gold answer; The numerical value and order need to be accurate, and there should be no hallucinations.\n    - Completeness: Referring to the reference answers, the assistant's answer should contain all the key points needed to answer the user's question; further elaboration on these key points can be omitted.\nPlease rate whether this answer is suitable for the question. Please note that the gold answer can be considered as a correct answer to the question.\n\nThe assistant receives an overall score on a scale of 1 to 100, where a higher score indicates better overall performance.\nPlease note that if the assistant's answer and the gold answer fully meet the above criteria, its overall rating should be the full marks (100).\nPlease first provide a comprehensive explanation of your evaluation, avoiding any potential bias.\nThen, output a line indicating the score of the Assistant.\n\nPLEASE OUTPUT WITH THE FOLLOWING FORMAT, WHERE THE SCORE IS A SCALE OF 1 TO 100 BY STRICTLY FOLLOWING THIS FORMAT: \"[[score]]\", FOR EXAMPLE \"Rating: [[100]]\":\n<start output>\nEvaluation evidence: your evluation explanation here, no more than 100 words\nRating: [[score]]\n<end output>\n\nNow, start your evaluation:",
        "generate_response": "{\"Reference\":[\"Interviewing the patient\", \"Contributions of the history, physical examination, and laboratory investigation in making medical diagnoses\", \"Relative contributions of history-taking, physical examination, and laboratory investigation to diagnosis and management of medical outpatients\", \"Teaching clinical medicine by iterative hypothesis testing: let's preach what we practice\", \"A study on relative contributions of the history, physical examination and investigations in making medical diagnosis\", \"The importance of the history in the medical clinic and the cost of unnecessary tests\", \"Skills for communicating with patients\", \"Global supply of health professionals\", \"GPT-4 Technical Report\", \"PaLM 2 Technical Report\", \"Gemini: A Family of Highly Capable Multimodal Models\", \"Large Language Models Encode Clinical Knowledge\", \"Towards expert-level medical question answering with large language models\", \"Can Generalist Foundation Models Outcompete Special-Purpose Tuning? Case Study in Medicine\", \"LaMDA: Language models for dialog applications\", \"Introducing ChatGPT\", \"Clinical Camel: An Open-Source Expert-Level Medical Language Model with Dialogue-Based Knowledge Encoding\", \"MEDITRON-70B: Scaling Medical Pretraining for Large Language Models\", \"History taking is a complex skill\", \"Best practice\" for patient-centered communication: a narrative review\", \"What disease does this patient have? a large-scale open domain question answering dataset from medical exams\", \"MIMIC-III, a freely accessible critical care database\", \"Speech recognition for medical conversations\", \"A computational approach to understanding empathy expressed in text-based mental health support\", \"Improving language model negotiation with self-play and in-context learning from ai feedback\", \"Overview of the mediqa-chat 2023 shared tasks on the summarization & generation of doctor-patient conversations\", \"Overview of the ImageCLLEF 2023: Multimedia Retrieval in Medical, Social Media and Internet Applications\", \"DialMed: A Dataset for Dialogue-based Medication Recommendation\", \"Incorporating Medical Knowledge to Transformer-based Language Models for Medical Dialogue Generation\", \"MRCP (UK) PART 2 Clinical Examination (PACES): a review of the first four examination sessions\", \"The Objective Structured Clinical Examination. The new gold standard for evaluating postgraduate clinical performance\", \"The objective structured clinical examination: a step in the direction of competency-based evaluation\", \"Defining and assessing professional competence\", \"Teaching history taking to medical students: a systematic review\", \"Effect of communications training on medical student performance\", \"Communication skills education in medical school and beyond\", \"Teaching and assessing communication skills in the postgraduate medical setting: a systematic scoping review\", \"Improving communication skills: a course for academic medical center surgery residents and faculty\", \"UK consensus statement on the content of communication curricula in undergraduate medical education\", \"Endpoints in medical communication research, proposing a framework of functions and outcomes\", \"Patient-centered communication in cancer care: promoting healing and reducing suffering\", \"Assessing communication competence: a review of current tools\", \"Medical history\", \"What are consultation models for?\", \"Implementation of virtual OSCE in health professions education: A systematic review\", \"Multiwoz-a large-scale multi-domain wizard-of-oz dataset for task-oriented dialogue modelling\", \"Airdialogue: An environment for goal-oriented dialogue research\", \"Attention is all you need\", \"Training language models to follow instructions with human feedback\", \"Ethical-advice taker: Do language models understand natural language interventions?\", \"Self-critiquing models for assisting human evaluators\", \"Training language models with language feedback at scale\", \"Improving alignment of dialogue agents via targeted human judgements\", \"Constitutional AI: Harmlessness from AI feedback\", \"A general language assistant as a laboratory for alignment\", \"Clinical BERTScore: An Improved Measure of Automatic Speech Recognition Performance in Clinical Settings\", \"Overview of the medical question answering task at TREC 2017 LiveQA\", \"The diagnostic and triage accuracy of digital and online symptom checker tools: a systematic review\", \"Testing the Limits of Language Models: A Conversational Framework for Medical AI Assessment\", \"Diagnostic accuracy of artificial intelligence in virtual primary care\", \"Comparing Physician and Artificial Intelligence Chatbot Responses to Patient Questions Posted to a Public Social Media Forum\", \"ChatGPT\", \"The effectiveness of teleconsultations in primary care: systematic review\", \"Virtual primary care: fragmentation or integration?\", \"Asynchronous Remote Communication as a Tool for Care Management in Primary Care: A Rapid Review of the Literature\", \"Comparing the content and quality of video, telephone, and face-to-face consultations: a non-randomised, quasi-experimental, exploratory study in UK primary care\", \"Patient satisfaction with time spent with their physician\", \"The effect of screen-to-screen versus face-to-face consultation on doctor-patient communication: an experimental study with simulated patients\", \"Trade-offs in high-volume primary care practice\", \"Enhancing the reliability and accuracy of AI-enabled diagnosis via complementarity-driven deferral to clinicians\", \"Methods in teaching consultation-liaison psychiatry\", \"What is missing from patient histories?\", \"Lessons for Clinicians From Physician-Patient\", \"Patient-centered interviewing: an evidence-based method\", \"Care, health, and cost\", \"From triple to quadruple aim: care of the patient requires care of the provider\", \"Physician communication skills and malpractice claims\", \"Doctors' non-verbal behaviour in consultations: look at the patient before you look at the computer\", \"Inter-Cultural Communication Skills Training in Medical Schools: A Systematic Review\", \"History taking as a diagnostic tool in children with chronic cough\", \"Reliability and validity of OSCE checklists used to assess the communication skills of undergraduate medical students: a systematic review\", \"Taxonomy of risks posed by language models\", \"Bias and Fairness in Large Language Models: A Survey\", \"Patient race/ethnicity and quality of patient-physician communication during medical visits\", \"Physician gender effects in medical communication: a meta-analytic review\", \"Red teaming language models with language models\", \"Red teaming language models to reduce harms: Methods, scaling behaviors, and lessons learned\", \"Gptfuzzer: Red teaming large language models with auto-generated jailbreak prompts\", \"MART: Improving LLM Safety with Multi-round Automatic Red-Teaming\", \"Model cards for model reporting\", \"Interactive model cards: A human-centered approach to model documentation\", \"Data cards: Purposeful and transparent dataset documentation for responsible ai\", \"How Linguistically Fair Are Multilingual Pre-Trained Language Models?\", \"You reap what you sow: On the challenges of bias evaluation under multilingual settings\", \"MEGAVERSE: Benchmarking Large Language Models Across Languages, Modalities, Models and Tasks\", \"Glot500: Scaling Multilingual Corpora and Language Models to 500 Languages\", \"Democratizing LLMs for Low-Resource Languages by Leveraging their English Dominant Abilities with Linguistically-Diverse Prompts\", \"Having Beer after Prayer? Measuring Cultural Bias in Large Language Models\", \"Fairness in Language Models Beyond English: Gaps and Challenges\", \"Are Large Language Model-based Evaluators the Solution to Scaling Up Multilingual Evaluation?\", \"Conformal Language Modeling\", \"Quantifying Uncertainty in Answers from any Language Model and Enhancing their Trustworthiness\", \"Look Before You Leap: An Exploratory Study of Uncertainty Measurement for Large Language Models\", \"Uncertainty-aware Language Modeling for Selective Question Answering\", \"Mind the gap: Assessing temporal generalization in neural language models\"], \"Citation\":[\"AgentClinic: a multimodal agent benchmark to evaluate Al in simulated clinical environments\"]}",
        "eval_response": "Evaluation evidence: The assistant's response contains a significant number of hallucinated references that are not mentioned in the gold answer, indicating a lack of accuracy and presence of hallucinations. The response also fails to include all the references and citations listed in the gold answer, showing incompleteness. The assistant's answer does not align well with the gold answer, missing key points and adding irrelevant information.\nRating: [[20]]",
        "score": 20
    },
    {
        "level": 3,
        "set": 2,
        "length": 82193,
        "type": "paper",
        "language": "en",
        "question": "Large Language Models for Mathematical Reasoning: Progresses and Challenges",
        "instruction": "We hope you will carefully study the provided papers and determine the citation relationships between them. Please follow the instructions below strictly to complete the task:\n\n#Specific Requirements:\n1. Reference: When a given paper mentions other provided papers, those other papers are considered as \"references\" for the given paper. To summarize in this specific context, references are about what the given paper is using.\n2. Citation: Conversely, when other provided papers mention the given paper in their works, the given paper is being \"cited\" by those other papers. To summarize in this specific context, citations are about who is using the given paper.\n3. Given a paper, you need to determine the citation or reference relationship between this paper and the other papers. Do not consider papers that are not provided.\n3. Please present the paper titles in a json format as follows: {{\"Reference\":[\"Reference Title 1\", \"Reference Title 2\", ..., \"Reference Title n\"], \"Citation\":[\"Citation Title 1\", \"Citation Title 2\", ..., \"Citation Title n\"]}}.\n4. If a paper does not have any references or citations, please leave the corresponding list empty, e.g.{{\"Refernce\":[]}}, {{\"Citation\":[]}}.",
        "prompt_template": "#Papers:\n{docs}\n\n{instruction}\n\n#The paper you need to analyze:\n{question}",
        "doc": [
            "2402.00157.md",
            "2405.19616.md",
            "2310.10677.md",
            "2310.13615.md"
        ],
        "answer": {
            "Reference": [
                "# LLMs as Potential Brainstorming Partners for Math and Science Problems - Case Studies and Analysis ",
                "# Three Questions Concerning the Use of Large Language Models to Facilitate Mathematics Learning "
            ],
            "Citation": [
                "# Easy Problems That LLMs Get Wrong "
            ]
        },
        "shuffle_doc": true,
        "id": "e9d92592-c21d-48b2-9f4a-9791fe5e26eb",
        "prompt": "[Question]\n#Papers:\n{docs}\n\nWe hope you will carefully study the provided papers and determine the citation relationships between them. Please follow the instructions below strictly to complete the task:\n\n#Specific Requirements:\n1. Reference: When a given paper mentions other provided papers, those other papers are considered as \"references\" for the given paper. To summarize in this specific context, references are about what the given paper is using.\n2. Citation: Conversely, when other provided papers mention the given paper in their works, the given paper is being \"cited\" by those other papers. To summarize in this specific context, citations are about who is using the given paper.\n3. Given a paper, you need to determine the citation or reference relationship between this paper and the other papers. Do not consider papers that are not provided.\n3. Please present the paper titles in a json format as follows: {{\"Reference\":[\"Reference Title 1\", \"Reference Title 2\", ..., \"Reference Title n\"], \"Citation\":[\"Citation Title 1\", \"Citation Title 2\", ..., \"Citation Title n\"]}}.\n4. If a paper does not have any references or citations, please leave the corresponding list empty, e.g.{{\"Refernce\":[]}}, {{\"Citation\":[]}}.\n\n#The paper you need to analyze:\nLarge Language Models for Mathematical Reasoning: Progresses and Challenges\n\n[Gold Answer]\n{'Reference': ['# LLMs as Potential Brainstorming Partners for Math and Science Problems - Case Studies and Analysis ', '# Three Questions Concerning the Use of Large Language Models to Facilitate Mathematics Learning '], 'Citation': ['# Easy Problems That LLMs Get Wrong ']}\n\n[The Start of Assistant's Predicted Answer]\n{\"Reference\": [\"A neural network solves, explains, and generates university math problems by program synthesis and fewshot learning at human level\", \"Large language models for mathematicians\", \"Mathematical capabilities of chatgpt\", \"Exploring pre-service teachers' perceptions of large language models-generated hints in online mathematics learning\", \"Reasoning in large language models through symbolic math word problems\", \"Llms as potential brainstorming partners for math and science problems\", \"Measuring mathematical problem solving with the MATH dataset\", \"Learning to solve arithmetic word problems with verb categorization\", \"Solving general arithmetic word problems\", \"An independent evaluation of chatgpt on mathematical word problems (MWP)\", \"A causal framework to quantify the robustness of mathematical reasoning with language models\", \"Solving olympiad geometry without human demonstrations\", \"Deep neural solver for math word problems\", \"Math word problem generation with mathematical consistency and problem context constraints\", \"Chain-of-thought prompting elicits reasoning in large language models\", \"CMATH: can your language model pass chinese elementary school math test?\", \"LPML: llm-prompting markup language for mathematical reasoning\", \"Metamath: Bootstrap your own mathematical questions for large language models\", \"How well do large language models perform in arithmetic tasks?\", \"Mammoth: Building math generalist models through hybrid instruction tuning\", \"GLM-130B: an open bilingual pre-trained model\", \"Evaluating and improving tool-augmented computation-intensive math reasoning\", \"Interpretable math word problem solution generation via step-by-step planning\", \"Ape210k: A large-scale and template-rich dataset of math word problems\", \"Minif2f: a cross-system benchmark for formal olympiad-level mathematics\", \"A human-centric benchmark for evaluating foundation models\", \"Solving challenging math word problems using GPT-4 code interpreter with code-based self-verification\", \"Mathattack: Attacking large language models towards math solving ability\", \"Solving math word problems via cooperative reasoning induced language models\", \"Solving math word problems concerning systems of equations with GPT-3\"], \"Citation\": [\"Three Questions Concerning the Use of Large Language Models to Facilitate Mathematics Learning\"]}\n[The End of Assistant's Predicted Answer]\n\n[System]\nWe would like to request your feedback on the performance of the AI assistant in response to the user question displayed above according to the gold answer. Please use the following listed aspects and their descriptions as evaluation criteria:\n    - Accuracy and Hallucinations: The assistant's answer is semantically consistent with the gold answer; The numerical value and order need to be accurate, and there should be no hallucinations.\n    - Completeness: Referring to the reference answers, the assistant's answer should contain all the key points needed to answer the user's question; further elaboration on these key points can be omitted.\nPlease rate whether this answer is suitable for the question. Please note that the gold answer can be considered as a correct answer to the question.\n\nThe assistant receives an overall score on a scale of 1 to 100, where a higher score indicates better overall performance.\nPlease note that if the assistant's answer and the gold answer fully meet the above criteria, its overall rating should be the full marks (100).\nPlease first provide a comprehensive explanation of your evaluation, avoiding any potential bias.\nThen, output a line indicating the score of the Assistant.\n\nPLEASE OUTPUT WITH THE FOLLOWING FORMAT, WHERE THE SCORE IS A SCALE OF 1 TO 100 BY STRICTLY FOLLOWING THIS FORMAT: \"[[score]]\", FOR EXAMPLE \"Rating: [[100]]\":\n<start output>\nEvaluation evidence: your evluation explanation here, no more than 100 words\nRating: [[score]]\n<end output>\n\nNow, start your evaluation:",
        "generate_response": "{\"Reference\": [\"A neural network solves, explains, and generates university math problems by program synthesis and fewshot learning at human level\", \"Large language models for mathematicians\", \"Mathematical capabilities of chatgpt\", \"Exploring pre-service teachers' perceptions of large language models-generated hints in online mathematics learning\", \"Reasoning in large language models through symbolic math word problems\", \"Llms as potential brainstorming partners for math and science problems\", \"Measuring mathematical problem solving with the MATH dataset\", \"Learning to solve arithmetic word problems with verb categorization\", \"Solving general arithmetic word problems\", \"An independent evaluation of chatgpt on mathematical word problems (MWP)\", \"A causal framework to quantify the robustness of mathematical reasoning with language models\", \"Solving olympiad geometry without human demonstrations\", \"Deep neural solver for math word problems\", \"Math word problem generation with mathematical consistency and problem context constraints\", \"Chain-of-thought prompting elicits reasoning in large language models\", \"CMATH: can your language model pass chinese elementary school math test?\", \"LPML: llm-prompting markup language for mathematical reasoning\", \"Metamath: Bootstrap your own mathematical questions for large language models\", \"How well do large language models perform in arithmetic tasks?\", \"Mammoth: Building math generalist models through hybrid instruction tuning\", \"GLM-130B: an open bilingual pre-trained model\", \"Evaluating and improving tool-augmented computation-intensive math reasoning\", \"Interpretable math word problem solution generation via step-by-step planning\", \"Ape210k: A large-scale and template-rich dataset of math word problems\", \"Minif2f: a cross-system benchmark for formal olympiad-level mathematics\", \"A human-centric benchmark for evaluating foundation models\", \"Solving challenging math word problems using GPT-4 code interpreter with code-based self-verification\", \"Mathattack: Attacking large language models towards math solving ability\", \"Solving math word problems via cooperative reasoning induced language models\", \"Solving math word problems concerning systems of equations with GPT-3\"], \"Citation\": [\"Three Questions Concerning the Use of Large Language Models to Facilitate Mathematics Learning\"]}",
        "eval_response": "Evaluation evidence: The assistant's answer does not match the gold answer in terms of accuracy and completeness. The references and citations listed by the assistant are entirely different from those in the gold answer, indicating a significant deviation and possible hallucination of data. The assistant's response includes a long list of unrelated papers, which are not mentioned in the gold answer, thus failing to meet the required accuracy and completeness.\n\nRating: [[10]]",
        "score": 10
    },
    {
        "level": 4,
        "set": 1,
        "length": 18721,
        "type": "paper",
        "language": "en",
        "question": "",
        "instruction": "We kindly ask you to thoroughly review the provided papers and construct a citation chain from them. Please adhere to the following instructions strictly while completing the task:\n\n#Task Instructions:\nGiven several papers, you are required to identify and list the longest citation chain, which demonstrates the citation relationship among the provided papers.\n\n#Specific Requirements:\n1.Please present the titles of the papers in the form of a list, as follows: [\"Title of Paper 1\", \"Title of Paper 2\", ..., \"Title of Paper n\"].\n2.Ensure that the citation chain in the list is linear and continuous, meaning that the first paper title in the list (Paper 1) should not cite any other works. Instead, it should be cited by the next paper in the list (Paper 2); subsequently, each paper should then be cited by the next one in the list, continuing up to the last paper (Paper n).\n3.Consider only the citation relationships within the supplied collection of papers, and ensure that the citation chain accurately reflects the sequential citation order among these documents.\n4.Do not take into account any articles not provided, and disregard other non-linear citation relationships.",
        "prompt_template": "{instruction}\n\n#Papers Provided:\n{docs}",
        "doc": [
            "1508.01745.md",
            "1709.03082.md"
        ],
        "answer": [
            "# Semantically Conditioned LSTM-based Natural Language Generation for Spoken Dialogue Systems ",
            "# A Neural Network Architecture Combining Gated Recurrent Unit (GRU) and Support Vector Machine (SVM) for Intrusion Detection in Network Traffic Data "
        ],
        "shuffle_doc": true,
        "id": "cd618a43-7176-432d-b21e-432e5c2ca4a7",
        "prompt": "[Question]\nWe kindly ask you to thoroughly review the provided papers and construct a citation chain from them. Please adhere to the following instructions strictly while completing the task:\n\n#Task Instructions:\nGiven several papers, you are required to identify and list the longest citation chain, which demonstrates the citation relationship among the provided papers.\n\n#Specific Requirements:\n1.Please present the titles of the papers in the form of a list, as follows: [\"Title of Paper 1\", \"Title of Paper 2\", ..., \"Title of Paper n\"].\n2.Ensure that the citation chain in the list is linear and continuous, meaning that the first paper title in the list (Paper 1) should not cite any other works. Instead, it should be cited by the next paper in the list (Paper 2); subsequently, each paper should then be cited by the next one in the list, continuing up to the last paper (Paper n).\n3.Consider only the citation relationships within the supplied collection of papers, and ensure that the citation chain accurately reflects the sequential citation order among these documents.\n4.Do not take into account any articles not provided, and disregard other non-linear citation relationships.\n\n#Papers Provided:\n{docs}\n\n[Gold Answer]\n['# Semantically Conditioned LSTM-based Natural Language Generation for Spoken Dialogue Systems ', '# A Neural Network Architecture Combining Gated Recurrent Unit (GRU) and Support Vector Machine (SVM) for Intrusion Detection in Network Traffic Data ']\n\n[The Start of Assistant's Predicted Answer]\n\"\n[The End of Assistant's Predicted Answer]\n\n[System]\nWe would like to request your feedback on the performance of the AI assistant in response to the user question displayed above according to the gold answer. Please use the following listed aspects and their descriptions as evaluation criteria:\n    - Accuracy and Hallucinations: The assistant's answer is semantically consistent with the gold answer; The numerical value and order need to be accurate, and there should be no hallucinations.\n    - Completeness: Referring to the reference answers, the assistant's answer should contain all the key points needed to answer the user's question; further elaboration on these key points can be omitted.\nPlease rate whether this answer is suitable for the question. Please note that the gold answer can be considered as a correct answer to the question.\n\nThe assistant receives an overall score on a scale of 1 to 100, where a higher score indicates better overall performance.\nPlease note that if the assistant's answer and the gold answer fully meet the above criteria, its overall rating should be the full marks (100).\nPlease first provide a comprehensive explanation of your evaluation, avoiding any potential bias.\nThen, output a line indicating the score of the Assistant.\n\nPLEASE OUTPUT WITH THE FOLLOWING FORMAT, WHERE THE SCORE IS A SCALE OF 1 TO 100 BY STRICTLY FOLLOWING THIS FORMAT: \"[[score]]\", FOR EXAMPLE \"Rating: [[100]]\":\n<start output>\nEvaluation evidence: your evluation explanation here, no more than 100 words\nRating: [[score]]\n<end output>\n\nNow, start your evaluation:",
        "generate_response": "\"",
        "eval_response": "Evaluation evidence: The assistant's response indicates an inability to determine the longest citation chain due to a lack of complete citation network information. This response does not align with the gold answer, which provides a specific citation chain. The assistant's answer fails to accurately reflect the citation relationships among the provided papers and does not list any titles, leading to inaccuracies and incompleteness in addressing the task requirements.\n\nRating: [[10]]",
        "score": 10
    },
    {
        "level": 4,
        "set": 3,
        "length": 131718,
        "type": "paper",
        "language": "en",
        "question": "",
        "instruction": "We kindly ask you to thoroughly review the provided papers and construct a citation chain from them. Please adhere to the following instructions strictly while completing the task:\n\n#Task Instructions:\nGiven several papers, you are required to identify and list the longest citation chain, which demonstrates the citation relationship among the provided papers.\n\n#Specific Requirements:\n1.Please present the titles of the papers in the form of a list, as follows: [\"Title of Paper 1\", \"Title of Paper 2\", ..., \"Title of Paper n\"].\n2.Ensure that the citation chain in the list is linear and continuous, meaning that the first paper title in the list (Paper 1) should not cite any other works. Instead, it should be cited by the next paper in the list (Paper 2); subsequently, each paper should then be cited by the next one in the list, continuing up to the last paper (Paper n).\n3.Consider only the citation relationships within the supplied collection of papers, and ensure that the citation chain accurately reflects the sequential citation order among these documents.\n4.Do not take into account any articles not provided, and disregard other non-linear citation relationships.",
        "prompt_template": "{instruction}\n\n#Papers Provided:\n{docs}",
        "doc": [
            "2203.15556.md",
            "2302.13971.md",
            "2303.08774.md"
        ],
        "answer": [
            "# Training Compute-Optimal Large Language Models ",
            "# LLaMA: Open and Efficient Foundation Language Models ",
            "# GPT-4 Technical Report "
        ],
        "shuffle_doc": true,
        "id": "d6940f31-9025-48c2-b0df-67d347a3a3e9",
        "prompt": "[Question]\nWe kindly ask you to thoroughly review the provided papers and construct a citation chain from them. Please adhere to the following instructions strictly while completing the task:\n\n#Task Instructions:\nGiven several papers, you are required to identify and list the longest citation chain, which demonstrates the citation relationship among the provided papers.\n\n#Specific Requirements:\n1.Please present the titles of the papers in the form of a list, as follows: [\"Title of Paper 1\", \"Title of Paper 2\", ..., \"Title of Paper n\"].\n2.Ensure that the citation chain in the list is linear and continuous, meaning that the first paper title in the list (Paper 1) should not cite any other works. Instead, it should be cited by the next paper in the list (Paper 2); subsequently, each paper should then be cited by the next one in the list, continuing up to the last paper (Paper n).\n3.Consider only the citation relationships within the supplied collection of papers, and ensure that the citation chain accurately reflects the sequential citation order among these documents.\n4.Do not take into account any articles not provided, and disregard other non-linear citation relationships.\n\n#Papers Provided:\n{docs}\n\n[Gold Answer]\n['# Training Compute-Optimal Large Language Models ', '# LLaMA: Open and Efficient Foundation Language Models ', '# GPT-4 Technical Report ']\n\n[The Start of Assistant's Predicted Answer]\n[\"Training Compute-Optimal Large Language Models\", \"Language models are few-shot learners\", \"Beyond the imitation game: Measuring and extrapolating the capabilities of language models\", \"PIQA: Reasoning about physical commonsense in natural language\", \"Improving language models by retrieving from trillions of tokens\", \"JAX: composable transformations of Python + NumPy programs\", \"On the dangers of stochastic parrots: Can language models be too big?\", \"Convex Optimization: Algorithms and Complexity\", \"Unified scaling laws for routed language models\", \"BoolQ: Exploring the surprising difficulty of natural yes/no questions\", \"Glam: Efficient scaling of language models with mixture-of-experts\", \"Switch transformers: Scaling to trillion parameter models with simple and efficient sparsity\", \"The Pile: An 800GB dataset of diverse text for language modeling\", \"An empirical model of large-batch training\", \"Pointer sentinel mixture models\", \"Model cards for model reporting\", \"Updating Quasi-Newton Matrices with Limited Storage\", \"The LAMBADA dataset: Word prediction requiring a broad discourse context\", \"Scaling language models: Methods, analysis \\& insights from training Gopher\", \"Compressive transformers for long-range sequence modelling\", \"Exploring the limits of transfer learning with a unified text-to-text transformer\", \"Zero: Memory optimizations toward training trillion parameter models\", \"A Stochastic Approximation Method\", \"Gender bias in coreference resolution\", \"Winogrande: An adversarial winograd schema challenge at scale\", \"SocialIQA: Commonsense reasoning about social interactions\", \"Measuring the effects of data parallelism on neural network training\", \"Approximation rates for neural networks with general activation functions\", \"Tuning large neural networks via zero-shot hyperparameter transfer\", \"HellaSwag: Can a machine really finish your sentence?\", \"Which algorithmic choices matter at which batch sizes? insights from a noisy quadratic model\", \"Designing effective sparse expert models\", \"GPT-4 Technical Report\", \"LLaMA: Open and Efficient Foundation Language Models\"]\n[The End of Assistant's Predicted Answer]\n\n[System]\nWe would like to request your feedback on the performance of the AI assistant in response to the user question displayed above according to the gold answer. Please use the following listed aspects and their descriptions as evaluation criteria:\n    - Accuracy and Hallucinations: The assistant's answer is semantically consistent with the gold answer; The numerical value and order need to be accurate, and there should be no hallucinations.\n    - Completeness: Referring to the reference answers, the assistant's answer should contain all the key points needed to answer the user's question; further elaboration on these key points can be omitted.\nPlease rate whether this answer is suitable for the question. Please note that the gold answer can be considered as a correct answer to the question.\n\nThe assistant receives an overall score on a scale of 1 to 100, where a higher score indicates better overall performance.\nPlease note that if the assistant's answer and the gold answer fully meet the above criteria, its overall rating should be the full marks (100).\nPlease first provide a comprehensive explanation of your evaluation, avoiding any potential bias.\nThen, output a line indicating the score of the Assistant.\n\nPLEASE OUTPUT WITH THE FOLLOWING FORMAT, WHERE THE SCORE IS A SCALE OF 1 TO 100 BY STRICTLY FOLLOWING THIS FORMAT: \"[[score]]\", FOR EXAMPLE \"Rating: [[100]]\":\n<start output>\nEvaluation evidence: your evluation explanation here, no more than 100 words\nRating: [[score]]\n<end output>\n\nNow, start your evaluation:",
        "generate_response": "[\"Training Compute-Optimal Large Language Models\", \"Language models are few-shot learners\", \"Beyond the imitation game: Measuring and extrapolating the capabilities of language models\", \"PIQA: Reasoning about physical commonsense in natural language\", \"Improving language models by retrieving from trillions of tokens\", \"JAX: composable transformations of Python + NumPy programs\", \"On the dangers of stochastic parrots: Can language models be too big?\", \"Convex Optimization: Algorithms and Complexity\", \"Unified scaling laws for routed language models\", \"BoolQ: Exploring the surprising difficulty of natural yes/no questions\", \"Glam: Efficient scaling of language models with mixture-of-experts\", \"Switch transformers: Scaling to trillion parameter models with simple and efficient sparsity\", \"The Pile: An 800GB dataset of diverse text for language modeling\", \"An empirical model of large-batch training\", \"Pointer sentinel mixture models\", \"Model cards for model reporting\", \"Updating Quasi-Newton Matrices with Limited Storage\", \"The LAMBADA dataset: Word prediction requiring a broad discourse context\", \"Scaling language models: Methods, analysis \\& insights from training Gopher\", \"Compressive transformers for long-range sequence modelling\", \"Exploring the limits of transfer learning with a unified text-to-text transformer\", \"Zero: Memory optimizations toward training trillion parameter models\", \"A Stochastic Approximation Method\", \"Gender bias in coreference resolution\", \"Winogrande: An adversarial winograd schema challenge at scale\", \"SocialIQA: Commonsense reasoning about social interactions\", \"Measuring the effects of data parallelism on neural network training\", \"Approximation rates for neural networks with general activation functions\", \"Tuning large neural networks via zero-shot hyperparameter transfer\", \"HellaSwag: Can a machine really finish your sentence?\", \"Which algorithmic choices matter at which batch sizes? insights from a noisy quadratic model\", \"Designing effective sparse expert models\", \"GPT-4 Technical Report\", \"LLaMA: Open and Efficient Foundation Language Models\"]",
        "eval_response": "Evaluation evidence: The assistant's response does not align with the task requirements. The gold answer specifies a citation chain of three papers, while the assistant's response lists a large number of unrelated papers. This indicates a failure in understanding the task of constructing a citation chain from the provided papers. The response shows a lack of accuracy and completeness concerning the task's specific instructions.\n\nRating: [[10]]",
        "score": 10
    },
    {
        "level": 4,
        "set": 2,
        "length": 94724,
        "type": "paper",
        "language": "en",
        "question": "",
        "instruction": "We kindly ask you to thoroughly review the provided papers and construct a citation chain from them. Please adhere to the following instructions strictly while completing the task:\n\n#Task Instructions:\nGiven several papers, you are required to identify and list the longest citation chain, which demonstrates the citation relationship among the provided papers.\n\n#Specific Requirements:\n1.Please present the titles of the papers in the form of a list, as follows: [\"Title of Paper 1\", \"Title of Paper 2\", ..., \"Title of Paper n\"].\n2.Ensure that the citation chain in the list is linear and continuous, meaning that the first paper title in the list (Paper 1) should not cite any other works. Instead, it should be cited by the next paper in the list (Paper 2); subsequently, each paper should then be cited by the next one in the list, continuing up to the last paper (Paper n).\n3.Consider only the citation relationships within the supplied collection of papers, and ensure that the citation chain accurately reflects the sequential citation order among these documents.\n4.Do not take into account any articles not provided, and disregard other non-linear citation relationships.",
        "prompt_template": "{instruction}\n\n#Papers Provided:\n{docs}",
        "doc": [
            "1804.04272.md",
            "1901.02039.md",
            "2112.13227.md",
            "2305.02536.md",
            "2405.08981.md"
        ],
        "answer": [
            "# Deep Neural Networks Motivated by Partial Differential Equations ",
            "# SPHERICAL CNNS ON UNSTRUCTURED GRIDS ",
            "# Pseudocylindrical Convolutions for Learned Omnidirectional Image Compression ",
            "# Scanpath Prediction in Panoramic Videos via Expected Code Length Minimization ",
            "# Impact of Design Decisions in Scanpath Modeling "
        ],
        "shuffle_doc": true,
        "id": "6e6c622e-fd69-4b58-9f5c-332e08ed120a",
        "prompt": "[Question]\nWe kindly ask you to thoroughly review the provided papers and construct a citation chain from them. Please adhere to the following instructions strictly while completing the task:\n\n#Task Instructions:\nGiven several papers, you are required to identify and list the longest citation chain, which demonstrates the citation relationship among the provided papers.\n\n#Specific Requirements:\n1.Please present the titles of the papers in the form of a list, as follows: [\"Title of Paper 1\", \"Title of Paper 2\", ..., \"Title of Paper n\"].\n2.Ensure that the citation chain in the list is linear and continuous, meaning that the first paper title in the list (Paper 1) should not cite any other works. Instead, it should be cited by the next paper in the list (Paper 2); subsequently, each paper should then be cited by the next one in the list, continuing up to the last paper (Paper n).\n3.Consider only the citation relationships within the supplied collection of papers, and ensure that the citation chain accurately reflects the sequential citation order among these documents.\n4.Do not take into account any articles not provided, and disregard other non-linear citation relationships.\n\n#Papers Provided:\n{docs}\n\n[Gold Answer]\n['# Deep Neural Networks Motivated by Partial Differential Equations ', '# SPHERICAL CNNS ON UNSTRUCTURED GRIDS ', '# Pseudocylindrical Convolutions for Learned Omnidirectional Image Compression ', '# Scanpath Prediction in Panoramic Videos via Expected Code Length Minimization ', '# Impact of Design Decisions in Scanpath Modeling ']\n\n[The Start of Assistant's Predicted Answer]\n'Pseudocylindrical Convolutions for Learned Omnidirectional Image Compression''Scanpath Prediction in Panoramic Videos via Expected Code Length Minimization'\"\n[The End of Assistant's Predicted Answer]\n\n[System]\nWe would like to request your feedback on the performance of the AI assistant in response to the user question displayed above according to the gold answer. Please use the following listed aspects and their descriptions as evaluation criteria:\n    - Accuracy and Hallucinations: The assistant's answer is semantically consistent with the gold answer; The numerical value and order need to be accurate, and there should be no hallucinations.\n    - Completeness: Referring to the reference answers, the assistant's answer should contain all the key points needed to answer the user's question; further elaboration on these key points can be omitted.\nPlease rate whether this answer is suitable for the question. Please note that the gold answer can be considered as a correct answer to the question.\n\nThe assistant receives an overall score on a scale of 1 to 100, where a higher score indicates better overall performance.\nPlease note that if the assistant's answer and the gold answer fully meet the above criteria, its overall rating should be the full marks (100).\nPlease first provide a comprehensive explanation of your evaluation, avoiding any potential bias.\nThen, output a line indicating the score of the Assistant.\n\nPLEASE OUTPUT WITH THE FOLLOWING FORMAT, WHERE THE SCORE IS A SCALE OF 1 TO 100 BY STRICTLY FOLLOWING THIS FORMAT: \"[[score]]\", FOR EXAMPLE \"Rating: [[100]]\":\n<start output>\nEvaluation evidence: your evluation explanation here, no more than 100 words\nRating: [[score]]\n<end output>\n\nNow, start your evaluation:",
        "generate_response": "'Pseudocylindrical Convolutions for Learned Omnidirectional Image Compression''Scanpath Prediction in Panoramic Videos via Expected Code Length Minimization'\"",
        "eval_response": "Evaluation evidence: The assistant's response does not match the gold answer and fails to list the citation chain as required. Instead, it provides an explanation about the inability to determine the citation chain due to lack of bidirectional citation information, which is not aligned with the task of listing the titles in a citation chain. This indicates a misunderstanding of the task requirements and results in both inaccuracies and incompleteness.\n\nRating: [[20]]",
        "score": 20
    },
    {
        "level": 3,
        "set": 2,
        "length": 78065,
        "type": "paper",
        "language": "en",
        "question": "Hallucination is Inevitable: <br> An Innate Limitation of Large Language Models",
        "instruction": "We hope you will carefully study the provided papers and determine the citation relationships between them. Please follow the instructions below strictly to complete the task:\n\n#Specific Requirements:\n1. Reference: When a given paper mentions other provided papers, those other papers are considered as \"references\" for the given paper. To summarize in this specific context, references are about what the given paper is using.\n2. Citation: Conversely, when other provided papers mention the given paper in their works, the given paper is being \"cited\" by those other papers. To summarize in this specific context, citations are about who is using the given paper.\n3. Given a paper, you need to determine the citation or reference relationship between this paper and the other papers. Do not consider papers that are not provided.\n3. Please present the paper titles in a json format as follows: {{\"Reference\":[\"Reference Title 1\", \"Reference Title 2\", ..., \"Reference Title n\"], \"Citation\":[\"Citation Title 1\", \"Citation Title 2\", ..., \"Citation Title n\"]}}.\n4. If a paper does not have any references or citations, please leave the corresponding list empty, e.g.{{\"Refernce\":[]}}, {{\"Citation\":[]}}.",
        "prompt_template": "#Papers:\n{docs}\n\n{instruction}\n\n#The paper you need to analyze:\n{question}",
        "doc": [
            "2401.11817.md",
            "2404.18852.md",
            "2405.12910.md",
            "2401.07897.md"
        ],
        "answer": {
            "Reference": [
                "# The Pitfalls of Defining Hallucination "
            ],
            "Citation": [
                "# VERT: Verified Equivalent Rust Transpilation with Large Language Models as Few-Shot Learners ",
                "# Topic Modelling Case Law Using a Large Language Model and a New Taxonomy for UK Law: AI Insights into Summary Judgment "
            ]
        },
        "shuffle_doc": true,
        "id": "0d2c067c-04d6-4424-bf4d-87457b7b19da",
        "prompt": "[Question]\n#Papers:\n{docs}\n\nWe hope you will carefully study the provided papers and determine the citation relationships between them. Please follow the instructions below strictly to complete the task:\n\n#Specific Requirements:\n1. Reference: When a given paper mentions other provided papers, those other papers are considered as \"references\" for the given paper. To summarize in this specific context, references are about what the given paper is using.\n2. Citation: Conversely, when other provided papers mention the given paper in their works, the given paper is being \"cited\" by those other papers. To summarize in this specific context, citations are about who is using the given paper.\n3. Given a paper, you need to determine the citation or reference relationship between this paper and the other papers. Do not consider papers that are not provided.\n3. Please present the paper titles in a json format as follows: {{\"Reference\":[\"Reference Title 1\", \"Reference Title 2\", ..., \"Reference Title n\"], \"Citation\":[\"Citation Title 1\", \"Citation Title 2\", ..., \"Citation Title n\"]}}.\n4. If a paper does not have any references or citations, please leave the corresponding list empty, e.g.{{\"Refernce\":[]}}, {{\"Citation\":[]}}.\n\n#The paper you need to analyze:\nHallucination is Inevitable: <br> An Innate Limitation of Large Language Models\n\n[Gold Answer]\n{'Reference': ['# The Pitfalls of Defining Hallucination '], 'Citation': ['# VERT: Verified Equivalent Rust Transpilation with Large Language Models as Few-Shot Learners ', '# Topic Modelling Case Law Using a Large Language Model and a New Taxonomy for UK Law: AI Insights into Summary Judgment ']}\n\n[The Start of Assistant's Predicted Answer]\n{\"Reference\": [\"A Simple Recipe towards Reducing Hallucination in Neural Surface Realisation\", \"The Curious Case of Hallucinations in Neural Machine Translation\", \"The Troubling Emergence of Hallucination in Large Language Models - An Extensive Definition, Quantification, and Prescriptive Remediations\", \"Retrieval Augmentation Reduces Hallucination in Conversation\", \"Chain-of-Verification Reduces Hallucination in Large Language Models\", \"Don't Pour Cereal into Coffee: Differentiable Temporal Logic for Temporal Action Segmentation\", \"Exposing Attention Glitches with Flip-Flop Language Modeling\", \"Factual-nucleus sampling, proposed by Lee et al. [32]\", \"Chain-of-Thought Prompting Elicits Reasoning in Large Language Models\", \"Tree of Thoughts: Deliberate Problem Solving with Large Language Models\", \"Verify-andEdit: A Knowledge-Enhanced Chain-of-Thought Framework\"], \"Citation\": []}\n[The End of Assistant's Predicted Answer]\n\n[System]\nWe would like to request your feedback on the performance of the AI assistant in response to the user question displayed above according to the gold answer. Please use the following listed aspects and their descriptions as evaluation criteria:\n    - Accuracy and Hallucinations: The assistant's answer is semantically consistent with the gold answer; The numerical value and order need to be accurate, and there should be no hallucinations.\n    - Completeness: Referring to the reference answers, the assistant's answer should contain all the key points needed to answer the user's question; further elaboration on these key points can be omitted.\nPlease rate whether this answer is suitable for the question. Please note that the gold answer can be considered as a correct answer to the question.\n\nThe assistant receives an overall score on a scale of 1 to 100, where a higher score indicates better overall performance.\nPlease note that if the assistant's answer and the gold answer fully meet the above criteria, its overall rating should be the full marks (100).\nPlease first provide a comprehensive explanation of your evaluation, avoiding any potential bias.\nThen, output a line indicating the score of the Assistant.\n\nPLEASE OUTPUT WITH THE FOLLOWING FORMAT, WHERE THE SCORE IS A SCALE OF 1 TO 100 BY STRICTLY FOLLOWING THIS FORMAT: \"[[score]]\", FOR EXAMPLE \"Rating: [[100]]\":\n<start output>\nEvaluation evidence: your evluation explanation here, no more than 100 words\nRating: [[score]]\n<end output>\n\nNow, start your evaluation:",
        "generate_response": "{\"Reference\": [\"A Simple Recipe towards Reducing Hallucination in Neural Surface Realisation\", \"The Curious Case of Hallucinations in Neural Machine Translation\", \"The Troubling Emergence of Hallucination in Large Language Models - An Extensive Definition, Quantification, and Prescriptive Remediations\", \"Retrieval Augmentation Reduces Hallucination in Conversation\", \"Chain-of-Verification Reduces Hallucination in Large Language Models\", \"Don't Pour Cereal into Coffee: Differentiable Temporal Logic for Temporal Action Segmentation\", \"Exposing Attention Glitches with Flip-Flop Language Modeling\", \"Factual-nucleus sampling, proposed by Lee et al. [32]\", \"Chain-of-Thought Prompting Elicits Reasoning in Large Language Models\", \"Tree of Thoughts: Deliberate Problem Solving with Large Language Models\", \"Verify-andEdit: A Knowledge-Enhanced Chain-of-Thought Framework\"], \"Citation\": []}",
        "eval_response": "Evaluation evidence: The assistant's answer does not match the gold answer in terms of accuracy and completeness. The references listed by the assistant are entirely different from those in the gold answer, indicating a significant hallucination issue. Additionally, the assistant failed to identify any citations, whereas the gold answer lists two. This discrepancy shows a lack of accuracy in identifying the correct citation relationships and a failure to include all necessary information.\n\nRating: [[10]]",
        "score": 10
    },
    {
        "level": 3,
        "set": 3,
        "length": 104867,
        "type": "paper",
        "language": "en",
        "question": "Risk Taxonomy, Mitigation, and Assessment Benchmarks of Large Language Model Systems",
        "instruction": "We hope you will carefully study the provided papers and determine the citation relationships between them. Please follow the instructions below strictly to complete the task:\n\n#Specific Requirements:\n1. Reference: When a given paper mentions other provided papers, those other papers are considered as \"references\" for the given paper. To summarize in this specific context, references are about what the given paper is using.\n2. Citation: Conversely, when other provided papers mention the given paper in their works, the given paper is being \"cited\" by those other papers. To summarize in this specific context, citations are about who is using the given paper.\n3. Given a paper, you need to determine the citation or reference relationship between this paper and the other papers. Do not consider papers that are not provided.\n3. Please present the paper titles in a json format as follows: {{\"Reference\":[\"Reference Title 1\", \"Reference Title 2\", ..., \"Reference Title n\"], \"Citation\":[\"Citation Title 1\", \"Citation Title 2\", ..., \"Citation Title n\"]}}.\n4. If a paper does not have any references or citations, please leave the corresponding list empty, e.g.{{\"Refernce\":[]}}, {{\"Citation\":[]}}.",
        "prompt_template": "#Papers:\n{docs}\n\n{instruction}\n\n#The paper you need to analyze:\n{question}",
        "doc": [
            "2401.05778.md",
            "2404.01349.md",
            "2404.12736.md"
        ],
        "answer": {
            "Reference": [],
            "Citation": [
                "# Fairness in Large Language Models: A Taxonomic Survey ",
                "# Large Language Model Supply Chain: A Research Agenda "
            ]
        },
        "shuffle_doc": true,
        "id": "a3e6935d-cf8c-4d69-a49f-0797bfdef58f",
        "prompt": "[Question]\n#Papers:\n{docs}\n\nWe hope you will carefully study the provided papers and determine the citation relationships between them. Please follow the instructions below strictly to complete the task:\n\n#Specific Requirements:\n1. Reference: When a given paper mentions other provided papers, those other papers are considered as \"references\" for the given paper. To summarize in this specific context, references are about what the given paper is using.\n2. Citation: Conversely, when other provided papers mention the given paper in their works, the given paper is being \"cited\" by those other papers. To summarize in this specific context, citations are about who is using the given paper.\n3. Given a paper, you need to determine the citation or reference relationship between this paper and the other papers. Do not consider papers that are not provided.\n3. Please present the paper titles in a json format as follows: {{\"Reference\":[\"Reference Title 1\", \"Reference Title 2\", ..., \"Reference Title n\"], \"Citation\":[\"Citation Title 1\", \"Citation Title 2\", ..., \"Citation Title n\"]}}.\n4. If a paper does not have any references or citations, please leave the corresponding list empty, e.g.{{\"Refernce\":[]}}, {{\"Citation\":[]}}.\n\n#The paper you need to analyze:\nRisk Taxonomy, Mitigation, and Assessment Benchmarks of Large Language Model Systems\n\n[Gold Answer]\n{'Reference': [], 'Citation': ['# Fairness in Large Language Models: A Taxonomic Survey ', '# Large Language Model Supply Chain: A Research Agenda ']}\n\n[The Start of Assistant's Predicted Answer]\n{\"Reference\":[\"Large language models (LLMs) [1]-[5]\", \"Safety Preprompt [54], [211]\", \"Adjusting the Order of Pre-Defined Prompt [212]\", \"Changing Input Format [214]\", \"Keyword Matching [216]-[222]\", \"Content Classifier [223]-[225]\", \"Private Data Interventions [228]-[231]\", \"Privacy Enhanced Techniques [233]-[238]\", \"Detoxifying and Debiasing [256]-[264]\", \"Enhancing the Quality of Training Data [1], [4], [118], [267]\", \"Learning from Human Feedback [268]\", \"Exploiting External Knowledge [122], [178]-[184]\", \"Improving Decoding Strategies [113], [282]-[284]\", \"Multi-Agent Interaction [286]\", \"Defenses for Software Development Tools [320]-[323]\", \"Defenses for LLM Hardware Systems [164]-[166]\", \"Defenses for External Tools [350]-[353]\", \"Detection [358]-[361]\", \"Intervention [362]-[365]\", \"Watermarking [384]-[391]\", \"PromptBench [397]\", \"AdvGLUE [398]\", \"ANLI [399]\", \"GLUE-X [400]\", \"BOSS [401]\", \"HaDes [403]\", \"Wikibro [404]\", \"Med-HALT [405]\", \"HaluEval [406]\", \"Levy/Holt [128]\", \"TruthfulQA [105]\", \"Concept-7 [407]\", \"CommonClaim [408]\", \"HateXplain [409]\", \"TrustGPT [410]\", \"TOXIGEN [366]\", \"COLD [411]\", \"SafetyPrompts [51]\", \"CVALUES [412]\", \"FaiRLLM [413]\", \"BOLD [414]\", \"StereoSet [438]\", \"HOLISTICBIAS [415]\", \"CDail-Bias [416]\"], \"Citation\":[]}\n[The End of Assistant's Predicted Answer]\n\n[System]\nWe would like to request your feedback on the performance of the AI assistant in response to the user question displayed above according to the gold answer. Please use the following listed aspects and their descriptions as evaluation criteria:\n    - Accuracy and Hallucinations: The assistant's answer is semantically consistent with the gold answer; The numerical value and order need to be accurate, and there should be no hallucinations.\n    - Completeness: Referring to the reference answers, the assistant's answer should contain all the key points needed to answer the user's question; further elaboration on these key points can be omitted.\nPlease rate whether this answer is suitable for the question. Please note that the gold answer can be considered as a correct answer to the question.\n\nThe assistant receives an overall score on a scale of 1 to 100, where a higher score indicates better overall performance.\nPlease note that if the assistant's answer and the gold answer fully meet the above criteria, its overall rating should be the full marks (100).\nPlease first provide a comprehensive explanation of your evaluation, avoiding any potential bias.\nThen, output a line indicating the score of the Assistant.\n\nPLEASE OUTPUT WITH THE FOLLOWING FORMAT, WHERE THE SCORE IS A SCALE OF 1 TO 100 BY STRICTLY FOLLOWING THIS FORMAT: \"[[score]]\", FOR EXAMPLE \"Rating: [[100]]\":\n<start output>\nEvaluation evidence: your evluation explanation here, no more than 100 words\nRating: [[score]]\n<end output>\n\nNow, start your evaluation:",
        "generate_response": "{\"Reference\":[\"Large language models (LLMs) [1]-[5]\", \"Safety Preprompt [54], [211]\", \"Adjusting the Order of Pre-Defined Prompt [212]\", \"Changing Input Format [214]\", \"Keyword Matching [216]-[222]\", \"Content Classifier [223]-[225]\", \"Private Data Interventions [228]-[231]\", \"Privacy Enhanced Techniques [233]-[238]\", \"Detoxifying and Debiasing [256]-[264]\", \"Enhancing the Quality of Training Data [1], [4], [118], [267]\", \"Learning from Human Feedback [268]\", \"Exploiting External Knowledge [122], [178]-[184]\", \"Improving Decoding Strategies [113], [282]-[284]\", \"Multi-Agent Interaction [286]\", \"Defenses for Software Development Tools [320]-[323]\", \"Defenses for LLM Hardware Systems [164]-[166]\", \"Defenses for External Tools [350]-[353]\", \"Detection [358]-[361]\", \"Intervention [362]-[365]\", \"Watermarking [384]-[391]\", \"PromptBench [397]\", \"AdvGLUE [398]\", \"ANLI [399]\", \"GLUE-X [400]\", \"BOSS [401]\", \"HaDes [403]\", \"Wikibro [404]\", \"Med-HALT [405]\", \"HaluEval [406]\", \"Levy/Holt [128]\", \"TruthfulQA [105]\", \"Concept-7 [407]\", \"CommonClaim [408]\", \"HateXplain [409]\", \"TrustGPT [410]\", \"TOXIGEN [366]\", \"COLD [411]\", \"SafetyPrompts [51]\", \"CVALUES [412]\", \"FaiRLLM [413]\", \"BOLD [414]\", \"StereoSet [438]\", \"HOLISTICBIAS [415]\", \"CDail-Bias [416]\"], \"Citation\":[]}",
        "eval_response": "Evaluation evidence: The assistant's answer does not align with the gold answer. The gold answer specifies no references and two citations, while the assistant lists numerous references and no citations. This discrepancy indicates inaccuracies and hallucinations in the assistant's response, as it introduces information not present in the gold answer. The assistant's answer fails to meet the completeness criterion as it does not accurately reflect the citation relationships specified in the gold answer.\n\nRating: [[10]]",
        "score": 10
    },
    {
        "level": 3,
        "set": 3,
        "length": 125253,
        "type": "paper",
        "language": "en",
        "question": "A Comprehensive Study of Knowledge Editing for Large Language Models",
        "instruction": "We hope you will carefully study the provided papers and determine the citation relationships between them. Please follow the instructions below strictly to complete the task:\n\n#Specific Requirements:\n1. Reference: When a given paper mentions other provided papers, those other papers are considered as \"references\" for the given paper. To summarize in this specific context, references are about what the given paper is using.\n2. Citation: Conversely, when other provided papers mention the given paper in their works, the given paper is being \"cited\" by those other papers. To summarize in this specific context, citations are about who is using the given paper.\n3. Given a paper, you need to determine the citation or reference relationship between this paper and the other papers. Do not consider papers that are not provided.\n3. Please present the paper titles in a json format as follows: {{\"Reference\":[\"Reference Title 1\", \"Reference Title 2\", ..., \"Reference Title n\"], \"Citation\":[\"Citation Title 1\", \"Citation Title 2\", ..., \"Citation Title n\"]}}.\n4. If a paper does not have any references or citations, please leave the corresponding list empty, e.g.{{\"Refernce\":[]}}, {{\"Citation\":[]}}.",
        "prompt_template": "#Papers:\n{docs}\n\n{instruction}\n\n#The paper you need to analyze:\n{question}",
        "doc": [
            "2401.01286.md",
            "2405.15349.md",
            "2312.14302.md",
            "2405.02421.md"
        ],
        "answer": {
            "Reference": [
                "# Exploiting Novel GPT-4 APIs ",
                "# WHAT DOES THE KNOWLEDGE NEURON THESIS HAVE TO DO WITH KNOWLEDGE? "
            ],
            "Citation": [
                "# UnKE: Unstructured Knowledge Editing in Large Language Models "
            ]
        },
        "shuffle_doc": true,
        "id": "4edda79f-07de-4004-8159-fb06fc566012",
        "prompt": "[Question]\n#Papers:\n{docs}\n\nWe hope you will carefully study the provided papers and determine the citation relationships between them. Please follow the instructions below strictly to complete the task:\n\n#Specific Requirements:\n1. Reference: When a given paper mentions other provided papers, those other papers are considered as \"references\" for the given paper. To summarize in this specific context, references are about what the given paper is using.\n2. Citation: Conversely, when other provided papers mention the given paper in their works, the given paper is being \"cited\" by those other papers. To summarize in this specific context, citations are about who is using the given paper.\n3. Given a paper, you need to determine the citation or reference relationship between this paper and the other papers. Do not consider papers that are not provided.\n3. Please present the paper titles in a json format as follows: {{\"Reference\":[\"Reference Title 1\", \"Reference Title 2\", ..., \"Reference Title n\"], \"Citation\":[\"Citation Title 1\", \"Citation Title 2\", ..., \"Citation Title n\"]}}.\n4. If a paper does not have any references or citations, please leave the corresponding list empty, e.g.{{\"Refernce\":[]}}, {{\"Citation\":[]}}.\n\n#The paper you need to analyze:\nA Comprehensive Study of Knowledge Editing for Large Language Models\n\n[Gold Answer]\n{'Reference': ['# Exploiting Novel GPT-4 APIs ', '# WHAT DOES THE KNOWLEDGE NEURON THESIS HAVE TO DO WITH KNOWLEDGE? '], 'Citation': ['# UnKE: Unstructured Knowledge Editing in Large Language Models ']}\n\n[The Start of Assistant's Predicted Answer]\n{\"Reference\":[\"Bleu: a method for automatic evaluation of machine translation\", \"Rouge: A package for automatic evaluation of summaries\", \"Factscore: Fine-grained atomic evaluation of factual precision in long form text generation\", \"Llama 2: Open foundation and fine-tuned chat models\", \"Qwen technical report\"], \"Citation\":[\"Comprehensive Study of Knowledge Editing for Large Language Models\", \"A Survey on Evaluation of Large Language Models\", \"Vicuna: An Open-Source Chatbot Impressing GPT-4 with $90 \\% *$ ChatGPT Quality\", \"ChatLaw: Open-Source Legal Large Language Model with Integrated External Knowledge Bases\", \"Compositional Semantic Parsing with Large Language Models\", \"Open LLM Leaderboard\", \"How Close is ChatGPT to Human Experts? Comparison Corpus, Evaluation, and Detection\", \"REALM: Retrieval-Augmented Language Model PreTraining\", \"Rethinking with Retrieval: Faithful Large Language Model Inference\", \"Measuring Massive Multitask Language Understanding\", \"Atlas: Few-shot Learning with Retrieval Augmented Language Models\", \"Efficient Machine Learning\", \"AI-Generated Content (AIGC)\", \"Trustworthy AI\", \"Human-Computer Interaction: Personalized Agents\"]}\n[The End of Assistant's Predicted Answer]\n\n[System]\nWe would like to request your feedback on the performance of the AI assistant in response to the user question displayed above according to the gold answer. Please use the following listed aspects and their descriptions as evaluation criteria:\n    - Accuracy and Hallucinations: The assistant's answer is semantically consistent with the gold answer; The numerical value and order need to be accurate, and there should be no hallucinations.\n    - Completeness: Referring to the reference answers, the assistant's answer should contain all the key points needed to answer the user's question; further elaboration on these key points can be omitted.\nPlease rate whether this answer is suitable for the question. Please note that the gold answer can be considered as a correct answer to the question.\n\nThe assistant receives an overall score on a scale of 1 to 100, where a higher score indicates better overall performance.\nPlease note that if the assistant's answer and the gold answer fully meet the above criteria, its overall rating should be the full marks (100).\nPlease first provide a comprehensive explanation of your evaluation, avoiding any potential bias.\nThen, output a line indicating the score of the Assistant.\n\nPLEASE OUTPUT WITH THE FOLLOWING FORMAT, WHERE THE SCORE IS A SCALE OF 1 TO 100 BY STRICTLY FOLLOWING THIS FORMAT: \"[[score]]\", FOR EXAMPLE \"Rating: [[100]]\":\n<start output>\nEvaluation evidence: your evluation explanation here, no more than 100 words\nRating: [[score]]\n<end output>\n\nNow, start your evaluation:",
        "generate_response": "{\"Reference\":[\"Bleu: a method for automatic evaluation of machine translation\", \"Rouge: A package for automatic evaluation of summaries\", \"Factscore: Fine-grained atomic evaluation of factual precision in long form text generation\", \"Llama 2: Open foundation and fine-tuned chat models\", \"Qwen technical report\"], \"Citation\":[\"Comprehensive Study of Knowledge Editing for Large Language Models\", \"A Survey on Evaluation of Large Language Models\", \"Vicuna: An Open-Source Chatbot Impressing GPT-4 with $90 \\% *$ ChatGPT Quality\", \"ChatLaw: Open-Source Legal Large Language Model with Integrated External Knowledge Bases\", \"Compositional Semantic Parsing with Large Language Models\", \"Open LLM Leaderboard\", \"How Close is ChatGPT to Human Experts? Comparison Corpus, Evaluation, and Detection\", \"REALM: Retrieval-Augmented Language Model PreTraining\", \"Rethinking with Retrieval: Faithful Large Language Model Inference\", \"Measuring Massive Multitask Language Understanding\", \"Atlas: Few-shot Learning with Retrieval Augmented Language Models\", \"Efficient Machine Learning\", \"AI-Generated Content (AIGC)\", \"Trustworthy AI\", \"Human-Computer Interaction: Personalized Agents\"]}",
        "eval_response": "Evaluation evidence: The assistant's answer does not match the gold answer in terms of accuracy and completeness. The references and citations listed in the assistant's answer are entirely different from those in the gold answer, indicating a significant deviation and hallucination. The assistant failed to identify the correct references and citations for the given paper, which are crucial for the task.\n\nRating: [[10]]",
        "score": 10
    },
    {
        "level": 4,
        "set": 1,
        "length": 32738,
        "type": "paper",
        "language": "en",
        "question": "",
        "instruction": "We kindly ask you to thoroughly review the provided papers and construct a citation chain from them. Please adhere to the following instructions strictly while completing the task:\n\n#Task Instructions:\nGiven several papers, you are required to identify and list the longest citation chain, which demonstrates the citation relationship among the provided papers.\n\n#Specific Requirements:\n1.Please present the titles of the papers in the form of a list, as follows: [\"Title of Paper 1\", \"Title of Paper 2\", ..., \"Title of Paper n\"].\n2.Ensure that the citation chain in the list is linear and continuous, meaning that the first paper title in the list (Paper 1) should not cite any other works. Instead, it should be cited by the next paper in the list (Paper 2); subsequently, each paper should then be cited by the next one in the list, continuing up to the last paper (Paper n).\n3.Consider only the citation relationships within the supplied collection of papers, and ensure that the citation chain accurately reflects the sequential citation order among these documents.\n4.Do not take into account any articles not provided, and disregard other non-linear citation relationships.",
        "prompt_template": "{instruction}\n\n#Papers Provided:\n{docs}",
        "doc": [
            "2208.12615.md",
            "2405.05136.md",
            "2405.08908.md"
        ],
        "answer": [
            "# MonaCoBERT: Monotonic attention based ConvBERT for Knowledge Tracing ",
            "# INTEGRATING LSTM AND BERT FOR LONG-SEQUENCE DATA ANALYSIS IN INTELLIGENT TUTORING SYSTEMS * ",
            "# THE IMPACT OF 2D AND 3D GAMIFIED VR ON LEARNING AMERICAN SIGN LANGUAGE "
        ],
        "shuffle_doc": true,
        "id": "b517cfc0-9245-4f1e-81b0-61d8c0dad060",
        "prompt": "[Question]\nWe kindly ask you to thoroughly review the provided papers and construct a citation chain from them. Please adhere to the following instructions strictly while completing the task:\n\n#Task Instructions:\nGiven several papers, you are required to identify and list the longest citation chain, which demonstrates the citation relationship among the provided papers.\n\n#Specific Requirements:\n1.Please present the titles of the papers in the form of a list, as follows: [\"Title of Paper 1\", \"Title of Paper 2\", ..., \"Title of Paper n\"].\n2.Ensure that the citation chain in the list is linear and continuous, meaning that the first paper title in the list (Paper 1) should not cite any other works. Instead, it should be cited by the next paper in the list (Paper 2); subsequently, each paper should then be cited by the next one in the list, continuing up to the last paper (Paper n).\n3.Consider only the citation relationships within the supplied collection of papers, and ensure that the citation chain accurately reflects the sequential citation order among these documents.\n4.Do not take into account any articles not provided, and disregard other non-linear citation relationships.\n\n#Papers Provided:\n{docs}\n\n[Gold Answer]\n['# MonaCoBERT: Monotonic attention based ConvBERT for Knowledge Tracing ', '# INTEGRATING LSTM AND BERT FOR LONG-SEQUENCE DATA ANALYSIS IN INTELLIGENT TUTORING SYSTEMS * ', '# THE IMPACT OF 2D AND 3D GAMIFIED VR ON LEARNING AMERICAN SIGN LANGUAGE ']\n\n[The Start of Assistant's Predicted Answer]\n[\"MonaCoBERT: Monotonic attention based ConvBERT for Knowledge Tracing\", \"Convbert: Improving bert with span-based dynamic convolution\", \"Bert: Pre-training of deep bidirectional transformers for language understanding\", \"Attention is all you need\"]\n[The End of Assistant's Predicted Answer]\n\n[System]\nWe would like to request your feedback on the performance of the AI assistant in response to the user question displayed above according to the gold answer. Please use the following listed aspects and their descriptions as evaluation criteria:\n    - Accuracy and Hallucinations: The assistant's answer is semantically consistent with the gold answer; The numerical value and order need to be accurate, and there should be no hallucinations.\n    - Completeness: Referring to the reference answers, the assistant's answer should contain all the key points needed to answer the user's question; further elaboration on these key points can be omitted.\nPlease rate whether this answer is suitable for the question. Please note that the gold answer can be considered as a correct answer to the question.\n\nThe assistant receives an overall score on a scale of 1 to 100, where a higher score indicates better overall performance.\nPlease note that if the assistant's answer and the gold answer fully meet the above criteria, its overall rating should be the full marks (100).\nPlease first provide a comprehensive explanation of your evaluation, avoiding any potential bias.\nThen, output a line indicating the score of the Assistant.\n\nPLEASE OUTPUT WITH THE FOLLOWING FORMAT, WHERE THE SCORE IS A SCALE OF 1 TO 100 BY STRICTLY FOLLOWING THIS FORMAT: \"[[score]]\", FOR EXAMPLE \"Rating: [[100]]\":\n<start output>\nEvaluation evidence: your evluation explanation here, no more than 100 words\nRating: [[score]]\n<end output>\n\nNow, start your evaluation:",
        "generate_response": "[\"MonaCoBERT: Monotonic attention based ConvBERT for Knowledge Tracing\", \"Convbert: Improving bert with span-based dynamic convolution\", \"Bert: Pre-training of deep bidirectional transformers for language understanding\", \"Attention is all you need\"]",
        "eval_response": "Evaluation evidence: The assistant's answer does not match the gold answer in terms of the titles of the papers and their citation chain. The titles listed by the assistant are not present in the gold answer, indicating a misunderstanding or misinterpretation of the task. The assistant's response fails to accurately reflect the citation chain among the provided papers, leading to a significant deviation from the task requirements.\n\nRating: [[10]]",
        "score": 10
    },
    {
        "level": 3,
        "set": 3,
        "length": 117369,
        "type": "paper",
        "language": "en",
        "question": "Contrastive Preference Optimization: Pushing the Boundaries of LLM Performance in Machine Translation",
        "instruction": "We hope you will carefully study the provided papers and determine the citation relationships between them. Please follow the instructions below strictly to complete the task:\n\n#Specific Requirements:\n1. Reference: When a given paper mentions other provided papers, those other papers are considered as \"references\" for the given paper. To summarize in this specific context, references are about what the given paper is using.\n2. Citation: Conversely, when other provided papers mention the given paper in their works, the given paper is being \"cited\" by those other papers. To summarize in this specific context, citations are about who is using the given paper.\n3. Given a paper, you need to determine the citation or reference relationship between this paper and the other papers. Do not consider papers that are not provided.\n3. Please present the paper titles in a json format as follows: {{\"Reference\":[\"Reference Title 1\", \"Reference Title 2\", ..., \"Reference Title n\"], \"Citation\":[\"Citation Title 1\", \"Citation Title 2\", ..., \"Citation Title n\"]}}.\n4. If a paper does not have any references or citations, please leave the corresponding list empty, e.g.{{\"Refernce\":[]}}, {{\"Citation\":[]}}.",
        "prompt_template": "#Papers:\n{docs}\n\n{instruction}\n\n#The paper you need to analyze:\n{question}",
        "doc": [
            "2401.08417.md",
            "2405.14734.md",
            "2405.16681.md",
            "2405.19534.md"
        ],
        "answer": {
            "Reference": [],
            "Citation": [
                "# SimPO: Simple Preference Optimization with a Reference-Free Reward ",
                "# Triple Preference Optimization: Achieving Better Alignment with Less Data in a Single Step Optimization ",
                "# Preference Learning Algorithms Do Not Learn Preference Rankings "
            ]
        },
        "shuffle_doc": true,
        "id": "e407d840-9268-48fb-b8d0-ef0af39bcef6",
        "prompt": "[Question]\n#Papers:\n{docs}\n\nWe hope you will carefully study the provided papers and determine the citation relationships between them. Please follow the instructions below strictly to complete the task:\n\n#Specific Requirements:\n1. Reference: When a given paper mentions other provided papers, those other papers are considered as \"references\" for the given paper. To summarize in this specific context, references are about what the given paper is using.\n2. Citation: Conversely, when other provided papers mention the given paper in their works, the given paper is being \"cited\" by those other papers. To summarize in this specific context, citations are about who is using the given paper.\n3. Given a paper, you need to determine the citation or reference relationship between this paper and the other papers. Do not consider papers that are not provided.\n3. Please present the paper titles in a json format as follows: {{\"Reference\":[\"Reference Title 1\", \"Reference Title 2\", ..., \"Reference Title n\"], \"Citation\":[\"Citation Title 1\", \"Citation Title 2\", ..., \"Citation Title n\"]}}.\n4. If a paper does not have any references or citations, please leave the corresponding list empty, e.g.{{\"Refernce\":[]}}, {{\"Citation\":[]}}.\n\n#The paper you need to analyze:\nContrastive Preference Optimization: Pushing the Boundaries of LLM Performance in Machine Translation\n\n[Gold Answer]\n{'Reference': [], 'Citation': ['# SimPO: Simple Preference Optimization with a Reference-Free Reward ', '# Triple Preference Optimization: Achieving Better Alignment with Less Data in a Single Step Optimization ', '# Preference Learning Algorithms Do Not Learn Preference Rankings ']}\n\n[The Start of Assistant's Predicted Answer]\n{\"Reference\":[\"A Survey on Evaluation of Large Language Models\", \"Vicuna: An Open-Source Chatbot Impressing GPT-4 with $90 \\% *$ ChatGPT Quality\", \"ChatLaw: Open-Source Legal Large Language Model with Integrated External Knowledge Bases\", \"Compositional Semantic Parsing with Large Language Models\", \"Open LLM Leaderboard\", \"How Close is ChatGPT to Human Experts? Comparison Corpus, Evaluation, and Detection\", \"REALM: Retrieval-Augmented Language Model PreTraining\", \"Rethinking with Retrieval: Faithful Large Language Model Inference\", \"Measuring Massive Multitask Language Understanding\", \"Atlas: Few-shot Learning with Retrieval Augmented Language Models\", \"Attention is all you need\", \"Beyond english-centric multilingual machine translation\", \"Improving translation faithfulness of large language models via augmenting instructions\", \"Small data, big impact: Leveraging minimal data for effective machine translation\", \"No language left behind: Scaling human-centered machine translation\", \"Representation learning with contrastive predictive coding\", \"Gpt-4 technical report\", \"Training language models to follow instructions with human feedback\", \"Multilingual and document-level large audited dataset\", \"Eliciting the translation ability of large language models via multilingual finetuning with translation instructions\", \"Multilingual representation distillation with contrastive learning\", \"Llama: Open and efficient foundation language models\", \"Llama 2: Open foundation and finetuned chat models\", \"Exploring prompt engineering with GPT language models for document-level machine translation: Insights and findings\", \"Bigtrans: Augmenting large language models with multilingual translation capability over 100 languages\", \"Bayling: Bridging cross-lingual alignment and instruction following through interactive translation for large language models\", \"Multilingual machine translation with large language models: Empirical results and analysis\", \"Extrapolating large language models to non-english by aligning languages\"], \"Citation\":[\"Preference Learning Algorithms Do Not Learn Preference Rankings\", \"Some Things Are More Cringe Than Others: Preference Optimization with the Pairwise Cringe Loss\", \"Is DPO Superior to PPO for LLM Alignment? A Comprehensive Study\"]}\n[The End of Assistant's Predicted Answer]\n\n[System]\nWe would like to request your feedback on the performance of the AI assistant in response to the user question displayed above according to the gold answer. Please use the following listed aspects and their descriptions as evaluation criteria:\n    - Accuracy and Hallucinations: The assistant's answer is semantically consistent with the gold answer; The numerical value and order need to be accurate, and there should be no hallucinations.\n    - Completeness: Referring to the reference answers, the assistant's answer should contain all the key points needed to answer the user's question; further elaboration on these key points can be omitted.\nPlease rate whether this answer is suitable for the question. Please note that the gold answer can be considered as a correct answer to the question.\n\nThe assistant receives an overall score on a scale of 1 to 100, where a higher score indicates better overall performance.\nPlease note that if the assistant's answer and the gold answer fully meet the above criteria, its overall rating should be the full marks (100).\nPlease first provide a comprehensive explanation of your evaluation, avoiding any potential bias.\nThen, output a line indicating the score of the Assistant.\n\nPLEASE OUTPUT WITH THE FOLLOWING FORMAT, WHERE THE SCORE IS A SCALE OF 1 TO 100 BY STRICTLY FOLLOWING THIS FORMAT: \"[[score]]\", FOR EXAMPLE \"Rating: [[100]]\":\n<start output>\nEvaluation evidence: your evluation explanation here, no more than 100 words\nRating: [[score]]\n<end output>\n\nNow, start your evaluation:",
        "generate_response": "{\"Reference\":[\"A Survey on Evaluation of Large Language Models\", \"Vicuna: An Open-Source Chatbot Impressing GPT-4 with $90 \\% *$ ChatGPT Quality\", \"ChatLaw: Open-Source Legal Large Language Model with Integrated External Knowledge Bases\", \"Compositional Semantic Parsing with Large Language Models\", \"Open LLM Leaderboard\", \"How Close is ChatGPT to Human Experts? Comparison Corpus, Evaluation, and Detection\", \"REALM: Retrieval-Augmented Language Model PreTraining\", \"Rethinking with Retrieval: Faithful Large Language Model Inference\", \"Measuring Massive Multitask Language Understanding\", \"Atlas: Few-shot Learning with Retrieval Augmented Language Models\", \"Attention is all you need\", \"Beyond english-centric multilingual machine translation\", \"Improving translation faithfulness of large language models via augmenting instructions\", \"Small data, big impact: Leveraging minimal data for effective machine translation\", \"No language left behind: Scaling human-centered machine translation\", \"Representation learning with contrastive predictive coding\", \"Gpt-4 technical report\", \"Training language models to follow instructions with human feedback\", \"Multilingual and document-level large audited dataset\", \"Eliciting the translation ability of large language models via multilingual finetuning with translation instructions\", \"Multilingual representation distillation with contrastive learning\", \"Llama: Open and efficient foundation language models\", \"Llama 2: Open foundation and finetuned chat models\", \"Exploring prompt engineering with GPT language models for document-level machine translation: Insights and findings\", \"Bigtrans: Augmenting large language models with multilingual translation capability over 100 languages\", \"Bayling: Bridging cross-lingual alignment and instruction following through interactive translation for large language models\", \"Multilingual machine translation with large language models: Empirical results and analysis\", \"Extrapolating large language models to non-english by aligning languages\"], \"Citation\":[\"Preference Learning Algorithms Do Not Learn Preference Rankings\", \"Some Things Are More Cringe Than Others: Preference Optimization with the Pairwise Cringe Loss\", \"Is DPO Superior to PPO for LLM Alignment? A Comprehensive Study\"]}",
        "eval_response": "Evaluation evidence: The assistant's answer does not align with the gold answer in terms of accuracy and completeness. The gold answer specifies that there are no references and lists three citations, whereas the assistant's answer incorrectly lists numerous references and different citations. This discrepancy indicates a significant error in understanding or processing the task requirements, leading to both inaccurate and incomplete responses.\n\nRating: [[10]]",
        "score": 10
    },
    {
        "level": 4,
        "set": 3,
        "length": 109959,
        "type": "paper",
        "language": "en",
        "question": "",
        "instruction": "We kindly ask you to thoroughly review the provided papers and construct a citation chain from them. Please adhere to the following instructions strictly while completing the task:\n\n#Task Instructions:\nGiven several papers, you are required to identify and list the longest citation chain, which demonstrates the citation relationship among the provided papers.\n\n#Specific Requirements:\n1.Please present the titles of the papers in the form of a list, as follows: [\"Title of Paper 1\", \"Title of Paper 2\", ..., \"Title of Paper n\"].\n2.Ensure that the citation chain in the list is linear and continuous, meaning that the first paper title in the list (Paper 1) should not cite any other works. Instead, it should be cited by the next paper in the list (Paper 2); subsequently, each paper should then be cited by the next one in the list, continuing up to the last paper (Paper n).\n3.Consider only the citation relationships within the supplied collection of papers, and ensure that the citation chain accurately reflects the sequential citation order among these documents.\n4.Do not take into account any articles not provided, and disregard other non-linear citation relationships.",
        "prompt_template": "{instruction}\n\n#Papers Provided:\n{docs}",
        "doc": [
            "2302.09751.md",
            "2312.15791.md",
            "2403.07059.md",
            "2405.07770.md"
        ],
        "answer": [
            "# VQE-generated Quantum Circuit Dataset for Machine Learning ",
            "# SantaQlaus: A resource-efficient method to leverage quantum shot-noise for optimization of variational quantum algorithms ",
            "# Better than classical? The subtle art of benchmarking quantum machine learning models ",
            "# Hype or Heuristic? Quantum Reinforcement Learning for Join Order Optimisation "
        ],
        "shuffle_doc": true,
        "id": "119c5961-858a-42f9-83eb-0a9458d0b5f5",
        "prompt": "[Question]\nWe kindly ask you to thoroughly review the provided papers and construct a citation chain from them. Please adhere to the following instructions strictly while completing the task:\n\n#Task Instructions:\nGiven several papers, you are required to identify and list the longest citation chain, which demonstrates the citation relationship among the provided papers.\n\n#Specific Requirements:\n1.Please present the titles of the papers in the form of a list, as follows: [\"Title of Paper 1\", \"Title of Paper 2\", ..., \"Title of Paper n\"].\n2.Ensure that the citation chain in the list is linear and continuous, meaning that the first paper title in the list (Paper 1) should not cite any other works. Instead, it should be cited by the next paper in the list (Paper 2); subsequently, each paper should then be cited by the next one in the list, continuing up to the last paper (Paper n).\n3.Consider only the citation relationships within the supplied collection of papers, and ensure that the citation chain accurately reflects the sequential citation order among these documents.\n4.Do not take into account any articles not provided, and disregard other non-linear citation relationships.\n\n#Papers Provided:\n{docs}\n\n[Gold Answer]\n['# VQE-generated Quantum Circuit Dataset for Machine Learning ', '# SantaQlaus: A resource-efficient method to leverage quantum shot-noise for optimization of variational quantum algorithms ', '# Better than classical? The subtle art of benchmarking quantum machine learning models ', '# Hype or Heuristic? Quantum Reinforcement Learning for Join Order Optimisation ']\n\n[The Start of Assistant's Predicted Answer]\n[\"Quantum Computation and Quantum Information - 10th Anniversary Edition\", \"A variational eigenvalue solver on a photonic quantum processor\", \"Scalable quantum simulation of molecular energies\", \"Hybrid quantum-classical approach to correlated materials\", \"Hardware-efficient variational quantum eigensolver for small molecules and quantum magnets\", \"A Quantum Approximate Optimization Algorithm\", \"Quantum Supremacy through the Quantum Approximate Optimization Algorithm\", \"Unsupervised Machine Learning on a Hybrid Quantum Computer\", \"Quantum circuit learning\", \"Classification with Quantum Neural Networks on Near Term Processors\", \"Quantum machine learning in feature hilbert spaces\", \"Supervised Learning with Quantum Computers\", \"Quantum machine learning beyond kernel methods\", \"Classical surrogates for quantum learning models\", \"Barren plateaus in quantum neu- ral network training landscapes\", \"Noise-induced barren plateaus in variational quantum algorithms\", \"Effect of barren plateaus on gradient-free optimization\", \"On barren plateaus and cost function locality in variational quantum algorithms\", \"Trainability of dissipative perceptron-based quantum neural networks\", \"Entanglement-induced barren plateaus\", \"Connecting ansatz expressibility to gradient magnitudes and barren plateaus\", \"Absence of barren plateaus in quantum convolutional neural networks\", \"Toward Trainability of Deep Quantum Neural Networks\", \"Entanglement devised barren plateau mitigation\", \"Adam: A method for stochastic optimization\", \"A simplex method for function minimization\", \"An efficient method for finding the minimum of a function of several variables without calculating derivatives\", \"A stochastic approximation technique for generating maximum likelihood parameter estimates\", \"Classical optimizers for noisy intermediatescale quantum devices\", \"An Adaptive Optimizer for Measurement-Frugal Variational Algorithms\", \"Variational quantum state diagonalization\", \"Operator Sampling for Shot-frugal Optimization in Variational Algorithms\", \"Quantum Natural Gradient\", \"Quantum natural gradient generalized to noisy and nonunitary circuits\", \"Sequential minimal optimization for quantum-classical hybrid algorithms\", \"A Jacobi Diagonalization and Anderson Acceleration Algorithm For Variational Quantum Algorithm Parameter Optimization\", \"Stochastic gradient descent for hybrid quantumclassical optimization\", \"Stochastic gradient line bayesian optimization for efficient noise-robust optimization of parameterized quantum circuits\", \"Fast gradient estimation for variational quantum algorithms\", \"Resource frugal optimizer for quantum machine learning\", \"Latency-aware adaptive shot allocation for run-time efficient variational quantum algorithms\", \"Bridging the gap between stochastic gradient mcmc and stochastic optimization\", \"Machine learning in a quantum world\", \"Quantumenhanced machine learning\", \"New trends in quantum machine learning(a)\", \"VQE-generated Quantum Circuit Dataset for Machine Learning\", \"Data re-uploading for a universal quantum classifier\", \"Effect of data encoding on the expressive power of variational quantum-machine-learning models\", \"Encoding-dependent generalization bounds for parametrized quantum circuits\", \"Trainability of dissipative perceptron-based quantum neural networks\", \"Training deep quantum neural networks\", \"Quantum convolutional neural networks\", \"QVECTOR: an algorithm for device-tailored quantum error correction\", \"Low-depth gradient measurements can improve convergence in variational hybrid quantum-classical algorithms\", \"General parameter-shift rules for quantum gradients\", \"Evaluating analytic gradients on quantum hardware\", \"Quantum autoencoders for efficient compression of quantum data\", \"Can cross entropy loss be robust to label noise?\", \"Predicting many properties of a quantum system from very few measurements\", \"Efficient evaluation of quantum observables using entangled measurements\", \"Efficient quantum measurement of Pauli operators in the presence of finite sampling error\", \"Quantum chemistry calculations on a trapped-ion quantum simulator\", \"Unitary partitioning approach to the measurement problem in the variational quantum eigensolver method\", \"Quantum computed moments correction to variational estimates\", \"Measurement optimization in the variational quantum eigensolver using a minimum clique cover\", \"Measurement reduction in variational quantum algorithms\", \"Overlapped grouping measurement: A unified framework for measuring quantum states\", \"A Class of Statistics with Asymptotically Normal Distribution\", \"Pac-bayesian stochastic model selection\", \"Minimum complexity density estimation\", \"On bayesian consistency\", \"From $\\epsilon$-entropy to kl-entropy: Analysis of minimum information complexity density estimation\", \"Learning bounds for a generalized family of bayesian posterior distributions\", \"Stochastic relaxation, gibbs distributions, and the bayesian restoration of images\", \"Bayesian learning via stochastic gradient langevin dynamics\", \"Stochastic gradient hamiltonian monte carlo\", \"On the importance of initialization and momentum in deep learning\", \"Bayesian sampling using stochastic gradient thermostats\", \"Scalable deep poisson factor analysis for topic modeling\", \"A unified formulation of the constant temperature molecular dynamics methods\", \"Canonical dynamics: Equilibrium phase-space distributions\", \"Equilibrated adaptive learning rates for non-convex optimization\", \"Preconditioned stochastic gradient descent\", \"Online Second Order Methods for NonConvex Stochastic Optimizations\", \"Adaptive subgradient methods for online learning and stochastic optimization\", \"Lecture 6.5 - rmsprop: Divide the gradient by a running average of its recent magnitude\", \"Riemann manifold langevin and hamiltonian monte carlo methods\", \"Stochastic gradient riemannian langevin dynamics on the probability simplex\", \"Preconditioned stochastic gradient langevin dynamics for deep neural networks\", \"Stochastic Stability of Differential Equations\", \"Exploration of the (non-)asymptotic bias and variance of stochastic gradient langevin dynamics\", \"On the convergence of stochastic gradient mcmc algorithms with high-order integrators\", \"High-order stochastic gradient thermostats for bayesian learning of deep models\", \"General forms of finite population central limit theorems with applications to causal inference\", \"Nonparametrics: Statistical Methods Based on Ranks\", \"Efficient variational quantum simulator incorporating active error minimization\", \"Hybrid quantum-classical algorithms and quantum error mitigation\", \"Error mitigation for short-depth quantum circuits\", \"Practical quantum error mitigation for near-future applications\", \"Error mitigation with Clifford quantum-circuit data\", \"Learning-based quantum error mitigation\", \"Probabilistic error cancellation with sparse pauli-lindblad models on noisy quantum processors\", \"Error mitigation extends computational reach of a noisy quantum processor\", \"Resource-efficient Purification-based Quantum Error Mitigation\", \"Extending quantum probabilistic error cancellation by noise scaling\", \"Scalable mitigation of measurement errors on quantum computers\", \"Efficient quantum readout-error mitigation for sparse measurement outcomes of near-term quantum devices\", \"Quantum Error Mitigation\", \"Evidence for the utility of quantum computing before fault tolerance\", \"Noise resilience of variational quantum compiling\", \"Evaluating the noise resilience of variational quantum algorithms\", \"Can Error Mitigation Improve Trainability of Noisy Variational Quantum Algorithms?\", \"Error-mitigation-aided optimization of parameterized quantum circuits: Convergence analysis\", \"Quantumnat: Quantum noise-aware training with noise injection, quantization and normalization\", \"Exponentially tighter bounds on limitations of quantum error mitigation\", \"Universal cost bound of quantum error mitigation based on quantum estimation theory\", \"Universal sampling lower bounds for quantum error mitigation\", \"Fundamental limits of quantum error mitigation\", \"Qulacs: a fast and versatile quantum circuit simulator for research purpose\", \"Fashion-MNIST: a Novel Image Dataset for Benchmarking Machine Learning Algorithms\", \"The species problem in iris\", \"Heuristic and randomized optimization for the join ordering problem\", \"Query simplification: Graceful degradation for joinorder optimization\", \"Adaptive optimization of very large join queries\", \"Solving the join ordering problem via mixed integer linear programming\", \"Dependency-aware reordering for parallelizing query optimization in multi-core cpus\", \"Join query optimization techniques for complex event processing applications\", \"Query join ordering optimization with evolutionary multi-agent systems\", \"Query optimization through the looking glass, and what we found running the join order benchmark\", \"Building query compilers\", \"On the complexity of generating optimal left-deep processing trees with cross products\", \"Optimization of large join queries: Combining heuristics and combinatorial techniques\", \"Optimization of nonrecursive queries\", \"Access path selection in a relational database management system\", \"Deep reinforcement learning for join order enumeration\", \"Neo: A learned query optimizer\", \"Learning to optimize join queries with deep reinforcement learning\", \"SkinnerDB: Regret-bounded query evaluation via reinforcement learning\", \"Efficient join order selection learning with graphbased representation\", \"Reinforcement learning with tree-lstm for join order selection\", \"Adopt: Adaptively optimizing attribute orders for worst-case optimal join algorithms via reinforcement learning\", \"Query join order optimization method based on dynamic double deep q-network\", \"Polynomial-time algorithms for prime factorization and discrete logarithms on a quantum computer\", \"A fast quantum mechanical algorithm for database search\", \"Quantum Computing in the NISQ era and beyond\", \"Effects of imperfections on quantum algorithms: A software engineering perspective\", \"An in-principle super-polynomial quantum advantage for approximating combinatorial optimization problems\", \"A quantum approximate optimization algorithm\", \"The theory of variational hybrid quantumclassical algorithms\", \"Variational quantum algorithms\", \"Disentangling hype from practicality: On realistically achieving quantum advantage\", \"Variational quantum circuits for deep reinforcement learning\", \"Quantum agents in the Gym: A variational quantum algorithm for deep Q-learning\", \"Uncovering instabilities in variational-quantum deep q-networks\", \"Data compression for quantum machine learning\", \"Quantum advantage in learning from experiments\", \"Expressive power of parametrized quantum circuits\", \"Power of data in quantum machine learning\", \"A rigorous and robust quantum speed-up in supervised machine learning\", \"On the Quantum versus Classical Learnability of Discrete Distributions\", \"Supervised learning with quantum-enhanced feature spaces\", \"Reinforcement learning with quantum variational circuit\", \"Parametrized quantum policies for reinforcement learning\", \"Opportunities for quantum acceleration of databases: Optimization of queries and transaction schedules\", \"Is quantum advantage the right goal for quantum machine learning?\", \"A survey on quantum reinforcement learning\", \"Quantum machine learning for join order optimization using variational quantum circuits\", \"1-2-3 reproducibility for quantum software experiments\", \"GPU-accelerated dynamic programming for join-order optimization\", \"Analysis of two existing and one new dynamic programming algorithm for the generation of optimal bushy join trees without cross products\", \"Rapid bushy join-order optimization with cartesian products\", \"Dynamic programming strikes back\", \"A genetic algorithm for database query optimization\", \"Polynomial heuristics for query optimization\", \"Randomized algorithms for optimizing large join queries\", \"Parallelizing query optimization on shared-nothing architectures\", \"Cardinality estimation in DBMS: A comprehensive benchmark evaluation\", \"Learned cardinality estimation: An in-depth study\", \"A machine learning approach to sparql query performance prediction\", \"Learning-based query performance modeling and prediction\", \"Multiple query optimization on the dwave 2x adiabatic quantum computer\", \"Quantum machine learning: Foundation, new techniques, and opportunities for database research\", \"Optimizing transaction schedules on universal quantum computers via code generation for grover's search algorithm\", \"Hardware accelerating the optimization of transaction schedules via quantum annealing by avoiding blocking\", \"Avoiding blocking by scheduling transactions using quantum annealing\", \"Ready to leap (by co-design)? Join order optimisation on quantum hardware\", \"Quantum-inspired digital annealing for join ordering\", \"Constructing optimal bushy join trees by solving qubo problems on quantum hardware and simulators\", \"Quantum optimisation of general join trees\", \"Optimizing the performance of a relational algebra database interface\", \"Systematic literature review: Cost estimation in relational databases\", \"Reinforcement learning: An introduction\", \"Proximal policy optimization algorithms\", \"Data re-uploading for a universal quantum classifier\", \"Multilayer feedforward networks are universal approximators\", \"Quantum Computation and Quantum Information\", \"Evaluating analytic gradients on quantum hardware\", \"Data encoding patterns for quantum computing\", \"Encoding patterns for quantum algorithms\", \"Incremental data-uploading for full-quantum classification\", \"Effect of data encoding on the expressive power of variational quantum-machine-learning models\", \"Quantum policy gradient algorithm with optimized action decoding\", \"Playing atari with hybrid quantumclassical reinforcement learning\", \"The influence of reward on the speed of reinforcement learning: An analysis of shaping\", \"How good are query optimizers, really?\", \"TPC-DS, taking decision support benchmarking to the next level\", \"TPC Benchmark H Standard Specification\", \"Estimation of prediction error by using k-fold crossvalidation\", \"Github repository: AI4DBCode\", \"Github repository: Rejoin\", \"TensorFlow: Large-scale machine learning on heterogeneous systems\", \"Tensorflow Quantum: A software framework for quantum machine learning\", \"Qiskit: An open-source framework for quantum computing\", \"Better than classical? the subtle art of benchmarking quantum machine learning models\", \"Impact of quantum noise on the training of quantum generative adversarial networks\", \"Review on methods to fix number of hidden neurons in neural networks\", \"General parameter-shift rules for quantum gradients\", \"Optimizing quantum optimization algorithms via faster quantum gradient computation\", \"Guided-spsa: Simultaneous perturbation stochastic approximation assisted by the parameter shift rule\", \"Quantum natural gradient\", \"Fast gradient estimation for variational quantum algorithms\", \"Multivariate stochastic approximation using a simultaneous perturbation gradient approximation\", \"Deep learning: A comprehensive overview on techniques, taxonomy, applications and research directions\", \"The tail at scale\"]\n[The End of Assistant's Predicted Answer]\n\n[System]\nWe would like to request your feedback on the performance of the AI assistant in response to the user question displayed above according to the gold answer. Please use the following listed aspects and their descriptions as evaluation criteria:\n    - Accuracy and Hallucinations: The assistant's answer is semantically consistent with the gold answer; The numerical value and order need to be accurate, and there should be no hallucinations.\n    - Completeness: Referring to the reference answers, the assistant's answer should contain all the key points needed to answer the user's question; further elaboration on these key points can be omitted.\nPlease rate whether this answer is suitable for the question. Please note that the gold answer can be considered as a correct answer to the question.\n\nThe assistant receives an overall score on a scale of 1 to 100, where a higher score indicates better overall performance.\nPlease note that if the assistant's answer and the gold answer fully meet the above criteria, its overall rating should be the full marks (100).\nPlease first provide a comprehensive explanation of your evaluation, avoiding any potential bias.\nThen, output a line indicating the score of the Assistant.\n\nPLEASE OUTPUT WITH THE FOLLOWING FORMAT, WHERE THE SCORE IS A SCALE OF 1 TO 100 BY STRICTLY FOLLOWING THIS FORMAT: \"[[score]]\", FOR EXAMPLE \"Rating: [[100]]\":\n<start output>\nEvaluation evidence: your evluation explanation here, no more than 100 words\nRating: [[score]]\n<end output>\n\nNow, start your evaluation:",
        "generate_response": "[\"Quantum Computation and Quantum Information - 10th Anniversary Edition\", \"A variational eigenvalue solver on a photonic quantum processor\", \"Scalable quantum simulation of molecular energies\", \"Hybrid quantum-classical approach to correlated materials\", \"Hardware-efficient variational quantum eigensolver for small molecules and quantum magnets\", \"A Quantum Approximate Optimization Algorithm\", \"Quantum Supremacy through the Quantum Approximate Optimization Algorithm\", \"Unsupervised Machine Learning on a Hybrid Quantum Computer\", \"Quantum circuit learning\", \"Classification with Quantum Neural Networks on Near Term Processors\", \"Quantum machine learning in feature hilbert spaces\", \"Supervised Learning with Quantum Computers\", \"Quantum machine learning beyond kernel methods\", \"Classical surrogates for quantum learning models\", \"Barren plateaus in quantum neu- ral network training landscapes\", \"Noise-induced barren plateaus in variational quantum algorithms\", \"Effect of barren plateaus on gradient-free optimization\", \"On barren plateaus and cost function locality in variational quantum algorithms\", \"Trainability of dissipative perceptron-based quantum neural networks\", \"Entanglement-induced barren plateaus\", \"Connecting ansatz expressibility to gradient magnitudes and barren plateaus\", \"Absence of barren plateaus in quantum convolutional neural networks\", \"Toward Trainability of Deep Quantum Neural Networks\", \"Entanglement devised barren plateau mitigation\", \"Adam: A method for stochastic optimization\", \"A simplex method for function minimization\", \"An efficient method for finding the minimum of a function of several variables without calculating derivatives\", \"A stochastic approximation technique for generating maximum likelihood parameter estimates\", \"Classical optimizers for noisy intermediatescale quantum devices\", \"An Adaptive Optimizer for Measurement-Frugal Variational Algorithms\", \"Variational quantum state diagonalization\", \"Operator Sampling for Shot-frugal Optimization in Variational Algorithms\", \"Quantum Natural Gradient\", \"Quantum natural gradient generalized to noisy and nonunitary circuits\", \"Sequential minimal optimization for quantum-classical hybrid algorithms\", \"A Jacobi Diagonalization and Anderson Acceleration Algorithm For Variational Quantum Algorithm Parameter Optimization\", \"Stochastic gradient descent for hybrid quantumclassical optimization\", \"Stochastic gradient line bayesian optimization for efficient noise-robust optimization of parameterized quantum circuits\", \"Fast gradient estimation for variational quantum algorithms\", \"Resource frugal optimizer for quantum machine learning\", \"Latency-aware adaptive shot allocation for run-time efficient variational quantum algorithms\", \"Bridging the gap between stochastic gradient mcmc and stochastic optimization\", \"Machine learning in a quantum world\", \"Quantumenhanced machine learning\", \"New trends in quantum machine learning(a)\", \"VQE-generated Quantum Circuit Dataset for Machine Learning\", \"Data re-uploading for a universal quantum classifier\", \"Effect of data encoding on the expressive power of variational quantum-machine-learning models\", \"Encoding-dependent generalization bounds for parametrized quantum circuits\", \"Trainability of dissipative perceptron-based quantum neural networks\", \"Training deep quantum neural networks\", \"Quantum convolutional neural networks\", \"QVECTOR: an algorithm for device-tailored quantum error correction\", \"Low-depth gradient measurements can improve convergence in variational hybrid quantum-classical algorithms\", \"General parameter-shift rules for quantum gradients\", \"Evaluating analytic gradients on quantum hardware\", \"Quantum autoencoders for efficient compression of quantum data\", \"Can cross entropy loss be robust to label noise?\", \"Predicting many properties of a quantum system from very few measurements\", \"Efficient evaluation of quantum observables using entangled measurements\", \"Efficient quantum measurement of Pauli operators in the presence of finite sampling error\", \"Quantum chemistry calculations on a trapped-ion quantum simulator\", \"Unitary partitioning approach to the measurement problem in the variational quantum eigensolver method\", \"Quantum computed moments correction to variational estimates\", \"Measurement optimization in the variational quantum eigensolver using a minimum clique cover\", \"Measurement reduction in variational quantum algorithms\", \"Overlapped grouping measurement: A unified framework for measuring quantum states\", \"A Class of Statistics with Asymptotically Normal Distribution\", \"Pac-bayesian stochastic model selection\", \"Minimum complexity density estimation\", \"On bayesian consistency\", \"From $\\epsilon$-entropy to kl-entropy: Analysis of minimum information complexity density estimation\", \"Learning bounds for a generalized family of bayesian posterior distributions\", \"Stochastic relaxation, gibbs distributions, and the bayesian restoration of images\", \"Bayesian learning via stochastic gradient langevin dynamics\", \"Stochastic gradient hamiltonian monte carlo\", \"On the importance of initialization and momentum in deep learning\", \"Bayesian sampling using stochastic gradient thermostats\", \"Scalable deep poisson factor analysis for topic modeling\", \"A unified formulation of the constant temperature molecular dynamics methods\", \"Canonical dynamics: Equilibrium phase-space distributions\", \"Equilibrated adaptive learning rates for non-convex optimization\", \"Preconditioned stochastic gradient descent\", \"Online Second Order Methods for NonConvex Stochastic Optimizations\", \"Adaptive subgradient methods for online learning and stochastic optimization\", \"Lecture 6.5 - rmsprop: Divide the gradient by a running average of its recent magnitude\", \"Riemann manifold langevin and hamiltonian monte carlo methods\", \"Stochastic gradient riemannian langevin dynamics on the probability simplex\", \"Preconditioned stochastic gradient langevin dynamics for deep neural networks\", \"Stochastic Stability of Differential Equations\", \"Exploration of the (non-)asymptotic bias and variance of stochastic gradient langevin dynamics\", \"On the convergence of stochastic gradient mcmc algorithms with high-order integrators\", \"High-order stochastic gradient thermostats for bayesian learning of deep models\", \"General forms of finite population central limit theorems with applications to causal inference\", \"Nonparametrics: Statistical Methods Based on Ranks\", \"Efficient variational quantum simulator incorporating active error minimization\", \"Hybrid quantum-classical algorithms and quantum error mitigation\", \"Error mitigation for short-depth quantum circuits\", \"Practical quantum error mitigation for near-future applications\", \"Error mitigation with Clifford quantum-circuit data\", \"Learning-based quantum error mitigation\", \"Probabilistic error cancellation with sparse pauli-lindblad models on noisy quantum processors\", \"Error mitigation extends computational reach of a noisy quantum processor\", \"Resource-efficient Purification-based Quantum Error Mitigation\", \"Extending quantum probabilistic error cancellation by noise scaling\", \"Scalable mitigation of measurement errors on quantum computers\", \"Efficient quantum readout-error mitigation for sparse measurement outcomes of near-term quantum devices\", \"Quantum Error Mitigation\", \"Evidence for the utility of quantum computing before fault tolerance\", \"Noise resilience of variational quantum compiling\", \"Evaluating the noise resilience of variational quantum algorithms\", \"Can Error Mitigation Improve Trainability of Noisy Variational Quantum Algorithms?\", \"Error-mitigation-aided optimization of parameterized quantum circuits: Convergence analysis\", \"Quantumnat: Quantum noise-aware training with noise injection, quantization and normalization\", \"Exponentially tighter bounds on limitations of quantum error mitigation\", \"Universal cost bound of quantum error mitigation based on quantum estimation theory\", \"Universal sampling lower bounds for quantum error mitigation\", \"Fundamental limits of quantum error mitigation\", \"Qulacs: a fast and versatile quantum circuit simulator for research purpose\", \"Fashion-MNIST: a Novel Image Dataset for Benchmarking Machine Learning Algorithms\", \"The species problem in iris\", \"Heuristic and randomized optimization for the join ordering problem\", \"Query simplification: Graceful degradation for joinorder optimization\", \"Adaptive optimization of very large join queries\", \"Solving the join ordering problem via mixed integer linear programming\", \"Dependency-aware reordering for parallelizing query optimization in multi-core cpus\", \"Join query optimization techniques for complex event processing applications\", \"Query join ordering optimization with evolutionary multi-agent systems\", \"Query optimization through the looking glass, and what we found running the join order benchmark\", \"Building query compilers\", \"On the complexity of generating optimal left-deep processing trees with cross products\", \"Optimization of large join queries: Combining heuristics and combinatorial techniques\", \"Optimization of nonrecursive queries\", \"Access path selection in a relational database management system\", \"Deep reinforcement learning for join order enumeration\", \"Neo: A learned query optimizer\", \"Learning to optimize join queries with deep reinforcement learning\", \"SkinnerDB: Regret-bounded query evaluation via reinforcement learning\", \"Efficient join order selection learning with graphbased representation\", \"Reinforcement learning with tree-lstm for join order selection\", \"Adopt: Adaptively optimizing attribute orders for worst-case optimal join algorithms via reinforcement learning\", \"Query join order optimization method based on dynamic double deep q-network\", \"Polynomial-time algorithms for prime factorization and discrete logarithms on a quantum computer\", \"A fast quantum mechanical algorithm for database search\", \"Quantum Computing in the NISQ era and beyond\", \"Effects of imperfections on quantum algorithms: A software engineering perspective\", \"An in-principle super-polynomial quantum advantage for approximating combinatorial optimization problems\", \"A quantum approximate optimization algorithm\", \"The theory of variational hybrid quantumclassical algorithms\", \"Variational quantum algorithms\", \"Disentangling hype from practicality: On realistically achieving quantum advantage\", \"Variational quantum circuits for deep reinforcement learning\", \"Quantum agents in the Gym: A variational quantum algorithm for deep Q-learning\", \"Uncovering instabilities in variational-quantum deep q-networks\", \"Data compression for quantum machine learning\", \"Quantum advantage in learning from experiments\", \"Expressive power of parametrized quantum circuits\", \"Power of data in quantum machine learning\", \"A rigorous and robust quantum speed-up in supervised machine learning\", \"On the Quantum versus Classical Learnability of Discrete Distributions\", \"Supervised learning with quantum-enhanced feature spaces\", \"Reinforcement learning with quantum variational circuit\", \"Parametrized quantum policies for reinforcement learning\", \"Opportunities for quantum acceleration of databases: Optimization of queries and transaction schedules\", \"Is quantum advantage the right goal for quantum machine learning?\", \"A survey on quantum reinforcement learning\", \"Quantum machine learning for join order optimization using variational quantum circuits\", \"1-2-3 reproducibility for quantum software experiments\", \"GPU-accelerated dynamic programming for join-order optimization\", \"Analysis of two existing and one new dynamic programming algorithm for the generation of optimal bushy join trees without cross products\", \"Rapid bushy join-order optimization with cartesian products\", \"Dynamic programming strikes back\", \"A genetic algorithm for database query optimization\", \"Polynomial heuristics for query optimization\", \"Randomized algorithms for optimizing large join queries\", \"Parallelizing query optimization on shared-nothing architectures\", \"Cardinality estimation in DBMS: A comprehensive benchmark evaluation\", \"Learned cardinality estimation: An in-depth study\", \"A machine learning approach to sparql query performance prediction\", \"Learning-based query performance modeling and prediction\", \"Multiple query optimization on the dwave 2x adiabatic quantum computer\", \"Quantum machine learning: Foundation, new techniques, and opportunities for database research\", \"Optimizing transaction schedules on universal quantum computers via code generation for grover's search algorithm\", \"Hardware accelerating the optimization of transaction schedules via quantum annealing by avoiding blocking\", \"Avoiding blocking by scheduling transactions using quantum annealing\", \"Ready to leap (by co-design)? Join order optimisation on quantum hardware\", \"Quantum-inspired digital annealing for join ordering\", \"Constructing optimal bushy join trees by solving qubo problems on quantum hardware and simulators\", \"Quantum optimisation of general join trees\", \"Optimizing the performance of a relational algebra database interface\", \"Systematic literature review: Cost estimation in relational databases\", \"Reinforcement learning: An introduction\", \"Proximal policy optimization algorithms\", \"Data re-uploading for a universal quantum classifier\", \"Multilayer feedforward networks are universal approximators\", \"Quantum Computation and Quantum Information\", \"Evaluating analytic gradients on quantum hardware\", \"Data encoding patterns for quantum computing\", \"Encoding patterns for quantum algorithms\", \"Incremental data-uploading for full-quantum classification\", \"Effect of data encoding on the expressive power of variational quantum-machine-learning models\", \"Quantum policy gradient algorithm with optimized action decoding\", \"Playing atari with hybrid quantumclassical reinforcement learning\", \"The influence of reward on the speed of reinforcement learning: An analysis of shaping\", \"How good are query optimizers, really?\", \"TPC-DS, taking decision support benchmarking to the next level\", \"TPC Benchmark H Standard Specification\", \"Estimation of prediction error by using k-fold crossvalidation\", \"Github repository: AI4DBCode\", \"Github repository: Rejoin\", \"TensorFlow: Large-scale machine learning on heterogeneous systems\", \"Tensorflow Quantum: A software framework for quantum machine learning\", \"Qiskit: An open-source framework for quantum computing\", \"Better than classical? the subtle art of benchmarking quantum machine learning models\", \"Impact of quantum noise on the training of quantum generative adversarial networks\", \"Review on methods to fix number of hidden neurons in neural networks\", \"General parameter-shift rules for quantum gradients\", \"Optimizing quantum optimization algorithms via faster quantum gradient computation\", \"Guided-spsa: Simultaneous perturbation stochastic approximation assisted by the parameter shift rule\", \"Quantum natural gradient\", \"Fast gradient estimation for variational quantum algorithms\", \"Multivariate stochastic approximation using a simultaneous perturbation gradient approximation\", \"Deep learning: A comprehensive overview on techniques, taxonomy, applications and research directions\", \"The tail at scale\"]",
        "eval_response": "Evaluation evidence: The assistant's response does not align with the task requirements or the gold answer. The task was to identify and list the longest citation chain among the provided papers, but the assistant generated a lengthy list of unrelated paper titles, which does not reflect a citation chain. The response shows a significant deviation from the task's specific requirement to present a linear and continuous citation chain from the provided papers. There are also numerous hallucinations with titles not relevant to the task.\n\nRating: [[10]]",
        "score": 10
    },
    {
        "level": 4,
        "set": 2,
        "length": 99568,
        "type": "paper",
        "language": "en",
        "question": "",
        "instruction": "We kindly ask you to thoroughly review the provided papers and construct a citation chain from them. Please adhere to the following instructions strictly while completing the task:\n\n#Task Instructions:\nGiven several papers, you are required to identify and list the longest citation chain, which demonstrates the citation relationship among the provided papers.\n\n#Specific Requirements:\n1.Please present the titles of the papers in the form of a list, as follows: [\"Title of Paper 1\", \"Title of Paper 2\", ..., \"Title of Paper n\"].\n2.Ensure that the citation chain in the list is linear and continuous, meaning that the first paper title in the list (Paper 1) should not cite any other works. Instead, it should be cited by the next paper in the list (Paper 2); subsequently, each paper should then be cited by the next one in the list, continuing up to the last paper (Paper n).\n3.Consider only the citation relationships within the supplied collection of papers, and ensure that the citation chain accurately reflects the sequential citation order among these documents.\n4.Do not take into account any articles not provided, and disregard other non-linear citation relationships.",
        "prompt_template": "{instruction}\n\n#Papers Provided:\n{docs}",
        "doc": [
            "1901.05353.md",
            "2301.10813.md",
            "2405.09251.md"
        ],
        "answer": [
            "# A PRIMER ON PAC-BAYESIAN LEARNING ",
            "# Increasing Fairness via Combination with Learning Guarantees ",
            "# Does Machine Bring in Extra Bias in Learning? Approximating Fairness in Models Promptly "
        ],
        "shuffle_doc": true,
        "id": "b2dc9c73-47fd-416b-ad81-2abc14df6ef0",
        "prompt": "[Question]\nWe kindly ask you to thoroughly review the provided papers and construct a citation chain from them. Please adhere to the following instructions strictly while completing the task:\n\n#Task Instructions:\nGiven several papers, you are required to identify and list the longest citation chain, which demonstrates the citation relationship among the provided papers.\n\n#Specific Requirements:\n1.Please present the titles of the papers in the form of a list, as follows: [\"Title of Paper 1\", \"Title of Paper 2\", ..., \"Title of Paper n\"].\n2.Ensure that the citation chain in the list is linear and continuous, meaning that the first paper title in the list (Paper 1) should not cite any other works. Instead, it should be cited by the next paper in the list (Paper 2); subsequently, each paper should then be cited by the next one in the list, continuing up to the last paper (Paper n).\n3.Consider only the citation relationships within the supplied collection of papers, and ensure that the citation chain accurately reflects the sequential citation order among these documents.\n4.Do not take into account any articles not provided, and disregard other non-linear citation relationships.\n\n#Papers Provided:\n{docs}\n\n[Gold Answer]\n['# A PRIMER ON PAC-BAYESIAN LEARNING ', '# Increasing Fairness via Combination with Learning Guarantees ', '# Does Machine Bring in Extra Bias in Learning? Approximating Fairness in Models Promptly ']\n\n[The Start of Assistant's Predicted Answer]\n[\"A reductions approach to fair classification\", \"Fair prediction with disparate impact: A study of bias in recidivism prediction instruments\"]\n[The End of Assistant's Predicted Answer]\n\n[System]\nWe would like to request your feedback on the performance of the AI assistant in response to the user question displayed above according to the gold answer. Please use the following listed aspects and their descriptions as evaluation criteria:\n    - Accuracy and Hallucinations: The assistant's answer is semantically consistent with the gold answer; The numerical value and order need to be accurate, and there should be no hallucinations.\n    - Completeness: Referring to the reference answers, the assistant's answer should contain all the key points needed to answer the user's question; further elaboration on these key points can be omitted.\nPlease rate whether this answer is suitable for the question. Please note that the gold answer can be considered as a correct answer to the question.\n\nThe assistant receives an overall score on a scale of 1 to 100, where a higher score indicates better overall performance.\nPlease note that if the assistant's answer and the gold answer fully meet the above criteria, its overall rating should be the full marks (100).\nPlease first provide a comprehensive explanation of your evaluation, avoiding any potential bias.\nThen, output a line indicating the score of the Assistant.\n\nPLEASE OUTPUT WITH THE FOLLOWING FORMAT, WHERE THE SCORE IS A SCALE OF 1 TO 100 BY STRICTLY FOLLOWING THIS FORMAT: \"[[score]]\", FOR EXAMPLE \"Rating: [[100]]\":\n<start output>\nEvaluation evidence: your evluation explanation here, no more than 100 words\nRating: [[score]]\n<end output>\n\nNow, start your evaluation:",
        "generate_response": "[\"A reductions approach to fair classification\", \"Fair prediction with disparate impact: A study of bias in recidivism prediction instruments\"]",
        "eval_response": "Evaluation evidence: The assistant's answer does not match the gold answer and fails to identify the correct citation chain among the provided papers. The titles listed in the assistant's answer do not appear in the gold answer, indicating a significant accuracy issue and a misunderstanding of the task requirements. The answer is incomplete as it does not reflect the correct citation relationships among the provided papers.\n\nRating: [[10]]",
        "score": 10
    },
    {
        "level": 3,
        "set": 4,
        "length": 237711,
        "type": "paper",
        "language": "en",
        "question": "Patchscopes: A Unifying Framework for Inspecting Hidden Representations of Language Models",
        "instruction": "We hope you will carefully study the provided papers and determine the citation relationships between them. Please follow the instructions below strictly to complete the task:\n\n#Specific Requirements:\n1. Reference: When a given paper mentions other provided papers, those other papers are considered as \"references\" for the given paper. To summarize in this specific context, references are about what the given paper is using.\n2. Citation: Conversely, when other provided papers mention the given paper in their works, the given paper is being \"cited\" by those other papers. To summarize in this specific context, citations are about who is using the given paper.\n3. Given a paper, you need to determine the citation or reference relationship between this paper and the other papers. Do not consider papers that are not provided.\n3. Please present the paper titles in a json format as follows: {{\"Reference\":[\"Reference Title 1\", \"Reference Title 2\", ..., \"Reference Title n\"], \"Citation\":[\"Citation Title 1\", \"Citation Title 2\", ..., \"Citation Title n\"]}}.\n4. If a paper does not have any references or citations, please leave the corresponding list empty, e.g.{{\"Refernce\":[]}}, {{\"Citation\":[]}}.",
        "prompt_template": "#Papers:\n{docs}\n\n{instruction}\n\n#The paper you need to analyze:\n{question}",
        "doc": [
            "2401.06102.md",
            "2404.05971.md",
            "2404.14082.md",
            "2405.17653.md",
            "2405.17969.md",
            "2405.20778.md",
            "2310.15916.md",
            "2310.16570.md",
            "2310.18969.md",
            "2311.04897.md",
            "2402.14328.md"
        ],
        "answer": {
            "Reference": [
                "# In-Context Learning Creates Task Vectors ",
                "# Analyzing Vision Transformers for Image Classification in Class Embedding Space ",
                "# Future Lens: Anticipating Subsequent Tokens from a Single Hidden State ",
                "# Understanding and Patching Compositional Reasoning in LLMs "
            ],
            "Citation": [
                "# Does Transformer Interpretability Transfer to RNNs? ",
                "# Mechanistic Interpretability for AI Safety A Review ",
                "# InversionView: A General-Purpose Method for Reading Information from Neural Activations ",
                "# Knowledge Circuits in Pretrained Transformers ",
                "# Improved Generation of Adversarial Examples Against Safety-aligned LLMs "
            ]
        },
        "shuffle_doc": true,
        "id": "495a9af7-4d73-4fdf-88e1-e772040a895c",
        "prompt": "[Question]\n#Papers:\n{docs}\n\nWe hope you will carefully study the provided papers and determine the citation relationships between them. Please follow the instructions below strictly to complete the task:\n\n#Specific Requirements:\n1. Reference: When a given paper mentions other provided papers, those other papers are considered as \"references\" for the given paper. To summarize in this specific context, references are about what the given paper is using.\n2. Citation: Conversely, when other provided papers mention the given paper in their works, the given paper is being \"cited\" by those other papers. To summarize in this specific context, citations are about who is using the given paper.\n3. Given a paper, you need to determine the citation or reference relationship between this paper and the other papers. Do not consider papers that are not provided.\n3. Please present the paper titles in a json format as follows: {{\"Reference\":[\"Reference Title 1\", \"Reference Title 2\", ..., \"Reference Title n\"], \"Citation\":[\"Citation Title 1\", \"Citation Title 2\", ..., \"Citation Title n\"]}}.\n4. If a paper does not have any references or citations, please leave the corresponding list empty, e.g.{{\"Refernce\":[]}}, {{\"Citation\":[]}}.\n\n#The paper you need to analyze:\nPatchscopes: A Unifying Framework for Inspecting Hidden Representations of Language Models\n\n[Gold Answer]\n{'Reference': ['# In-Context Learning Creates Task Vectors ', '# Analyzing Vision Transformers for Image Classification in Class Embedding Space ', '# Future Lens: Anticipating Subsequent Tokens from a Single Hidden State ', '# Understanding and Patching Compositional Reasoning in LLMs '], 'Citation': ['# Does Transformer Interpretability Transfer to RNNs? ', '# Mechanistic Interpretability for AI Safety A Review ', '# InversionView: A General-Purpose Method for Reading Information from Neural Activations ', '# Knowledge Circuits in Pretrained Transformers ', '# Improved Generation of Adversarial Examples Against Safety-aligned LLMs ']}\n\n[The Start of Assistant's Predicted Answer]\n{\"Reference\":[\"Understanding intermediate layers using linear classifier probes\", \"Revisiting model stitching to compare neural representations\", \"Analyzing transformers in embedding space\", \"The Pile: An 800GB dataset of diverse text for language modeling\", \"Causal abstractions of neural networks\", \"How does GPT-2 compute greater-than?: Interpreting mathematical abilities in a pre-trained language model\", \"Does localization inform editing? surprising differences in causality-based localization vs. knowledge editing in language models\", \"In-context learning creates task vectors\", \"Natural language descriptions of deep features\", \"Measuring and manipulating knowledge representations in language models\", \"Linearity of relation decoding in transformer language models\", \"Understanding and patching compositional reasoning in llms\", \"Does circuit analysis interpretability scale? evidence from multiple choice capabilities in chinchilla\", \"ROUGE: A package for automatic evaluation of summaries\", \"Self-refine: Iterative refinement with self-feedback\", \"Locating and editing factual associations in gpt\", \"Mass-editing memory in a transformer\", \"Pointer sentinel mixture models\", \"The expressive power of transformers with chain of thought\", \"Linearly mapping from image to text space\", \"A mechanism for solving relational tasks in transformer language models\", \"Can llms facilitate interpretation of pre-trained language models?\", \"Mapping language models to grounded conceptual spaces\", \"Sentence-bert: Sentence embeddings using siamese bert-networks\", \"Multimodal neurons in pretrained text-only transformers\", \"Explaining black box text modules in natural language with language models\", \"A mechanistic interpretation of arithmetic reasoning in language models using causal mediation analysis\", \"Lstmvis: A tool for visual analysis of hidden state dynamics in recurrent neural networks\", \"Bert rediscovers the classical nlp pipeline\", \"Llama: Open and efficient foundation language models\", \"Llama 2: Open foundation and finetuned chat models\", \"Investigating gender bias in language models using causal mediation analysis\", \"GPT-J-6B: A 6 Billion Parameter Autoregressive Language Model\", \"Interpretability in the wild: a circuit for indirect object identification in GPT-2 small\", \"Gaussian Process Probes (GPP) for uncertainty-aware probing\", \"Chain-of-thought prompting elicits reasoning in large language models\", \"Tree of thoughts: Deliberate problem solving with large language models\", \"Give me the facts! a survey on factual knowledge probing in pre-trained language models\", \"Towards best practices of activation patching in language models: Metrics and methods\", \"MQuAKE: Assessing knowledge editing in language models via multi-hop questions\"], \"Citation\":[\"InversionView: A General-Purpose Method for Reading Information from Neural Activations\"]}\n[The End of Assistant's Predicted Answer]\n\n[System]\nWe would like to request your feedback on the performance of the AI assistant in response to the user question displayed above according to the gold answer. Please use the following listed aspects and their descriptions as evaluation criteria:\n    - Accuracy and Hallucinations: The assistant's answer is semantically consistent with the gold answer; The numerical value and order need to be accurate, and there should be no hallucinations.\n    - Completeness: Referring to the reference answers, the assistant's answer should contain all the key points needed to answer the user's question; further elaboration on these key points can be omitted.\nPlease rate whether this answer is suitable for the question. Please note that the gold answer can be considered as a correct answer to the question.\n\nThe assistant receives an overall score on a scale of 1 to 100, where a higher score indicates better overall performance.\nPlease note that if the assistant's answer and the gold answer fully meet the above criteria, its overall rating should be the full marks (100).\nPlease first provide a comprehensive explanation of your evaluation, avoiding any potential bias.\nThen, output a line indicating the score of the Assistant.\n\nPLEASE OUTPUT WITH THE FOLLOWING FORMAT, WHERE THE SCORE IS A SCALE OF 1 TO 100 BY STRICTLY FOLLOWING THIS FORMAT: \"[[score]]\", FOR EXAMPLE \"Rating: [[100]]\":\n<start output>\nEvaluation evidence: your evluation explanation here, no more than 100 words\nRating: [[score]]\n<end output>\n\nNow, start your evaluation:",
        "generate_response": "{\"Reference\":[\"Understanding intermediate layers using linear classifier probes\", \"Revisiting model stitching to compare neural representations\", \"Analyzing transformers in embedding space\", \"The Pile: An 800GB dataset of diverse text for language modeling\", \"Causal abstractions of neural networks\", \"How does GPT-2 compute greater-than?: Interpreting mathematical abilities in a pre-trained language model\", \"Does localization inform editing? surprising differences in causality-based localization vs. knowledge editing in language models\", \"In-context learning creates task vectors\", \"Natural language descriptions of deep features\", \"Measuring and manipulating knowledge representations in language models\", \"Linearity of relation decoding in transformer language models\", \"Understanding and patching compositional reasoning in llms\", \"Does circuit analysis interpretability scale? evidence from multiple choice capabilities in chinchilla\", \"ROUGE: A package for automatic evaluation of summaries\", \"Self-refine: Iterative refinement with self-feedback\", \"Locating and editing factual associations in gpt\", \"Mass-editing memory in a transformer\", \"Pointer sentinel mixture models\", \"The expressive power of transformers with chain of thought\", \"Linearly mapping from image to text space\", \"A mechanism for solving relational tasks in transformer language models\", \"Can llms facilitate interpretation of pre-trained language models?\", \"Mapping language models to grounded conceptual spaces\", \"Sentence-bert: Sentence embeddings using siamese bert-networks\", \"Multimodal neurons in pretrained text-only transformers\", \"Explaining black box text modules in natural language with language models\", \"A mechanistic interpretation of arithmetic reasoning in language models using causal mediation analysis\", \"Lstmvis: A tool for visual analysis of hidden state dynamics in recurrent neural networks\", \"Bert rediscovers the classical nlp pipeline\", \"Llama: Open and efficient foundation language models\", \"Llama 2: Open foundation and finetuned chat models\", \"Investigating gender bias in language models using causal mediation analysis\", \"GPT-J-6B: A 6 Billion Parameter Autoregressive Language Model\", \"Interpretability in the wild: a circuit for indirect object identification in GPT-2 small\", \"Gaussian Process Probes (GPP) for uncertainty-aware probing\", \"Chain-of-thought prompting elicits reasoning in large language models\", \"Tree of thoughts: Deliberate problem solving with large language models\", \"Give me the facts! a survey on factual knowledge probing in pre-trained language models\", \"Towards best practices of activation patching in language models: Metrics and methods\", \"MQuAKE: Assessing knowledge editing in language models via multi-hop questions\"], \"Citation\":[\"InversionView: A General-Purpose Method for Reading Information from Neural Activations\"]}",
        "eval_response": "Evaluation evidence: The assistant's answer does not match the gold answer in terms of accuracy and completeness. The references and citations listed are largely incorrect or hallucinated, not aligning with those specified in the gold answer. The assistant's response includes many titles that are not mentioned in the gold answer, indicating a significant deviation from the required task. This results in a failure to meet the criteria of accuracy, hallucinations, and completeness.\n\nRating: [[10]]",
        "score": 10
    },
    {
        "level": 4,
        "set": 1,
        "length": 28795,
        "type": "paper",
        "language": "en",
        "question": "",
        "instruction": "We kindly ask you to thoroughly review the provided papers and construct a citation chain from them. Please adhere to the following instructions strictly while completing the task:\n\n#Task Instructions:\nGiven several papers, you are required to identify and list the longest citation chain, which demonstrates the citation relationship among the provided papers.\n\n#Specific Requirements:\n1.Please present the titles of the papers in the form of a list, as follows: [\"Title of Paper 1\", \"Title of Paper 2\", ..., \"Title of Paper n\"].\n2.Ensure that the citation chain in the list is linear and continuous, meaning that the first paper title in the list (Paper 1) should not cite any other works. Instead, it should be cited by the next paper in the list (Paper 2); subsequently, each paper should then be cited by the next one in the list, continuing up to the last paper (Paper n).\n3.Consider only the citation relationships within the supplied collection of papers, and ensure that the citation chain accurately reflects the sequential citation order among these documents.\n4.Do not take into account any articles not provided, and disregard other non-linear citation relationships.",
        "prompt_template": "{instruction}\n\n#Papers Provided:\n{docs}",
        "doc": [
            "1709.03082.md",
            "1803.08375.md",
            "2405.07657.md"
        ],
        "answer": [
            "# A Neural Network Architecture Combining Gated Recurrent Unit (GRU) and Support Vector Machine (SVM) for Intrusion Detection in Network Traffic Data ",
            "# Deep Learning using Rectified Linear Units (ReLU) ",
            "# BEYOND TRADITIONAL MAGNETIC RESONANCE PROCESSING WITH ARTIFICIAL INTELLIGENCE * "
        ],
        "shuffle_doc": true,
        "id": "28f4bc5c-284c-4bf0-b844-35f3554c2c92",
        "prompt": "[Question]\nWe kindly ask you to thoroughly review the provided papers and construct a citation chain from them. Please adhere to the following instructions strictly while completing the task:\n\n#Task Instructions:\nGiven several papers, you are required to identify and list the longest citation chain, which demonstrates the citation relationship among the provided papers.\n\n#Specific Requirements:\n1.Please present the titles of the papers in the form of a list, as follows: [\"Title of Paper 1\", \"Title of Paper 2\", ..., \"Title of Paper n\"].\n2.Ensure that the citation chain in the list is linear and continuous, meaning that the first paper title in the list (Paper 1) should not cite any other works. Instead, it should be cited by the next paper in the list (Paper 2); subsequently, each paper should then be cited by the next one in the list, continuing up to the last paper (Paper n).\n3.Consider only the citation relationships within the supplied collection of papers, and ensure that the citation chain accurately reflects the sequential citation order among these documents.\n4.Do not take into account any articles not provided, and disregard other non-linear citation relationships.\n\n#Papers Provided:\n{docs}\n\n[Gold Answer]\n['# A Neural Network Architecture Combining Gated Recurrent Unit (GRU) and Support Vector Machine (SVM) for Intrusion Detection in Network Traffic Data ', '# Deep Learning using Rectified Linear Units (ReLU) ', '# BEYOND TRADITIONAL MAGNETIC RESONANCE PROCESSING WITH ARTIFICIAL INTELLIGENCE * ']\n\n[The Start of Assistant's Predicted Answer]\n[\"Deep Learning using Rectified Linear Units (ReLU)\", \"Adam: A method for stochastic optimization\"]\n[The End of Assistant's Predicted Answer]\n\n[System]\nWe would like to request your feedback on the performance of the AI assistant in response to the user question displayed above according to the gold answer. Please use the following listed aspects and their descriptions as evaluation criteria:\n    - Accuracy and Hallucinations: The assistant's answer is semantically consistent with the gold answer; The numerical value and order need to be accurate, and there should be no hallucinations.\n    - Completeness: Referring to the reference answers, the assistant's answer should contain all the key points needed to answer the user's question; further elaboration on these key points can be omitted.\nPlease rate whether this answer is suitable for the question. Please note that the gold answer can be considered as a correct answer to the question.\n\nThe assistant receives an overall score on a scale of 1 to 100, where a higher score indicates better overall performance.\nPlease note that if the assistant's answer and the gold answer fully meet the above criteria, its overall rating should be the full marks (100).\nPlease first provide a comprehensive explanation of your evaluation, avoiding any potential bias.\nThen, output a line indicating the score of the Assistant.\n\nPLEASE OUTPUT WITH THE FOLLOWING FORMAT, WHERE THE SCORE IS A SCALE OF 1 TO 100 BY STRICTLY FOLLOWING THIS FORMAT: \"[[score]]\", FOR EXAMPLE \"Rating: [[100]]\":\n<start output>\nEvaluation evidence: your evluation explanation here, no more than 100 words\nRating: [[score]]\n<end output>\n\nNow, start your evaluation:",
        "generate_response": "[\"Deep Learning using Rectified Linear Units (ReLU)\", \"Adam: A method for stochastic optimization\"]",
        "eval_response": "Evaluation evidence: The assistant's answer does not match the gold answer in terms of the titles of the papers listed. The gold answer lists a specific citation chain, while the assistant's answer includes different paper titles and does not form a complete citation chain as required. The assistant's response is incomplete and inaccurate, missing the correct titles and the sequence from the gold answer.\n\nRating: [[20]]",
        "score": 20
    },
    {
        "level": 3,
        "set": 2,
        "length": 74820,
        "type": "paper",
        "language": "en",
        "question": "MedusA: Simple LLM Inference Acceleration Framework with Multiple Decoding Heads",
        "instruction": "We hope you will carefully study the provided papers and determine the citation relationships between them. Please follow the instructions below strictly to complete the task:\n\n#Specific Requirements:\n1. Reference: When a given paper mentions other provided papers, those other papers are considered as \"references\" for the given paper. To summarize in this specific context, references are about what the given paper is using.\n2. Citation: Conversely, when other provided papers mention the given paper in their works, the given paper is being \"cited\" by those other papers. To summarize in this specific context, citations are about who is using the given paper.\n3. Given a paper, you need to determine the citation or reference relationship between this paper and the other papers. Do not consider papers that are not provided.\n3. Please present the paper titles in a json format as follows: {{\"Reference\":[\"Reference Title 1\", \"Reference Title 2\", ..., \"Reference Title n\"], \"Citation\":[\"Citation Title 1\", \"Citation Title 2\", ..., \"Citation Title n\"]}}.\n4. If a paper does not have any references or citations, please leave the corresponding list empty, e.g.{{\"Refernce\":[]}}, {{\"Citation\":[]}}.",
        "prompt_template": "#Papers:\n{docs}\n\n{instruction}\n\n#The paper you need to analyze:\n{question}",
        "doc": [
            "2401.10774.md",
            "2405.20314.md",
            "2309.06180.md",
            "2402.02057.md"
        ],
        "answer": {
            "Reference": [
                "# Efficient Memory Management for Large Language Model Serving with PagedAttention ",
                "# Break the Sequential Dependency of LLM Inference Using LOOKAHEAD DECODING "
            ],
            "Citation": [
                "# S3D: A Simple and Cost-Effective Self-Speculative Decoding Scheme for Low-Memory GPUs "
            ]
        },
        "shuffle_doc": true,
        "id": "d8b62c27-60b8-4244-9d76-ea76de5042a7",
        "prompt": "[Question]\n#Papers:\n{docs}\n\nWe hope you will carefully study the provided papers and determine the citation relationships between them. Please follow the instructions below strictly to complete the task:\n\n#Specific Requirements:\n1. Reference: When a given paper mentions other provided papers, those other papers are considered as \"references\" for the given paper. To summarize in this specific context, references are about what the given paper is using.\n2. Citation: Conversely, when other provided papers mention the given paper in their works, the given paper is being \"cited\" by those other papers. To summarize in this specific context, citations are about who is using the given paper.\n3. Given a paper, you need to determine the citation or reference relationship between this paper and the other papers. Do not consider papers that are not provided.\n3. Please present the paper titles in a json format as follows: {{\"Reference\":[\"Reference Title 1\", \"Reference Title 2\", ..., \"Reference Title n\"], \"Citation\":[\"Citation Title 1\", \"Citation Title 2\", ..., \"Citation Title n\"]}}.\n4. If a paper does not have any references or citations, please leave the corresponding list empty, e.g.{{\"Refernce\":[]}}, {{\"Citation\":[]}}.\n\n#The paper you need to analyze:\nMedusA: Simple LLM Inference Acceleration Framework with Multiple Decoding Heads\n\n[Gold Answer]\n{'Reference': ['# Efficient Memory Management for Large Language Model Serving with PagedAttention ', '# Break the Sequential Dependency of LLM Inference Using LOOKAHEAD DECODING '], 'Citation': ['# S3D: A Simple and Cost-Effective Self-Speculative Decoding Scheme for Low-Memory GPUs ']}\n\n[The Start of Assistant's Predicted Answer]\n{\"Citation\":[],\"Reference\":[\"A Survey on Evaluation of Large Language Models\",\"Vicuna: An open-source chatbot impressing GPT-4 with $90 \\% *$ ChatGPT quality\",\"Llama 2: Open foundation and fine-tuned chat models\",\"Zephyr: Direct distillation of LM alignment\",\"GQA: Training Generalized Multi-Query Transformer Models from Multi-Head Checkpoints\",\"8-bit optimizers via block-wise quantization\",\"Llm.int8(): 8-bit Matrix Multiplication for Transformers at Scale\",\"Qlora: Efficient fine-tuning of quantized LLMs\",\"Enhancing chat language models by scaling high-quality instructional conversations\",\"Alpacafarm: A Simulation Framework for Methods that Learn from Human Feedback\",\"Sigmoid-weighted linear units for neural network function approximation in reinforcement learning\",\"Hierarchical Neural Story Generation\",\"GPTQ: Accurate Post-Training Quantization for Generative Pre-Trained Transformers\",\"Breaking the sequential dependency of $11 \\mathrm{~m}$ inference using lookahead decoding\",\"Palm 2 technical report\",\"Rest: Retrieval-based Speculative Decoding\",\"Truncation Sampling as Language Model Desmoothing\",\"Training Compute-Optimal Large Language Models\",\"The curious case of neural text degeneration\",\"Lora: Low-Rank Adaptation of Large Language Models\",\"Online Speculative Decoding\",\"Locally Typical Sampling\",\"Specinfer: Accelerating Generative LLM Serving with Speculative Inference and Token Tree Verification\",\"Gpt-4 technical report\",\"Training Language Models to Follow Instructions with Human Feedback\",\"Efficiently Scaling Transformer Inference\",\"ShareGPT\",\"Fast Transformer Decoding: One Write-Head is all you need\",\"Accelerating LLM Inference with Staged Speculative Decoding\",\"Blockwise Parallel Decoding for Deep Autoregressive Models\",\"Do Transformers Really Perform Badly for Graph Representation?\",\"Smoothquant: Accurate and Efficient Post-Training Quantization for Large Language Models\",\"A Survey on Non-Autoregressive Generation for Neural Machine Translation and Beyond\",\"H _2 o: Heavy-Hitter Oracle for Efficient Generative Inference of Large Language Models\",\"Judging LLM-as-a-Judge with MT-Bench and Chatbot Arena\",\"DistillSpec: Improving Speculative Decoding via Knowledge Distillation\"]}\n[The End of Assistant's Predicted Answer]\n\n[System]\nWe would like to request your feedback on the performance of the AI assistant in response to the user question displayed above according to the gold answer. Please use the following listed aspects and their descriptions as evaluation criteria:\n    - Accuracy and Hallucinations: The assistant's answer is semantically consistent with the gold answer; The numerical value and order need to be accurate, and there should be no hallucinations.\n    - Completeness: Referring to the reference answers, the assistant's answer should contain all the key points needed to answer the user's question; further elaboration on these key points can be omitted.\nPlease rate whether this answer is suitable for the question. Please note that the gold answer can be considered as a correct answer to the question.\n\nThe assistant receives an overall score on a scale of 1 to 100, where a higher score indicates better overall performance.\nPlease note that if the assistant's answer and the gold answer fully meet the above criteria, its overall rating should be the full marks (100).\nPlease first provide a comprehensive explanation of your evaluation, avoiding any potential bias.\nThen, output a line indicating the score of the Assistant.\n\nPLEASE OUTPUT WITH THE FOLLOWING FORMAT, WHERE THE SCORE IS A SCALE OF 1 TO 100 BY STRICTLY FOLLOWING THIS FORMAT: \"[[score]]\", FOR EXAMPLE \"Rating: [[100]]\":\n<start output>\nEvaluation evidence: your evluation explanation here, no more than 100 words\nRating: [[score]]\n<end output>\n\nNow, start your evaluation:",
        "generate_response": "{\"Citation\":[],\"Reference\":[\"A Survey on Evaluation of Large Language Models\",\"Vicuna: An open-source chatbot impressing GPT-4 with $90 \\% *$ ChatGPT quality\",\"Llama 2: Open foundation and fine-tuned chat models\",\"Zephyr: Direct distillation of LM alignment\",\"GQA: Training Generalized Multi-Query Transformer Models from Multi-Head Checkpoints\",\"8-bit optimizers via block-wise quantization\",\"Llm.int8(): 8-bit Matrix Multiplication for Transformers at Scale\",\"Qlora: Efficient fine-tuning of quantized LLMs\",\"Enhancing chat language models by scaling high-quality instructional conversations\",\"Alpacafarm: A Simulation Framework for Methods that Learn from Human Feedback\",\"Sigmoid-weighted linear units for neural network function approximation in reinforcement learning\",\"Hierarchical Neural Story Generation\",\"GPTQ: Accurate Post-Training Quantization for Generative Pre-Trained Transformers\",\"Breaking the sequential dependency of $11 \\mathrm{~m}$ inference using lookahead decoding\",\"Palm 2 technical report\",\"Rest: Retrieval-based Speculative Decoding\",\"Truncation Sampling as Language Model Desmoothing\",\"Training Compute-Optimal Large Language Models\",\"The curious case of neural text degeneration\",\"Lora: Low-Rank Adaptation of Large Language Models\",\"Online Speculative Decoding\",\"Locally Typical Sampling\",\"Specinfer: Accelerating Generative LLM Serving with Speculative Inference and Token Tree Verification\",\"Gpt-4 technical report\",\"Training Language Models to Follow Instructions with Human Feedback\",\"Efficiently Scaling Transformer Inference\",\"ShareGPT\",\"Fast Transformer Decoding: One Write-Head is all you need\",\"Accelerating LLM Inference with Staged Speculative Decoding\",\"Blockwise Parallel Decoding for Deep Autoregressive Models\",\"Do Transformers Really Perform Badly for Graph Representation?\",\"Smoothquant: Accurate and Efficient Post-Training Quantization for Large Language Models\",\"A Survey on Non-Autoregressive Generation for Neural Machine Translation and Beyond\",\"H _2 o: Heavy-Hitter Oracle for Efficient Generative Inference of Large Language Models\",\"Judging LLM-as-a-Judge with MT-Bench and Chatbot Arena\",\"DistillSpec: Improving Speculative Decoding via Knowledge Distillation\"]}",
        "eval_response": "Evaluation evidence: The assistant's answer does not match the gold answer in terms of accuracy and completeness. The assistant failed to identify any of the correct references or citations from the provided paper. Instead, it listed numerous unrelated titles, indicating a significant hallucination issue. The correct references and citation mentioned in the gold answer are entirely missing in the assistant's response.\n\nRating: [[10]]",
        "score": 10
    },
    {
        "level": 3,
        "set": 2,
        "length": 56216,
        "type": "paper",
        "language": "en",
        "question": "Patchscopes: A Unifying Framework for Inspecting Hidden Representations of Language Models",
        "instruction": "We hope you will carefully study the provided papers and determine the citation relationships between them. Please follow the instructions below strictly to complete the task:\n\n#Specific Requirements:\n1. Reference: When a given paper mentions other provided papers, those other papers are considered as \"references\" for the given paper. To summarize in this specific context, references are about what the given paper is using.\n2. Citation: Conversely, when other provided papers mention the given paper in their works, the given paper is being \"cited\" by those other papers. To summarize in this specific context, citations are about who is using the given paper.\n3. Given a paper, you need to determine the citation or reference relationship between this paper and the other papers. Do not consider papers that are not provided.\n3. Please present the paper titles in a json format as follows: {{\"Reference\":[\"Reference Title 1\", \"Reference Title 2\", ..., \"Reference Title n\"], \"Citation\":[\"Citation Title 1\", \"Citation Title 2\", ..., \"Citation Title n\"]}}.\n4. If a paper does not have any references or citations, please leave the corresponding list empty, e.g.{{\"Refernce\":[]}}, {{\"Citation\":[]}}.",
        "prompt_template": "#Papers:\n{docs}\n\n{instruction}\n\n#The paper you need to analyze:\n{question}",
        "doc": [
            "2401.06102.md",
            "2310.15916.md",
            "2311.04897.md"
        ],
        "answer": {
            "Reference": [
                "# In-Context Learning Creates Task Vectors ",
                "# Future Lens: Anticipating Subsequent Tokens from a Single Hidden State "
            ],
            "Citation": []
        },
        "shuffle_doc": true,
        "id": "11c5c175-bc71-4895-b803-9d356cd27688",
        "prompt": "[Question]\n#Papers:\n{docs}\n\nWe hope you will carefully study the provided papers and determine the citation relationships between them. Please follow the instructions below strictly to complete the task:\n\n#Specific Requirements:\n1. Reference: When a given paper mentions other provided papers, those other papers are considered as \"references\" for the given paper. To summarize in this specific context, references are about what the given paper is using.\n2. Citation: Conversely, when other provided papers mention the given paper in their works, the given paper is being \"cited\" by those other papers. To summarize in this specific context, citations are about who is using the given paper.\n3. Given a paper, you need to determine the citation or reference relationship between this paper and the other papers. Do not consider papers that are not provided.\n3. Please present the paper titles in a json format as follows: {{\"Reference\":[\"Reference Title 1\", \"Reference Title 2\", ..., \"Reference Title n\"], \"Citation\":[\"Citation Title 1\", \"Citation Title 2\", ..., \"Citation Title n\"]}}.\n4. If a paper does not have any references or citations, please leave the corresponding list empty, e.g.{{\"Refernce\":[]}}, {{\"Citation\":[]}}.\n\n#The paper you need to analyze:\nPatchscopes: A Unifying Framework for Inspecting Hidden Representations of Language Models\n\n[Gold Answer]\n{'Reference': ['# In-Context Learning Creates Task Vectors ', '# Future Lens: Anticipating Subsequent Tokens from a Single Hidden State '], 'Citation': []}\n\n[The Start of Assistant's Predicted Answer]\n{\"Reference\":[\"Vicuna: An open-source chatbot impressing GPT-4 with $90 \\% *$ ChatGPT quality\", \"Towards automated circuit discovery for mechanistic interpretability\", \"Similarity and matching of neural network representations\", \"Analyzing transformers in embedding space\", \"The Pile: An 800GB dataset of diverse text for language modeling\", \"Causal abstractions of neural networks\", \"Distributional vectors encode referential attributes\", \"Interpreting mathematical abilities in a pre-trained language model\", \"Does localization inform editing? surprising differences in causality-based localization vs. knowledge editing in language models\", \"In-context learning creates task vectors\", \"Natural language descriptions of deep features\", \"Measuring and manipulating knowledge representations in language models\", \"Measuring the expressive power of transformers with chain of thought\", \"Linearly mapping from image to text space\", \"A mechanism for solving relational tasks in transformer language models\", \"The curious case of hallucinatory (un)answerability: Finding truths in the hidden states of over-confident large language models\", \"A mechanistic interpretation of arithmetic reasoning in language models using causal mediation analysis\", \"GPT-J-6B: A 6 Billion Parameter Autoregressive Language Model\", \"Interpretability in the wild: a circuit for indirect object identification in GPT-2 small\", \"Gaussian Process Probes (GPP) for uncertainty-aware probing\", \"Chain-of-thought prompting elicits reasoning in large language models\", \"Tree of thoughts: Deliberate problem solving with large language models\", \"Give me the facts! a survey on factual knowledge probing in pre-trained language models\", \"Towards best practices of activation patching in language models: Metrics and methods\", \"Assessing knowledge editing in language models via multi-hop questions\"], \"Citation\":[]}\n[The End of Assistant's Predicted Answer]\n\n[System]\nWe would like to request your feedback on the performance of the AI assistant in response to the user question displayed above according to the gold answer. Please use the following listed aspects and their descriptions as evaluation criteria:\n    - Accuracy and Hallucinations: The assistant's answer is semantically consistent with the gold answer; The numerical value and order need to be accurate, and there should be no hallucinations.\n    - Completeness: Referring to the reference answers, the assistant's answer should contain all the key points needed to answer the user's question; further elaboration on these key points can be omitted.\nPlease rate whether this answer is suitable for the question. Please note that the gold answer can be considered as a correct answer to the question.\n\nThe assistant receives an overall score on a scale of 1 to 100, where a higher score indicates better overall performance.\nPlease note that if the assistant's answer and the gold answer fully meet the above criteria, its overall rating should be the full marks (100).\nPlease first provide a comprehensive explanation of your evaluation, avoiding any potential bias.\nThen, output a line indicating the score of the Assistant.\n\nPLEASE OUTPUT WITH THE FOLLOWING FORMAT, WHERE THE SCORE IS A SCALE OF 1 TO 100 BY STRICTLY FOLLOWING THIS FORMAT: \"[[score]]\", FOR EXAMPLE \"Rating: [[100]]\":\n<start output>\nEvaluation evidence: your evluation explanation here, no more than 100 words\nRating: [[score]]\n<end output>\n\nNow, start your evaluation:",
        "generate_response": "{\"Reference\":[\"Vicuna: An open-source chatbot impressing GPT-4 with $90 \\% *$ ChatGPT quality\", \"Towards automated circuit discovery for mechanistic interpretability\", \"Similarity and matching of neural network representations\", \"Analyzing transformers in embedding space\", \"The Pile: An 800GB dataset of diverse text for language modeling\", \"Causal abstractions of neural networks\", \"Distributional vectors encode referential attributes\", \"Interpreting mathematical abilities in a pre-trained language model\", \"Does localization inform editing? surprising differences in causality-based localization vs. knowledge editing in language models\", \"In-context learning creates task vectors\", \"Natural language descriptions of deep features\", \"Measuring and manipulating knowledge representations in language models\", \"Measuring the expressive power of transformers with chain of thought\", \"Linearly mapping from image to text space\", \"A mechanism for solving relational tasks in transformer language models\", \"The curious case of hallucinatory (un)answerability: Finding truths in the hidden states of over-confident large language models\", \"A mechanistic interpretation of arithmetic reasoning in language models using causal mediation analysis\", \"GPT-J-6B: A 6 Billion Parameter Autoregressive Language Model\", \"Interpretability in the wild: a circuit for indirect object identification in GPT-2 small\", \"Gaussian Process Probes (GPP) for uncertainty-aware probing\", \"Chain-of-thought prompting elicits reasoning in large language models\", \"Tree of thoughts: Deliberate problem solving with large language models\", \"Give me the facts! a survey on factual knowledge probing in pre-trained language models\", \"Towards best practices of activation patching in language models: Metrics and methods\", \"Assessing knowledge editing in language models via multi-hop questions\"], \"Citation\":[]}",
        "eval_response": "Evaluation evidence: The assistant's answer is inaccurate and contains hallucinations. The provided answer lists numerous papers that are not mentioned in the gold answer, indicating a failure to correctly identify the references for the specified paper. The gold answer lists two specific papers as references, which are not included in the assistant's response. Additionally, the assistant's answer includes an extensive list of references that are not relevant to the task, demonstrating a misunderstanding of the task requirements.\n\nRating: [[10]]",
        "score": 10
    },
    {
        "level": 3,
        "set": 3,
        "length": 110252,
        "type": "paper",
        "language": "en",
        "question": "LLM4Vuln: A Unified Evaluation Framework for Decoupling and Enhancing LLMs' Vulnerability Reasoning",
        "instruction": "We hope you will carefully study the provided papers and determine the citation relationships between them. Please follow the instructions below strictly to complete the task:\n\n#Specific Requirements:\n1. Reference: When a given paper mentions other provided papers, those other papers are considered as \"references\" for the given paper. To summarize in this specific context, references are about what the given paper is using.\n2. Citation: Conversely, when other provided papers mention the given paper in their works, the given paper is being \"cited\" by those other papers. To summarize in this specific context, citations are about who is using the given paper.\n3. Given a paper, you need to determine the citation or reference relationship between this paper and the other papers. Do not consider papers that are not provided.\n3. Please present the paper titles in a json format as follows: {{\"Reference\":[\"Reference Title 1\", \"Reference Title 2\", ..., \"Reference Title n\"], \"Citation\":[\"Citation Title 1\", \"Citation Title 2\", ..., \"Citation Title n\"]}}.\n4. If a paper does not have any references or citations, please leave the corresponding list empty, e.g.{{\"Refernce\":[]}}, {{\"Citation\":[]}}.",
        "prompt_template": "#Papers:\n{docs}\n\n{instruction}\n\n#The paper you need to analyze:\n{question}",
        "doc": [
            "2401.16185.md",
            "2404.17833.md",
            "2405.15614.md",
            "2310.01152.md",
            "2311.13721.md"
        ],
        "answer": {
            "Reference": [
                "# Large Language Model-Powered Smart Contract Vulnerability Detection: New Perspectives ",
                "# Nova ${ }^{+}$: Generative Language Models for Binaries "
            ],
            "Citation": [
                "# Testing and Understanding Erroneous Planning in LLM Agents THROUGH SYntHESIZEd UsER InPUTs ",
                "# Harnessing Large Language Models for Software Vulnerability Detection: A Comprehensive Benchmarking Study "
            ]
        },
        "shuffle_doc": true,
        "id": "3b29c259-fb2a-497e-84de-3cf812f92db1",
        "prompt": "[Question]\n#Papers:\n{docs}\n\nWe hope you will carefully study the provided papers and determine the citation relationships between them. Please follow the instructions below strictly to complete the task:\n\n#Specific Requirements:\n1. Reference: When a given paper mentions other provided papers, those other papers are considered as \"references\" for the given paper. To summarize in this specific context, references are about what the given paper is using.\n2. Citation: Conversely, when other provided papers mention the given paper in their works, the given paper is being \"cited\" by those other papers. To summarize in this specific context, citations are about who is using the given paper.\n3. Given a paper, you need to determine the citation or reference relationship between this paper and the other papers. Do not consider papers that are not provided.\n3. Please present the paper titles in a json format as follows: {{\"Reference\":[\"Reference Title 1\", \"Reference Title 2\", ..., \"Reference Title n\"], \"Citation\":[\"Citation Title 1\", \"Citation Title 2\", ..., \"Citation Title n\"]}}.\n4. If a paper does not have any references or citations, please leave the corresponding list empty, e.g.{{\"Refernce\":[]}}, {{\"Citation\":[]}}.\n\n#The paper you need to analyze:\nLLM4Vuln: A Unified Evaluation Framework for Decoupling and Enhancing LLMs' Vulnerability Reasoning\n\n[Gold Answer]\n{'Reference': ['# Large Language Model-Powered Smart Contract Vulnerability Detection: New Perspectives ', '# Nova ${ }^{+}$: Generative Language Models for Binaries '], 'Citation': ['# Testing and Understanding Erroneous Planning in LLM Agents THROUGH SYntHESIZEd UsER InPUTs ', '# Harnessing Large Language Models for Software Vulnerability Detection: A Comprehensive Benchmarking Study ']}\n\n[The Start of Assistant's Predicted Answer]\n{\"Reference\":[\"A Survey on Evaluation of Large Language Models\", \"Vicuna: An Open-Source Chatbot Impressing GPT-4 with $90 \\% *$ ChatGPT Quality\", \"ChatLaw: Open-Source Legal Large Language Model with Integrated External Knowledge Bases\", \"Compositional Semantic Parsing with Large Language Models\", \"Open LLM Leaderboard\", \"How Close is ChatGPT to Human Experts? Comparison Corpus, Evaluation, and Detection\", \"REALM: Retrieval-Augmented Language Model PreTraining\", \"Rethinking with Retrieval: Faithful Large Language Model Inference\", \"Measuring Massive Multitask Language Understanding\", \"Atlas: Few-shot Learning with Retrieval Augmented Language Models\", \"Generative Pre-training Transformer (GPT) models, exemplified by GPT-3.5\", \"TitanFuzz\", \"FuzzGPT\", \"Fuzz4All\", \"ChatAFL\", \"GPTScan\", \"GPTLens\", \"LLift\", \"LATTE\", \"Thapa et al. conducted a pioneer work\", \"Apebond\", \"Chen et al. [19] conducted an empirical study to investigate the performance of LLM in detecting smart contracts vulnerabilities\", \"David et al. [24] evaluate the performance of LLMs in security audit on 52 DeFi smart contracts\", \"Gao et al. [31] introduced a dataset of vulnerabilities and evacuated the LLMs, deep learning-based method and traditional static analysis models in vulnerability detection\", \"Khare et al. [37] also benchmarked the effectiveness of LLMs in Java and C++ vulnerability detection\", \"Lacomis et al. [39] proposed a technique for creating corpora specifically designed to train models for renaming decompiled code\", \"Pal et al. [49] presented a novel model to predict the real variable name from the decompilation output for security analysis\", \"Pei et al. [51] introduced a new mechanism of transformers for learning different binary and source code semantics in code security analysis tasks\", \"Chen et al. [20] proposed a novel model to improve the output of the decompilation results for security analysis\", \"Ding et al. [27] introduced an execution-aware pre-training strategy to improve model performance in the complicated execution logic\", \"Gai et al. [30] presented a blockchain-oriented model to detect anomalous blockchain transactions\", \"Guthula et al. [32] proposed a network security model pre-trained on unlabeled network packet traces\", \"Jiang et al. [35] focused on the binary corpora, and presented a specific LLM for binary code analysis\", \"Li et al. [43] also proposed a specific LLM for binary code analysis by capturing the unique characteristics of disassembly\", \"Dai et al. [23] Why Can GPT Learn In-Context? Language Models Implicitly Perform Gradient Descent as Meta-Optimizers\", \"Sun et al. [57] combined GPT with static analysis to detect logic vulnerabilities in smart contracts\", \"Li et al. [42] proposed an automated framework to interface static analysis tools and LLMs\", \"Deng et al. [25] introduce TitanFuzz to leverage LLMs to generate input cases for fuzzing DL libraries\", \"FuzzGPT [26] is another work that uses LLMs to synthesize unusual programs for fuzzing vulnerabilities\", \"Meng et al. [47] proposed an LLM-based protocol implementation fuzzing method called ChatAFL\", \"Xia et al. [67] presented Fuzz4All, a tool that employs LLMs to generate fuzzing inputs for all kinds of programs\", \"Chen et al. [19] conducted an empirical study to investigate the performance of LLM in detecting smart contracts vulnerabilities\", \"David et al. [24] evaluate the performance of LLMs in security audit on 52 DeFi smart contracts\", \"Khare et al. [37] also benchmarked the effectiveness of LLMs in Java and C++ vulnerability detection\", \"Pearce et al. [50] Examining Zero-Shot Vulnerability Repair with Large Language Models\", \"Pei et al. [51] Symmetry-Preserving Program Representations for Learning Code Semantics\", \"Tang et al. [58] CSGVD: A deep learning approach combining sequence and graph embedding for source code vulnerability detection\", \"Thapa et al. [59] Transformer-Based Language Models for Software Vulnerability Detection\", \"Ullah et al. [62] Can Large Language Models Identify And Reason About Security Vulnerabilities? Not Yet\", \"Xia et al. [67] Fuzz4All: Universal Fuzzing with Large Language Models\", \"Yi et al. [69] BlockScope: Detecting and Investigating Propagated Vulnerabilities in Forked Blockchain Projects\", \"Zhu et al. [71] kTrans: Knowledge-Aware Transformer for Binary Code Embedding\"], \"Citation\":[]}\n[The End of Assistant's Predicted Answer]\n\n[System]\nWe would like to request your feedback on the performance of the AI assistant in response to the user question displayed above according to the gold answer. Please use the following listed aspects and their descriptions as evaluation criteria:\n    - Accuracy and Hallucinations: The assistant's answer is semantically consistent with the gold answer; The numerical value and order need to be accurate, and there should be no hallucinations.\n    - Completeness: Referring to the reference answers, the assistant's answer should contain all the key points needed to answer the user's question; further elaboration on these key points can be omitted.\nPlease rate whether this answer is suitable for the question. Please note that the gold answer can be considered as a correct answer to the question.\n\nThe assistant receives an overall score on a scale of 1 to 100, where a higher score indicates better overall performance.\nPlease note that if the assistant's answer and the gold answer fully meet the above criteria, its overall rating should be the full marks (100).\nPlease first provide a comprehensive explanation of your evaluation, avoiding any potential bias.\nThen, output a line indicating the score of the Assistant.\n\nPLEASE OUTPUT WITH THE FOLLOWING FORMAT, WHERE THE SCORE IS A SCALE OF 1 TO 100 BY STRICTLY FOLLOWING THIS FORMAT: \"[[score]]\", FOR EXAMPLE \"Rating: [[100]]\":\n<start output>\nEvaluation evidence: your evluation explanation here, no more than 100 words\nRating: [[score]]\n<end output>\n\nNow, start your evaluation:",
        "generate_response": "{\"Reference\":[\"A Survey on Evaluation of Large Language Models\", \"Vicuna: An Open-Source Chatbot Impressing GPT-4 with $90 \\% *$ ChatGPT Quality\", \"ChatLaw: Open-Source Legal Large Language Model with Integrated External Knowledge Bases\", \"Compositional Semantic Parsing with Large Language Models\", \"Open LLM Leaderboard\", \"How Close is ChatGPT to Human Experts? Comparison Corpus, Evaluation, and Detection\", \"REALM: Retrieval-Augmented Language Model PreTraining\", \"Rethinking with Retrieval: Faithful Large Language Model Inference\", \"Measuring Massive Multitask Language Understanding\", \"Atlas: Few-shot Learning with Retrieval Augmented Language Models\", \"Generative Pre-training Transformer (GPT) models, exemplified by GPT-3.5\", \"TitanFuzz\", \"FuzzGPT\", \"Fuzz4All\", \"ChatAFL\", \"GPTScan\", \"GPTLens\", \"LLift\", \"LATTE\", \"Thapa et al. conducted a pioneer work\", \"Apebond\", \"Chen et al. [19] conducted an empirical study to investigate the performance of LLM in detecting smart contracts vulnerabilities\", \"David et al. [24] evaluate the performance of LLMs in security audit on 52 DeFi smart contracts\", \"Gao et al. [31] introduced a dataset of vulnerabilities and evacuated the LLMs, deep learning-based method and traditional static analysis models in vulnerability detection\", \"Khare et al. [37] also benchmarked the effectiveness of LLMs in Java and C++ vulnerability detection\", \"Lacomis et al. [39] proposed a technique for creating corpora specifically designed to train models for renaming decompiled code\", \"Pal et al. [49] presented a novel model to predict the real variable name from the decompilation output for security analysis\", \"Pei et al. [51] introduced a new mechanism of transformers for learning different binary and source code semantics in code security analysis tasks\", \"Chen et al. [20] proposed a novel model to improve the output of the decompilation results for security analysis\", \"Ding et al. [27] introduced an execution-aware pre-training strategy to improve model performance in the complicated execution logic\", \"Gai et al. [30] presented a blockchain-oriented model to detect anomalous blockchain transactions\", \"Guthula et al. [32] proposed a network security model pre-trained on unlabeled network packet traces\", \"Jiang et al. [35] focused on the binary corpora, and presented a specific LLM for binary code analysis\", \"Li et al. [43] also proposed a specific LLM for binary code analysis by capturing the unique characteristics of disassembly\", \"Dai et al. [23] Why Can GPT Learn In-Context? Language Models Implicitly Perform Gradient Descent as Meta-Optimizers\", \"Sun et al. [57] combined GPT with static analysis to detect logic vulnerabilities in smart contracts\", \"Li et al. [42] proposed an automated framework to interface static analysis tools and LLMs\", \"Deng et al. [25] introduce TitanFuzz to leverage LLMs to generate input cases for fuzzing DL libraries\", \"FuzzGPT [26] is another work that uses LLMs to synthesize unusual programs for fuzzing vulnerabilities\", \"Meng et al. [47] proposed an LLM-based protocol implementation fuzzing method called ChatAFL\", \"Xia et al. [67] presented Fuzz4All, a tool that employs LLMs to generate fuzzing inputs for all kinds of programs\", \"Chen et al. [19] conducted an empirical study to investigate the performance of LLM in detecting smart contracts vulnerabilities\", \"David et al. [24] evaluate the performance of LLMs in security audit on 52 DeFi smart contracts\", \"Khare et al. [37] also benchmarked the effectiveness of LLMs in Java and C++ vulnerability detection\", \"Pearce et al. [50] Examining Zero-Shot Vulnerability Repair with Large Language Models\", \"Pei et al. [51] Symmetry-Preserving Program Representations for Learning Code Semantics\", \"Tang et al. [58] CSGVD: A deep learning approach combining sequence and graph embedding for source code vulnerability detection\", \"Thapa et al. [59] Transformer-Based Language Models for Software Vulnerability Detection\", \"Ullah et al. [62] Can Large Language Models Identify And Reason About Security Vulnerabilities? Not Yet\", \"Xia et al. [67] Fuzz4All: Universal Fuzzing with Large Language Models\", \"Yi et al. [69] BlockScope: Detecting and Investigating Propagated Vulnerabilities in Forked Blockchain Projects\", \"Zhu et al. [71] kTrans: Knowledge-Aware Transformer for Binary Code Embedding\"], \"Citation\":[]}",
        "eval_response": "Evaluation evidence: The assistant's answer does not match the gold answer in terms of accuracy and completeness. The references and citations listed in the assistant's response do not align with those in the gold answer, indicating a significant deviation and hallucination of content. The assistant incorrectly lists numerous unrelated papers, which are not mentioned in the gold answer, thus failing to meet the task requirements.\n\nRating: [[10]]",
        "score": 10
    },
    {
        "level": 3,
        "set": 2,
        "length": 63041,
        "type": "paper",
        "language": "en",
        "question": "LLM4Vuln: A Unified Evaluation Framework for Decoupling and Enhancing LLMs' Vulnerability Reasoning",
        "instruction": "We hope you will carefully study the provided papers and determine the citation relationships between them. Please follow the instructions below strictly to complete the task:\n\n#Specific Requirements:\n1. Reference: When a given paper mentions other provided papers, those other papers are considered as \"references\" for the given paper. To summarize in this specific context, references are about what the given paper is using.\n2. Citation: Conversely, when other provided papers mention the given paper in their works, the given paper is being \"cited\" by those other papers. To summarize in this specific context, citations are about who is using the given paper.\n3. Given a paper, you need to determine the citation or reference relationship between this paper and the other papers. Do not consider papers that are not provided.\n3. Please present the paper titles in a json format as follows: {{\"Reference\":[\"Reference Title 1\", \"Reference Title 2\", ..., \"Reference Title n\"], \"Citation\":[\"Citation Title 1\", \"Citation Title 2\", ..., \"Citation Title n\"]}}.\n4. If a paper does not have any references or citations, please leave the corresponding list empty, e.g.{{\"Refernce\":[]}}, {{\"Citation\":[]}}.",
        "prompt_template": "#Papers:\n{docs}\n\n{instruction}\n\n#The paper you need to analyze:\n{question}",
        "doc": [
            "2401.16185.md",
            "2404.17833.md",
            "2310.01152.md"
        ],
        "answer": {
            "Reference": [
                "# Large Language Model-Powered Smart Contract Vulnerability Detection: New Perspectives "
            ],
            "Citation": [
                "# Testing and Understanding Erroneous Planning in LLM Agents THROUGH SYntHESIZEd UsER InPUTs "
            ]
        },
        "shuffle_doc": true,
        "id": "03204820-ffc2-455a-99de-e2f2fd00c537",
        "prompt": "[Question]\n#Papers:\n{docs}\n\nWe hope you will carefully study the provided papers and determine the citation relationships between them. Please follow the instructions below strictly to complete the task:\n\n#Specific Requirements:\n1. Reference: When a given paper mentions other provided papers, those other papers are considered as \"references\" for the given paper. To summarize in this specific context, references are about what the given paper is using.\n2. Citation: Conversely, when other provided papers mention the given paper in their works, the given paper is being \"cited\" by those other papers. To summarize in this specific context, citations are about who is using the given paper.\n3. Given a paper, you need to determine the citation or reference relationship between this paper and the other papers. Do not consider papers that are not provided.\n3. Please present the paper titles in a json format as follows: {{\"Reference\":[\"Reference Title 1\", \"Reference Title 2\", ..., \"Reference Title n\"], \"Citation\":[\"Citation Title 1\", \"Citation Title 2\", ..., \"Citation Title n\"]}}.\n4. If a paper does not have any references or citations, please leave the corresponding list empty, e.g.{{\"Refernce\":[]}}, {{\"Citation\":[]}}.\n\n#The paper you need to analyze:\nLLM4Vuln: A Unified Evaluation Framework for Decoupling and Enhancing LLMs' Vulnerability Reasoning\n\n[Gold Answer]\n{'Reference': ['# Large Language Model-Powered Smart Contract Vulnerability Detection: New Perspectives '], 'Citation': ['# Testing and Understanding Erroneous Planning in LLM Agents THROUGH SYntHESIZEd UsER InPUTs ']}\n\n[The Start of Assistant's Predicted Answer]\n{\"Reference\": [\"A Survey on Evaluation of Large Language Models\", \"Vicuna: An Open-Source Chatbot Impressing GPT-4 with $90 \\% *$ ChatGPT Quality\", \"ChatLaw: Open-Source Legal Large Language Model with Integrated External Knowledge Bases\", \"Compositional Semantic Parsing with Large Language Models\", \"Open LLM Leaderboard\", \"How Close is ChatGPT to Human Experts? Comparison Corpus, Evaluation, and Detection\", \"REALM: Retrieval-Augmented Language Model PreTraining\", \"Rethinking with Retrieval: Faithful Large Language Model Inference\", \"Measuring Massive Multitask Language Understanding\", \"Atlas: Few-shot Learning with Retrieval Augmented Language Models\", \"Generative Pre-training Transformer (GPT) models\", \"Low level source code vulnerability detection using advanced bert language model\", \"FlowDroid: precise context, flow, field, object-sensitive and lifecycle-aware taint analysis for Android apps\", \"KLEE Unassisted and Automatic Generation of High-Coverage Tests for Complex Systems Programs\", \"Deep Learning Based Vulnerability Detection: Are We There Yet?\", \"When ChatGPT Meets Smart Contract Vulnerability Detection: How Far Are We?\", \"Augmenting Decompiler Output with Learned Variable Names and Types\", \"DiverseVul: A New Vulnerable Source Code Dataset for Deep Learning Based Vulnerability Detection\", \"Can Large Language Models Be an Alternative to Human Evaluations?\", \"Why Can GPT Learn In-Context? Language Models Implicitly Perform Gradient Descent as Meta-Optimizers\", \"Do you still need a manual smart contract audit?\", \"Large Language Models Are Zero-Shot Fuzzers: Fuzzing DeepLearning Libraries via Large Language Models\", \"Large Language Models are Edge-Case Generators: Crafting Unusual Programs for Fuzzing Deep Learning Libraries\", \"TRACED: Execution-aware Pre-training for Source Code\", \"Beyond 'Protected' and 'Private': An Empirical Security Analysis of Custom Function Modifiers in Smart Contracts\", \"AFL++: Combining Incremental Steps of Fuzzing Research\", \"Blockchain Large Language Models\", \"How Far Have We Gone in Vulnerability Detection Using Large Language Models\", \"netFound: Foundation Model for Network Security\", \"Large Language Model-Powered Smart Contract Vulnerability Detection: New Perspectives\", \"Mixtral of Experts\", \"Nova $^{+}$: Generative Language Models for Binaries\", \"Billion-scale similarity search with GPUs\", \"Understanding the Effectiveness of Large Language Models in Detecting Security Vulnerabilities\", \"Large language models are zero-shot reasoners\", \"DIRE: A Neural Approach to Decompiled Identifier Naming\", \"Illustrating Reinforcement Learning from Human Feedback (RLHF)\", \"Retrieval-Augmented Generation for Knowledge-Intensive NLP Tasks\", \"The Hitchhiker's Guide to Program Analysis: A Journey with Large Language Models\", \"PalmTree: Learning an Assembly Language Model for Instruction Embedding\", \"Split and Merge: Aligning Position Biases in Large Language Model based Evaluators\", \"Vuldeepecker: A deep learning-based system for vulnerability detection\", \"Harnessing the Power of LLM to Support Binary Taint Analysis\", \"Large Language Model guided Protocol Fuzzing\", \"Examining Zero-Shot Vulnerability Repair with Large Language Models\", \"Symmetry-Preserving Program Representations for Learning Code Semantics\", \"Code Llama: Open Foundation Models for Code\", \"Toolformer: Language Models Can Teach Themselves to Use Tools\", \"Neuzz: Efficient fuzzing with neural program smoothing\", \"SOK: (State of) The Art of War: Offensive Techniques in Binary Analysis\", \"GPTScan: Detecting Logic Vulnerabilities in Smart Contracts by Combining GPT with Program Analysis\", \"CSGVD: A deep learning approach combining sequence and graph embedding for source code vulnerability detection\", \"Transformer-Based Language Models for Software Vulnerability Detection\", \"LLama 2: Open Foundation and Fine-Tuned Chat Models\", \"Can Large Language Models Identify And Reason About Security Vulnerabilities? Not Yet\", \"Attention Is All You Need\", \"TaintScope: A ChecksumAware Directed Fuzzing Tool for Automatic Software Vulnerability Detection\", \"Chain-of-Thought Prompting Elicits Reasoning in Large Language Models\", \"When Program Analysis Meets Bytecode Search: Targeted and Efficient Inter-procedural Analysis of Modern Android Apps in BackDroid\", \"Neural Network-based Graph Embedding for Cross-Platform Binary Code Similarity Detection\", \"BlockScope: Detecting and Investigating Propagated Vulnerabilities in Forked Blockchain Projects\", \"Devign: Effective Vulnerability Identification by Learning Comprehensive Program Semantics via Graph Neural Networks\", \"kTrans: Knowledge-Aware Transformer for Binary Code Embedding\"], \"Citation\": []}\n[The End of Assistant's Predicted Answer]\n\n[System]\nWe would like to request your feedback on the performance of the AI assistant in response to the user question displayed above according to the gold answer. Please use the following listed aspects and their descriptions as evaluation criteria:\n    - Accuracy and Hallucinations: The assistant's answer is semantically consistent with the gold answer; The numerical value and order need to be accurate, and there should be no hallucinations.\n    - Completeness: Referring to the reference answers, the assistant's answer should contain all the key points needed to answer the user's question; further elaboration on these key points can be omitted.\nPlease rate whether this answer is suitable for the question. Please note that the gold answer can be considered as a correct answer to the question.\n\nThe assistant receives an overall score on a scale of 1 to 100, where a higher score indicates better overall performance.\nPlease note that if the assistant's answer and the gold answer fully meet the above criteria, its overall rating should be the full marks (100).\nPlease first provide a comprehensive explanation of your evaluation, avoiding any potential bias.\nThen, output a line indicating the score of the Assistant.\n\nPLEASE OUTPUT WITH THE FOLLOWING FORMAT, WHERE THE SCORE IS A SCALE OF 1 TO 100 BY STRICTLY FOLLOWING THIS FORMAT: \"[[score]]\", FOR EXAMPLE \"Rating: [[100]]\":\n<start output>\nEvaluation evidence: your evluation explanation here, no more than 100 words\nRating: [[score]]\n<end output>\n\nNow, start your evaluation:",
        "generate_response": "{\"Reference\": [\"A Survey on Evaluation of Large Language Models\", \"Vicuna: An Open-Source Chatbot Impressing GPT-4 with $90 \\% *$ ChatGPT Quality\", \"ChatLaw: Open-Source Legal Large Language Model with Integrated External Knowledge Bases\", \"Compositional Semantic Parsing with Large Language Models\", \"Open LLM Leaderboard\", \"How Close is ChatGPT to Human Experts? Comparison Corpus, Evaluation, and Detection\", \"REALM: Retrieval-Augmented Language Model PreTraining\", \"Rethinking with Retrieval: Faithful Large Language Model Inference\", \"Measuring Massive Multitask Language Understanding\", \"Atlas: Few-shot Learning with Retrieval Augmented Language Models\", \"Generative Pre-training Transformer (GPT) models\", \"Low level source code vulnerability detection using advanced bert language model\", \"FlowDroid: precise context, flow, field, object-sensitive and lifecycle-aware taint analysis for Android apps\", \"KLEE Unassisted and Automatic Generation of High-Coverage Tests for Complex Systems Programs\", \"Deep Learning Based Vulnerability Detection: Are We There Yet?\", \"When ChatGPT Meets Smart Contract Vulnerability Detection: How Far Are We?\", \"Augmenting Decompiler Output with Learned Variable Names and Types\", \"DiverseVul: A New Vulnerable Source Code Dataset for Deep Learning Based Vulnerability Detection\", \"Can Large Language Models Be an Alternative to Human Evaluations?\", \"Why Can GPT Learn In-Context? Language Models Implicitly Perform Gradient Descent as Meta-Optimizers\", \"Do you still need a manual smart contract audit?\", \"Large Language Models Are Zero-Shot Fuzzers: Fuzzing DeepLearning Libraries via Large Language Models\", \"Large Language Models are Edge-Case Generators: Crafting Unusual Programs for Fuzzing Deep Learning Libraries\", \"TRACED: Execution-aware Pre-training for Source Code\", \"Beyond 'Protected' and 'Private': An Empirical Security Analysis of Custom Function Modifiers in Smart Contracts\", \"AFL++: Combining Incremental Steps of Fuzzing Research\", \"Blockchain Large Language Models\", \"How Far Have We Gone in Vulnerability Detection Using Large Language Models\", \"netFound: Foundation Model for Network Security\", \"Large Language Model-Powered Smart Contract Vulnerability Detection: New Perspectives\", \"Mixtral of Experts\", \"Nova $^{+}$: Generative Language Models for Binaries\", \"Billion-scale similarity search with GPUs\", \"Understanding the Effectiveness of Large Language Models in Detecting Security Vulnerabilities\", \"Large language models are zero-shot reasoners\", \"DIRE: A Neural Approach to Decompiled Identifier Naming\", \"Illustrating Reinforcement Learning from Human Feedback (RLHF)\", \"Retrieval-Augmented Generation for Knowledge-Intensive NLP Tasks\", \"The Hitchhiker's Guide to Program Analysis: A Journey with Large Language Models\", \"PalmTree: Learning an Assembly Language Model for Instruction Embedding\", \"Split and Merge: Aligning Position Biases in Large Language Model based Evaluators\", \"Vuldeepecker: A deep learning-based system for vulnerability detection\", \"Harnessing the Power of LLM to Support Binary Taint Analysis\", \"Large Language Model guided Protocol Fuzzing\", \"Examining Zero-Shot Vulnerability Repair with Large Language Models\", \"Symmetry-Preserving Program Representations for Learning Code Semantics\", \"Code Llama: Open Foundation Models for Code\", \"Toolformer: Language Models Can Teach Themselves to Use Tools\", \"Neuzz: Efficient fuzzing with neural program smoothing\", \"SOK: (State of) The Art of War: Offensive Techniques in Binary Analysis\", \"GPTScan: Detecting Logic Vulnerabilities in Smart Contracts by Combining GPT with Program Analysis\", \"CSGVD: A deep learning approach combining sequence and graph embedding for source code vulnerability detection\", \"Transformer-Based Language Models for Software Vulnerability Detection\", \"LLama 2: Open Foundation and Fine-Tuned Chat Models\", \"Can Large Language Models Identify And Reason About Security Vulnerabilities? Not Yet\", \"Attention Is All You Need\", \"TaintScope: A ChecksumAware Directed Fuzzing Tool for Automatic Software Vulnerability Detection\", \"Chain-of-Thought Prompting Elicits Reasoning in Large Language Models\", \"When Program Analysis Meets Bytecode Search: Targeted and Efficient Inter-procedural Analysis of Modern Android Apps in BackDroid\", \"Neural Network-based Graph Embedding for Cross-Platform Binary Code Similarity Detection\", \"BlockScope: Detecting and Investigating Propagated Vulnerabilities in Forked Blockchain Projects\", \"Devign: Effective Vulnerability Identification by Learning Comprehensive Program Semantics via Graph Neural Networks\", \"kTrans: Knowledge-Aware Transformer for Binary Code Embedding\"], \"Citation\": []}",
        "eval_response": "Evaluation evidence: The assistant's answer does not match the gold answer in terms of accuracy and completeness. The assistant lists numerous references that are not mentioned in the gold answer, indicating a significant hallucination issue. Additionally, the assistant fails to list any citations, whereas the gold answer specifies one citation. This discrepancy shows a lack of accuracy and completeness in the assistant's response.\n\nRating: [[10]]",
        "score": 10
    },
    {
        "level": 3,
        "set": 2,
        "length": 97705,
        "type": "paper",
        "language": "en",
        "question": "CHAIN-OF-TABLE: EVOLVING TABLES IN THE REASONING CHAIN FOR TABLE UNDERSTANDING",
        "instruction": "We hope you will carefully study the provided papers and determine the citation relationships between them. Please follow the instructions below strictly to complete the task:\n\n#Specific Requirements:\n1. Reference: When a given paper mentions other provided papers, those other papers are considered as \"references\" for the given paper. To summarize in this specific context, references are about what the given paper is using.\n2. Citation: Conversely, when other provided papers mention the given paper in their works, the given paper is being \"cited\" by those other papers. To summarize in this specific context, citations are about who is using the given paper.\n3. Given a paper, you need to determine the citation or reference relationship between this paper and the other papers. Do not consider papers that are not provided.\n3. Please present the paper titles in a json format as follows: {{\"Reference\":[\"Reference Title 1\", \"Reference Title 2\", ..., \"Reference Title n\"], \"Citation\":[\"Citation Title 1\", \"Citation Title 2\", ..., \"Citation Title n\"]}}.\n4. If a paper does not have any references or citations, please leave the corresponding list empty, e.g.{{\"Refernce\":[]}}, {{\"Citation\":[]}}.",
        "prompt_template": "#Papers:\n{docs}\n\n{instruction}\n\n#The paper you need to analyze:\n{question}",
        "doc": [
            "2401.04398.md",
            "2404.10150.md",
            "2405.01147.md",
            "2305.14992.md",
            "2305.17812.md"
        ],
        "answer": {
            "Reference": [
                "# Reasoning with Language Model is Planning with World Model ",
                "# Tab-CoT: Zero-shot Tabular Chain of Thought "
            ],
            "Citation": [
                "# TabSQLify: Enhancing Reasoning Capabilities of LLMs Through Table Decomposition ",
                "# Position: Why Tabular Foundation Models Should Be a Research Priority "
            ]
        },
        "shuffle_doc": true,
        "id": "f33380a1-9078-415a-8049-743c40e6df08",
        "prompt": "[Question]\n#Papers:\n{docs}\n\nWe hope you will carefully study the provided papers and determine the citation relationships between them. Please follow the instructions below strictly to complete the task:\n\n#Specific Requirements:\n1. Reference: When a given paper mentions other provided papers, those other papers are considered as \"references\" for the given paper. To summarize in this specific context, references are about what the given paper is using.\n2. Citation: Conversely, when other provided papers mention the given paper in their works, the given paper is being \"cited\" by those other papers. To summarize in this specific context, citations are about who is using the given paper.\n3. Given a paper, you need to determine the citation or reference relationship between this paper and the other papers. Do not consider papers that are not provided.\n3. Please present the paper titles in a json format as follows: {{\"Reference\":[\"Reference Title 1\", \"Reference Title 2\", ..., \"Reference Title n\"], \"Citation\":[\"Citation Title 1\", \"Citation Title 2\", ..., \"Citation Title n\"]}}.\n4. If a paper does not have any references or citations, please leave the corresponding list empty, e.g.{{\"Refernce\":[]}}, {{\"Citation\":[]}}.\n\n#The paper you need to analyze:\nCHAIN-OF-TABLE: EVOLVING TABLES IN THE REASONING CHAIN FOR TABLE UNDERSTANDING\n\n[Gold Answer]\n{'Reference': ['# Reasoning with Language Model is Planning with World Model ', '# Tab-CoT: Zero-shot Tabular Chain of Thought '], 'Citation': ['# TabSQLify: Enhancing Reasoning Capabilities of LLMs Through Table Decomposition ', '# Position: Why Tabular Foundation Models Should Be a Research Priority ']}\n\n[The Start of Assistant's Predicted Answer]\n{\"Reference\": [\"Table-based reasoning with large language models (LLMs)\", \"Table-based fact verification (Chen et al., 2019)\", \"table-based question answering (Jin et al., 2022)\", \"Distilling step-by-step! outperforming larger language models with less training data and smaller model sizes (Cheng-Yu Hsieh et al., 2023)\", \"MathPrompter: Mathematical reasoning using large language models (Shima Imani et al., 2023)\", \"A survey on table question answering: recent advances (Nengzheng Jin et al., 2022)\", \"Tab-cot: Zero-shot tabular chain of thought (Ziqi Jin and Wei Lu, 2023)\", \"A survey on deep learning approaches for text-to-sql (George Katsogiannis-Meimarakis and Georgia Koutrika, 2023)\", \"Decomposed prompting: A modular approach for solving complex tasks (Tushar Khot et al., 2022)\", \"Large language models are zero-shot reasoners (Takeshi Kojima et al., 2022)\"], \"Citation\": [\"mask language modeling (MLM) proposed in BERT (Devlin et al., 2019)\", \"TaPas (Herzig et al., 2020)\", \"Pasta (Gu et al., 2022)\", \"TUTA (Wang et al., 2021)\", \"TAPEX (Liu et al., 2021)\", \"Chain-of-Thought (CoT) (Wei et al., 2022)\", \"Least-to-Most (Zhou et al., 2022)\", \"Decomp (Khot et al., 2022)\", \"Text-to-SQL (Rajkumar et al., 2022)\", \"Binder (Cheng et al., 2022)\"]}\n[The End of Assistant's Predicted Answer]\n\n[System]\nWe would like to request your feedback on the performance of the AI assistant in response to the user question displayed above according to the gold answer. Please use the following listed aspects and their descriptions as evaluation criteria:\n    - Accuracy and Hallucinations: The assistant's answer is semantically consistent with the gold answer; The numerical value and order need to be accurate, and there should be no hallucinations.\n    - Completeness: Referring to the reference answers, the assistant's answer should contain all the key points needed to answer the user's question; further elaboration on these key points can be omitted.\nPlease rate whether this answer is suitable for the question. Please note that the gold answer can be considered as a correct answer to the question.\n\nThe assistant receives an overall score on a scale of 1 to 100, where a higher score indicates better overall performance.\nPlease note that if the assistant's answer and the gold answer fully meet the above criteria, its overall rating should be the full marks (100).\nPlease first provide a comprehensive explanation of your evaluation, avoiding any potential bias.\nThen, output a line indicating the score of the Assistant.\n\nPLEASE OUTPUT WITH THE FOLLOWING FORMAT, WHERE THE SCORE IS A SCALE OF 1 TO 100 BY STRICTLY FOLLOWING THIS FORMAT: \"[[score]]\", FOR EXAMPLE \"Rating: [[100]]\":\n<start output>\nEvaluation evidence: your evluation explanation here, no more than 100 words\nRating: [[score]]\n<end output>\n\nNow, start your evaluation:",
        "generate_response": "{\"Reference\": [\"Table-based reasoning with large language models (LLMs)\", \"Table-based fact verification (Chen et al., 2019)\", \"table-based question answering (Jin et al., 2022)\", \"Distilling step-by-step! outperforming larger language models with less training data and smaller model sizes (Cheng-Yu Hsieh et al., 2023)\", \"MathPrompter: Mathematical reasoning using large language models (Shima Imani et al., 2023)\", \"A survey on table question answering: recent advances (Nengzheng Jin et al., 2022)\", \"Tab-cot: Zero-shot tabular chain of thought (Ziqi Jin and Wei Lu, 2023)\", \"A survey on deep learning approaches for text-to-sql (George Katsogiannis-Meimarakis and Georgia Koutrika, 2023)\", \"Decomposed prompting: A modular approach for solving complex tasks (Tushar Khot et al., 2022)\", \"Large language models are zero-shot reasoners (Takeshi Kojima et al., 2022)\"], \"Citation\": [\"mask language modeling (MLM) proposed in BERT (Devlin et al., 2019)\", \"TaPas (Herzig et al., 2020)\", \"Pasta (Gu et al., 2022)\", \"TUTA (Wang et al., 2021)\", \"TAPEX (Liu et al., 2021)\", \"Chain-of-Thought (CoT) (Wei et al., 2022)\", \"Least-to-Most (Zhou et al., 2022)\", \"Decomp (Khot et al., 2022)\", \"Text-to-SQL (Rajkumar et al., 2022)\", \"Binder (Cheng et al., 2022)\"]}",
        "eval_response": "Evaluation evidence: The assistant's answer does not match the gold answer in terms of accuracy and completeness. The references and citations listed by the assistant are entirely different from those in the gold answer, indicating a significant deviation and possible hallucination of content. The assistant fails to identify the correct papers that reference or cite the given paper, which is crucial for the task.\n\nRating: [[10]]",
        "score": 10
    },
    {
        "level": 4,
        "set": 3,
        "length": 147897,
        "type": "paper",
        "language": "en",
        "question": "",
        "instruction": "We kindly ask you to thoroughly review the provided papers and construct a citation chain from them. Please adhere to the following instructions strictly while completing the task:\n\n#Task Instructions:\nGiven several papers, you are required to identify and list the longest citation chain, which demonstrates the citation relationship among the provided papers.\n\n#Specific Requirements:\n1.Please present the titles of the papers in the form of a list, as follows: [\"Title of Paper 1\", \"Title of Paper 2\", ..., \"Title of Paper n\"].\n2.Ensure that the citation chain in the list is linear and continuous, meaning that the first paper title in the list (Paper 1) should not cite any other works. Instead, it should be cited by the next paper in the list (Paper 2); subsequently, each paper should then be cited by the next one in the list, continuing up to the last paper (Paper n).\n3.Consider only the citation relationships within the supplied collection of papers, and ensure that the citation chain accurately reflects the sequential citation order among these documents.\n4.Do not take into account any articles not provided, and disregard other non-linear citation relationships.",
        "prompt_template": "{instruction}\n\n#Papers Provided:\n{docs}",
        "doc": [
            "2108.08612.md",
            "2109.11251.md",
            "2209.12681.md",
            "2209.08244.md",
            "2405.06161.md"
        ],
        "answer": [
            "# Settling the Variance of Multi-Agent Policy Gradients ",
            "# TRUST REGION POLICY OPTIMISATION IN MULTI-AGENT REINFORCEMENT LEARNING ",
            "# More Centralized Training, Still DecentralIZED EXECUTION: MULTI-AGENT CONDITIONAL POLICY FACTORIZATION ",
            "# MA2QL: A Minimalist Approach to Fully Decentralized Multi-Agent Reinforcement Learning ",
            "# (A Partial Survey of) Decentralized, Cooperative Multi-Agent Reinforcement Learning "
        ],
        "shuffle_doc": true,
        "id": "2126abac-1728-4bc7-ba58-b6c3955a6d9a",
        "prompt": "[Question]\nWe kindly ask you to thoroughly review the provided papers and construct a citation chain from them. Please adhere to the following instructions strictly while completing the task:\n\n#Task Instructions:\nGiven several papers, you are required to identify and list the longest citation chain, which demonstrates the citation relationship among the provided papers.\n\n#Specific Requirements:\n1.Please present the titles of the papers in the form of a list, as follows: [\"Title of Paper 1\", \"Title of Paper 2\", ..., \"Title of Paper n\"].\n2.Ensure that the citation chain in the list is linear and continuous, meaning that the first paper title in the list (Paper 1) should not cite any other works. Instead, it should be cited by the next paper in the list (Paper 2); subsequently, each paper should then be cited by the next one in the list, continuing up to the last paper (Paper n).\n3.Consider only the citation relationships within the supplied collection of papers, and ensure that the citation chain accurately reflects the sequential citation order among these documents.\n4.Do not take into account any articles not provided, and disregard other non-linear citation relationships.\n\n#Papers Provided:\n{docs}\n\n[Gold Answer]\n['# Settling the Variance of Multi-Agent Policy Gradients ', '# TRUST REGION POLICY OPTIMISATION IN MULTI-AGENT REINFORCEMENT LEARNING ', '# More Centralized Training, Still DecentralIZED EXECUTION: MULTI-AGENT CONDITIONAL POLICY FACTORIZATION ', '# MA2QL: A Minimalist Approach to Fully Decentralized Multi-Agent Reinforcement Learning ', '# (A Partial Survey of) Decentralized, Cooperative Multi-Agent Reinforcement Learning ']\n\n[The Start of Assistant's Predicted Answer]\n[\"MA2QL: A Minimalist Approach to Fully Decentralized Multi-Agent Reinforcement Learning\", \"Valuedecomposition networks for cooperative multi-agent learning based on team reward\", \"Multi-agent cooperation and competition with deep reinforcement learning\", \"Multi-agent actor-critic for mixed cooperative-competitive environments\"]\n[The End of Assistant's Predicted Answer]\n\n[System]\nWe would like to request your feedback on the performance of the AI assistant in response to the user question displayed above according to the gold answer. Please use the following listed aspects and their descriptions as evaluation criteria:\n    - Accuracy and Hallucinations: The assistant's answer is semantically consistent with the gold answer; The numerical value and order need to be accurate, and there should be no hallucinations.\n    - Completeness: Referring to the reference answers, the assistant's answer should contain all the key points needed to answer the user's question; further elaboration on these key points can be omitted.\nPlease rate whether this answer is suitable for the question. Please note that the gold answer can be considered as a correct answer to the question.\n\nThe assistant receives an overall score on a scale of 1 to 100, where a higher score indicates better overall performance.\nPlease note that if the assistant's answer and the gold answer fully meet the above criteria, its overall rating should be the full marks (100).\nPlease first provide a comprehensive explanation of your evaluation, avoiding any potential bias.\nThen, output a line indicating the score of the Assistant.\n\nPLEASE OUTPUT WITH THE FOLLOWING FORMAT, WHERE THE SCORE IS A SCALE OF 1 TO 100 BY STRICTLY FOLLOWING THIS FORMAT: \"[[score]]\", FOR EXAMPLE \"Rating: [[100]]\":\n<start output>\nEvaluation evidence: your evluation explanation here, no more than 100 words\nRating: [[score]]\n<end output>\n\nNow, start your evaluation:",
        "generate_response": "[\"MA2QL: A Minimalist Approach to Fully Decentralized Multi-Agent Reinforcement Learning\", \"Valuedecomposition networks for cooperative multi-agent learning based on team reward\", \"Multi-agent cooperation and competition with deep reinforcement learning\", \"Multi-agent actor-critic for mixed cooperative-competitive environments\"]",
        "eval_response": "Evaluation evidence: The assistant's answer does not match the gold answer in terms of the titles of the papers listed. The titles in the assistant's answer do not appear in the gold answer, indicating a significant deviation and inaccuracy. The citation chain provided by the assistant does not reflect the correct sequence or titles as per the task's requirements, leading to a failure in both accuracy and completeness.\n\nRating: [[10]]",
        "score": 10
    },
    {
        "level": 4,
        "set": 2,
        "length": 53441,
        "type": "paper",
        "language": "en",
        "question": "",
        "instruction": "We kindly ask you to thoroughly review the provided papers and construct a citation chain from them. Please adhere to the following instructions strictly while completing the task:\n\n#Task Instructions:\nGiven several papers, you are required to identify and list the longest citation chain, which demonstrates the citation relationship among the provided papers.\n\n#Specific Requirements:\n1.Please present the titles of the papers in the form of a list, as follows: [\"Title of Paper 1\", \"Title of Paper 2\", ..., \"Title of Paper n\"].\n2.Ensure that the citation chain in the list is linear and continuous, meaning that the first paper title in the list (Paper 1) should not cite any other works. Instead, it should be cited by the next paper in the list (Paper 2); subsequently, each paper should then be cited by the next one in the list, continuing up to the last paper (Paper n).\n3.Consider only the citation relationships within the supplied collection of papers, and ensure that the citation chain accurately reflects the sequential citation order among these documents.\n4.Do not take into account any articles not provided, and disregard other non-linear citation relationships.",
        "prompt_template": "{instruction}\n\n#Papers Provided:\n{docs}",
        "doc": [
            "1804.04272.md",
            "1901.02039.md",
            "2112.13227.md"
        ],
        "answer": [
            "# Deep Neural Networks Motivated by Partial Differential Equations ",
            "# SPHERICAL CNNS ON UNSTRUCTURED GRIDS ",
            "# Pseudocylindrical Convolutions for Learned Omnidirectional Image Compression "
        ],
        "shuffle_doc": true,
        "id": "d652b46e-4fc8-4cdc-ac7f-36aaaba74439",
        "prompt": "[Question]\nWe kindly ask you to thoroughly review the provided papers and construct a citation chain from them. Please adhere to the following instructions strictly while completing the task:\n\n#Task Instructions:\nGiven several papers, you are required to identify and list the longest citation chain, which demonstrates the citation relationship among the provided papers.\n\n#Specific Requirements:\n1.Please present the titles of the papers in the form of a list, as follows: [\"Title of Paper 1\", \"Title of Paper 2\", ..., \"Title of Paper n\"].\n2.Ensure that the citation chain in the list is linear and continuous, meaning that the first paper title in the list (Paper 1) should not cite any other works. Instead, it should be cited by the next paper in the list (Paper 2); subsequently, each paper should then be cited by the next one in the list, continuing up to the last paper (Paper n).\n3.Consider only the citation relationships within the supplied collection of papers, and ensure that the citation chain accurately reflects the sequential citation order among these documents.\n4.Do not take into account any articles not provided, and disregard other non-linear citation relationships.\n\n#Papers Provided:\n{docs}\n\n[Gold Answer]\n['# Deep Neural Networks Motivated by Partial Differential Equations ', '# SPHERICAL CNNS ON UNSTRUCTURED GRIDS ', '# Pseudocylindrical Convolutions for Learned Omnidirectional Image Compression ']\n\n[The Start of Assistant's Predicted Answer]\n[\"SPHERICAL CNNS ON UNSTRUCTURED GRIDS\"]\n[The End of Assistant's Predicted Answer]\n\n[System]\nWe would like to request your feedback on the performance of the AI assistant in response to the user question displayed above according to the gold answer. Please use the following listed aspects and their descriptions as evaluation criteria:\n    - Accuracy and Hallucinations: The assistant's answer is semantically consistent with the gold answer; The numerical value and order need to be accurate, and there should be no hallucinations.\n    - Completeness: Referring to the reference answers, the assistant's answer should contain all the key points needed to answer the user's question; further elaboration on these key points can be omitted.\nPlease rate whether this answer is suitable for the question. Please note that the gold answer can be considered as a correct answer to the question.\n\nThe assistant receives an overall score on a scale of 1 to 100, where a higher score indicates better overall performance.\nPlease note that if the assistant's answer and the gold answer fully meet the above criteria, its overall rating should be the full marks (100).\nPlease first provide a comprehensive explanation of your evaluation, avoiding any potential bias.\nThen, output a line indicating the score of the Assistant.\n\nPLEASE OUTPUT WITH THE FOLLOWING FORMAT, WHERE THE SCORE IS A SCALE OF 1 TO 100 BY STRICTLY FOLLOWING THIS FORMAT: \"[[score]]\", FOR EXAMPLE \"Rating: [[100]]\":\n<start output>\nEvaluation evidence: your evluation explanation here, no more than 100 words\nRating: [[score]]\n<end output>\n\nNow, start your evaluation:",
        "generate_response": "[\"SPHERICAL CNNS ON UNSTRUCTURED GRIDS\"]",
        "eval_response": "Evaluation evidence: The assistant's answer is incomplete and inaccurate. It only lists one paper title, \"SPHERICAL CNNS ON UNSTRUCTURED GRIDS,\" whereas the gold answer provides a citation chain of three papers. The assistant's response fails to capture the linear and continuous citation relationship among the papers, which is a critical requirement of the task. Therefore, the answer does not meet the completeness and accuracy criteria.\n\nRating: [[20]]",
        "score": 20
    },
    {
        "level": 4,
        "set": 2,
        "length": 74085,
        "type": "paper",
        "language": "en",
        "question": "",
        "instruction": "We kindly ask you to thoroughly review the provided papers and construct a citation chain from them. Please adhere to the following instructions strictly while completing the task:\n\n#Task Instructions:\nGiven several papers, you are required to identify and list the longest citation chain, which demonstrates the citation relationship among the provided papers.\n\n#Specific Requirements:\n1.Please present the titles of the papers in the form of a list, as follows: [\"Title of Paper 1\", \"Title of Paper 2\", ..., \"Title of Paper n\"].\n2.Ensure that the citation chain in the list is linear and continuous, meaning that the first paper title in the list (Paper 1) should not cite any other works. Instead, it should be cited by the next paper in the list (Paper 2); subsequently, each paper should then be cited by the next one in the list, continuing up to the last paper (Paper n).\n3.Consider only the citation relationships within the supplied collection of papers, and ensure that the citation chain accurately reflects the sequential citation order among these documents.\n4.Do not take into account any articles not provided, and disregard other non-linear citation relationships.",
        "prompt_template": "{instruction}\n\n#Papers Provided:\n{docs}",
        "doc": [
            "2305.17560.md",
            "2310.12487.md",
            "2405.09285.md"
        ],
        "answer": [
            "# Scalable Transformer for PDE Surrogate Modeling ",
            "# IMPROVED OPERATOR LEARNING BY ORTHOGONAL ATTENTION ",
            "# Positional Knowledge is All You Need: Position-induced Transformer (PiT) for Operator Learning "
        ],
        "shuffle_doc": true,
        "id": "8449c961-673e-4bb9-a1c6-538761165167",
        "prompt": "[Question]\nWe kindly ask you to thoroughly review the provided papers and construct a citation chain from them. Please adhere to the following instructions strictly while completing the task:\n\n#Task Instructions:\nGiven several papers, you are required to identify and list the longest citation chain, which demonstrates the citation relationship among the provided papers.\n\n#Specific Requirements:\n1.Please present the titles of the papers in the form of a list, as follows: [\"Title of Paper 1\", \"Title of Paper 2\", ..., \"Title of Paper n\"].\n2.Ensure that the citation chain in the list is linear and continuous, meaning that the first paper title in the list (Paper 1) should not cite any other works. Instead, it should be cited by the next paper in the list (Paper 2); subsequently, each paper should then be cited by the next one in the list, continuing up to the last paper (Paper n).\n3.Consider only the citation relationships within the supplied collection of papers, and ensure that the citation chain accurately reflects the sequential citation order among these documents.\n4.Do not take into account any articles not provided, and disregard other non-linear citation relationships.\n\n#Papers Provided:\n{docs}\n\n[Gold Answer]\n['# Scalable Transformer for PDE Surrogate Modeling ', '# IMPROVED OPERATOR LEARNING BY ORTHOGONAL ATTENTION ', '# Positional Knowledge is All You Need: Position-induced Transformer (PiT) for Operator Learning ']\n\n[The Start of Assistant's Predicted Answer]\n[\"IMPROVED OPERATOR LEARNING BY ORTHOGONAL ATTENTION\", \"Fourier neural operator for parametric partial differential equations\", \"Scalable Transformer for PDE Surrogate Modeling\", \"Learning Operators with Coupled Attention\", \"Mesh-Independent Operator Learning for Partial Differential Equations\", \"Nonlinear Manifold Decoders for Operator Learning\"]\n[The End of Assistant's Predicted Answer]\n\n[System]\nWe would like to request your feedback on the performance of the AI assistant in response to the user question displayed above according to the gold answer. Please use the following listed aspects and their descriptions as evaluation criteria:\n    - Accuracy and Hallucinations: The assistant's answer is semantically consistent with the gold answer; The numerical value and order need to be accurate, and there should be no hallucinations.\n    - Completeness: Referring to the reference answers, the assistant's answer should contain all the key points needed to answer the user's question; further elaboration on these key points can be omitted.\nPlease rate whether this answer is suitable for the question. Please note that the gold answer can be considered as a correct answer to the question.\n\nThe assistant receives an overall score on a scale of 1 to 100, where a higher score indicates better overall performance.\nPlease note that if the assistant's answer and the gold answer fully meet the above criteria, its overall rating should be the full marks (100).\nPlease first provide a comprehensive explanation of your evaluation, avoiding any potential bias.\nThen, output a line indicating the score of the Assistant.\n\nPLEASE OUTPUT WITH THE FOLLOWING FORMAT, WHERE THE SCORE IS A SCALE OF 1 TO 100 BY STRICTLY FOLLOWING THIS FORMAT: \"[[score]]\", FOR EXAMPLE \"Rating: [[100]]\":\n<start output>\nEvaluation evidence: your evluation explanation here, no more than 100 words\nRating: [[score]]\n<end output>\n\nNow, start your evaluation:",
        "generate_response": "[\"IMPROVED OPERATOR LEARNING BY ORTHOGONAL ATTENTION\", \"Fourier neural operator for parametric partial differential equations\", \"Scalable Transformer for PDE Surrogate Modeling\", \"Learning Operators with Coupled Attention\", \"Mesh-Independent Operator Learning for Partial Differential Equations\", \"Nonlinear Manifold Decoders for Operator Learning\"]",
        "eval_response": "Evaluation evidence: The assistant's answer does not match the gold answer in terms of the titles and the sequence of the citation chain. The gold answer lists a specific sequence of three papers, while the assistant's answer includes different titles and more papers, which are not in a linear citation chain as required. This indicates issues with both accuracy and completeness, as the assistant's response does not accurately reflect the citation relationships specified in the task instructions and includes additional, unrelated papers.\n\nRating: [[20]]",
        "score": 20
    },
    {
        "level": 3,
        "set": 1,
        "length": 48373,
        "type": "paper",
        "language": "en",
        "question": "Eyes Wide Shut? Exploring the Visual Shortcomings of Multimodal LLMs",
        "instruction": "We hope you will carefully study the provided papers and determine the citation relationships between them. Please follow the instructions below strictly to complete the task:\n\n#Specific Requirements:\n1. Reference: When a given paper mentions other provided papers, those other papers are considered as \"references\" for the given paper. To summarize in this specific context, references are about what the given paper is using.\n2. Citation: Conversely, when other provided papers mention the given paper in their works, the given paper is being \"cited\" by those other papers. To summarize in this specific context, citations are about who is using the given paper.\n3. Given a paper, you need to determine the citation or reference relationship between this paper and the other papers. Do not consider papers that are not provided.\n3. Please present the paper titles in a json format as follows: {{\"Reference\":[\"Reference Title 1\", \"Reference Title 2\", ..., \"Reference Title n\"], \"Citation\":[\"Citation Title 1\", \"Citation Title 2\", ..., \"Citation Title n\"]}}.\n4. If a paper does not have any references or citations, please leave the corresponding list empty, e.g.{{\"Refernce\":[]}}, {{\"Citation\":[]}}.",
        "prompt_template": "#Papers:\n{docs}\n\n{instruction}\n\n#The paper you need to analyze:\n{question}",
        "doc": [
            "2401.06209.md",
            "2405.19226.md",
            "2311.09215.md"
        ],
        "answer": {
            "Reference": [
                "# ConvNet vs Transformer, Supervised vs CLIP: Beyond ImageNet Accuracy "
            ],
            "Citation": [
                "# ContextBLIP: Doubly Contextual Alignment for Contrastive Image Retrieval from Linguistically Complex Descriptions "
            ]
        },
        "shuffle_doc": true,
        "id": "6e59cc47-82ae-4421-be67-f6ca0992f718",
        "prompt": "[Question]\n#Papers:\n{docs}\n\nWe hope you will carefully study the provided papers and determine the citation relationships between them. Please follow the instructions below strictly to complete the task:\n\n#Specific Requirements:\n1. Reference: When a given paper mentions other provided papers, those other papers are considered as \"references\" for the given paper. To summarize in this specific context, references are about what the given paper is using.\n2. Citation: Conversely, when other provided papers mention the given paper in their works, the given paper is being \"cited\" by those other papers. To summarize in this specific context, citations are about who is using the given paper.\n3. Given a paper, you need to determine the citation or reference relationship between this paper and the other papers. Do not consider papers that are not provided.\n3. Please present the paper titles in a json format as follows: {{\"Reference\":[\"Reference Title 1\", \"Reference Title 2\", ..., \"Reference Title n\"], \"Citation\":[\"Citation Title 1\", \"Citation Title 2\", ..., \"Citation Title n\"]}}.\n4. If a paper does not have any references or citations, please leave the corresponding list empty, e.g.{{\"Refernce\":[]}}, {{\"Citation\":[]}}.\n\n#The paper you need to analyze:\nEyes Wide Shut? Exploring the Visual Shortcomings of Multimodal LLMs\n\n[Gold Answer]\n{'Reference': ['# ConvNet vs Transformer, Supervised vs CLIP: Beyond ImageNet Accuracy '], 'Citation': ['# ContextBLIP: Doubly Contextual Alignment for Contrastive Image Retrieval from Linguistically Complex Descriptions ']}\n\n[The Start of Assistant's Predicted Answer]\n{\"Reference\":[\"GPT-4V [41]\", \"Contrastive Language-Image PreTraining (CLIP) model [43]\", \"DINOv2 [42]\", \"GPT-4 [41]\", \"ImageNet [47]\", \"LAION-Aesthetic datasets [48]\", \"EVA01 ViT-g-14+ [54]\", \"LLaVA-1.5 [30]\", \"Mini-GPT4 [71]\", \"GPT-4V [40]\", \"Gemini [14]\", \"Bard [13]\", \"EVA02 ViT-bigE-14+ [54]\", \"LLaVA [30, 31]\", \"DINOV2-ViT-L-14 [42]\", \"CLIP-ViT-L-14 [43]\", \"MAE\", \"MoCoV3\", \"MMBench [32]\", \"GQA [21]\", \"Multimodal LLMs [8, 13, 30, 31, 40]\", \"Large Language Models [3, 41, 58, 59, 69]\", \"TextVQA [52]\", \"VQAv2 [15]\", \"MM-Vet [64]\", \"SLIP [39]\", \"Tschannen et al. [60]\"], \"Citation\":[]}\n[The End of Assistant's Predicted Answer]\n\n[System]\nWe would like to request your feedback on the performance of the AI assistant in response to the user question displayed above according to the gold answer. Please use the following listed aspects and their descriptions as evaluation criteria:\n    - Accuracy and Hallucinations: The assistant's answer is semantically consistent with the gold answer; The numerical value and order need to be accurate, and there should be no hallucinations.\n    - Completeness: Referring to the reference answers, the assistant's answer should contain all the key points needed to answer the user's question; further elaboration on these key points can be omitted.\nPlease rate whether this answer is suitable for the question. Please note that the gold answer can be considered as a correct answer to the question.\n\nThe assistant receives an overall score on a scale of 1 to 100, where a higher score indicates better overall performance.\nPlease note that if the assistant's answer and the gold answer fully meet the above criteria, its overall rating should be the full marks (100).\nPlease first provide a comprehensive explanation of your evaluation, avoiding any potential bias.\nThen, output a line indicating the score of the Assistant.\n\nPLEASE OUTPUT WITH THE FOLLOWING FORMAT, WHERE THE SCORE IS A SCALE OF 1 TO 100 BY STRICTLY FOLLOWING THIS FORMAT: \"[[score]]\", FOR EXAMPLE \"Rating: [[100]]\":\n<start output>\nEvaluation evidence: your evluation explanation here, no more than 100 words\nRating: [[score]]\n<end output>\n\nNow, start your evaluation:",
        "generate_response": "{\"Reference\":[\"GPT-4V [41]\", \"Contrastive Language-Image PreTraining (CLIP) model [43]\", \"DINOv2 [42]\", \"GPT-4 [41]\", \"ImageNet [47]\", \"LAION-Aesthetic datasets [48]\", \"EVA01 ViT-g-14+ [54]\", \"LLaVA-1.5 [30]\", \"Mini-GPT4 [71]\", \"GPT-4V [40]\", \"Gemini [14]\", \"Bard [13]\", \"EVA02 ViT-bigE-14+ [54]\", \"LLaVA [30, 31]\", \"DINOV2-ViT-L-14 [42]\", \"CLIP-ViT-L-14 [43]\", \"MAE\", \"MoCoV3\", \"MMBench [32]\", \"GQA [21]\", \"Multimodal LLMs [8, 13, 30, 31, 40]\", \"Large Language Models [3, 41, 58, 59, 69]\", \"TextVQA [52]\", \"VQAv2 [15]\", \"MM-Vet [64]\", \"SLIP [39]\", \"Tschannen et al. [60]\"], \"Citation\":[]}",
        "eval_response": "Evaluation evidence: The assistant's answer does not align with the gold answer in terms of accuracy. The references listed in the assistant's answer do not match the single reference provided in the gold answer, indicating a significant discrepancy and potential hallucination of data. Additionally, the assistant's answer fails to include the citation mentioned in the gold answer, leading to incompleteness in addressing the user's question. The assistant's response contains multiple references and no citations, which does not reflect the specific relationships outlined in the gold answer.\n\nRating: [[20]]",
        "score": 20
    },
    {
        "level": 3,
        "set": 2,
        "length": 64332,
        "type": "paper",
        "language": "en",
        "question": "Self-Discover: Large Language Models Self-Compose Reasoning Structures",
        "instruction": "We hope you will carefully study the provided papers and determine the citation relationships between them. Please follow the instructions below strictly to complete the task:\n\n#Specific Requirements:\n1. Reference: When a given paper mentions other provided papers, those other papers are considered as \"references\" for the given paper. To summarize in this specific context, references are about what the given paper is using.\n2. Citation: Conversely, when other provided papers mention the given paper in their works, the given paper is being \"cited\" by those other papers. To summarize in this specific context, citations are about who is using the given paper.\n3. Given a paper, you need to determine the citation or reference relationship between this paper and the other papers. Do not consider papers that are not provided.\n3. Please present the paper titles in a json format as follows: {{\"Reference\":[\"Reference Title 1\", \"Reference Title 2\", ..., \"Reference Title n\"], \"Citation\":[\"Citation Title 1\", \"Citation Title 2\", ..., \"Citation Title n\"]}}.\n4. If a paper does not have any references or citations, please leave the corresponding list empty, e.g.{{\"Refernce\":[]}}, {{\"Citation\":[]}}.",
        "prompt_template": "#Papers:\n{docs}\n\n{instruction}\n\n#The paper you need to analyze:\n{question}",
        "doc": [
            "2402.03620.md",
            "2402.15205.md",
            "2404.15488.md",
            "2310.03051.md",
            "2310.14628.md"
        ],
        "answer": {
            "Reference": [
                "# How FaR ARE Large LanguAGE MoDELS FROM AGENTS WITH THEORY-OF-MIND? ",
                "# Plan, Verify and Switch: Integrated Reasoning with Diverse X-of-Thoughts "
            ],
            "Citation": [
                "# Enhancing ICU Patient Recovery: Using LLMs to Assist Nurses in Diary Writing ",
                "# IryNLP at MEDIQA-CORR 2024: Tackling the Medical Error Detection \\& Correction Task On the Shoulders of Medical Agents "
            ]
        },
        "shuffle_doc": true,
        "id": "6e540221-f25d-4b6d-b6ce-8318973a1ef1",
        "prompt": "[Question]\n#Papers:\n{docs}\n\nWe hope you will carefully study the provided papers and determine the citation relationships between them. Please follow the instructions below strictly to complete the task:\n\n#Specific Requirements:\n1. Reference: When a given paper mentions other provided papers, those other papers are considered as \"references\" for the given paper. To summarize in this specific context, references are about what the given paper is using.\n2. Citation: Conversely, when other provided papers mention the given paper in their works, the given paper is being \"cited\" by those other papers. To summarize in this specific context, citations are about who is using the given paper.\n3. Given a paper, you need to determine the citation or reference relationship between this paper and the other papers. Do not consider papers that are not provided.\n3. Please present the paper titles in a json format as follows: {{\"Reference\":[\"Reference Title 1\", \"Reference Title 2\", ..., \"Reference Title n\"], \"Citation\":[\"Citation Title 1\", \"Citation Title 2\", ..., \"Citation Title n\"]}}.\n4. If a paper does not have any references or citations, please leave the corresponding list empty, e.g.{{\"Refernce\":[]}}, {{\"Citation\":[]}}.\n\n#The paper you need to analyze:\nSelf-Discover: Large Language Models Self-Compose Reasoning Structures\n\n[Gold Answer]\n{'Reference': ['# How FaR ARE Large LanguAGE MoDELS FROM AGENTS WITH THEORY-OF-MIND? ', '# Plan, Verify and Switch: Integrated Reasoning with Diverse X-of-Thoughts '], 'Citation': ['# Enhancing ICU Patient Recovery: Using LLMs to Assist Nurses in Diary Writing ', '# IryNLP at MEDIQA-CORR 2024: Tackling the Medical Error Detection \\\\& Correction Task On the Shoulders of Medical Agents ']}\n\n[The Start of Assistant's Predicted Answer]\n{{\"Reference\":[\"Language models are few-shot learners (Brown et al., 2020)\", \"Palm: Scaling language modeling with pathways (Chowdhery et al., 2022)\", \"Measuring mathematical problem solving with the math dataset (Hendrycks et al., 2021)\", \"Less is more: Summary of long instructions is better for program synthesis (Kuznia et al., 2022)\", \"Challenging big-bench tasks and whether chain-of-thought can solve them (Suzgun et al., 2022)\", \"Lila: A unified benchmark for mathematical reasoning (Mishra et al., 2022a)\", \"Refining instructional prompts to gptk's language (Mishra et al., 2022b)\", \"Scaling instruction-finetuned language models (Chung et al., 2022)\", \"Training language models to follow instructions with human feedback (Ouyang et al., 2022)\", \"Least-to-most prompting enables complex reasoning in large language models (Zhou et al., 2022a)\", \"Chain-of-thought prompting elicits reasoning in large language models (Wei et al., 2022)\", \"Pal: Program-aided language models (Gao et al., 2023b)\", \"Reasoning with language model is planning with world model (Hao et al., 2023)\", \"Plan-and-solve prompting: Improving zero-shot chain-of-thought reasoning by large language models (Wang et al., 2023)\", \"Large language models as optimizers (Yang et al., 2023)\", \"Take a step back: Evoking reasoning via abstraction in large language models (Zheng et al., 2023)\"], \"Citation\":[\"How FaR ARE Large LanguAGE MoDELS FROM AGENTS WITH THEORY-OF-MIND?\", \"IryNLP at MEDIQA-CORR 2024: Tackling the Medical Error Detection \\\\& Correction Task On the Shoulders of Medical Agents\"]}}\"\n[The End of Assistant's Predicted Answer]\n\n[System]\nWe would like to request your feedback on the performance of the AI assistant in response to the user question displayed above according to the gold answer. Please use the following listed aspects and their descriptions as evaluation criteria:\n    - Accuracy and Hallucinations: The assistant's answer is semantically consistent with the gold answer; The numerical value and order need to be accurate, and there should be no hallucinations.\n    - Completeness: Referring to the reference answers, the assistant's answer should contain all the key points needed to answer the user's question; further elaboration on these key points can be omitted.\nPlease rate whether this answer is suitable for the question. Please note that the gold answer can be considered as a correct answer to the question.\n\nThe assistant receives an overall score on a scale of 1 to 100, where a higher score indicates better overall performance.\nPlease note that if the assistant's answer and the gold answer fully meet the above criteria, its overall rating should be the full marks (100).\nPlease first provide a comprehensive explanation of your evaluation, avoiding any potential bias.\nThen, output a line indicating the score of the Assistant.\n\nPLEASE OUTPUT WITH THE FOLLOWING FORMAT, WHERE THE SCORE IS A SCALE OF 1 TO 100 BY STRICTLY FOLLOWING THIS FORMAT: \"[[score]]\", FOR EXAMPLE \"Rating: [[100]]\":\n<start output>\nEvaluation evidence: your evluation explanation here, no more than 100 words\nRating: [[score]]\n<end output>\n\nNow, start your evaluation:",
        "generate_response": "{{\"Reference\":[\"Language models are few-shot learners (Brown et al., 2020)\", \"Palm: Scaling language modeling with pathways (Chowdhery et al., 2022)\", \"Measuring mathematical problem solving with the math dataset (Hendrycks et al., 2021)\", \"Less is more: Summary of long instructions is better for program synthesis (Kuznia et al., 2022)\", \"Challenging big-bench tasks and whether chain-of-thought can solve them (Suzgun et al., 2022)\", \"Lila: A unified benchmark for mathematical reasoning (Mishra et al., 2022a)\", \"Refining instructional prompts to gptk's language (Mishra et al., 2022b)\", \"Scaling instruction-finetuned language models (Chung et al., 2022)\", \"Training language models to follow instructions with human feedback (Ouyang et al., 2022)\", \"Least-to-most prompting enables complex reasoning in large language models (Zhou et al., 2022a)\", \"Chain-of-thought prompting elicits reasoning in large language models (Wei et al., 2022)\", \"Pal: Program-aided language models (Gao et al., 2023b)\", \"Reasoning with language model is planning with world model (Hao et al., 2023)\", \"Plan-and-solve prompting: Improving zero-shot chain-of-thought reasoning by large language models (Wang et al., 2023)\", \"Large language models as optimizers (Yang et al., 2023)\", \"Take a step back: Evoking reasoning via abstraction in large language models (Zheng et al., 2023)\"], \"Citation\":[\"How FaR ARE Large LanguAGE MoDELS FROM AGENTS WITH THEORY-OF-MIND?\", \"IryNLP at MEDIQA-CORR 2024: Tackling the Medical Error Detection \\\\& Correction Task On the Shoulders of Medical Agents\"]}}\"",
        "eval_response": "Evaluation evidence: The assistant's answer does not match the gold answer in terms of accuracy and completeness. The references and citations listed in the assistant's answer are entirely different from those in the gold answer, indicating a significant deviation and hallucination of content. The assistant's response fails to identify the correct papers that reference or cite the given paper, which is crucial for the task.\n\nRating: [[10]]",
        "score": 10
    },
    {
        "level": 3,
        "set": 2,
        "length": 88512,
        "type": "paper",
        "language": "en",
        "question": "A Comprehensive Study of Knowledge Editing for Large Language Models",
        "instruction": "We hope you will carefully study the provided papers and determine the citation relationships between them. Please follow the instructions below strictly to complete the task:\n\n#Specific Requirements:\n1. Reference: When a given paper mentions other provided papers, those other papers are considered as \"references\" for the given paper. To summarize in this specific context, references are about what the given paper is using.\n2. Citation: Conversely, when other provided papers mention the given paper in their works, the given paper is being \"cited\" by those other papers. To summarize in this specific context, citations are about who is using the given paper.\n3. Given a paper, you need to determine the citation or reference relationship between this paper and the other papers. Do not consider papers that are not provided.\n3. Please present the paper titles in a json format as follows: {{\"Reference\":[\"Reference Title 1\", \"Reference Title 2\", ..., \"Reference Title n\"], \"Citation\":[\"Citation Title 1\", \"Citation Title 2\", ..., \"Citation Title n\"]}}.\n4. If a paper does not have any references or citations, please leave the corresponding list empty, e.g.{{\"Refernce\":[]}}, {{\"Citation\":[]}}.",
        "prompt_template": "#Papers:\n{docs}\n\n{instruction}\n\n#The paper you need to analyze:\n{question}",
        "doc": [
            "2401.01286.md",
            "2405.19010.md",
            "2312.17173.md"
        ],
        "answer": {
            "Reference": [
                "# Non-Vacuous Generalization Bounds for Large Language Models "
            ],
            "Citation": [
                "# Evaluating the External and Parametric Knowledge Fusion of Large Language Models "
            ]
        },
        "shuffle_doc": true,
        "id": "4e9c9c0f-962a-48eb-ae3f-376a92c935e5",
        "prompt": "[Question]\n#Papers:\n{docs}\n\nWe hope you will carefully study the provided papers and determine the citation relationships between them. Please follow the instructions below strictly to complete the task:\n\n#Specific Requirements:\n1. Reference: When a given paper mentions other provided papers, those other papers are considered as \"references\" for the given paper. To summarize in this specific context, references are about what the given paper is using.\n2. Citation: Conversely, when other provided papers mention the given paper in their works, the given paper is being \"cited\" by those other papers. To summarize in this specific context, citations are about who is using the given paper.\n3. Given a paper, you need to determine the citation or reference relationship between this paper and the other papers. Do not consider papers that are not provided.\n3. Please present the paper titles in a json format as follows: {{\"Reference\":[\"Reference Title 1\", \"Reference Title 2\", ..., \"Reference Title n\"], \"Citation\":[\"Citation Title 1\", \"Citation Title 2\", ..., \"Citation Title n\"]}}.\n4. If a paper does not have any references or citations, please leave the corresponding list empty, e.g.{{\"Refernce\":[]}}, {{\"Citation\":[]}}.\n\n#The paper you need to analyze:\nA Comprehensive Study of Knowledge Editing for Large Language Models\n\n[Gold Answer]\n{'Reference': ['# Non-Vacuous Generalization Bounds for Large Language Models '], 'Citation': ['# Evaluating the External and Parametric Knowledge Fusion of Large Language Models ']}\n\n[The Start of Assistant's Predicted Answer]\n{\"Reference\": [\"Language Patches\", \"CaliNET\", \"T-Patcher\", \"REMEDI\", \"GRACE\", \"LoRA\", \"MELO\", \"FT-Constrained\", \"ENN\", \"KE\", \"SLAG\", \"MEND\", \"Knowledge Neuron\", \"ROME\", \"MEMIT\", \"PMET\", \"MALMEN\", \"BIRD\", \"SERAC\", \"AdaLoRA\", \"IKE\", \"ICE\", \"PokeMQA\", \"MEND\", \"WikiData recent\", \"ZsRE\", \"WikiBio\", \"WikiData counter fact\", \"Convsent\", \"Sanitation\", \"Measuring Massive Multitask Language Understanding\", \"Eval-KLLM\", \"Bi-ZsRE\", \"Retrieval-augmented Multilingual Knowledge Editor (ReMaKE)\", \"ENTITY INFERENCES\", \"DepEdit\", \"Bias in Bios dataset\", \"WinoBias\", \"StereoSet\", \"PrivQA\", \"A Survey on Evaluation of Large Language Models\", \"Vicuna: An Open-Source Chatbot Impressing GPT-4 with $90 \\% *$ ChatGPT Quality\", \"ChatLaw: Open-Source Legal Large Language Model with Integrated External Knowledge Bases\", \"Compositional Semantic Parsing with Large Language Models\", \"Open LLM Leaderboard\", \"How Close is ChatGPT to Human Experts? Comparison Corpus, Evaluation, and Detection\", \"REALM: Retrieval-Augmented Language Model PreTraining\", \"Rethinking with Retrieval: Faithful Large Language Model Inference\", \"Measuring Massive Multitask Language Understanding\", \"Atlas: Few-shot Learning with Retrieval Augmented Language Models\"], \"Citation\": [\"Attention is all you need\", \"What does BERT learn about the structure of language?\", \"Locating and editing factual associations in GPT\", \"Linearity of relation decoding in transformer language models\", \"Linguistic knowledge and transferability of contextual representations\", \"Physics of language models: Part 3.1, knowledge storage and extraction\", \"Physics of language models: Part 3.2, knowledge manipulation\", \"Editing factual knowledge in language models\", \"Methods for measuring, updating, and visualizing factual beliefs in language models\", \"Fast model editing at scale\", \"Massediting memory in a transformer\", \"Precise model editing in a transformer\", \"Massive editing for large language models via meta learning\", \"Untying the reversal curse via bidirectional language model editing\", \"Combating misinformation in the age of llms: Opportunities and challenges\", \"Editing language model-based knowledge graph embeddings\", \"GNNDelete: A general strategy for unlearning in graph neural networks\", \"Editable graph neural network for node classifications\", \"Seeking neural nuggets: Knowledge transfer in large language models from a parametric perspective\", \"Discovering knowledge-critical subnetworks in pretrained language models\", \"A comprehensive survey of continual learning: Theory, method and application\", \"Never-ending learning\", \"A survey of chain of thought reasoning: Advances, frontiers and future\", \"Emergent abilities of large language models\", \"Non-vacuous generalization bounds for large language models\", \"Grokking as simplification: A nonlinear complexity perspective\", \"Language modeling is compression\", \"Polyglot or not? measuring multilingual encyclopedic knowledge in foundation models\", \"The life cycle of knowledge in big language models: A survey\", \"Give me the facts! A survey on factual knowledge probing in pre-trained language models\", \"Unifying large language models and knowledge graphs: A roadmap\", \"Large language models and knowledge graphs: Opportunities and challenges\", \"How do large language models capture the ever-changing world knowledge? A review of recent advances\", \"Can language models serve as temporal knowledge bases?\", \"Time-aware language models as temporal knowledge bases\", \"A review on language models as knowledge bases\", \"Self-knowledge guided retrieval augmentation for large language models\", \"Recall: A benchmark for llms robustness against external counterfactual knowledge\", \"Resolving knowledge conflicts in large language models\", \"Adaptive chameleon or stubborn sloth: Revealing the behavior of large language models in knowledge conflicts\", \"Characterizing mechanisms for factual recall in language models\", \"Forgetting before learning: Utilizing parametric arithmetic for knowledge updating in large language models\", \"A divide and conquer framework for knowledge editing\", \"The reversal curse: Llms trained on \\\"a is b\\\" fail to learn \\\"b is a\\\"\", \"Emptying the ocean with a spoon: Should we edit models?\", \"Zero-shot relation extraction via reading comprehension\", \"Selfcheckgpt: Zero-resource black-box hallucination detection for generative large language models\", \"When not to trust language models: Investigating effectiveness of parametric and non-parametric memories\", \"Knowledge sanitization of large language models\", \"Evaluating dependencies in fact editing for language models: Specificity and implication awareness\", \"Language anisotropic cross-lingual model editing\", \"Cross-lingual knowledge editing in large language models\", \"Retrieval-augmented multilingual knowledge editing\", \"DUNE: Dataset for unified editing\", \"Editing models with task arithmetic\", \"Editing a classifier by rewriting its prediction rules\", \"Edit at your own risk: evaluating the robustness of edited models to distribution shifts\", \"Task arithmetic in the tangent space: Improved editing of pre-trained models\", \"Neuron patching: Neuron-level model editing on code generation and llms\", \"Seeking neural nuggets: Knowledge transfer in large language models from a parametric perspective\", \"Discovering knowledge-critical subnetworks in pretrained language models\", \"Deep model fusion: A survey\", \"An empirical study of multimodal model merging\", \"Composing parameter-efficient modules with arithmetic operations\"]}\n[The End of Assistant's Predicted Answer]\n\n[System]\nWe would like to request your feedback on the performance of the AI assistant in response to the user question displayed above according to the gold answer. Please use the following listed aspects and their descriptions as evaluation criteria:\n    - Accuracy and Hallucinations: The assistant's answer is semantically consistent with the gold answer; The numerical value and order need to be accurate, and there should be no hallucinations.\n    - Completeness: Referring to the reference answers, the assistant's answer should contain all the key points needed to answer the user's question; further elaboration on these key points can be omitted.\nPlease rate whether this answer is suitable for the question. Please note that the gold answer can be considered as a correct answer to the question.\n\nThe assistant receives an overall score on a scale of 1 to 100, where a higher score indicates better overall performance.\nPlease note that if the assistant's answer and the gold answer fully meet the above criteria, its overall rating should be the full marks (100).\nPlease first provide a comprehensive explanation of your evaluation, avoiding any potential bias.\nThen, output a line indicating the score of the Assistant.\n\nPLEASE OUTPUT WITH THE FOLLOWING FORMAT, WHERE THE SCORE IS A SCALE OF 1 TO 100 BY STRICTLY FOLLOWING THIS FORMAT: \"[[score]]\", FOR EXAMPLE \"Rating: [[100]]\":\n<start output>\nEvaluation evidence: your evluation explanation here, no more than 100 words\nRating: [[score]]\n<end output>\n\nNow, start your evaluation:",
        "generate_response": "{\"Reference\": [\"Language Patches\", \"CaliNET\", \"T-Patcher\", \"REMEDI\", \"GRACE\", \"LoRA\", \"MELO\", \"FT-Constrained\", \"ENN\", \"KE\", \"SLAG\", \"MEND\", \"Knowledge Neuron\", \"ROME\", \"MEMIT\", \"PMET\", \"MALMEN\", \"BIRD\", \"SERAC\", \"AdaLoRA\", \"IKE\", \"ICE\", \"PokeMQA\", \"MEND\", \"WikiData recent\", \"ZsRE\", \"WikiBio\", \"WikiData counter fact\", \"Convsent\", \"Sanitation\", \"Measuring Massive Multitask Language Understanding\", \"Eval-KLLM\", \"Bi-ZsRE\", \"Retrieval-augmented Multilingual Knowledge Editor (ReMaKE)\", \"ENTITY INFERENCES\", \"DepEdit\", \"Bias in Bios dataset\", \"WinoBias\", \"StereoSet\", \"PrivQA\", \"A Survey on Evaluation of Large Language Models\", \"Vicuna: An Open-Source Chatbot Impressing GPT-4 with $90 \\% *$ ChatGPT Quality\", \"ChatLaw: Open-Source Legal Large Language Model with Integrated External Knowledge Bases\", \"Compositional Semantic Parsing with Large Language Models\", \"Open LLM Leaderboard\", \"How Close is ChatGPT to Human Experts? Comparison Corpus, Evaluation, and Detection\", \"REALM: Retrieval-Augmented Language Model PreTraining\", \"Rethinking with Retrieval: Faithful Large Language Model Inference\", \"Measuring Massive Multitask Language Understanding\", \"Atlas: Few-shot Learning with Retrieval Augmented Language Models\"], \"Citation\": [\"Attention is all you need\", \"What does BERT learn about the structure of language?\", \"Locating and editing factual associations in GPT\", \"Linearity of relation decoding in transformer language models\", \"Linguistic knowledge and transferability of contextual representations\", \"Physics of language models: Part 3.1, knowledge storage and extraction\", \"Physics of language models: Part 3.2, knowledge manipulation\", \"Editing factual knowledge in language models\", \"Methods for measuring, updating, and visualizing factual beliefs in language models\", \"Fast model editing at scale\", \"Massediting memory in a transformer\", \"Precise model editing in a transformer\", \"Massive editing for large language models via meta learning\", \"Untying the reversal curse via bidirectional language model editing\", \"Combating misinformation in the age of llms: Opportunities and challenges\", \"Editing language model-based knowledge graph embeddings\", \"GNNDelete: A general strategy for unlearning in graph neural networks\", \"Editable graph neural network for node classifications\", \"Seeking neural nuggets: Knowledge transfer in large language models from a parametric perspective\", \"Discovering knowledge-critical subnetworks in pretrained language models\", \"A comprehensive survey of continual learning: Theory, method and application\", \"Never-ending learning\", \"A survey of chain of thought reasoning: Advances, frontiers and future\", \"Emergent abilities of large language models\", \"Non-vacuous generalization bounds for large language models\", \"Grokking as simplification: A nonlinear complexity perspective\", \"Language modeling is compression\", \"Polyglot or not? measuring multilingual encyclopedic knowledge in foundation models\", \"The life cycle of knowledge in big language models: A survey\", \"Give me the facts! A survey on factual knowledge probing in pre-trained language models\", \"Unifying large language models and knowledge graphs: A roadmap\", \"Large language models and knowledge graphs: Opportunities and challenges\", \"How do large language models capture the ever-changing world knowledge? A review of recent advances\", \"Can language models serve as temporal knowledge bases?\", \"Time-aware language models as temporal knowledge bases\", \"A review on language models as knowledge bases\", \"Self-knowledge guided retrieval augmentation for large language models\", \"Recall: A benchmark for llms robustness against external counterfactual knowledge\", \"Resolving knowledge conflicts in large language models\", \"Adaptive chameleon or stubborn sloth: Revealing the behavior of large language models in knowledge conflicts\", \"Characterizing mechanisms for factual recall in language models\", \"Forgetting before learning: Utilizing parametric arithmetic for knowledge updating in large language models\", \"A divide and conquer framework for knowledge editing\", \"The reversal curse: Llms trained on \\\"a is b\\\" fail to learn \\\"b is a\\\"\", \"Emptying the ocean with a spoon: Should we edit models?\", \"Zero-shot relation extraction via reading comprehension\", \"Selfcheckgpt: Zero-resource black-box hallucination detection for generative large language models\", \"When not to trust language models: Investigating effectiveness of parametric and non-parametric memories\", \"Knowledge sanitization of large language models\", \"Evaluating dependencies in fact editing for language models: Specificity and implication awareness\", \"Language anisotropic cross-lingual model editing\", \"Cross-lingual knowledge editing in large language models\", \"Retrieval-augmented multilingual knowledge editing\", \"DUNE: Dataset for unified editing\", \"Editing models with task arithmetic\", \"Editing a classifier by rewriting its prediction rules\", \"Edit at your own risk: evaluating the robustness of edited models to distribution shifts\", \"Task arithmetic in the tangent space: Improved editing of pre-trained models\", \"Neuron patching: Neuron-level model editing on code generation and llms\", \"Seeking neural nuggets: Knowledge transfer in large language models from a parametric perspective\", \"Discovering knowledge-critical subnetworks in pretrained language models\", \"Deep model fusion: A survey\", \"An empirical study of multimodal model merging\", \"Composing parameter-efficient modules with arithmetic operations\"]}",
        "eval_response": "Evaluation evidence: The assistant's answer is not accurate and contains hallucinations. The provided answer lists numerous references and citations that are not mentioned in the gold answer, indicating a significant deviation from the required information. The gold answer specifies one reference and one citation, which are completely absent in the assistant's response. This shows a lack of accuracy and introduces irrelevant data, which does not meet the task requirements.\n\nRating: [[10]]",
        "score": 10
    },
    {
        "level": 3,
        "set": 2,
        "length": 93401,
        "type": "paper",
        "language": "en",
        "question": "TOFU: A Task of Fictitious Unlearning for LLMs",
        "instruction": "We hope you will carefully study the provided papers and determine the citation relationships between them. Please follow the instructions below strictly to complete the task:\n\n#Specific Requirements:\n1. Reference: When a given paper mentions other provided papers, those other papers are considered as \"references\" for the given paper. To summarize in this specific context, references are about what the given paper is using.\n2. Citation: Conversely, when other provided papers mention the given paper in their works, the given paper is being \"cited\" by those other papers. To summarize in this specific context, citations are about who is using the given paper.\n3. Given a paper, you need to determine the citation or reference relationship between this paper and the other papers. Do not consider papers that are not provided.\n3. Please present the paper titles in a json format as follows: {{\"Reference\":[\"Reference Title 1\", \"Reference Title 2\", ..., \"Reference Title n\"], \"Citation\":[\"Citation Title 1\", \"Citation Title 2\", ..., \"Citation Title n\"]}}.\n4. If a paper does not have any references or citations, please leave the corresponding list empty, e.g.{{\"Refernce\":[]}}, {{\"Citation\":[]}}.",
        "prompt_template": "#Papers:\n{docs}\n\n{instruction}\n\n#The paper you need to analyze:\n{question}",
        "doc": [
            "2401.06121.md",
            "2405.03097.md",
            "2405.12523.md",
            "2405.19563.md"
        ],
        "answer": {
            "Reference": [],
            "Citation": [
                "# To Each (Textual Sequence) Its Own: Improving Memorized-Data Unlearning in Large Language Models ",
                "# Single Image Unlearning: Efficient Machine Unlearning in Multimodal Large Language Models ",
                "# Unlearning Climate Misinformation in Large Language Models "
            ]
        },
        "shuffle_doc": true,
        "id": "1ee9e393-74b2-45e0-a02e-d715a7fec886",
        "prompt": "[Question]\n#Papers:\n{docs}\n\nWe hope you will carefully study the provided papers and determine the citation relationships between them. Please follow the instructions below strictly to complete the task:\n\n#Specific Requirements:\n1. Reference: When a given paper mentions other provided papers, those other papers are considered as \"references\" for the given paper. To summarize in this specific context, references are about what the given paper is using.\n2. Citation: Conversely, when other provided papers mention the given paper in their works, the given paper is being \"cited\" by those other papers. To summarize in this specific context, citations are about who is using the given paper.\n3. Given a paper, you need to determine the citation or reference relationship between this paper and the other papers. Do not consider papers that are not provided.\n3. Please present the paper titles in a json format as follows: {{\"Reference\":[\"Reference Title 1\", \"Reference Title 2\", ..., \"Reference Title n\"], \"Citation\":[\"Citation Title 1\", \"Citation Title 2\", ..., \"Citation Title n\"]}}.\n4. If a paper does not have any references or citations, please leave the corresponding list empty, e.g.{{\"Refernce\":[]}}, {{\"Citation\":[]}}.\n\n#The paper you need to analyze:\nTOFU: A Task of Fictitious Unlearning for LLMs\n\n[Gold Answer]\n{'Reference': [], 'Citation': ['# To Each (Textual Sequence) Its Own: Improving Memorized-Data Unlearning in Large Language Models ', '# Single Image Unlearning: Efficient Machine Unlearning in Multimodal Large Language Models ', '# Unlearning Climate Misinformation in Large Language Models ']}\n\n[The Start of Assistant's Predicted Answer]\n{\"Reference\":[\"Unlearn what you want to forget: Efficient unlearning for llms\", \"Who's harry potter? approximate unlearning in llms\", \"Towards adversarial evaluations for inexact machine unlearning\", \"Extracting training data from large language models\", \"Editing factual knowledge in language models\", \"A Survey on Evaluation of Large Language Models\", \"Certified data removal from machine learning models\", \"Separate the wheat from the chaff: Model deficiency unlearning via parameter-efficient module operation\", \"Are large pre-trained language models leaking your personal information?\", \"Auditing differentially private machine learning: How private is private sgd?\", \"Knowledge unlearning for mitigating privacy risks in language models\", \"Propile: Probing privacy leakage in large language models\", \"Scalable unlearning by selectively disobeying the teacher\", \"Towards unbounded machine unlearning\", \"Textbooks are all you need ii: phi-1.5 technical report\", \"Rouge: A package for automatic evaluation of summaries\", \"Quark: Controllable text generation with reinforced unlearning\", \"Dataset inference: Ownership resolution in machine learning\", \"Locating and editing factual associations in gpt\", \"Adversary instantiation: Lower bounds for differentially private machine learning\", \"Ccpa regulations: Final regulation text\", \"Can sensitive information be deleted from llms? objectives for defending against extraction attacks\", \"In-context unlearning: Language models as few shot unlearners\", \"Direct preference optimization: Your language model is secretly a reward model\", \"Remember what you want to forget: Algorithms for machine unlearning\", \"Privacy auditing with one (1) training run\", \"The eu general data protection regulation (gdpr). A Practical Guide\", \"Kga: A general machine unlearning framework based on knowledge gap alignment\", \"Jailbroken: How does llm safety training fail?\", \"Large language model unlearning\", \"Right to be forgotten in the era of large language models: Implications, challenges, and solutions\", \"A comprehensive study of knowledge editing for large language models\", \"Universal and transferable adversarial attacks on aligned language models\"], \"Citation\":[\"To Each (Textual Sequence) Its Own: Improving Memorized-Data Unlearning in Large Language Models\"]}\n[The End of Assistant's Predicted Answer]\n\n[System]\nWe would like to request your feedback on the performance of the AI assistant in response to the user question displayed above according to the gold answer. Please use the following listed aspects and their descriptions as evaluation criteria:\n    - Accuracy and Hallucinations: The assistant's answer is semantically consistent with the gold answer; The numerical value and order need to be accurate, and there should be no hallucinations.\n    - Completeness: Referring to the reference answers, the assistant's answer should contain all the key points needed to answer the user's question; further elaboration on these key points can be omitted.\nPlease rate whether this answer is suitable for the question. Please note that the gold answer can be considered as a correct answer to the question.\n\nThe assistant receives an overall score on a scale of 1 to 100, where a higher score indicates better overall performance.\nPlease note that if the assistant's answer and the gold answer fully meet the above criteria, its overall rating should be the full marks (100).\nPlease first provide a comprehensive explanation of your evaluation, avoiding any potential bias.\nThen, output a line indicating the score of the Assistant.\n\nPLEASE OUTPUT WITH THE FOLLOWING FORMAT, WHERE THE SCORE IS A SCALE OF 1 TO 100 BY STRICTLY FOLLOWING THIS FORMAT: \"[[score]]\", FOR EXAMPLE \"Rating: [[100]]\":\n<start output>\nEvaluation evidence: your evluation explanation here, no more than 100 words\nRating: [[score]]\n<end output>\n\nNow, start your evaluation:",
        "generate_response": "{\"Reference\":[\"Unlearn what you want to forget: Efficient unlearning for llms\", \"Who's harry potter? approximate unlearning in llms\", \"Towards adversarial evaluations for inexact machine unlearning\", \"Extracting training data from large language models\", \"Editing factual knowledge in language models\", \"A Survey on Evaluation of Large Language Models\", \"Certified data removal from machine learning models\", \"Separate the wheat from the chaff: Model deficiency unlearning via parameter-efficient module operation\", \"Are large pre-trained language models leaking your personal information?\", \"Auditing differentially private machine learning: How private is private sgd?\", \"Knowledge unlearning for mitigating privacy risks in language models\", \"Propile: Probing privacy leakage in large language models\", \"Scalable unlearning by selectively disobeying the teacher\", \"Towards unbounded machine unlearning\", \"Textbooks are all you need ii: phi-1.5 technical report\", \"Rouge: A package for automatic evaluation of summaries\", \"Quark: Controllable text generation with reinforced unlearning\", \"Dataset inference: Ownership resolution in machine learning\", \"Locating and editing factual associations in gpt\", \"Adversary instantiation: Lower bounds for differentially private machine learning\", \"Ccpa regulations: Final regulation text\", \"Can sensitive information be deleted from llms? objectives for defending against extraction attacks\", \"In-context unlearning: Language models as few shot unlearners\", \"Direct preference optimization: Your language model is secretly a reward model\", \"Remember what you want to forget: Algorithms for machine unlearning\", \"Privacy auditing with one (1) training run\", \"The eu general data protection regulation (gdpr). A Practical Guide\", \"Kga: A general machine unlearning framework based on knowledge gap alignment\", \"Jailbroken: How does llm safety training fail?\", \"Large language model unlearning\", \"Right to be forgotten in the era of large language models: Implications, challenges, and solutions\", \"A comprehensive study of knowledge editing for large language models\", \"Universal and transferable adversarial attacks on aligned language models\"], \"Citation\":[\"To Each (Textual Sequence) Its Own: Improving Memorized-Data Unlearning in Large Language Models\"]}",
        "eval_response": "Evaluation evidence: The assistant's answer does not match the gold answer in terms of accuracy and completeness. The assistant lists numerous references that are not mentioned in the gold answer, indicating a significant hallucination issue. Additionally, the citation list in the assistant's answer only partially matches the gold answer, missing two of the three citations listed in the gold answer. This shows a lack of completeness and accuracy.\n\nRating: [[20]]",
        "score": 20
    },
    {
        "level": 3,
        "set": 4,
        "length": 218125,
        "type": "paper",
        "language": "en",
        "question": "Mastering Text-to-Image Diffusion: Recaptioning, Planning, and Generating with Multimodal LLMs",
        "instruction": "We hope you will carefully study the provided papers and determine the citation relationships between them. Please follow the instructions below strictly to complete the task:\n\n#Specific Requirements:\n1. Reference: When a given paper mentions other provided papers, those other papers are considered as \"references\" for the given paper. To summarize in this specific context, references are about what the given paper is using.\n2. Citation: Conversely, when other provided papers mention the given paper in their works, the given paper is being \"cited\" by those other papers. To summarize in this specific context, citations are about who is using the given paper.\n3. Given a paper, you need to determine the citation or reference relationship between this paper and the other papers. Do not consider papers that are not provided.\n3. Please present the paper titles in a json format as follows: {{\"Reference\":[\"Reference Title 1\", \"Reference Title 2\", ..., \"Reference Title n\"], \"Citation\":[\"Citation Title 1\", \"Citation Title 2\", ..., \"Citation Title n\"]}}.\n4. If a paper does not have any references or citations, please leave the corresponding list empty, e.g.{{\"Refernce\":[]}}, {{\"Citation\":[]}}.",
        "prompt_template": "#Papers:\n{docs}\n\n{instruction}\n\n#The paper you need to analyze:\n{question}",
        "doc": [
            "2401.11708.md",
            "2405.05945.md",
            "2405.08748.md",
            "2405.14785.md",
            "2405.14828.md",
            "2405.15287.md",
            "2311.16090.md",
            "2311.17946.md",
            "2401.02015.md",
            "2402.16627.md",
            "2402.17563.md"
        ],
        "answer": {
            "Reference": [
                "# Self-correcting LLM-controlled Diffusion Models ",
                "# DreamSync: Aligning Text-to-Image Generation with Image Understanding Feedback ",
                "# Improving Diffusion-Based Image Synthesis with Context Prediction ",
                "# Cross-ModAL ConTEXTUALIZED DifFUSION MODELS FOR TEXT-GUIDED VISUAL GENERATION AND EDITING ",
                "# Structure-Guided Adversarial Training of Diffusion Models "
            ],
            "Citation": [
                "# Lumina-T2X: Transforming Text into ",
                "# Hunyuan-DiT : A Powerful Multi-Resolution Diffusion Transformer with Fine-Grained Chinese Understanding ",
                "# EDITWORLD: Simulating World Dynamics for Instruction-Following Image Editing ",
                "# Good Seed Makes a Good Crop: Discovering Secret Seeds in Text-to-Image Diffusion Models ",
                "# StyleMaster: Towards Flexible Stylized Image Generation with Diffusion Models "
            ]
        },
        "shuffle_doc": true,
        "id": "c0677ef8-5a9d-4f95-9b2a-adcff1e2d14d",
        "prompt": "[Question]\n#Papers:\n{docs}\n\nWe hope you will carefully study the provided papers and determine the citation relationships between them. Please follow the instructions below strictly to complete the task:\n\n#Specific Requirements:\n1. Reference: When a given paper mentions other provided papers, those other papers are considered as \"references\" for the given paper. To summarize in this specific context, references are about what the given paper is using.\n2. Citation: Conversely, when other provided papers mention the given paper in their works, the given paper is being \"cited\" by those other papers. To summarize in this specific context, citations are about who is using the given paper.\n3. Given a paper, you need to determine the citation or reference relationship between this paper and the other papers. Do not consider papers that are not provided.\n3. Please present the paper titles in a json format as follows: {{\"Reference\":[\"Reference Title 1\", \"Reference Title 2\", ..., \"Reference Title n\"], \"Citation\":[\"Citation Title 1\", \"Citation Title 2\", ..., \"Citation Title n\"]}}.\n4. If a paper does not have any references or citations, please leave the corresponding list empty, e.g.{{\"Refernce\":[]}}, {{\"Citation\":[]}}.\n\n#The paper you need to analyze:\nMastering Text-to-Image Diffusion: Recaptioning, Planning, and Generating with Multimodal LLMs\n\n[Gold Answer]\n{'Reference': ['# Self-correcting LLM-controlled Diffusion Models ', '# DreamSync: Aligning Text-to-Image Generation with Image Understanding Feedback ', '# Improving Diffusion-Based Image Synthesis with Context Prediction ', '# Cross-ModAL ConTEXTUALIZED DifFUSION MODELS FOR TEXT-GUIDED VISUAL GENERATION AND EDITING ', '# Structure-Guided Adversarial Training of Diffusion Models '], 'Citation': ['# Lumina-T2X: Transforming Text into ', '# Hunyuan-DiT : A Powerful Multi-Resolution Diffusion Transformer with Fine-Grained Chinese Understanding ', '# EDITWORLD: Simulating World Dynamics for Instruction-Following Image Editing ', '# Good Seed Makes a Good Crop: Discovering Secret Seeds in Text-to-Image Diffusion Models ', '# StyleMaster: Towards Flexible Stylized Image Generation with Diffusion Models ']}\n\n[The Start of Assistant's Predicted Answer]\n{\"Reference\":[\"Composable Diffusion\", \"Stable Diffusion v1.4\", \"Stable Diffusion v2\", \"GLIDE\", \"Imagen\", \"Latent Diffusion Models\", \"SDXL\", \"ContextDiff\", \"DALL-E 3\", \"T2I-Adapter\", \"GILL\", \"GORS\", \"LMD\", \"LayoutGPT\", \"DreamLLM\", \"Composer\", \"Opt-IML\", \"Segment Anything\", \"GPT-4\", \"Prompt-to-Prompt Image Editing with Cross Attention Control\", \"Improving Image Generation with Better Captions\", \"InstructPix2Pix: Learning to Follow Image Editing Instructions\", \"Masactrl: Tuning-Free Mutual Self-Attention Control for Consistent Image Synthesis and Editing\", \"Denosing Diffusion Probabilistic Models\", \"Photorealistic Text-to-Image Diffusion Models with Deep Language Understanding\", \"Learning Transferable Visual Models from Natural Language Supervision\", \"High-Resolution Image Synthesis with Latent Diffusion Models\", \"Compositional Text-to-Image Synthesis with Attention Map Control of Diffusion Models\", \"Bloom: A 176B-Parameter Open-Access Multilingual Language Model\", \"Visual ChatGPT: Talking, Drawing and Editing with Visual Foundation Models\", \"Self-Correcting LLM-Controlled Diffusion Models\", \"BoxDiff: Text-to-Image Synthesis with Training-Free Box-Constrained Diffusion\", \"ImageReward: Learning and Evaluating Human Preferences for Text-to-Image Generation\", \"SPIRAL: Self-Supervised Pre-Training for Instructional Reasoning and Learning\", \"Scaling Language Model Instruction Meta Learning through the Lens of Generalization\", \"Generating Images with Multimodal Language Models\", \"BLIP-2: Bootstrapping Language-Image Pre-Training with Frozen Image Encoders and Large Language Models\", \"LayoutLLM$t$2$i$: Eliciting Layout Guidance from LLM for Text-to-Image Generation\", \"Hierarchical Text-Conditional Image Generation with CLIP Latents\", \"Linguistic Binding in Diffusion Models: Enhancing Attribute Correspondence Through Attention Map Alignment\", \"Score-Based Generative Modeling Through Stochastic Differential Equations\", \"Reinforcement Learning from Human Feedback\", \"Learning to Summarize with Human Feedback\", \"DreamSync: Aligning Text-to-Image Generation with Image Understanding Feedback\", \"Generative Pretraining in Multimodality\", \"VIPERGPT: Visual Inference via Python Execution for Reasoning\", \"Galactica: A Large Language Model for Science\", \"Llama: Open and Efficient Foundation Language Models\", \"Llama 2: Open Foundation and Fine-Tuned Chat Models\", \"Scaling Autoregressive Multi-Modal Models: Pretraining and Instruction Tuning\", \"GLM-130B: An Open Bilingual Pre-Trained Model\", \"Adding Conditional Control to Text-to-Image Diffusion Models\", \"Opt: Open Pre-Trained Transformer Language Models\", \"Controllable Text-to-Image Generation with GPT-4\", \"Enhanced Visual Instruction Tuning for Text-Rich Image Understanding\", \"Multimodal Chain-of-Thought Reasoning in Language Models\", \"Generalized Decoding for Pixel, Image, and Language\"], \"Citation\":[\"Self-correcting LLM-Controlled Diffusion Models\"]}\n[The End of Assistant's Predicted Answer]\n\n[System]\nWe would like to request your feedback on the performance of the AI assistant in response to the user question displayed above according to the gold answer. Please use the following listed aspects and their descriptions as evaluation criteria:\n    - Accuracy and Hallucinations: The assistant's answer is semantically consistent with the gold answer; The numerical value and order need to be accurate, and there should be no hallucinations.\n    - Completeness: Referring to the reference answers, the assistant's answer should contain all the key points needed to answer the user's question; further elaboration on these key points can be omitted.\nPlease rate whether this answer is suitable for the question. Please note that the gold answer can be considered as a correct answer to the question.\n\nThe assistant receives an overall score on a scale of 1 to 100, where a higher score indicates better overall performance.\nPlease note that if the assistant's answer and the gold answer fully meet the above criteria, its overall rating should be the full marks (100).\nPlease first provide a comprehensive explanation of your evaluation, avoiding any potential bias.\nThen, output a line indicating the score of the Assistant.\n\nPLEASE OUTPUT WITH THE FOLLOWING FORMAT, WHERE THE SCORE IS A SCALE OF 1 TO 100 BY STRICTLY FOLLOWING THIS FORMAT: \"[[score]]\", FOR EXAMPLE \"Rating: [[100]]\":\n<start output>\nEvaluation evidence: your evluation explanation here, no more than 100 words\nRating: [[score]]\n<end output>\n\nNow, start your evaluation:",
        "generate_response": "{\"Reference\":[\"Composable Diffusion\", \"Stable Diffusion v1.4\", \"Stable Diffusion v2\", \"GLIDE\", \"Imagen\", \"Latent Diffusion Models\", \"SDXL\", \"ContextDiff\", \"DALL-E 3\", \"T2I-Adapter\", \"GILL\", \"GORS\", \"LMD\", \"LayoutGPT\", \"DreamLLM\", \"Composer\", \"Opt-IML\", \"Segment Anything\", \"GPT-4\", \"Prompt-to-Prompt Image Editing with Cross Attention Control\", \"Improving Image Generation with Better Captions\", \"InstructPix2Pix: Learning to Follow Image Editing Instructions\", \"Masactrl: Tuning-Free Mutual Self-Attention Control for Consistent Image Synthesis and Editing\", \"Denosing Diffusion Probabilistic Models\", \"Photorealistic Text-to-Image Diffusion Models with Deep Language Understanding\", \"Learning Transferable Visual Models from Natural Language Supervision\", \"High-Resolution Image Synthesis with Latent Diffusion Models\", \"Compositional Text-to-Image Synthesis with Attention Map Control of Diffusion Models\", \"Bloom: A 176B-Parameter Open-Access Multilingual Language Model\", \"Visual ChatGPT: Talking, Drawing and Editing with Visual Foundation Models\", \"Self-Correcting LLM-Controlled Diffusion Models\", \"BoxDiff: Text-to-Image Synthesis with Training-Free Box-Constrained Diffusion\", \"ImageReward: Learning and Evaluating Human Preferences for Text-to-Image Generation\", \"SPIRAL: Self-Supervised Pre-Training for Instructional Reasoning and Learning\", \"Scaling Language Model Instruction Meta Learning through the Lens of Generalization\", \"Generating Images with Multimodal Language Models\", \"BLIP-2: Bootstrapping Language-Image Pre-Training with Frozen Image Encoders and Large Language Models\", \"LayoutLLM$t$2$i$: Eliciting Layout Guidance from LLM for Text-to-Image Generation\", \"Hierarchical Text-Conditional Image Generation with CLIP Latents\", \"Linguistic Binding in Diffusion Models: Enhancing Attribute Correspondence Through Attention Map Alignment\", \"Score-Based Generative Modeling Through Stochastic Differential Equations\", \"Reinforcement Learning from Human Feedback\", \"Learning to Summarize with Human Feedback\", \"DreamSync: Aligning Text-to-Image Generation with Image Understanding Feedback\", \"Generative Pretraining in Multimodality\", \"VIPERGPT: Visual Inference via Python Execution for Reasoning\", \"Galactica: A Large Language Model for Science\", \"Llama: Open and Efficient Foundation Language Models\", \"Llama 2: Open Foundation and Fine-Tuned Chat Models\", \"Scaling Autoregressive Multi-Modal Models: Pretraining and Instruction Tuning\", \"GLM-130B: An Open Bilingual Pre-Trained Model\", \"Adding Conditional Control to Text-to-Image Diffusion Models\", \"Opt: Open Pre-Trained Transformer Language Models\", \"Controllable Text-to-Image Generation with GPT-4\", \"Enhanced Visual Instruction Tuning for Text-Rich Image Understanding\", \"Multimodal Chain-of-Thought Reasoning in Language Models\", \"Generalized Decoding for Pixel, Image, and Language\"], \"Citation\":[\"Self-correcting LLM-Controlled Diffusion Models\"]}",
        "eval_response": "Evaluation evidence: The assistant's answer does not match the gold answer in terms of accuracy and completeness. The references and citations listed in the assistant's answer are entirely different from those in the gold answer, indicating a significant deviation and possible hallucination of content. The assistant's response fails to accurately identify the correct references and citations for the given paper, which is crucial for the task.\n\nRating: [[10]]",
        "score": 10
    },
    {
        "level": 4,
        "set": 2,
        "length": 54336,
        "type": "paper",
        "language": "en",
        "question": "",
        "instruction": "We kindly ask you to thoroughly review the provided papers and construct a citation chain from them. Please adhere to the following instructions strictly while completing the task:\n\n#Task Instructions:\nGiven several papers, you are required to identify and list the longest citation chain, which demonstrates the citation relationship among the provided papers.\n\n#Specific Requirements:\n1.Please present the titles of the papers in the form of a list, as follows: [\"Title of Paper 1\", \"Title of Paper 2\", ..., \"Title of Paper n\"].\n2.Ensure that the citation chain in the list is linear and continuous, meaning that the first paper title in the list (Paper 1) should not cite any other works. Instead, it should be cited by the next paper in the list (Paper 2); subsequently, each paper should then be cited by the next one in the list, continuing up to the last paper (Paper n).\n3.Consider only the citation relationships within the supplied collection of papers, and ensure that the citation chain accurately reflects the sequential citation order among these documents.\n4.Do not take into account any articles not provided, and disregard other non-linear citation relationships.",
        "prompt_template": "{instruction}\n\n#Papers Provided:\n{docs}",
        "doc": [
            "2307.04684.md",
            "2404.02152.md",
            "2405.08054.md"
        ],
        "answer": [
            "# FreeDrag: Feature Dragging for Reliable Point-based Image Editing ",
            "# GeneAvatar: Generic Expression-Aware Volumetric Head Avatar Editing from a Single Image ",
            "# Coin3D: Controllable and Interactive 3D Assets Generation with Proxy-Guided Conditioning "
        ],
        "shuffle_doc": true,
        "id": "23a93e4b-f627-4dd4-87f5-24e76f0f625f",
        "prompt": "[Question]\nWe kindly ask you to thoroughly review the provided papers and construct a citation chain from them. Please adhere to the following instructions strictly while completing the task:\n\n#Task Instructions:\nGiven several papers, you are required to identify and list the longest citation chain, which demonstrates the citation relationship among the provided papers.\n\n#Specific Requirements:\n1.Please present the titles of the papers in the form of a list, as follows: [\"Title of Paper 1\", \"Title of Paper 2\", ..., \"Title of Paper n\"].\n2.Ensure that the citation chain in the list is linear and continuous, meaning that the first paper title in the list (Paper 1) should not cite any other works. Instead, it should be cited by the next paper in the list (Paper 2); subsequently, each paper should then be cited by the next one in the list, continuing up to the last paper (Paper n).\n3.Consider only the citation relationships within the supplied collection of papers, and ensure that the citation chain accurately reflects the sequential citation order among these documents.\n4.Do not take into account any articles not provided, and disregard other non-linear citation relationships.\n\n#Papers Provided:\n{docs}\n\n[Gold Answer]\n['# FreeDrag: Feature Dragging for Reliable Point-based Image Editing ', '# GeneAvatar: Generic Expression-Aware Volumetric Head Avatar Editing from a Single Image ', '# Coin3D: Controllable and Interactive 3D Assets Generation with Proxy-Guided Conditioning ']\n\n[The Start of Assistant's Predicted Answer]\n[\"FreeDrag: Feature Dragging for Reliable Point-based Image Editing\", \"Diffusionclip: Text-guided diffusion models for robust image manipulation\", \"Sine: Single image editing with text-to-image diffusion models\"]\n[The End of Assistant's Predicted Answer]\n\n[System]\nWe would like to request your feedback on the performance of the AI assistant in response to the user question displayed above according to the gold answer. Please use the following listed aspects and their descriptions as evaluation criteria:\n    - Accuracy and Hallucinations: The assistant's answer is semantically consistent with the gold answer; The numerical value and order need to be accurate, and there should be no hallucinations.\n    - Completeness: Referring to the reference answers, the assistant's answer should contain all the key points needed to answer the user's question; further elaboration on these key points can be omitted.\nPlease rate whether this answer is suitable for the question. Please note that the gold answer can be considered as a correct answer to the question.\n\nThe assistant receives an overall score on a scale of 1 to 100, where a higher score indicates better overall performance.\nPlease note that if the assistant's answer and the gold answer fully meet the above criteria, its overall rating should be the full marks (100).\nPlease first provide a comprehensive explanation of your evaluation, avoiding any potential bias.\nThen, output a line indicating the score of the Assistant.\n\nPLEASE OUTPUT WITH THE FOLLOWING FORMAT, WHERE THE SCORE IS A SCALE OF 1 TO 100 BY STRICTLY FOLLOWING THIS FORMAT: \"[[score]]\", FOR EXAMPLE \"Rating: [[100]]\":\n<start output>\nEvaluation evidence: your evluation explanation here, no more than 100 words\nRating: [[score]]\n<end output>\n\nNow, start your evaluation:",
        "generate_response": "[\"FreeDrag: Feature Dragging for Reliable Point-based Image Editing\", \"Diffusionclip: Text-guided diffusion models for robust image manipulation\", \"Sine: Single image editing with text-to-image diffusion models\"]",
        "eval_response": "Evaluation evidence: The assistant's answer does not match the gold answer in terms of the titles of the papers listed. The titles in the assistant's response do not correspond to those in the gold answer, indicating a significant deviation and a failure to accurately identify the correct citation chain from the provided papers. This results in both inaccuracies and hallucinations as the titles and citation chain mentioned do not align with the correct documents provided.\n\nRating: [[10]]",
        "score": 10
    },
    {
        "level": 3,
        "set": 4,
        "length": 234663,
        "type": "paper",
        "language": "en",
        "question": "Small LLMs Are Weak Tool Learners: A Multi-LLM Agent",
        "instruction": "We hope you will carefully study the provided papers and determine the citation relationships between them. Please follow the instructions below strictly to complete the task:\n\n#Specific Requirements:\n1. Reference: When a given paper mentions other provided papers, those other papers are considered as \"references\" for the given paper. To summarize in this specific context, references are about what the given paper is using.\n2. Citation: Conversely, when other provided papers mention the given paper in their works, the given paper is being \"cited\" by those other papers. To summarize in this specific context, citations are about who is using the given paper.\n3. Given a paper, you need to determine the citation or reference relationship between this paper and the other papers. Do not consider papers that are not provided.\n3. Please present the paper titles in a json format as follows: {{\"Reference\":[\"Reference Title 1\", \"Reference Title 2\", ..., \"Reference Title n\"], \"Citation\":[\"Citation Title 1\", \"Citation Title 2\", ..., \"Citation Title n\"]}}.\n4. If a paper does not have any references or citations, please leave the corresponding list empty, e.g.{{\"Refernce\":[]}}, {{\"Citation\":[]}}.",
        "prompt_template": "#Papers:\n{docs}\n\n{instruction}\n\n#The paper you need to analyze:\n{question}",
        "doc": [
            "2401.07324.md",
            "2405.13050.md",
            "2405.14205.md",
            "2405.15307.md",
            "2405.16533.md",
            "2405.17935.md",
            "2307.16789.md",
            "2308.00352.md",
            "2310.05492.md",
            "2310.05915.md",
            "2310.12823.md"
        ],
        "answer": {
            "Reference": [
                "# ToolLLM: Facilitating Large LanguAGE MODELS TO MASTER 16000+ REAL-WORLD APIS ",
                "# METAGPT: META PROGRAMMING FOR A MulTi-AGENT COLLABORATIVE FRAMEWORK ",
                "# How Abilities in Large Language Models are Affected by Supervised Fine-tuning Data Composition ",
                "# FireAct: TOWARd LANGUAGE AGENT FinE-TUNING ",
                "# AGENTTUNing: EnABLING GENERALIZED AGENT ABILITIES FOR LLMS "
            ],
            "Citation": [
                "# Human-Centered LLM-Agent User Interface: A Position Paper ",
                "# Agent Planning with World Knowledge Model ",
                "# Before Generation, Align it! A Novel and Effective Strategy for Mitigating Hallucinations in Text-to-SQL Generation ",
                "# Chain of Tools: Large Language Model is an Automatic Multi-tool Learner ",
                "# Tool Learning with Large Language Models: A Survey "
            ]
        },
        "shuffle_doc": true,
        "id": "5900e327-814c-452e-b7db-87700ef28c2c",
        "prompt": "[Question]\n#Papers:\n{docs}\n\nWe hope you will carefully study the provided papers and determine the citation relationships between them. Please follow the instructions below strictly to complete the task:\n\n#Specific Requirements:\n1. Reference: When a given paper mentions other provided papers, those other papers are considered as \"references\" for the given paper. To summarize in this specific context, references are about what the given paper is using.\n2. Citation: Conversely, when other provided papers mention the given paper in their works, the given paper is being \"cited\" by those other papers. To summarize in this specific context, citations are about who is using the given paper.\n3. Given a paper, you need to determine the citation or reference relationship between this paper and the other papers. Do not consider papers that are not provided.\n3. Please present the paper titles in a json format as follows: {{\"Reference\":[\"Reference Title 1\", \"Reference Title 2\", ..., \"Reference Title n\"], \"Citation\":[\"Citation Title 1\", \"Citation Title 2\", ..., \"Citation Title n\"]}}.\n4. If a paper does not have any references or citations, please leave the corresponding list empty, e.g.{{\"Refernce\":[]}}, {{\"Citation\":[]}}.\n\n#The paper you need to analyze:\nSmall LLMs Are Weak Tool Learners: A Multi-LLM Agent\n\n[Gold Answer]\n{'Reference': ['# ToolLLM: Facilitating Large LanguAGE MODELS TO MASTER 16000+ REAL-WORLD APIS ', '# METAGPT: META PROGRAMMING FOR A MulTi-AGENT COLLABORATIVE FRAMEWORK ', '# How Abilities in Large Language Models are Affected by Supervised Fine-tuning Data Composition ', '# FireAct: TOWARd LANGUAGE AGENT FinE-TUNING ', '# AGENTTUNing: EnABLING GENERALIZED AGENT ABILITIES FOR LLMS '], 'Citation': ['# Human-Centered LLM-Agent User Interface: A Position Paper ', '# Agent Planning with World Knowledge Model ', '# Before Generation, Align it! A Novel and Effective Strategy for Mitigating Hallucinations in Text-to-SQL Generation ', '# Chain of Tools: Large Language Model is an Automatic Multi-tool Learner ', '# Tool Learning with Large Language Models: A Survey ']}\n\n[The Start of Assistant's Predicted Answer]\n{\"Reference\":[\"Fireact: Toward language agent fine-tuning\", \"Scaling instruction-finetuned language models\", \"Training verifiers to solve math word problems\", \"How abilities in large language models are affected by supervised fine-tuning data composition\", \"Tora: A tool-integrated reasoning agent for mathematical problem solving\", \"Autogpt: the heart of the open-source agent ecosystem\", \"Measuring massive multitask language understanding\", \"Babyagi\", \"Introducing chatgpt\", \"Gpt-4 technical report\", \"Gpt-engineer\", \"Training language models to follow instructions with human feedback\", \"Toolllm: Facilitating large language models to master 16000+ real-world apis\", \"Robots that ask for help: Uncertainty alignment for large language model planners\", \"AutoGPT\", \"Multitask prompted training enables zero-shot task generalization\", \"Distilling the knowledge in a neural network\", \"Crosslingual generalization through multitask finetuning\", \"Webshop: Towards scalable real-world web interaction with grounded language agents\", \"React: Synergizing reasoning and acting in language models\", \"Reflexion: Language agents with verbal reinforcement learning\", \"Tree of thoughts: Deliberate problem solving with large language models\", \"STaR: Bootstrapping reasoning with reasoning\", \"Generative agents: Interactive simulacra of human behavior\", \"Did aristotle use a laptop? A question answering benchmark with implicit reasoning strategies\", \"Measuring massive multitask language understanding\", \"Retrieval augmented language model pre-training\", \"Retrieval augmented generation benchmark (rgb): A new benchmark for evaluating retrieval augmented language generation models\", \"Cross-task generalization via natural language crowdsourcing instructions\", \"Training language models to follow instructions with human feedback\", \"The compositionality gap in language models\", \"Language models can teach themselves to use tools\", \"Chain of thought prompting elicits reasoning in large language models\", \"A question answering benchmark with implicit reasoning strategies\", \"Retrieval augmented language model pre-training\", \"Robustness and generalization in language agents\", \"Efficient finetuning of quantized llms\", \"Parameter-efficient fine-tuning of large-scale pre-trained language models\", \"Palm-e: An embodied multimodal language model\", \"Lima: Less is more for alignment\", \"Multimodal-gpt: A vision and language model for dialogue with humans\", \"Textbooks are all you need\", \"Video-llama: An instruction-tuned audio-visual language model for video understanding\", \"Llavar: Enhanced visual instruction tuning for text-rich image understanding\", \"Instruction tuning for large language models: A survey\", \"Robots that ask for help: Uncertainty alignment for large language model planners\", \"Webarena: A realistic web environment for building autonomous agents\"], \"Citation\":[]}\n[The End of Assistant's Predicted Answer]\n\n[System]\nWe would like to request your feedback on the performance of the AI assistant in response to the user question displayed above according to the gold answer. Please use the following listed aspects and their descriptions as evaluation criteria:\n    - Accuracy and Hallucinations: The assistant's answer is semantically consistent with the gold answer; The numerical value and order need to be accurate, and there should be no hallucinations.\n    - Completeness: Referring to the reference answers, the assistant's answer should contain all the key points needed to answer the user's question; further elaboration on these key points can be omitted.\nPlease rate whether this answer is suitable for the question. Please note that the gold answer can be considered as a correct answer to the question.\n\nThe assistant receives an overall score on a scale of 1 to 100, where a higher score indicates better overall performance.\nPlease note that if the assistant's answer and the gold answer fully meet the above criteria, its overall rating should be the full marks (100).\nPlease first provide a comprehensive explanation of your evaluation, avoiding any potential bias.\nThen, output a line indicating the score of the Assistant.\n\nPLEASE OUTPUT WITH THE FOLLOWING FORMAT, WHERE THE SCORE IS A SCALE OF 1 TO 100 BY STRICTLY FOLLOWING THIS FORMAT: \"[[score]]\", FOR EXAMPLE \"Rating: [[100]]\":\n<start output>\nEvaluation evidence: your evluation explanation here, no more than 100 words\nRating: [[score]]\n<end output>\n\nNow, start your evaluation:",
        "generate_response": "{\"Reference\":[\"Fireact: Toward language agent fine-tuning\", \"Scaling instruction-finetuned language models\", \"Training verifiers to solve math word problems\", \"How abilities in large language models are affected by supervised fine-tuning data composition\", \"Tora: A tool-integrated reasoning agent for mathematical problem solving\", \"Autogpt: the heart of the open-source agent ecosystem\", \"Measuring massive multitask language understanding\", \"Babyagi\", \"Introducing chatgpt\", \"Gpt-4 technical report\", \"Gpt-engineer\", \"Training language models to follow instructions with human feedback\", \"Toolllm: Facilitating large language models to master 16000+ real-world apis\", \"Robots that ask for help: Uncertainty alignment for large language model planners\", \"AutoGPT\", \"Multitask prompted training enables zero-shot task generalization\", \"Distilling the knowledge in a neural network\", \"Crosslingual generalization through multitask finetuning\", \"Webshop: Towards scalable real-world web interaction with grounded language agents\", \"React: Synergizing reasoning and acting in language models\", \"Reflexion: Language agents with verbal reinforcement learning\", \"Tree of thoughts: Deliberate problem solving with large language models\", \"STaR: Bootstrapping reasoning with reasoning\", \"Generative agents: Interactive simulacra of human behavior\", \"Did aristotle use a laptop? A question answering benchmark with implicit reasoning strategies\", \"Measuring massive multitask language understanding\", \"Retrieval augmented language model pre-training\", \"Retrieval augmented generation benchmark (rgb): A new benchmark for evaluating retrieval augmented language generation models\", \"Cross-task generalization via natural language crowdsourcing instructions\", \"Training language models to follow instructions with human feedback\", \"The compositionality gap in language models\", \"Language models can teach themselves to use tools\", \"Chain of thought prompting elicits reasoning in large language models\", \"A question answering benchmark with implicit reasoning strategies\", \"Retrieval augmented language model pre-training\", \"Robustness and generalization in language agents\", \"Efficient finetuning of quantized llms\", \"Parameter-efficient fine-tuning of large-scale pre-trained language models\", \"Palm-e: An embodied multimodal language model\", \"Lima: Less is more for alignment\", \"Multimodal-gpt: A vision and language model for dialogue with humans\", \"Textbooks are all you need\", \"Video-llama: An instruction-tuned audio-visual language model for video understanding\", \"Llavar: Enhanced visual instruction tuning for text-rich image understanding\", \"Instruction tuning for large language models: A survey\", \"Robots that ask for help: Uncertainty alignment for large language model planners\", \"Webarena: A realistic web environment for building autonomous agents\"], \"Citation\":[]}",
        "eval_response": "Evaluation evidence: The assistant's answer does not match the gold answer in terms of accuracy and completeness. The references listed in the assistant's answer are significantly different from those in the gold answer, indicating a high level of hallucination and a lack of accuracy. The citations section is also incorrect as it is empty, whereas the gold answer lists several citations. This discrepancy shows that the assistant failed to identify the correct citation relationships, leading to an incomplete and inaccurate response.\n\nRating: [[10]]",
        "score": 10
    },
    {
        "level": 3,
        "set": 3,
        "length": 146228,
        "type": "paper",
        "language": "en",
        "question": "OpenMoE: An Early Effort on Open Mixture-of-Experts Language Models",
        "instruction": "We hope you will carefully study the provided papers and determine the citation relationships between them. Please follow the instructions below strictly to complete the task:\n\n#Specific Requirements:\n1. Reference: When a given paper mentions other provided papers, those other papers are considered as \"references\" for the given paper. To summarize in this specific context, references are about what the given paper is using.\n2. Citation: Conversely, when other provided papers mention the given paper in their works, the given paper is being \"cited\" by those other papers. To summarize in this specific context, citations are about who is using the given paper.\n3. Given a paper, you need to determine the citation or reference relationship between this paper and the other papers. Do not consider papers that are not provided.\n3. Please present the paper titles in a json format as follows: {{\"Reference\":[\"Reference Title 1\", \"Reference Title 2\", ..., \"Reference Title n\"], \"Citation\":[\"Citation Title 1\", \"Citation Title 2\", ..., \"Citation Title n\"]}}.\n4. If a paper does not have any references or citations, please leave the corresponding list empty, e.g.{{\"Refernce\":[]}}, {{\"Citation\":[]}}.",
        "prompt_template": "#Papers:\n{docs}\n\n{instruction}\n\n#The paper you need to analyze:\n{question}",
        "doc": [
            "2402.01739.md",
            "2404.15247.md",
            "2405.15984.md",
            "2402.00159.md"
        ],
        "answer": {
            "Reference": [
                "# dIIO: an Open Corpus of Three Trillion Tokens for Language Model Pretraining Research "
            ],
            "Citation": [
                "# $\\mathcal{X}$ FT: Unlocking the Power of Code Instruction Tuning by Simply Merging Upcycled Mixture-of-Experts ",
                "# Evaluating the Adversarial Robustness of Retrieval-Based In-Context Learning for Large Language Models "
            ]
        },
        "shuffle_doc": true,
        "id": "e0ed8817-b431-4da4-b820-30ee6efa1f75",
        "prompt": "[Question]\n#Papers:\n{docs}\n\nWe hope you will carefully study the provided papers and determine the citation relationships between them. Please follow the instructions below strictly to complete the task:\n\n#Specific Requirements:\n1. Reference: When a given paper mentions other provided papers, those other papers are considered as \"references\" for the given paper. To summarize in this specific context, references are about what the given paper is using.\n2. Citation: Conversely, when other provided papers mention the given paper in their works, the given paper is being \"cited\" by those other papers. To summarize in this specific context, citations are about who is using the given paper.\n3. Given a paper, you need to determine the citation or reference relationship between this paper and the other papers. Do not consider papers that are not provided.\n3. Please present the paper titles in a json format as follows: {{\"Reference\":[\"Reference Title 1\", \"Reference Title 2\", ..., \"Reference Title n\"], \"Citation\":[\"Citation Title 1\", \"Citation Title 2\", ..., \"Citation Title n\"]}}.\n4. If a paper does not have any references or citations, please leave the corresponding list empty, e.g.{{\"Refernce\":[]}}, {{\"Citation\":[]}}.\n\n#The paper you need to analyze:\nOpenMoE: An Early Effort on Open Mixture-of-Experts Language Models\n\n[Gold Answer]\n{'Reference': ['# dIIO: an Open Corpus of Three Trillion Tokens for Language Model Pretraining Research '], 'Citation': ['# $\\\\mathcal{X}$ FT: Unlocking the Power of Code Instruction Tuning by Simply Merging Upcycled Mixture-of-Experts ', '# Evaluating the Adversarial Robustness of Retrieval-Based In-Context Learning for Large Language Models ']}\n\n[The Start of Assistant's Predicted Answer]\n{\"Reference\": [\"Palm 2 technical report\", \"(inthe)wildchat: 570k chatGPT interaction logs in the wild\", \"Beyond the imitation game: Quantifying and extrapolating the capabilities of language models\", \"Efficient training of language models to fill in the middle\", \"Findings of the 2016 conference on machine translation\", \"Language models are few-shot learners\", \"Evaluating large language models trained on code\", \"Palm: Scaling language modeling with pathways\", \"Switch transformers: Scaling to trillion parameter models with simple and efficient sparsity\", \"How does gpt obtain its ability? tracing emergent abilities of language models to their sources\", \"Megablocks: Efficient sparse training with mixture-of-experts\", \"A framework for few-shot language model evaluation\", \"Openllama: An open reproduction of llama\", \"Measuring massive multitask language understanding\", \"Training compute-optimal large language models\", \"Mixtral of experts\", \"Base layers: Simplifying training of large, sparse models\", \"Self-prompting large language models for open-domain qa\", \"Starcoder: May the source be with you!\", \"Roberta: A robustly optimized bert pretraining approach\", \"Cross-token modeling with conditional computation\", \"Multimodal contrastive learning with limoe: The language-image mixture of experts\", \"Xgen-7b technical report\", \"From sparse to soft mixtures of experts\", \"Scaling language models: Methods, analysis \\& insights from training gopher\", \"Exploring the limits of transfer learning with a unified text-to-text transformer\", \"Scaling vision with sparse mixture of experts\", \"Hash layers for large sparse models\", \"Code llama: Open foundation models for code\", \"Glu variants improve transformer\", \"Outrageously large neural networks: The sparsely-gated mixture-of-experts layer\", \"Megatron-lm: Training multi-billion parameter language models using model parallelism\", \"Dolma: An Open Corpus of Three Trillion Tokens for Language Model Pretraining Research\", \"Roformer: Enhanced transformer with rotary position embedding\", \"U12: Unifying language learning paradigms\", \"Unifying language learning paradigms\", \"Llama: Open and efficient foundation language models\", \"GPT-J-6B: A 6 Billion Parameter Autoregressive Language Model\", \"CCNet: Extracting high quality monolingual datasets from web crawl data\", \"Gspmd: General and scalable parallelization for ml computation graphs\", \"One student knows all experts know: From sparse to dense\", \"Go wider instead of deeper\", \"Efficient language modeling with sparse all-mlp\", \"Tinyllama: An open-source small language model\", \"Deep long-tailed learning: A survey\", \"Judging llm-as-a-judge with mt-bench and chatbot arena\", \"Brainformers: Trading simplicity for efficiency\", \"Mixture-of-experts with expert choice routing\", \"St-moe: Designing stable and transferable sparse expert models\"], \"Citation\": [\"Language models are few-shot learners\", \"Evaluating large language models trained on code\", \"Palm: Scaling language modeling with pathways\", \"Switch transformers: Scaling to trillion parameter models with simple and efficient sparsity\", \"Base layers: Simplifying training of large, sparse models\", \"Self-prompting large language models for open-domain qa\", \"Exploring the limits of transfer learning with a unified text-to-text transformer\", \"Megatron-lm: Training multi-billion parameter language models using model parallelism\", \"Roformer: Enhanced transformer with rotary position embedding\", \"U12: Unifying language learning paradigms\", \"Unifying language learning paradigms\", \"St-moe: Designing stable and transferable sparse expert models\"]}\n[The End of Assistant's Predicted Answer]\n\n[System]\nWe would like to request your feedback on the performance of the AI assistant in response to the user question displayed above according to the gold answer. Please use the following listed aspects and their descriptions as evaluation criteria:\n    - Accuracy and Hallucinations: The assistant's answer is semantically consistent with the gold answer; The numerical value and order need to be accurate, and there should be no hallucinations.\n    - Completeness: Referring to the reference answers, the assistant's answer should contain all the key points needed to answer the user's question; further elaboration on these key points can be omitted.\nPlease rate whether this answer is suitable for the question. Please note that the gold answer can be considered as a correct answer to the question.\n\nThe assistant receives an overall score on a scale of 1 to 100, where a higher score indicates better overall performance.\nPlease note that if the assistant's answer and the gold answer fully meet the above criteria, its overall rating should be the full marks (100).\nPlease first provide a comprehensive explanation of your evaluation, avoiding any potential bias.\nThen, output a line indicating the score of the Assistant.\n\nPLEASE OUTPUT WITH THE FOLLOWING FORMAT, WHERE THE SCORE IS A SCALE OF 1 TO 100 BY STRICTLY FOLLOWING THIS FORMAT: \"[[score]]\", FOR EXAMPLE \"Rating: [[100]]\":\n<start output>\nEvaluation evidence: your evluation explanation here, no more than 100 words\nRating: [[score]]\n<end output>\n\nNow, start your evaluation:",
        "generate_response": "{\"Reference\": [\"Palm 2 technical report\", \"(inthe)wildchat: 570k chatGPT interaction logs in the wild\", \"Beyond the imitation game: Quantifying and extrapolating the capabilities of language models\", \"Efficient training of language models to fill in the middle\", \"Findings of the 2016 conference on machine translation\", \"Language models are few-shot learners\", \"Evaluating large language models trained on code\", \"Palm: Scaling language modeling with pathways\", \"Switch transformers: Scaling to trillion parameter models with simple and efficient sparsity\", \"How does gpt obtain its ability? tracing emergent abilities of language models to their sources\", \"Megablocks: Efficient sparse training with mixture-of-experts\", \"A framework for few-shot language model evaluation\", \"Openllama: An open reproduction of llama\", \"Measuring massive multitask language understanding\", \"Training compute-optimal large language models\", \"Mixtral of experts\", \"Base layers: Simplifying training of large, sparse models\", \"Self-prompting large language models for open-domain qa\", \"Starcoder: May the source be with you!\", \"Roberta: A robustly optimized bert pretraining approach\", \"Cross-token modeling with conditional computation\", \"Multimodal contrastive learning with limoe: The language-image mixture of experts\", \"Xgen-7b technical report\", \"From sparse to soft mixtures of experts\", \"Scaling language models: Methods, analysis \\& insights from training gopher\", \"Exploring the limits of transfer learning with a unified text-to-text transformer\", \"Scaling vision with sparse mixture of experts\", \"Hash layers for large sparse models\", \"Code llama: Open foundation models for code\", \"Glu variants improve transformer\", \"Outrageously large neural networks: The sparsely-gated mixture-of-experts layer\", \"Megatron-lm: Training multi-billion parameter language models using model parallelism\", \"Dolma: An Open Corpus of Three Trillion Tokens for Language Model Pretraining Research\", \"Roformer: Enhanced transformer with rotary position embedding\", \"U12: Unifying language learning paradigms\", \"Unifying language learning paradigms\", \"Llama: Open and efficient foundation language models\", \"GPT-J-6B: A 6 Billion Parameter Autoregressive Language Model\", \"CCNet: Extracting high quality monolingual datasets from web crawl data\", \"Gspmd: General and scalable parallelization for ml computation graphs\", \"One student knows all experts know: From sparse to dense\", \"Go wider instead of deeper\", \"Efficient language modeling with sparse all-mlp\", \"Tinyllama: An open-source small language model\", \"Deep long-tailed learning: A survey\", \"Judging llm-as-a-judge with mt-bench and chatbot arena\", \"Brainformers: Trading simplicity for efficiency\", \"Mixture-of-experts with expert choice routing\", \"St-moe: Designing stable and transferable sparse expert models\"], \"Citation\": [\"Language models are few-shot learners\", \"Evaluating large language models trained on code\", \"Palm: Scaling language modeling with pathways\", \"Switch transformers: Scaling to trillion parameter models with simple and efficient sparsity\", \"Base layers: Simplifying training of large, sparse models\", \"Self-prompting large language models for open-domain qa\", \"Exploring the limits of transfer learning with a unified text-to-text transformer\", \"Megatron-lm: Training multi-billion parameter language models using model parallelism\", \"Roformer: Enhanced transformer with rotary position embedding\", \"U12: Unifying language learning paradigms\", \"Unifying language learning paradigms\", \"St-moe: Designing stable and transferable sparse expert models\"]}",
        "eval_response": "Evaluation evidence: The assistant's answer does not match the gold answer in terms of accuracy and completeness. The references and citations listed in the assistant's answer are entirely different from those in the gold answer, indicating a significant deviation and hallucination. The assistant's response includes a large number of unrelated papers, which are not mentioned in the gold answer, showing a lack of focus on the specific task requirements.\n\nRating: [[10]]",
        "score": 10
    },
    {
        "level": 3,
        "set": 3,
        "length": 128146,
        "type": "paper",
        "language": "en",
        "question": "Patchscopes: A Unifying Framework for Inspecting Hidden Representations of Language Models",
        "instruction": "We hope you will carefully study the provided papers and determine the citation relationships between them. Please follow the instructions below strictly to complete the task:\n\n#Specific Requirements:\n1. Reference: When a given paper mentions other provided papers, those other papers are considered as \"references\" for the given paper. To summarize in this specific context, references are about what the given paper is using.\n2. Citation: Conversely, when other provided papers mention the given paper in their works, the given paper is being \"cited\" by those other papers. To summarize in this specific context, citations are about who is using the given paper.\n3. Given a paper, you need to determine the citation or reference relationship between this paper and the other papers. Do not consider papers that are not provided.\n3. Please present the paper titles in a json format as follows: {{\"Reference\":[\"Reference Title 1\", \"Reference Title 2\", ..., \"Reference Title n\"], \"Citation\":[\"Citation Title 1\", \"Citation Title 2\", ..., \"Citation Title n\"]}}.\n4. If a paper does not have any references or citations, please leave the corresponding list empty, e.g.{{\"Refernce\":[]}}, {{\"Citation\":[]}}.",
        "prompt_template": "#Papers:\n{docs}\n\n{instruction}\n\n#The paper you need to analyze:\n{question}",
        "doc": [
            "2401.06102.md",
            "2404.05971.md",
            "2405.17653.md",
            "2310.15916.md",
            "2310.18969.md"
        ],
        "answer": {
            "Reference": [
                "# In-Context Learning Creates Task Vectors ",
                "# Analyzing Vision Transformers for Image Classification in Class Embedding Space "
            ],
            "Citation": [
                "# Does Transformer Interpretability Transfer to RNNs? ",
                "# InversionView: A General-Purpose Method for Reading Information from Neural Activations "
            ]
        },
        "shuffle_doc": true,
        "id": "1a9f0ce6-846a-4c0d-9027-28b77541f284",
        "prompt": "[Question]\n#Papers:\n{docs}\n\nWe hope you will carefully study the provided papers and determine the citation relationships between them. Please follow the instructions below strictly to complete the task:\n\n#Specific Requirements:\n1. Reference: When a given paper mentions other provided papers, those other papers are considered as \"references\" for the given paper. To summarize in this specific context, references are about what the given paper is using.\n2. Citation: Conversely, when other provided papers mention the given paper in their works, the given paper is being \"cited\" by those other papers. To summarize in this specific context, citations are about who is using the given paper.\n3. Given a paper, you need to determine the citation or reference relationship between this paper and the other papers. Do not consider papers that are not provided.\n3. Please present the paper titles in a json format as follows: {{\"Reference\":[\"Reference Title 1\", \"Reference Title 2\", ..., \"Reference Title n\"], \"Citation\":[\"Citation Title 1\", \"Citation Title 2\", ..., \"Citation Title n\"]}}.\n4. If a paper does not have any references or citations, please leave the corresponding list empty, e.g.{{\"Refernce\":[]}}, {{\"Citation\":[]}}.\n\n#The paper you need to analyze:\nPatchscopes: A Unifying Framework for Inspecting Hidden Representations of Language Models\n\n[Gold Answer]\n{'Reference': ['# In-Context Learning Creates Task Vectors ', '# Analyzing Vision Transformers for Image Classification in Class Embedding Space '], 'Citation': ['# Does Transformer Interpretability Transfer to RNNs? ', '# InversionView: A General-Purpose Method for Reading Information from Neural Activations ']}\n\n[The Start of Assistant's Predicted Answer]\n{\"Reference\":[\"Understanding intermediate layers using linear classifier probes\", \"Revisiting model stitching to compare neural representations\", \"Probing classifiers: Promises, shortcomings, and advances\", \"Analysis methods in neural language processing: A survey\", \"A Survey on Evaluation of Large Language Models\", \"Vicuna: An open-source chatbot impressing GPT-4 with $90 \\% *$ ChatGPT quality\", \"Eliciting latent predictions from transformers with the tuned lens\", \"Graph of thoughts: Solving elaborate problems with large language models\", \"Pythia: A suite for analyzing large language models across training and scaling\", \"Language models can explain neurons in language models\", \"Sok: Toward transparent ai: A survey on interpreting the inner structures of deep neural networks\", \"The Pile: An 800GB dataset of diverse text for language modeling\", \"Causal abstractions of neural networks\", \"Localizing model behavior with path patching\", \"Distributional vectors encode referential attributes\", \"Does localization inform editing? surprising differences in causality-based localization vs. knowledge editing in language models\", \"In-context learning creates task vectors\", \"Natural language descriptions of deep features\", \"Measuring and manipulating knowledge representations in language models\", \"Linearity of relation decoding in transformer language models\", \"Self-refine: Iterative refinement with self-feedback\", \"Post-hoc interpretability for neural nlp: A survey\", \"When not to trust language models: Investigating effectiveness of parametric and nonparametric memories\", \"Locating and editing factual associations in gpt\", \"Pointer sentinel mixture models\", \"The expressive power of transformers with chain of thought\", \"Linearly mapping from image to text space\", \"A mechanism for solving relational tasks in transformer language models\", \"Can llms facilitate interpretation of pre-trained language models?\", \"Interpreting gpt: the logit lens\", \"Future lens: Anticipating subsequent tokens from a single hidden state\", \"Mapping language models to grounded conceptual spaces\", \"Sentence-bert: Sentence embeddings using siamese bertnetworks\", \"Multimodal neurons in pretrained text-only transformers\", \"Explaining black box text modules in natural language with language models\", \"A mechanistic interpretation of arithmetic reasoning in language models using causal mediation analysis\", \"Lstmvis: A tool for visual analysis of hidden state dynamics in recurrent neural networks\", \"Bert rediscovers the classical nlp pipeline\", \"Llama: Open and efficient foundation language models\", \"Llama 2: Open foundation and finetuned chat models\", \"Investigating gender bias in language models using causal mediation analysis\", \"Gaussian Process Probes (GPP) for uncertainty-aware probing\", \"Chain-of-thought prompting elicits reasoning in large language models\", \"Tree of thoughts: Deliberate problem solving with large language models\", \"Give me the facts! a survey on factual knowledge probing in pre-trained language models\", \"Towards best practices of activation patching in language models: Metrics and methods\", \"Interpreting deep visual representations via network dissection\"], \"Citation\": [\"InversionView: A General-Purpose Method for Reading Information from Neural Activations\"]}\n[The End of Assistant's Predicted Answer]\n\n[System]\nWe would like to request your feedback on the performance of the AI assistant in response to the user question displayed above according to the gold answer. Please use the following listed aspects and their descriptions as evaluation criteria:\n    - Accuracy and Hallucinations: The assistant's answer is semantically consistent with the gold answer; The numerical value and order need to be accurate, and there should be no hallucinations.\n    - Completeness: Referring to the reference answers, the assistant's answer should contain all the key points needed to answer the user's question; further elaboration on these key points can be omitted.\nPlease rate whether this answer is suitable for the question. Please note that the gold answer can be considered as a correct answer to the question.\n\nThe assistant receives an overall score on a scale of 1 to 100, where a higher score indicates better overall performance.\nPlease note that if the assistant's answer and the gold answer fully meet the above criteria, its overall rating should be the full marks (100).\nPlease first provide a comprehensive explanation of your evaluation, avoiding any potential bias.\nThen, output a line indicating the score of the Assistant.\n\nPLEASE OUTPUT WITH THE FOLLOWING FORMAT, WHERE THE SCORE IS A SCALE OF 1 TO 100 BY STRICTLY FOLLOWING THIS FORMAT: \"[[score]]\", FOR EXAMPLE \"Rating: [[100]]\":\n<start output>\nEvaluation evidence: your evluation explanation here, no more than 100 words\nRating: [[score]]\n<end output>\n\nNow, start your evaluation:",
        "generate_response": "{\"Reference\":[\"Understanding intermediate layers using linear classifier probes\", \"Revisiting model stitching to compare neural representations\", \"Probing classifiers: Promises, shortcomings, and advances\", \"Analysis methods in neural language processing: A survey\", \"A Survey on Evaluation of Large Language Models\", \"Vicuna: An open-source chatbot impressing GPT-4 with $90 \\% *$ ChatGPT quality\", \"Eliciting latent predictions from transformers with the tuned lens\", \"Graph of thoughts: Solving elaborate problems with large language models\", \"Pythia: A suite for analyzing large language models across training and scaling\", \"Language models can explain neurons in language models\", \"Sok: Toward transparent ai: A survey on interpreting the inner structures of deep neural networks\", \"The Pile: An 800GB dataset of diverse text for language modeling\", \"Causal abstractions of neural networks\", \"Localizing model behavior with path patching\", \"Distributional vectors encode referential attributes\", \"Does localization inform editing? surprising differences in causality-based localization vs. knowledge editing in language models\", \"In-context learning creates task vectors\", \"Natural language descriptions of deep features\", \"Measuring and manipulating knowledge representations in language models\", \"Linearity of relation decoding in transformer language models\", \"Self-refine: Iterative refinement with self-feedback\", \"Post-hoc interpretability for neural nlp: A survey\", \"When not to trust language models: Investigating effectiveness of parametric and nonparametric memories\", \"Locating and editing factual associations in gpt\", \"Pointer sentinel mixture models\", \"The expressive power of transformers with chain of thought\", \"Linearly mapping from image to text space\", \"A mechanism for solving relational tasks in transformer language models\", \"Can llms facilitate interpretation of pre-trained language models?\", \"Interpreting gpt: the logit lens\", \"Future lens: Anticipating subsequent tokens from a single hidden state\", \"Mapping language models to grounded conceptual spaces\", \"Sentence-bert: Sentence embeddings using siamese bertnetworks\", \"Multimodal neurons in pretrained text-only transformers\", \"Explaining black box text modules in natural language with language models\", \"A mechanistic interpretation of arithmetic reasoning in language models using causal mediation analysis\", \"Lstmvis: A tool for visual analysis of hidden state dynamics in recurrent neural networks\", \"Bert rediscovers the classical nlp pipeline\", \"Llama: Open and efficient foundation language models\", \"Llama 2: Open foundation and finetuned chat models\", \"Investigating gender bias in language models using causal mediation analysis\", \"Gaussian Process Probes (GPP) for uncertainty-aware probing\", \"Chain-of-thought prompting elicits reasoning in large language models\", \"Tree of thoughts: Deliberate problem solving with large language models\", \"Give me the facts! a survey on factual knowledge probing in pre-trained language models\", \"Towards best practices of activation patching in language models: Metrics and methods\", \"Interpreting deep visual representations via network dissection\"], \"Citation\": [\"InversionView: A General-Purpose Method for Reading Information from Neural Activations\"]}",
        "eval_response": "Evaluation evidence: The assistant's answer contains a significant number of hallucinated references that are not mentioned in the gold answer, indicating a lack of accuracy and the presence of hallucinations. The references listed in the gold answer are missing from the assistant's response, and the citation list also includes an incorrect entry not found in the gold answer. This shows a lack of completeness and accuracy in the response.\n\nRating: [[10]]",
        "score": 10
    },
    {
        "level": 3,
        "set": 1,
        "length": 44863,
        "type": "paper",
        "language": "en",
        "question": "WebVoyager : Building an End-to-End Web Agent with Large Multimodal Models",
        "instruction": "We hope you will carefully study the provided papers and determine the citation relationships between them. Please follow the instructions below strictly to complete the task:\n\n#Specific Requirements:\n1. Reference: When a given paper mentions other provided papers, those other papers are considered as \"references\" for the given paper. To summarize in this specific context, references are about what the given paper is using.\n2. Citation: Conversely, when other provided papers mention the given paper in their works, the given paper is being \"cited\" by those other papers. To summarize in this specific context, citations are about who is using the given paper.\n3. Given a paper, you need to determine the citation or reference relationship between this paper and the other papers. Do not consider papers that are not provided.\n3. Please present the paper titles in a json format as follows: {{\"Reference\":[\"Reference Title 1\", \"Reference Title 2\", ..., \"Reference Title n\"], \"Citation\":[\"Citation Title 1\", \"Citation Title 2\", ..., \"Citation Title n\"]}}.\n4. If a paper does not have any references or citations, please leave the corresponding list empty, e.g.{{\"Refernce\":[]}}, {{\"Citation\":[]}}.",
        "prompt_template": "#Papers:\n{docs}\n\n{instruction}\n\n#The paper you need to analyze:\n{question}",
        "doc": [
            "2401.13919.md",
            "2404.05955.md",
            "2312.13771.md"
        ],
        "answer": {
            "Reference": [
                "# AppAgent: Multimodal Agents as Smartphone Users "
            ],
            "Citation": [
                "# VisualWebBench: How Far Have Multimodal LLMs Evolved in Web Page Understanding and Grounding? "
            ]
        },
        "shuffle_doc": true,
        "id": "427e9ff9-c027-4b84-bb8b-d1f78c0d9b21",
        "prompt": "[Question]\n#Papers:\n{docs}\n\nWe hope you will carefully study the provided papers and determine the citation relationships between them. Please follow the instructions below strictly to complete the task:\n\n#Specific Requirements:\n1. Reference: When a given paper mentions other provided papers, those other papers are considered as \"references\" for the given paper. To summarize in this specific context, references are about what the given paper is using.\n2. Citation: Conversely, when other provided papers mention the given paper in their works, the given paper is being \"cited\" by those other papers. To summarize in this specific context, citations are about who is using the given paper.\n3. Given a paper, you need to determine the citation or reference relationship between this paper and the other papers. Do not consider papers that are not provided.\n3. Please present the paper titles in a json format as follows: {{\"Reference\":[\"Reference Title 1\", \"Reference Title 2\", ..., \"Reference Title n\"], \"Citation\":[\"Citation Title 1\", \"Citation Title 2\", ..., \"Citation Title n\"]}}.\n4. If a paper does not have any references or citations, please leave the corresponding list empty, e.g.{{\"Refernce\":[]}}, {{\"Citation\":[]}}.\n\n#The paper you need to analyze:\nWebVoyager : Building an End-to-End Web Agent with Large Multimodal Models\n\n[Gold Answer]\n{'Reference': ['# AppAgent: Multimodal Agents as Smartphone Users '], 'Citation': ['# VisualWebBench: How Far Have Multimodal LLMs Evolved in Web Page Understanding and Grounding? ']}\n\n[The Start of Assistant's Predicted Answer]\n{\"Reference\":[\"A Survey on Evaluation of Large Language Models\", \"Vicuna: An Open-Source Chatbot Impressing GPT-4 with $90 \\% *$ ChatGPT Quality\", \"ChatLaw: Open-Source Legal Large Language Model with Integrated External Knowledge Bases\", \"Compositional Semantic Parsing with Large Language Models\", \"Open LLM Leaderboard\", \"How Close is ChatGPT to Human Experts? Comparison Corpus, Evaluation, and Detection\", \"REALM: Retrieval-Augmented Language Model PreTraining\", \"Rethinking with Retrieval: Faithful Large Language Model Inference\", \"Measuring Massive Multitask Language Understanding\", \"Pix2Struct: Screenshot parsing as pretraining for visual language understanding\", \"WebArena: Evaluating multimodal agents on realistic visual web tasks\", \"GAIA: A Benchmark for General AI Assistants\", \"SeeAct: Harnessing GUI Grounding for Advanced Visual GUI Agents\", \"Cm3: A causal masked multimodal model of the internet\", \"Language models are few-shot learners\", \"Evaluating large language models trained on code\", \"Webvln: Vision-and-language navigation on websites\", \"Scaling language modeling with pathways\", \"A coefficient of agreement for nominal scales\", \"Mind2web: Towards a generalist agent for the web\", \"A survey for in-context learning\", \"An image is worth $16 \\\\times 16$ words: Transformers for image recognition at scale\", \"Measuring nominal scale agreement among many raters\", \"Multimodal web navigation with instruction-finetuned foundation models\", \"Making the v in vqa matter: Elevating the role of image understanding in visual question answering\", \"A real-world webagent with planning, long context understanding, and program synthesis\", \"The abduction of sherlock holmes: A dataset for visual abductive reasoning\", \"Chameleon: Plug-and-play compositional reasoning with large language models\", \"Laser: LLM Agent with State-Space Exploration for Web Navigation\", \"GAIA: A Benchmark for General AI Assistants\", \"WebGPT: Browser-Assisted Question-Answearing with Human Feedback\", \"GPT-4 Technical Report\", \"Training language models to follow instructions with human feedback\", \"ToolLLM: Facilitating Large Language Models to Master 16000+ Real-World APIs\", \"Exploring the limits of transfer learning with a unified text-to-text transformer\", \"Toolformer: Language models can teach themselves to use tools\", \"From pixels to UI actions: Learning to follow instructions via graphical user interfaces\", \"World of bits: An open-domain platform for web-based agents\", \"Reflexion: Language agents with verbal reinforcement learning\", \"Gemini: A family of highly capable multimodal models\", \"Self-instruct: Aligning language model with self generated instructions\", \"Simple visual language model pretraining with weak supervision\", \"LLM-powered autonomous agents\", \"GPT-4V in Wonderland: Large Multimodal Models for Zero-Shot Smartphone GUI Navigation\", \"Set-of-mark prompting unleashes extraordinary visual grounding in GPT-4V\", \"The dawn of $1\\\\mathrm{mms}$ : Preliminary explorations with GPT-4V (ision)\", \"WebShop: Towards Scalable Real-World Web Interaction with Grounded Language Agents\", \"React: Synergizing Reasoning and Acting in Language Models\", \"Lumos: Learning agents with unified data, modular design, and open-source LLMs\", \"Visualwebarena: Evaluating multimodal agents on realistic visual web tasks\", \"AppAgent: Multimodal Agents as Smartphone Users\", \"GPT-4V (ision) is a Generalist Web Agent, if Grounded\", \"Object Detection in 20 Years: A Survey\"], \"Citation\":[]}\n[The End of Assistant's Predicted Answer]\n\n[System]\nWe would like to request your feedback on the performance of the AI assistant in response to the user question displayed above according to the gold answer. Please use the following listed aspects and their descriptions as evaluation criteria:\n    - Accuracy and Hallucinations: The assistant's answer is semantically consistent with the gold answer; The numerical value and order need to be accurate, and there should be no hallucinations.\n    - Completeness: Referring to the reference answers, the assistant's answer should contain all the key points needed to answer the user's question; further elaboration on these key points can be omitted.\nPlease rate whether this answer is suitable for the question. Please note that the gold answer can be considered as a correct answer to the question.\n\nThe assistant receives an overall score on a scale of 1 to 100, where a higher score indicates better overall performance.\nPlease note that if the assistant's answer and the gold answer fully meet the above criteria, its overall rating should be the full marks (100).\nPlease first provide a comprehensive explanation of your evaluation, avoiding any potential bias.\nThen, output a line indicating the score of the Assistant.\n\nPLEASE OUTPUT WITH THE FOLLOWING FORMAT, WHERE THE SCORE IS A SCALE OF 1 TO 100 BY STRICTLY FOLLOWING THIS FORMAT: \"[[score]]\", FOR EXAMPLE \"Rating: [[100]]\":\n<start output>\nEvaluation evidence: your evluation explanation here, no more than 100 words\nRating: [[score]]\n<end output>\n\nNow, start your evaluation:",
        "generate_response": "{\"Reference\":[\"A Survey on Evaluation of Large Language Models\", \"Vicuna: An Open-Source Chatbot Impressing GPT-4 with $90 \\% *$ ChatGPT Quality\", \"ChatLaw: Open-Source Legal Large Language Model with Integrated External Knowledge Bases\", \"Compositional Semantic Parsing with Large Language Models\", \"Open LLM Leaderboard\", \"How Close is ChatGPT to Human Experts? Comparison Corpus, Evaluation, and Detection\", \"REALM: Retrieval-Augmented Language Model PreTraining\", \"Rethinking with Retrieval: Faithful Large Language Model Inference\", \"Measuring Massive Multitask Language Understanding\", \"Pix2Struct: Screenshot parsing as pretraining for visual language understanding\", \"WebArena: Evaluating multimodal agents on realistic visual web tasks\", \"GAIA: A Benchmark for General AI Assistants\", \"SeeAct: Harnessing GUI Grounding for Advanced Visual GUI Agents\", \"Cm3: A causal masked multimodal model of the internet\", \"Language models are few-shot learners\", \"Evaluating large language models trained on code\", \"Webvln: Vision-and-language navigation on websites\", \"Scaling language modeling with pathways\", \"A coefficient of agreement for nominal scales\", \"Mind2web: Towards a generalist agent for the web\", \"A survey for in-context learning\", \"An image is worth $16 \\\\times 16$ words: Transformers for image recognition at scale\", \"Measuring nominal scale agreement among many raters\", \"Multimodal web navigation with instruction-finetuned foundation models\", \"Making the v in vqa matter: Elevating the role of image understanding in visual question answering\", \"A real-world webagent with planning, long context understanding, and program synthesis\", \"The abduction of sherlock holmes: A dataset for visual abductive reasoning\", \"Chameleon: Plug-and-play compositional reasoning with large language models\", \"Laser: LLM Agent with State-Space Exploration for Web Navigation\", \"GAIA: A Benchmark for General AI Assistants\", \"WebGPT: Browser-Assisted Question-Answearing with Human Feedback\", \"GPT-4 Technical Report\", \"Training language models to follow instructions with human feedback\", \"ToolLLM: Facilitating Large Language Models to Master 16000+ Real-World APIs\", \"Exploring the limits of transfer learning with a unified text-to-text transformer\", \"Toolformer: Language models can teach themselves to use tools\", \"From pixels to UI actions: Learning to follow instructions via graphical user interfaces\", \"World of bits: An open-domain platform for web-based agents\", \"Reflexion: Language agents with verbal reinforcement learning\", \"Gemini: A family of highly capable multimodal models\", \"Self-instruct: Aligning language model with self generated instructions\", \"Simple visual language model pretraining with weak supervision\", \"LLM-powered autonomous agents\", \"GPT-4V in Wonderland: Large Multimodal Models for Zero-Shot Smartphone GUI Navigation\", \"Set-of-mark prompting unleashes extraordinary visual grounding in GPT-4V\", \"The dawn of $1\\\\mathrm{mms}$ : Preliminary explorations with GPT-4V (ision)\", \"WebShop: Towards Scalable Real-World Web Interaction with Grounded Language Agents\", \"React: Synergizing Reasoning and Acting in Language Models\", \"Lumos: Learning agents with unified data, modular design, and open-source LLMs\", \"Visualwebarena: Evaluating multimodal agents on realistic visual web tasks\", \"AppAgent: Multimodal Agents as Smartphone Users\", \"GPT-4V (ision) is a Generalist Web Agent, if Grounded\", \"Object Detection in 20 Years: A Survey\"], \"Citation\":[]}",
        "eval_response": "Evaluation evidence: The assistant's answer inaccurately lists numerous references that are not mentioned in the gold answer, indicating a significant hallucination issue. The gold answer specifies one reference and one citation, but the assistant fails to mention the correct citation and instead lists an extensive array of unrelated references. This shows a lack of accuracy and completeness in relation to the task requirements.\n\nRating: [[10]]",
        "score": 10
    },
    {
        "level": 3,
        "set": 3,
        "length": 122082,
        "type": "paper",
        "language": "en",
        "question": "WARM: On the Benefits of Weight Averaged Reward Models",
        "instruction": "We hope you will carefully study the provided papers and determine the citation relationships between them. Please follow the instructions below strictly to complete the task:\n\n#Specific Requirements:\n1. Reference: When a given paper mentions other provided papers, those other papers are considered as \"references\" for the given paper. To summarize in this specific context, references are about what the given paper is using.\n2. Citation: Conversely, when other provided papers mention the given paper in their works, the given paper is being \"cited\" by those other papers. To summarize in this specific context, citations are about who is using the given paper.\n3. Given a paper, you need to determine the citation or reference relationship between this paper and the other papers. Do not consider papers that are not provided.\n3. Please present the paper titles in a json format as follows: {{\"Reference\":[\"Reference Title 1\", \"Reference Title 2\", ..., \"Reference Title n\"], \"Citation\":[\"Citation Title 1\", \"Citation Title 2\", ..., \"Citation Title n\"]}}.\n4. If a paper does not have any references or citations, please leave the corresponding list empty, e.g.{{\"Refernce\":[]}}, {{\"Citation\":[]}}.",
        "prompt_template": "#Papers:\n{docs}\n\n{instruction}\n\n#The paper you need to analyze:\n{question}",
        "doc": [
            "2401.12187.md",
            "2405.16276.md",
            "2405.19107.md",
            "2405.19316.md"
        ],
        "answer": {
            "Reference": [],
            "Citation": [
                "# MECHANISM DESIGN FOR LLM FINE-TUNING WITH MULTIPLE REWARD MODELS ",
                "# Offline Regularised Reinforcement Learning for Large Language Models Alignment ",
                "# Robust Preference Optimization through Reward Model Distillation "
            ]
        },
        "shuffle_doc": true,
        "id": "a4dea00c-f25c-447e-bcf1-ed270fdd6051",
        "prompt": "[Question]\n#Papers:\n{docs}\n\nWe hope you will carefully study the provided papers and determine the citation relationships between them. Please follow the instructions below strictly to complete the task:\n\n#Specific Requirements:\n1. Reference: When a given paper mentions other provided papers, those other papers are considered as \"references\" for the given paper. To summarize in this specific context, references are about what the given paper is using.\n2. Citation: Conversely, when other provided papers mention the given paper in their works, the given paper is being \"cited\" by those other papers. To summarize in this specific context, citations are about who is using the given paper.\n3. Given a paper, you need to determine the citation or reference relationship between this paper and the other papers. Do not consider papers that are not provided.\n3. Please present the paper titles in a json format as follows: {{\"Reference\":[\"Reference Title 1\", \"Reference Title 2\", ..., \"Reference Title n\"], \"Citation\":[\"Citation Title 1\", \"Citation Title 2\", ..., \"Citation Title n\"]}}.\n4. If a paper does not have any references or citations, please leave the corresponding list empty, e.g.{{\"Refernce\":[]}}, {{\"Citation\":[]}}.\n\n#The paper you need to analyze:\nWARM: On the Benefits of Weight Averaged Reward Models\n\n[Gold Answer]\n{'Reference': [], 'Citation': ['# MECHANISM DESIGN FOR LLM FINE-TUNING WITH MULTIPLE REWARD MODELS ', '# Offline Regularised Reinforcement Learning for Large Language Models Alignment ', '# Robust Preference Optimization through Reward Model Distillation ']}\n\n[The Start of Assistant's Predicted Answer]\n{\"Reference\":[],\"Citation\":[\"HELPING OR HERDING? REWARD MODEL ENSEMBLES MITIGATE BUT DO NOT ELIMINATE REWARD HACKING\",\"REWARMED SOUPS: TOWARDS PARETO-OPTIMAL ALIGNMENT BY INTERPOLATING WEIGHTS FINE-TUNED ON DIVERSE REWARDS\",\"REWARD MODEL ENSEMBLES HELP MITIGATE OVEROPTIMIZATION\",\"IS DPO SUPERIOR TO PPO FOR LLM ALIGNMENT? A COMPREHENSIVE STUDY\",\"COMBO: CONSERVATIVE OFFLINE MODEL-BASED POLICY OPTIMIZATION\",\"DISCOVERING LANGUAGE MODEL BEHAVIORS WITH MODEL-WRITTEN EVALUATIONS\",\"SEQUENCE TUTOR: CONSERVATIVE FINE-TUNING OF SEQUENCE GENERATION MODELS WITH KL-CONTROL\",\"SEASONING MODEL SOUPS FOR ROBUSTNESS TO ADVERSARIAL AND NATURAL DISTRIBUTION SHIFTS\",\"FINE-TUNING CAN DISTORT PRETRAINED FEATURES AND UNDERPERFORM OUT-OF-DISTRIBUTION\",\"IMPROVING STABILITY IN DEEP REINFORCEMENT LEARNING WITH WEIGHT AVERAGING\",\"MAPPING THE NEURAL NETWORK LANDSCAPE: CONNECTIVITY STRUCTURE AND LOSS FUNCTION GEOMETRY\",\"LEARN YOUR REFERENCE MODEL FOR REAL GOOD ALIGNMENT\",\"CONFRONTING REWARD MODEL OVEROPTIMIZATION WITH CONSTRAINED RLHF\",\"IMPROVING LANGUAGE UNDERSTANDING BY GENERATIVE PRE-TRAINING\",\"BERT: PRE-TRAINING OF DEEP BIDIRECTIONAL TRANSFORMERS FOR LANGUAGE UNDERSTANDING\",\"TRAINING LANGUAGE MODELS TO FOLLOW INSTRUCTIONS WITH HUMAN FEEDBACK\",\"MODEL SOUPS: AVERAGING WEIGHTS OF MULTIPLE FINE-TUNED MODELS IMPROVES ACCURACY WITHOUT INCREASING INFERENCE TIME\",\"DIVERSE WEIGHT AVERAGING FOR OUT-OF-DISTRIBUTION GENERALIZATION\",\"MODEL RATATOUILLE: RECYCLING DIVERSE MODELS FOR OUT-OF-DISTRIBUTION GENERALIZATION\",\"SWAD: DOMAIN GENERALIZATION BY SEEKING FLAT MINIMA\",\"DOMAIN GENERALIZATION VIA INVARIANT FEATURE REPRESENTATION\",\"INVARIANT RISK MINIMIZATION\",\"BIAS PLUS VARIANCE DECOMPOSITION FOR ZERO-ONE LOSS FUNCTIONS\",\"GENERALIZATION ERROR OF ENSEMBLE ESTIMATORS\",\"ENSEMBLE LEARNING IN THE PRESENCE OF NOISE\",\"SIMPLE AND SCALABLE PREDICTIVE UNCERTAINTY ESTIMATION USING DEEP ENSEMBLES\",\"AVERAGING WEIGHTS LEADS TO WIDER OPTIMA AND BETTER GENERALIZATION\",\"ENSEMBLE OF AVERAGES: IMPROVING MODEL SELECTION AND BOOSTING PERFORMANCE IN DOMAIN GENERALIZATION\",\"IMPROVING DIALOGUE AGENTS VIA TARGETED HUMAN JUDGEMENTS\",\"POLICY SHAPING: INTEGRATING HUMAN FEEDBACK WITH REINFORCEMENT LEARNING\",\"RLHF: TRAINING AN AGENT MANUALLY VIA EVALUATIVE REINFORCEMENT\",\"WEBGPT: BROWSER-ASSISTED QUESTION-ANSWERING WITH HUMAN FEEDBACK\",\"LEARNING TRANSFERABLE VISUAL MODELS FROM NATURAL LANGUAGE SUPERVISION\",\"TRAINING LANGUAGE MODELS TO FOLLOW INSTRUCTIONS WITH HUMAN FEEDBACK\",\"FINE-GRAINED HUMAN FEEDBACK GIVES BETTER REWARDS FOR LANGUAGE MODEL TRAINING\",\"REINFORCEMENT LEARNING FROM HUMAN FEEDBACK WITH EFFICIENT REWARD MODEL ENSEMBLE\"]}\n[The End of Assistant's Predicted Answer]\n\n[System]\nWe would like to request your feedback on the performance of the AI assistant in response to the user question displayed above according to the gold answer. Please use the following listed aspects and their descriptions as evaluation criteria:\n    - Accuracy and Hallucinations: The assistant's answer is semantically consistent with the gold answer; The numerical value and order need to be accurate, and there should be no hallucinations.\n    - Completeness: Referring to the reference answers, the assistant's answer should contain all the key points needed to answer the user's question; further elaboration on these key points can be omitted.\nPlease rate whether this answer is suitable for the question. Please note that the gold answer can be considered as a correct answer to the question.\n\nThe assistant receives an overall score on a scale of 1 to 100, where a higher score indicates better overall performance.\nPlease note that if the assistant's answer and the gold answer fully meet the above criteria, its overall rating should be the full marks (100).\nPlease first provide a comprehensive explanation of your evaluation, avoiding any potential bias.\nThen, output a line indicating the score of the Assistant.\n\nPLEASE OUTPUT WITH THE FOLLOWING FORMAT, WHERE THE SCORE IS A SCALE OF 1 TO 100 BY STRICTLY FOLLOWING THIS FORMAT: \"[[score]]\", FOR EXAMPLE \"Rating: [[100]]\":\n<start output>\nEvaluation evidence: your evluation explanation here, no more than 100 words\nRating: [[score]]\n<end output>\n\nNow, start your evaluation:",
        "generate_response": "{\"Reference\":[],\"Citation\":[\"HELPING OR HERDING? REWARD MODEL ENSEMBLES MITIGATE BUT DO NOT ELIMINATE REWARD HACKING\",\"REWARMED SOUPS: TOWARDS PARETO-OPTIMAL ALIGNMENT BY INTERPOLATING WEIGHTS FINE-TUNED ON DIVERSE REWARDS\",\"REWARD MODEL ENSEMBLES HELP MITIGATE OVEROPTIMIZATION\",\"IS DPO SUPERIOR TO PPO FOR LLM ALIGNMENT? A COMPREHENSIVE STUDY\",\"COMBO: CONSERVATIVE OFFLINE MODEL-BASED POLICY OPTIMIZATION\",\"DISCOVERING LANGUAGE MODEL BEHAVIORS WITH MODEL-WRITTEN EVALUATIONS\",\"SEQUENCE TUTOR: CONSERVATIVE FINE-TUNING OF SEQUENCE GENERATION MODELS WITH KL-CONTROL\",\"SEASONING MODEL SOUPS FOR ROBUSTNESS TO ADVERSARIAL AND NATURAL DISTRIBUTION SHIFTS\",\"FINE-TUNING CAN DISTORT PRETRAINED FEATURES AND UNDERPERFORM OUT-OF-DISTRIBUTION\",\"IMPROVING STABILITY IN DEEP REINFORCEMENT LEARNING WITH WEIGHT AVERAGING\",\"MAPPING THE NEURAL NETWORK LANDSCAPE: CONNECTIVITY STRUCTURE AND LOSS FUNCTION GEOMETRY\",\"LEARN YOUR REFERENCE MODEL FOR REAL GOOD ALIGNMENT\",\"CONFRONTING REWARD MODEL OVEROPTIMIZATION WITH CONSTRAINED RLHF\",\"IMPROVING LANGUAGE UNDERSTANDING BY GENERATIVE PRE-TRAINING\",\"BERT: PRE-TRAINING OF DEEP BIDIRECTIONAL TRANSFORMERS FOR LANGUAGE UNDERSTANDING\",\"TRAINING LANGUAGE MODELS TO FOLLOW INSTRUCTIONS WITH HUMAN FEEDBACK\",\"MODEL SOUPS: AVERAGING WEIGHTS OF MULTIPLE FINE-TUNED MODELS IMPROVES ACCURACY WITHOUT INCREASING INFERENCE TIME\",\"DIVERSE WEIGHT AVERAGING FOR OUT-OF-DISTRIBUTION GENERALIZATION\",\"MODEL RATATOUILLE: RECYCLING DIVERSE MODELS FOR OUT-OF-DISTRIBUTION GENERALIZATION\",\"SWAD: DOMAIN GENERALIZATION BY SEEKING FLAT MINIMA\",\"DOMAIN GENERALIZATION VIA INVARIANT FEATURE REPRESENTATION\",\"INVARIANT RISK MINIMIZATION\",\"BIAS PLUS VARIANCE DECOMPOSITION FOR ZERO-ONE LOSS FUNCTIONS\",\"GENERALIZATION ERROR OF ENSEMBLE ESTIMATORS\",\"ENSEMBLE LEARNING IN THE PRESENCE OF NOISE\",\"SIMPLE AND SCALABLE PREDICTIVE UNCERTAINTY ESTIMATION USING DEEP ENSEMBLES\",\"AVERAGING WEIGHTS LEADS TO WIDER OPTIMA AND BETTER GENERALIZATION\",\"ENSEMBLE OF AVERAGES: IMPROVING MODEL SELECTION AND BOOSTING PERFORMANCE IN DOMAIN GENERALIZATION\",\"IMPROVING DIALOGUE AGENTS VIA TARGETED HUMAN JUDGEMENTS\",\"POLICY SHAPING: INTEGRATING HUMAN FEEDBACK WITH REINFORCEMENT LEARNING\",\"RLHF: TRAINING AN AGENT MANUALLY VIA EVALUATIVE REINFORCEMENT\",\"WEBGPT: BROWSER-ASSISTED QUESTION-ANSWERING WITH HUMAN FEEDBACK\",\"LEARNING TRANSFERABLE VISUAL MODELS FROM NATURAL LANGUAGE SUPERVISION\",\"TRAINING LANGUAGE MODELS TO FOLLOW INSTRUCTIONS WITH HUMAN FEEDBACK\",\"FINE-GRAINED HUMAN FEEDBACK GIVES BETTER REWARDS FOR LANGUAGE MODEL TRAINING\",\"REINFORCEMENT LEARNING FROM HUMAN FEEDBACK WITH EFFICIENT REWARD MODEL ENSEMBLE\"]}",
        "eval_response": "Evaluation evidence: The assistant's answer does not match the gold answer in terms of the citations listed. The gold answer lists three specific papers citing the analyzed paper, while the assistant's answer includes a long list of unrelated papers, indicating a significant hallucination issue. This discrepancy shows a lack of accuracy and introduces irrelevant information, failing to meet the completeness criterion as it does not correctly identify the papers that cite the given paper.\n\nRating: [[10]]",
        "score": 10
    },
    {
        "level": 3,
        "set": 3,
        "length": 162536,
        "type": "paper",
        "language": "en",
        "question": "Mixtral of Experts",
        "instruction": "We hope you will carefully study the provided papers and determine the citation relationships between them. Please follow the instructions below strictly to complete the task:\n\n#Specific Requirements:\n1. Reference: When a given paper mentions other provided papers, those other papers are considered as \"references\" for the given paper. To summarize in this specific context, references are about what the given paper is using.\n2. Citation: Conversely, when other provided papers mention the given paper in their works, the given paper is being \"cited\" by those other papers. To summarize in this specific context, citations are about who is using the given paper.\n3. Given a paper, you need to determine the citation or reference relationship between this paper and the other papers. Do not consider papers that are not provided.\n3. Please present the paper titles in a json format as follows: {{\"Reference\":[\"Reference Title 1\", \"Reference Title 2\", ..., \"Reference Title n\"], \"Citation\":[\"Citation Title 1\", \"Citation Title 2\", ..., \"Citation Title n\"]}}.\n4. If a paper does not have any references or citations, please leave the corresponding list empty, e.g.{{\"Refernce\":[]}}, {{\"Citation\":[]}}.",
        "prompt_template": "#Papers:\n{docs}\n\n{instruction}\n\n#The paper you need to analyze:\n{question}",
        "doc": [
            "2401.04088.md",
            "2405.15130.md",
            "2405.16009.md",
            "2405.16783.md",
            "2405.17233.md",
            "2405.17249.md",
            "2305.18290.md",
            "2306.05685.md",
            "2310.06825.md",
            "2310.10631.md",
            "2312.17238.md"
        ],
        "answer": {
            "Reference": [
                "# Direct Preference Optimization: Your Language Model is Secretly a Reward Model ",
                "# Judging LLM-as-a-Judge with MT-Bench and Chatbot Arena ",
                "# Mistral 7B ",
                "# LLEMMA: AN OPEN LANGUAGE MODEL FOR MATHEMATICS ",
                "# Fast Inference of Mixture-of-Experts Language Models with Offloading "
            ],
            "Citation": [
                "# OptLLM: Optimal Assignment of Queries to Large Language Models ",
                "# Streaming Long Video Understanding with Large Language Models ",
                "# TrojFM: Resource-efficient Backdoor Attacks against Very Large Foundation Models ",
                "# CLAQ: Pushing the Limits of Low-Bit Post-Training Quantization for $\\mathbf{L L M s}$ ",
                "# Assessing LLMs Suitability for Knowledge Graph Completion "
            ]
        },
        "shuffle_doc": true,
        "id": "1f0650f8-8239-472e-8fa2-f9cb2d90a820",
        "prompt": "[Question]\n#Papers:\n{docs}\n\nWe hope you will carefully study the provided papers and determine the citation relationships between them. Please follow the instructions below strictly to complete the task:\n\n#Specific Requirements:\n1. Reference: When a given paper mentions other provided papers, those other papers are considered as \"references\" for the given paper. To summarize in this specific context, references are about what the given paper is using.\n2. Citation: Conversely, when other provided papers mention the given paper in their works, the given paper is being \"cited\" by those other papers. To summarize in this specific context, citations are about who is using the given paper.\n3. Given a paper, you need to determine the citation or reference relationship between this paper and the other papers. Do not consider papers that are not provided.\n3. Please present the paper titles in a json format as follows: {{\"Reference\":[\"Reference Title 1\", \"Reference Title 2\", ..., \"Reference Title n\"], \"Citation\":[\"Citation Title 1\", \"Citation Title 2\", ..., \"Citation Title n\"]}}.\n4. If a paper does not have any references or citations, please leave the corresponding list empty, e.g.{{\"Refernce\":[]}}, {{\"Citation\":[]}}.\n\n#The paper you need to analyze:\nMixtral of Experts\n\n[Gold Answer]\n{'Reference': ['# Direct Preference Optimization: Your Language Model is Secretly a Reward Model ', '# Judging LLM-as-a-Judge with MT-Bench and Chatbot Arena ', '# Mistral 7B ', '# LLEMMA: AN OPEN LANGUAGE MODEL FOR MATHEMATICS ', '# Fast Inference of Mixture-of-Experts Language Models with Offloading '], 'Citation': ['# OptLLM: Optimal Assignment of Queries to Large Language Models ', '# Streaming Long Video Understanding with Large Language Models ', '# TrojFM: Resource-efficient Backdoor Attacks against Very Large Foundation Models ', '# CLAQ: Pushing the Limits of Low-Bit Post-Training Quantization for $\\\\mathbf{L L M s}$ ', '# Assessing LLMs Suitability for Knowledge Graph Completion ']}\n\n[The Start of Assistant's Predicted Answer]\n{\"Reference\":[\"Program synthesis with large language models\", \"Llemma: An open language model for mathematics\", \"Piqa: Reasoning about physical commonsense in natural language\", \"Evaluating large language models trained on code\", \"Quac: Question answering in context\", \"Dselect-k: Differentiable selection in the mixture of experts with applications to multi-task learning\", \"The pile: An $800 \\\\mathrm{gb}$ dataset of diverse text for language modeling\", \"Gshard: Scaling giant models with conditional computation and automatic sharding\", \"Can a suit of armor conduct electricity? a new dataset for open book question answering\", \"Landmark attention: Random-access infinite context length for transformers\", \"A hand-built bias benchmark for question answering\", \"Direct preference optimization: Your language model is secretly a reward model\", \"Winogrande: An adversarial winograd schema challenge at scale\", \"Socialiqa: Commonsense reasoning about social interactions\", \"Outrageously large neural networks: The sparsely-gated mixture-of-experts layer\", \"Challenging big-bench tasks and whether chain-of-thought can solve them\", \"Commonsenseqa: A question answering challenge targeting commonsense knowledge\", \"Attention is all you need\", \"Hellaswag: Can a machine really finish your sentence?\", \"Judging llm-as-a-judge with mt-bench and chatbot arena\", \"Agieval: A human-centric benchmark for evaluating foundation models\", \"Mixture-of-experts with expert choice routing\"], \"Citation\":[\"Assessing LLMs Suitability for Knowledge Graph Completion\", \"OptLLM: Optimal Assignment of Queries to Large Language Models\", \"Fast Inference of Mixture-of-Experts Language Models with Offloading\", \"TrojFM: Resource-efficient Backdoor Attacks against Very Large Foundation Models\"]}\n[The End of Assistant's Predicted Answer]\n\n[System]\nWe would like to request your feedback on the performance of the AI assistant in response to the user question displayed above according to the gold answer. Please use the following listed aspects and their descriptions as evaluation criteria:\n    - Accuracy and Hallucinations: The assistant's answer is semantically consistent with the gold answer; The numerical value and order need to be accurate, and there should be no hallucinations.\n    - Completeness: Referring to the reference answers, the assistant's answer should contain all the key points needed to answer the user's question; further elaboration on these key points can be omitted.\nPlease rate whether this answer is suitable for the question. Please note that the gold answer can be considered as a correct answer to the question.\n\nThe assistant receives an overall score on a scale of 1 to 100, where a higher score indicates better overall performance.\nPlease note that if the assistant's answer and the gold answer fully meet the above criteria, its overall rating should be the full marks (100).\nPlease first provide a comprehensive explanation of your evaluation, avoiding any potential bias.\nThen, output a line indicating the score of the Assistant.\n\nPLEASE OUTPUT WITH THE FOLLOWING FORMAT, WHERE THE SCORE IS A SCALE OF 1 TO 100 BY STRICTLY FOLLOWING THIS FORMAT: \"[[score]]\", FOR EXAMPLE \"Rating: [[100]]\":\n<start output>\nEvaluation evidence: your evluation explanation here, no more than 100 words\nRating: [[score]]\n<end output>\n\nNow, start your evaluation:",
        "generate_response": "{\"Reference\":[\"Program synthesis with large language models\", \"Llemma: An open language model for mathematics\", \"Piqa: Reasoning about physical commonsense in natural language\", \"Evaluating large language models trained on code\", \"Quac: Question answering in context\", \"Dselect-k: Differentiable selection in the mixture of experts with applications to multi-task learning\", \"The pile: An $800 \\\\mathrm{gb}$ dataset of diverse text for language modeling\", \"Gshard: Scaling giant models with conditional computation and automatic sharding\", \"Can a suit of armor conduct electricity? a new dataset for open book question answering\", \"Landmark attention: Random-access infinite context length for transformers\", \"A hand-built bias benchmark for question answering\", \"Direct preference optimization: Your language model is secretly a reward model\", \"Winogrande: An adversarial winograd schema challenge at scale\", \"Socialiqa: Commonsense reasoning about social interactions\", \"Outrageously large neural networks: The sparsely-gated mixture-of-experts layer\", \"Challenging big-bench tasks and whether chain-of-thought can solve them\", \"Commonsenseqa: A question answering challenge targeting commonsense knowledge\", \"Attention is all you need\", \"Hellaswag: Can a machine really finish your sentence?\", \"Judging llm-as-a-judge with mt-bench and chatbot arena\", \"Agieval: A human-centric benchmark for evaluating foundation models\", \"Mixture-of-experts with expert choice routing\"], \"Citation\":[\"Assessing LLMs Suitability for Knowledge Graph Completion\", \"OptLLM: Optimal Assignment of Queries to Large Language Models\", \"Fast Inference of Mixture-of-Experts Language Models with Offloading\", \"TrojFM: Resource-efficient Backdoor Attacks against Very Large Foundation Models\"]}",
        "eval_response": "Evaluation evidence: The assistant's answer includes a list of references and citations that are not consistent with the gold answer. The references and citations listed in the assistant's answer are significantly different from those in the gold answer, indicating inaccuracies and hallucinations. The assistant's answer fails to capture the correct references and citations specific to the \"Mixtral of Experts\" paper, leading to a lack of completeness and accuracy.\n\nRating: [[20]]",
        "score": 20
    },
    {
        "level": 3,
        "set": 3,
        "length": 131208,
        "type": "paper",
        "language": "en",
        "question": "Scalable Pre-training of Large Autoregressive Image Models",
        "instruction": "We hope you will carefully study the provided papers and determine the citation relationships between them. Please follow the instructions below strictly to complete the task:\n\n#Specific Requirements:\n1. Reference: When a given paper mentions other provided papers, those other papers are considered as \"references\" for the given paper. To summarize in this specific context, references are about what the given paper is using.\n2. Citation: Conversely, when other provided papers mention the given paper in their works, the given paper is being \"cited\" by those other papers. To summarize in this specific context, citations are about who is using the given paper.\n3. Given a paper, you need to determine the citation or reference relationship between this paper and the other papers. Do not consider papers that are not provided.\n3. Please present the paper titles in a json format as follows: {{\"Reference\":[\"Reference Title 1\", \"Reference Title 2\", ..., \"Reference Title n\"], \"Citation\":[\"Citation Title 1\", \"Citation Title 2\", ..., \"Citation Title n\"]}}.\n4. If a paper does not have any references or citations, please leave the corresponding list empty, e.g.{{\"Refernce\":[]}}, {{\"Citation\":[]}}.",
        "prompt_template": "#Papers:\n{docs}\n\n{instruction}\n\n#The paper you need to analyze:\n{question}",
        "doc": [
            "2401.08541.md",
            "2405.14930.md",
            "2405.16847.md",
            "2304.14108.md"
        ],
        "answer": {
            "Reference": [
                "# DATACOMP: In search of the next generation of multimodal datasets "
            ],
            "Citation": [
                "# AstroPT: Scaling Large Observation Models for Astronomy ",
                "# TokenUnify: Scalable Autoregressive Visual Pre-training with Mixture Token Prediction "
            ]
        },
        "shuffle_doc": true,
        "id": "129cb5dc-b1ed-4415-a441-e74e38d7ad81",
        "prompt": "[Question]\n#Papers:\n{docs}\n\nWe hope you will carefully study the provided papers and determine the citation relationships between them. Please follow the instructions below strictly to complete the task:\n\n#Specific Requirements:\n1. Reference: When a given paper mentions other provided papers, those other papers are considered as \"references\" for the given paper. To summarize in this specific context, references are about what the given paper is using.\n2. Citation: Conversely, when other provided papers mention the given paper in their works, the given paper is being \"cited\" by those other papers. To summarize in this specific context, citations are about who is using the given paper.\n3. Given a paper, you need to determine the citation or reference relationship between this paper and the other papers. Do not consider papers that are not provided.\n3. Please present the paper titles in a json format as follows: {{\"Reference\":[\"Reference Title 1\", \"Reference Title 2\", ..., \"Reference Title n\"], \"Citation\":[\"Citation Title 1\", \"Citation Title 2\", ..., \"Citation Title n\"]}}.\n4. If a paper does not have any references or citations, please leave the corresponding list empty, e.g.{{\"Refernce\":[]}}, {{\"Citation\":[]}}.\n\n#The paper you need to analyze:\nScalable Pre-training of Large Autoregressive Image Models\n\n[Gold Answer]\n{'Reference': ['# DATACOMP: In search of the next generation of multimodal datasets '], 'Citation': ['# AstroPT: Scaling Large Observation Models for Astronomy ', '# TokenUnify: Scalable Autoregressive Visual Pre-training with Mixture Token Prediction ']}\n\n[The Start of Assistant's Predicted Answer]\n{\"Reference\": [\"V-JEPA: Latent video prediction for visual representation learning\", \"Self-supervised learning from images with a joint-embedding predictive architecture\", \"Sequential modeling enables scalable learning for large vision models\", \"From detection of individual metastases to classification of lymph node status at the patient level: the camelyon17 challenge\", \"BEiT: Bert pretraining of image transformers\", \"Vicreg: Variance-invariance-covariance regularization for selfsupervised learning\", \"Deep unsupervised exemplar learning\", \"The iwildcam 2020 competition dataset\", \"A neural probabilistic language model\", \"Unsupervised learning by predicting noise\", \"Unsupervised pre-training of image features on non-curated data\", \"Self-supervised learning from images with a joint-embedding predictive architecture\", \"Unsupervised learning of visual representations by solving jigsaw puzzles\", \"Wavenet: A generative model for raw audio\", \"Representation learning with contrastive predictive coding\", \"Dinov2: Learning robust visual features without supervision\", \"Training language models to follow instructions with human feedback\", \"Exploring the limits of transfer learning with a unified text-to-text transformer\", \"Zero-shot text-to-image generation\", \"Pixelcnn++: Improving the pixelcnn with discretized logistic mixture likelihood and other modifications\", \"Prediction and entropy of printed english\", \"The effectiveness of mae pre-pretraining for billionscale pretraining\", \"Divide and contrast: Self-supervised learning from uncurated data\", \"Training data-efficient image transformers & distillation through attention\", \"Llama: Open and efficient foundation language models\", \"Conditional image generation with pixelcnn decoders\", \"Pixel recurrent neural networks\", \"Neurips\", \"Rotation equivariant cnns for digital pathology\", \"Stacked denoising autoencoders: Learning useful representations in a deep network with a local denoising criterion\", \"Diffusion models as masked autoencoders\", \"ClusterFit: Improving Generalization of Visual Representations\", \"Coca: Contrastive captioners are image-text foundation models\", \"Barlow twins: Self-supervised learning via redundancy reduction\", \"mixup: Beyond empirical risk minimization\", \"Colorful image colorization\"], \"Citation\": [\"AstroPT: Scaling Large Observation Models for Astronomy\", \"DATACOMP: In search of the next generation of multimodal datasets\"]}\n[The End of Assistant's Predicted Answer]\n\n[System]\nWe would like to request your feedback on the performance of the AI assistant in response to the user question displayed above according to the gold answer. Please use the following listed aspects and their descriptions as evaluation criteria:\n    - Accuracy and Hallucinations: The assistant's answer is semantically consistent with the gold answer; The numerical value and order need to be accurate, and there should be no hallucinations.\n    - Completeness: Referring to the reference answers, the assistant's answer should contain all the key points needed to answer the user's question; further elaboration on these key points can be omitted.\nPlease rate whether this answer is suitable for the question. Please note that the gold answer can be considered as a correct answer to the question.\n\nThe assistant receives an overall score on a scale of 1 to 100, where a higher score indicates better overall performance.\nPlease note that if the assistant's answer and the gold answer fully meet the above criteria, its overall rating should be the full marks (100).\nPlease first provide a comprehensive explanation of your evaluation, avoiding any potential bias.\nThen, output a line indicating the score of the Assistant.\n\nPLEASE OUTPUT WITH THE FOLLOWING FORMAT, WHERE THE SCORE IS A SCALE OF 1 TO 100 BY STRICTLY FOLLOWING THIS FORMAT: \"[[score]]\", FOR EXAMPLE \"Rating: [[100]]\":\n<start output>\nEvaluation evidence: your evluation explanation here, no more than 100 words\nRating: [[score]]\n<end output>\n\nNow, start your evaluation:",
        "generate_response": "{\"Reference\": [\"V-JEPA: Latent video prediction for visual representation learning\", \"Self-supervised learning from images with a joint-embedding predictive architecture\", \"Sequential modeling enables scalable learning for large vision models\", \"From detection of individual metastases to classification of lymph node status at the patient level: the camelyon17 challenge\", \"BEiT: Bert pretraining of image transformers\", \"Vicreg: Variance-invariance-covariance regularization for selfsupervised learning\", \"Deep unsupervised exemplar learning\", \"The iwildcam 2020 competition dataset\", \"A neural probabilistic language model\", \"Unsupervised learning by predicting noise\", \"Unsupervised pre-training of image features on non-curated data\", \"Self-supervised learning from images with a joint-embedding predictive architecture\", \"Unsupervised learning of visual representations by solving jigsaw puzzles\", \"Wavenet: A generative model for raw audio\", \"Representation learning with contrastive predictive coding\", \"Dinov2: Learning robust visual features without supervision\", \"Training language models to follow instructions with human feedback\", \"Exploring the limits of transfer learning with a unified text-to-text transformer\", \"Zero-shot text-to-image generation\", \"Pixelcnn++: Improving the pixelcnn with discretized logistic mixture likelihood and other modifications\", \"Prediction and entropy of printed english\", \"The effectiveness of mae pre-pretraining for billionscale pretraining\", \"Divide and contrast: Self-supervised learning from uncurated data\", \"Training data-efficient image transformers & distillation through attention\", \"Llama: Open and efficient foundation language models\", \"Conditional image generation with pixelcnn decoders\", \"Pixel recurrent neural networks\", \"Neurips\", \"Rotation equivariant cnns for digital pathology\", \"Stacked denoising autoencoders: Learning useful representations in a deep network with a local denoising criterion\", \"Diffusion models as masked autoencoders\", \"ClusterFit: Improving Generalization of Visual Representations\", \"Coca: Contrastive captioners are image-text foundation models\", \"Barlow twins: Self-supervised learning via redundancy reduction\", \"mixup: Beyond empirical risk minimization\", \"Colorful image colorization\"], \"Citation\": [\"AstroPT: Scaling Large Observation Models for Astronomy\", \"DATACOMP: In search of the next generation of multimodal datasets\"]}",
        "eval_response": "Evaluation evidence: The assistant's answer contains significant hallucinations and inaccuracies. The provided references and citations do not match the gold answer, except for one correct citation. The assistant lists numerous unrelated papers, which are not mentioned in the gold answer, indicating a misunderstanding of the task. This results in a failure to meet the accuracy and completeness criteria.\n\nRating: [[10]]",
        "score": 10
    },
    {
        "level": 3,
        "set": 3,
        "length": 151066,
        "type": "paper",
        "language": "en",
        "question": "Scalable Pre-training of Large Autoregressive Image Models",
        "instruction": "We hope you will carefully study the provided papers and determine the citation relationships between them. Please follow the instructions below strictly to complete the task:\n\n#Specific Requirements:\n1. Reference: When a given paper mentions other provided papers, those other papers are considered as \"references\" for the given paper. To summarize in this specific context, references are about what the given paper is using.\n2. Citation: Conversely, when other provided papers mention the given paper in their works, the given paper is being \"cited\" by those other papers. To summarize in this specific context, citations are about who is using the given paper.\n3. Given a paper, you need to determine the citation or reference relationship between this paper and the other papers. Do not consider papers that are not provided.\n3. Please present the paper titles in a json format as follows: {{\"Reference\":[\"Reference Title 1\", \"Reference Title 2\", ..., \"Reference Title n\"], \"Citation\":[\"Citation Title 1\", \"Citation Title 2\", ..., \"Citation Title n\"]}}.\n4. If a paper does not have any references or citations, please leave the corresponding list empty, e.g.{{\"Refernce\":[]}}, {{\"Citation\":[]}}.",
        "prompt_template": "#Papers:\n{docs}\n\n{instruction}\n\n#The paper you need to analyze:\n{question}",
        "doc": [
            "2401.08541.md",
            "2405.14930.md",
            "2405.16847.md",
            "2304.03283.md",
            "2304.14108.md"
        ],
        "answer": {
            "Reference": [
                "# Diffusion Models as Masked Autoencoders ",
                "# DATACOMP: In search of the next generation of multimodal datasets "
            ],
            "Citation": [
                "# AstroPT: Scaling Large Observation Models for Astronomy ",
                "# TokenUnify: Scalable Autoregressive Visual Pre-training with Mixture Token Prediction "
            ]
        },
        "shuffle_doc": true,
        "id": "d381d3eb-af5e-46f7-a225-c5d24e300b31",
        "prompt": "[Question]\n#Papers:\n{docs}\n\nWe hope you will carefully study the provided papers and determine the citation relationships between them. Please follow the instructions below strictly to complete the task:\n\n#Specific Requirements:\n1. Reference: When a given paper mentions other provided papers, those other papers are considered as \"references\" for the given paper. To summarize in this specific context, references are about what the given paper is using.\n2. Citation: Conversely, when other provided papers mention the given paper in their works, the given paper is being \"cited\" by those other papers. To summarize in this specific context, citations are about who is using the given paper.\n3. Given a paper, you need to determine the citation or reference relationship between this paper and the other papers. Do not consider papers that are not provided.\n3. Please present the paper titles in a json format as follows: {{\"Reference\":[\"Reference Title 1\", \"Reference Title 2\", ..., \"Reference Title n\"], \"Citation\":[\"Citation Title 1\", \"Citation Title 2\", ..., \"Citation Title n\"]}}.\n4. If a paper does not have any references or citations, please leave the corresponding list empty, e.g.{{\"Refernce\":[]}}, {{\"Citation\":[]}}.\n\n#The paper you need to analyze:\nScalable Pre-training of Large Autoregressive Image Models\n\n[Gold Answer]\n{'Reference': ['# Diffusion Models as Masked Autoencoders ', '# DATACOMP: In search of the next generation of multimodal datasets '], 'Citation': ['# AstroPT: Scaling Large Observation Models for Astronomy ', '# TokenUnify: Scalable Autoregressive Visual Pre-training with Mixture Token Prediction ']}\n\n[The Start of Assistant's Predicted Answer]\n{\"Reference\":[\"V-JEPA: Latent video prediction for visual representation learning\", \"Self-supervised learning from images with a joint-embedding predictive architecture\", \"Sequential modeling enables scalable learning for large vision models\", \"Vicreg: Variance-invariance-covariance regularization for selfsupervised learning\", \"Cliquecnn: Deep unsupervised exemplar learning\", \"The iwildcam 2020 competition dataset\", \"A neural probabilistic language model\", \"Unsupervised learning by predicting noise\", \"Unsupervised pre-training of image features on non-curated data\", \"BEiT: Bert pretraining of image transformers\", \"Vicreg: Variance-invariance-covariance regularization for selfsupervised learning\", \"Self-supervised learning from images with a joint-embedding predictive architecture\", \"Generative pretraining from pixels\", \"A simple framework for contrastive learning of visual representations\", \"An empirical study of training self-supervised vision transformers\", \"Unsupervised visual representation learning by predicting image rotations\", \"Generative adversarial nets\", \"Scaling and benchmarking self-supervised visual representation learning\", \"Vision models are more robust and fair when pretrained on uncurated images without supervision\", \"Bootstrap your own latent-a new approach to self-supervised learning\", \"Mask r-cnn\", \"Momentum contrast for unsupervised visual representation learning\", \"Masked autoencoders are scalable vision learners\", \"Training compute-optimal large language models\", \"Lora: Low-rank adaptation of large language models\", \"Deep networks with stochastic depth\", \"3d object representations for fine-grained categorization\", \"Learning multiple layers of features from tiny images\", \"The neural autoregressive distribution estimator\", \"Set transformer: A framework for attention-based permutation-invariant neural networks\", \"Sgdr: Stochastic gradient descent with warm restarts\", \"Decoupled weight decay regularization\", \"Recurrent neural network based language model\", \"Self-supervised learning of pretext-invariant representations\", \"Unsupervised learning of visual representations by solving jigsaw puzzles\", \"Wavenet: A generative model for raw audio\", \"Representation learning with contrastive predictive coding\", \"Dinov2: Learning robust visual features without supervision\", \"Training language models to follow instructions with human feedback\", \"Exploring the limits of transfer learning with a unified text-to-text transformer\", \"Zero-shot text-to-image generation\", \"Pixelcnn++: Improving the pixelcnn with discretized logistic mixture likelihood and other modifications\", \"Prediction and entropy of printed english\", \"The effectiveness of mae pre-pretraining for billionscale pretraining\", \"Divide and contrast: Self-supervised learning from uncurated data\", \"Training data-efficient image transformers  distillation through attention\", \"Going deeper with image transformers\", \"Llama: Open and efficient foundation language models\", \"Conditional image generation with pixelcnn decoders\", \"Pixel recurrent neural networks\", \"Neurips\", \"Attention is all you need\", \"Rotation equivariant cnns for digital pathology\", \"Stacked denoising autoencoders: Learning useful representations in a deep network with a local denoising criterion\", \"Diffusion models as masked autoencoders\", \"ClusterFit: Improving Generalization of Visual Representations\", \"Coca: Contrastive captioners are image-text foundation models\", \"Barlow twins: Self-supervised learning via redundancy reduction\", \"Mixup: Beyond empirical risk minimization\", \"Colorful image colorization\"], \"Citation\":[]}\n[The End of Assistant's Predicted Answer]\n\n[System]\nWe would like to request your feedback on the performance of the AI assistant in response to the user question displayed above according to the gold answer. Please use the following listed aspects and their descriptions as evaluation criteria:\n    - Accuracy and Hallucinations: The assistant's answer is semantically consistent with the gold answer; The numerical value and order need to be accurate, and there should be no hallucinations.\n    - Completeness: Referring to the reference answers, the assistant's answer should contain all the key points needed to answer the user's question; further elaboration on these key points can be omitted.\nPlease rate whether this answer is suitable for the question. Please note that the gold answer can be considered as a correct answer to the question.\n\nThe assistant receives an overall score on a scale of 1 to 100, where a higher score indicates better overall performance.\nPlease note that if the assistant's answer and the gold answer fully meet the above criteria, its overall rating should be the full marks (100).\nPlease first provide a comprehensive explanation of your evaluation, avoiding any potential bias.\nThen, output a line indicating the score of the Assistant.\n\nPLEASE OUTPUT WITH THE FOLLOWING FORMAT, WHERE THE SCORE IS A SCALE OF 1 TO 100 BY STRICTLY FOLLOWING THIS FORMAT: \"[[score]]\", FOR EXAMPLE \"Rating: [[100]]\":\n<start output>\nEvaluation evidence: your evluation explanation here, no more than 100 words\nRating: [[score]]\n<end output>\n\nNow, start your evaluation:",
        "generate_response": "{\"Reference\":[\"V-JEPA: Latent video prediction for visual representation learning\", \"Self-supervised learning from images with a joint-embedding predictive architecture\", \"Sequential modeling enables scalable learning for large vision models\", \"Vicreg: Variance-invariance-covariance regularization for selfsupervised learning\", \"Cliquecnn: Deep unsupervised exemplar learning\", \"The iwildcam 2020 competition dataset\", \"A neural probabilistic language model\", \"Unsupervised learning by predicting noise\", \"Unsupervised pre-training of image features on non-curated data\", \"BEiT: Bert pretraining of image transformers\", \"Vicreg: Variance-invariance-covariance regularization for selfsupervised learning\", \"Self-supervised learning from images with a joint-embedding predictive architecture\", \"Generative pretraining from pixels\", \"A simple framework for contrastive learning of visual representations\", \"An empirical study of training self-supervised vision transformers\", \"Unsupervised visual representation learning by predicting image rotations\", \"Generative adversarial nets\", \"Scaling and benchmarking self-supervised visual representation learning\", \"Vision models are more robust and fair when pretrained on uncurated images without supervision\", \"Bootstrap your own latent-a new approach to self-supervised learning\", \"Mask r-cnn\", \"Momentum contrast for unsupervised visual representation learning\", \"Masked autoencoders are scalable vision learners\", \"Training compute-optimal large language models\", \"Lora: Low-rank adaptation of large language models\", \"Deep networks with stochastic depth\", \"3d object representations for fine-grained categorization\", \"Learning multiple layers of features from tiny images\", \"The neural autoregressive distribution estimator\", \"Set transformer: A framework for attention-based permutation-invariant neural networks\", \"Sgdr: Stochastic gradient descent with warm restarts\", \"Decoupled weight decay regularization\", \"Recurrent neural network based language model\", \"Self-supervised learning of pretext-invariant representations\", \"Unsupervised learning of visual representations by solving jigsaw puzzles\", \"Wavenet: A generative model for raw audio\", \"Representation learning with contrastive predictive coding\", \"Dinov2: Learning robust visual features without supervision\", \"Training language models to follow instructions with human feedback\", \"Exploring the limits of transfer learning with a unified text-to-text transformer\", \"Zero-shot text-to-image generation\", \"Pixelcnn++: Improving the pixelcnn with discretized logistic mixture likelihood and other modifications\", \"Prediction and entropy of printed english\", \"The effectiveness of mae pre-pretraining for billionscale pretraining\", \"Divide and contrast: Self-supervised learning from uncurated data\", \"Training data-efficient image transformers  distillation through attention\", \"Going deeper with image transformers\", \"Llama: Open and efficient foundation language models\", \"Conditional image generation with pixelcnn decoders\", \"Pixel recurrent neural networks\", \"Neurips\", \"Attention is all you need\", \"Rotation equivariant cnns for digital pathology\", \"Stacked denoising autoencoders: Learning useful representations in a deep network with a local denoising criterion\", \"Diffusion models as masked autoencoders\", \"ClusterFit: Improving Generalization of Visual Representations\", \"Coca: Contrastive captioners are image-text foundation models\", \"Barlow twins: Self-supervised learning via redundancy reduction\", \"Mixup: Beyond empirical risk minimization\", \"Colorful image colorization\"], \"Citation\":[]}",
        "eval_response": "Evaluation evidence: The assistant's answer does not match the gold answer in terms of accuracy and completeness. The references and citations listed in the assistant's answer do not align with those in the gold answer. The assistant's answer includes a long list of unrelated papers, indicating a significant hallucination issue. The correct references and citations from the gold answer are missing in the assistant's response, leading to a lack of completeness.\n\nRating: [[10]]",
        "score": 10
    },
    {
        "level": 3,
        "set": 3,
        "length": 102858,
        "type": "paper",
        "language": "en",
        "question": "Self-Discover: Large Language Models Self-Compose Reasoning Structures",
        "instruction": "We hope you will carefully study the provided papers and determine the citation relationships between them. Please follow the instructions below strictly to complete the task:\n\n#Specific Requirements:\n1. Reference: When a given paper mentions other provided papers, those other papers are considered as \"references\" for the given paper. To summarize in this specific context, references are about what the given paper is using.\n2. Citation: Conversely, when other provided papers mention the given paper in their works, the given paper is being \"cited\" by those other papers. To summarize in this specific context, citations are about who is using the given paper.\n3. Given a paper, you need to determine the citation or reference relationship between this paper and the other papers. Do not consider papers that are not provided.\n3. Please present the paper titles in a json format as follows: {{\"Reference\":[\"Reference Title 1\", \"Reference Title 2\", ..., \"Reference Title n\"], \"Citation\":[\"Citation Title 1\", \"Citation Title 2\", ..., \"Citation Title n\"]}}.\n4. If a paper does not have any references or citations, please leave the corresponding list empty, e.g.{{\"Refernce\":[]}}, {{\"Citation\":[]}}.",
        "prompt_template": "#Papers:\n{docs}\n\n{instruction}\n\n#The paper you need to analyze:\n{question}",
        "doc": [
            "2402.03620.md",
            "2405.14804.md",
            "2310.14628.md",
            "2311.08803.md"
        ],
        "answer": {
            "Reference": [
                "# Plan, Verify and Switch: Integrated Reasoning with Diverse X-of-Thoughts ",
                "# StrategyLLM: Large Language Models as Strategy Generators, Executors, Optimizers, and Evaluators for Problem Solving "
            ],
            "Citation": [
                "# Can LLMs Solve Longer Math Word Problems Better? "
            ]
        },
        "shuffle_doc": true,
        "id": "eee66b88-7fea-4144-b939-a2e66a7f158a",
        "prompt": "[Question]\n#Papers:\n{docs}\n\nWe hope you will carefully study the provided papers and determine the citation relationships between them. Please follow the instructions below strictly to complete the task:\n\n#Specific Requirements:\n1. Reference: When a given paper mentions other provided papers, those other papers are considered as \"references\" for the given paper. To summarize in this specific context, references are about what the given paper is using.\n2. Citation: Conversely, when other provided papers mention the given paper in their works, the given paper is being \"cited\" by those other papers. To summarize in this specific context, citations are about who is using the given paper.\n3. Given a paper, you need to determine the citation or reference relationship between this paper and the other papers. Do not consider papers that are not provided.\n3. Please present the paper titles in a json format as follows: {{\"Reference\":[\"Reference Title 1\", \"Reference Title 2\", ..., \"Reference Title n\"], \"Citation\":[\"Citation Title 1\", \"Citation Title 2\", ..., \"Citation Title n\"]}}.\n4. If a paper does not have any references or citations, please leave the corresponding list empty, e.g.{{\"Refernce\":[]}}, {{\"Citation\":[]}}.\n\n#The paper you need to analyze:\nSelf-Discover: Large Language Models Self-Compose Reasoning Structures\n\n[Gold Answer]\n{'Reference': ['# Plan, Verify and Switch: Integrated Reasoning with Diverse X-of-Thoughts ', '# StrategyLLM: Large Language Models as Strategy Generators, Executors, Optimizers, and Evaluators for Problem Solving '], 'Citation': ['# Can LLMs Solve Longer Math Word Problems Better? ']}\n\n[The Start of Assistant's Predicted Answer]\n{\"Reference\":[\"BIG-Bench Hard (BBH) (Suzgun et al., 2022)\", \"Thinking for Doing (T4D) (Zhou et al., 2023)\", \"MATH (Hendrycks et al., 2021)\", \"LLMs as optimizers (OPRO) (Yang et al., 2023)\", \"Palm 2 technical report (Anil et al., 2023)\", \"Graph of thoughts: Solving elaborate problems with large language models (Besta et al., 2023)\", \"Language models are few-shot learners (Brown et al., 2020)\", \"Skills-in-context prompting: Unlocking compositionality in large language models (Chen et al., 2023)\", \"Program of thoughts prompting: Disentangling computation from reasoning for numerical reasoning tasks (Chen et al., 2022)\", \"Palm: Scaling language modeling with pathways (Chowdhery et al., 2022)\", \"Scaling instruction-finetuned language models (Chung et al., 2022)\", \"Training verifiers to solve math word problems (Cobbe et al., 2021)\", \"Compositional semantic parsing with large language models (Drozdov et al., 2022)\", \"Promptbreeder: Self-referential self-improvement via prompt evolution (Fernando et al., 2023)\", \"Strategyllm: Large language models as strategy generators, executors, optimizers, and evaluators for problem solving (Gao et al., 2023a)\", \"Pal: Program-aided language models (Gao et al., 2023b)\", \"Reasoning with language model is planning with world model (Hao et al., 2023)\", \"Measuring mathematical problem solving with the math dataset (Hendrycks et al., 2021)\", \"Decomposed prompting: A modular approach for solving complex tasks (Khot et al., 2022)\", \"Large language models are zero-shot reasoners (Kojima et al., 2022)\", \"Less is more: Summary of long instructions is better for program synthesis (Kuznia et al., 2022)\", \"Plan, verify and switch: Integrated reasoning with diverse x-of-thoughts (Liu et al., 2023)\", \"HELP ME THINK: A simple prompting strategy for non-experts to create customized content with models (Mishra and Nouri, 2023)\", \"Lila: A unified benchmark for mathematical reasoning (Mishra et al., 2022a)\", \"Reframing instructional prompts to gptk's language (Mishra et al., 2022b)\", \"Crosstask generalization via natural language crowdsourcing instructions (Mishra et al., 2022c)\", \"Elements of a theory of human problem solving (Newell et al., 1958)\", \"Show your work: Scratchpads for intermediate computation with language models (Nye et al., 2021)\", \"Chatgpt: Optimizing language models for dialogue (OpenAI, 2022)\", \"Json generation mode (OpenAI, 2023a)\", \"Gpt-4 technical report (OpenAI, 2023b)\", \"Training language models to follow instructions with human feedback (Ouyang et al., 2022)\", \"Is a question decomposition unit all we need? (Patel et al., 2022)\", \"How to solve it: A new aspect of mathematical method (Polya, 2004)\", \"Skills, rules, and knowledge; signals, signs, and symbols, and other distinctions in human performance models (Rasmussen, 1983)\", \"Branch-solve-merge improves large language model evaluation and generation (Saha et al., 2023)\", \"Beyond the imitation game: Quantifying and extrapolating the capabilities of language models (Srivastava et al., 2023)\", \"Challenging big-bench tasks and whether chain-of-thought can solve them (Suzgun et al., 2022)\", \"Llama 2: Open foundation and finetuned chat models (Touvron et al., 2023)\", \"Attention is all you need (Vaswani et al., 2017)\", \"Selfconsistency improves chain of thought reasoning in language models (Wang et al., 2022)\", \"Plan-and-solve prompting: Improving zero-shot chain-of-thought reasoning by large language models (Wang et al., 2023)\", \"Finetuned language models are zero-shot learners (Wei et al., 2021)\", \"Chain-of-thought prompting elicits reasoning in large language models (Wei et al., 2022)\", \"Large language models as optimizers (Yang et al., 2023)\", \"Tree of thoughts: Deliberate problem solving with large language models (Yao et al., 2023a)\", \"Beyond chain-of-thought, effective graph-of-thought reasoning in large language models (Yao et al., 2023b)\", \"Large language models as analogical reasoners (Yasunaga et al., 2023)\", \"Take a step back: Evoking reasoning via abstraction in large language models (Zheng et al., 2023)\", \"Adapting language models for zero-shot learning by metatuning on dataset and prompt collections (Zhong et al., 2021)\", \"Least-to-most prompting enables complex reasoning in large language models (Zhou et al., 2022a)\", \"How far are large language models from agents with theory-of-mind? (Zhou et al., 2023)\", \"Large language models are humanlevel prompt engineers (Zhou et al., 2022b)\"], \"Citation\":[]}\n[The End of Assistant's Predicted Answer]\n\n[System]\nWe would like to request your feedback on the performance of the AI assistant in response to the user question displayed above according to the gold answer. Please use the following listed aspects and their descriptions as evaluation criteria:\n    - Accuracy and Hallucinations: The assistant's answer is semantically consistent with the gold answer; The numerical value and order need to be accurate, and there should be no hallucinations.\n    - Completeness: Referring to the reference answers, the assistant's answer should contain all the key points needed to answer the user's question; further elaboration on these key points can be omitted.\nPlease rate whether this answer is suitable for the question. Please note that the gold answer can be considered as a correct answer to the question.\n\nThe assistant receives an overall score on a scale of 1 to 100, where a higher score indicates better overall performance.\nPlease note that if the assistant's answer and the gold answer fully meet the above criteria, its overall rating should be the full marks (100).\nPlease first provide a comprehensive explanation of your evaluation, avoiding any potential bias.\nThen, output a line indicating the score of the Assistant.\n\nPLEASE OUTPUT WITH THE FOLLOWING FORMAT, WHERE THE SCORE IS A SCALE OF 1 TO 100 BY STRICTLY FOLLOWING THIS FORMAT: \"[[score]]\", FOR EXAMPLE \"Rating: [[100]]\":\n<start output>\nEvaluation evidence: your evluation explanation here, no more than 100 words\nRating: [[score]]\n<end output>\n\nNow, start your evaluation:",
        "generate_response": "{\"Reference\":[\"BIG-Bench Hard (BBH) (Suzgun et al., 2022)\", \"Thinking for Doing (T4D) (Zhou et al., 2023)\", \"MATH (Hendrycks et al., 2021)\", \"LLMs as optimizers (OPRO) (Yang et al., 2023)\", \"Palm 2 technical report (Anil et al., 2023)\", \"Graph of thoughts: Solving elaborate problems with large language models (Besta et al., 2023)\", \"Language models are few-shot learners (Brown et al., 2020)\", \"Skills-in-context prompting: Unlocking compositionality in large language models (Chen et al., 2023)\", \"Program of thoughts prompting: Disentangling computation from reasoning for numerical reasoning tasks (Chen et al., 2022)\", \"Palm: Scaling language modeling with pathways (Chowdhery et al., 2022)\", \"Scaling instruction-finetuned language models (Chung et al., 2022)\", \"Training verifiers to solve math word problems (Cobbe et al., 2021)\", \"Compositional semantic parsing with large language models (Drozdov et al., 2022)\", \"Promptbreeder: Self-referential self-improvement via prompt evolution (Fernando et al., 2023)\", \"Strategyllm: Large language models as strategy generators, executors, optimizers, and evaluators for problem solving (Gao et al., 2023a)\", \"Pal: Program-aided language models (Gao et al., 2023b)\", \"Reasoning with language model is planning with world model (Hao et al., 2023)\", \"Measuring mathematical problem solving with the math dataset (Hendrycks et al., 2021)\", \"Decomposed prompting: A modular approach for solving complex tasks (Khot et al., 2022)\", \"Large language models are zero-shot reasoners (Kojima et al., 2022)\", \"Less is more: Summary of long instructions is better for program synthesis (Kuznia et al., 2022)\", \"Plan, verify and switch: Integrated reasoning with diverse x-of-thoughts (Liu et al., 2023)\", \"HELP ME THINK: A simple prompting strategy for non-experts to create customized content with models (Mishra and Nouri, 2023)\", \"Lila: A unified benchmark for mathematical reasoning (Mishra et al., 2022a)\", \"Reframing instructional prompts to gptk's language (Mishra et al., 2022b)\", \"Crosstask generalization via natural language crowdsourcing instructions (Mishra et al., 2022c)\", \"Elements of a theory of human problem solving (Newell et al., 1958)\", \"Show your work: Scratchpads for intermediate computation with language models (Nye et al., 2021)\", \"Chatgpt: Optimizing language models for dialogue (OpenAI, 2022)\", \"Json generation mode (OpenAI, 2023a)\", \"Gpt-4 technical report (OpenAI, 2023b)\", \"Training language models to follow instructions with human feedback (Ouyang et al., 2022)\", \"Is a question decomposition unit all we need? (Patel et al., 2022)\", \"How to solve it: A new aspect of mathematical method (Polya, 2004)\", \"Skills, rules, and knowledge; signals, signs, and symbols, and other distinctions in human performance models (Rasmussen, 1983)\", \"Branch-solve-merge improves large language model evaluation and generation (Saha et al., 2023)\", \"Beyond the imitation game: Quantifying and extrapolating the capabilities of language models (Srivastava et al., 2023)\", \"Challenging big-bench tasks and whether chain-of-thought can solve them (Suzgun et al., 2022)\", \"Llama 2: Open foundation and finetuned chat models (Touvron et al., 2023)\", \"Attention is all you need (Vaswani et al., 2017)\", \"Selfconsistency improves chain of thought reasoning in language models (Wang et al., 2022)\", \"Plan-and-solve prompting: Improving zero-shot chain-of-thought reasoning by large language models (Wang et al., 2023)\", \"Finetuned language models are zero-shot learners (Wei et al., 2021)\", \"Chain-of-thought prompting elicits reasoning in large language models (Wei et al., 2022)\", \"Large language models as optimizers (Yang et al., 2023)\", \"Tree of thoughts: Deliberate problem solving with large language models (Yao et al., 2023a)\", \"Beyond chain-of-thought, effective graph-of-thought reasoning in large language models (Yao et al., 2023b)\", \"Large language models as analogical reasoners (Yasunaga et al., 2023)\", \"Take a step back: Evoking reasoning via abstraction in large language models (Zheng et al., 2023)\", \"Adapting language models for zero-shot learning by metatuning on dataset and prompt collections (Zhong et al., 2021)\", \"Least-to-most prompting enables complex reasoning in large language models (Zhou et al., 2022a)\", \"How far are large language models from agents with theory-of-mind? (Zhou et al., 2023)\", \"Large language models are humanlevel prompt engineers (Zhou et al., 2022b)\"], \"Citation\":[]}",
        "eval_response": "Evaluation evidence: The assistant's answer does not match the gold answer in terms of accuracy and completeness. The assistant listed a large number of references that are not mentioned in the gold answer, indicating a significant hallucination issue. Additionally, the assistant failed to identify any citations, whereas the gold answer lists one citation. This discrepancy shows a lack of accuracy in identifying the correct citation relationships and a failure to include all necessary information.\n\nRating: [[20]]",
        "score": 20
    },
    {
        "level": 3,
        "set": 3,
        "length": 108334,
        "type": "paper",
        "language": "en",
        "question": "Executable Code Actions Elicit Better LLM Agents",
        "instruction": "We hope you will carefully study the provided papers and determine the citation relationships between them. Please follow the instructions below strictly to complete the task:\n\n#Specific Requirements:\n1. Reference: When a given paper mentions other provided papers, those other papers are considered as \"references\" for the given paper. To summarize in this specific context, references are about what the given paper is using.\n2. Citation: Conversely, when other provided papers mention the given paper in their works, the given paper is being \"cited\" by those other papers. To summarize in this specific context, citations are about who is using the given paper.\n3. Given a paper, you need to determine the citation or reference relationship between this paper and the other papers. Do not consider papers that are not provided.\n3. Please present the paper titles in a json format as follows: {{\"Reference\":[\"Reference Title 1\", \"Reference Title 2\", ..., \"Reference Title n\"], \"Citation\":[\"Citation Title 1\", \"Citation Title 2\", ..., \"Citation Title n\"]}}.\n4. If a paper does not have any references or citations, please leave the corresponding list empty, e.g.{{\"Refernce\":[]}}, {{\"Citation\":[]}}.",
        "prompt_template": "#Papers:\n{docs}\n\n{instruction}\n\n#The paper you need to analyze:\n{question}",
        "doc": [
            "2402.01030.md",
            "2405.16533.md",
            "2405.20092.md",
            "2402.14658.md"
        ],
        "answer": {
            "Reference": [
                "# OpenCodeInterpreter: Integrating Code Generation with Execution and Refinement "
            ],
            "Citation": [
                "# Chain of Tools: Large Language Model is an Automatic Multi-tool Learner ",
                "# Divide-and-Conquer Meets Consensus: Unleashing the Power of Functions in Code Generation "
            ]
        },
        "shuffle_doc": true,
        "id": "3867e89a-e288-4dd8-ae75-fdc1df962ae0",
        "prompt": "[Question]\n#Papers:\n{docs}\n\nWe hope you will carefully study the provided papers and determine the citation relationships between them. Please follow the instructions below strictly to complete the task:\n\n#Specific Requirements:\n1. Reference: When a given paper mentions other provided papers, those other papers are considered as \"references\" for the given paper. To summarize in this specific context, references are about what the given paper is using.\n2. Citation: Conversely, when other provided papers mention the given paper in their works, the given paper is being \"cited\" by those other papers. To summarize in this specific context, citations are about who is using the given paper.\n3. Given a paper, you need to determine the citation or reference relationship between this paper and the other papers. Do not consider papers that are not provided.\n3. Please present the paper titles in a json format as follows: {{\"Reference\":[\"Reference Title 1\", \"Reference Title 2\", ..., \"Reference Title n\"], \"Citation\":[\"Citation Title 1\", \"Citation Title 2\", ..., \"Citation Title n\"]}}.\n4. If a paper does not have any references or citations, please leave the corresponding list empty, e.g.{{\"Refernce\":[]}}, {{\"Citation\":[]}}.\n\n#The paper you need to analyze:\nExecutable Code Actions Elicit Better LLM Agents\n\n[Gold Answer]\n{'Reference': ['# OpenCodeInterpreter: Integrating Code Generation with Execution and Refinement '], 'Citation': ['# Chain of Tools: Large Language Model is an Automatic Multi-tool Learner ', '# Divide-and-Conquer Meets Consensus: Unleashing the Power of Functions in Code Generation ']}\n\n[The Start of Assistant's Predicted Answer]\n{\"Reference\":[\"A Survey on Evaluation of Large Language Models\", \"Do as i can and not as i say: Grounding language in robotic affordances\", \"Webgpt: Browser-assisted question-answering with human feedback\", \"Augmented language models: a survey\", \"Tool learning with foundation models\", \"Toolllm: Facilitating large language models to master 16000+ real-world apis\", \"Toolformer: Language models can teach themselves to use tools\", \"Progprompt: Generating situated robot task plans using large language models\", \"Leti: Learning to generate from textual interactions\", \"Mint: Evaluating llms in multi-turn interaction with tools and language feedback\", \"React: Synergizing reasoning and acting in language models\", \"Retroformer: Retrospective large language agents with policy gradient optimization\", \"Agenttuning: Enabling generalized agent abilities for llms\", \"Opencodeinterpreter: Integrating code generation with execution and refinement\", \"Ghost in the minecraft: Generally capable agents for open-world enviroments via large language models\", \"Chain-of-thought prompting elicits reasoning in large language models\", \"Self-consistency improves chain of thought reasoning in language models\", \"On the tool manipulation capability of open-source large language models\", \"Evaluating large language models trained on code\", \"Code as policies: Language model programs for embodied control\", \"Intercode: Standardizing and benchmarking interactive coding with execution feedback\", \"Voyager: An openended embodied agent with large language models\", \"Eureka: Human-level reward design via coding large language models\", \"Code4struct: Code generation for few-shot event structure prediction\", \"Hugginggpt: Solving ai tasks with chatgpt and its friends in huggingface\", \"Scaling instruction-finetuned language models\", \"Training verifiers to solve math word problems\", \"Measuring massive multitask language understanding\", \"Measuring coding challenge competence with apps\", \"Measuring mathematical problem solving with the math dataset\", \"Metagpt: Meta programming for multi-agent collaborative framework\", \"Data interpreter: An llm agent for data science\", \"Voxposer: Composable 3d value maps for robotic manipulation with language models\", \"Towards unified alignment between agents, humans, and environment\", \"Webshop: Towards scalable real-world web interaction with grounded language agents\", \"Tree of thoughts: Deliberating problem solving with large language models\", \"Pre-train, prompt, and predict: A systematic survey of prompting methods in natural language processing\", \"Training socially aligned language models in simulated human society\", \"Communicative agents for software development\", \"If llm is the wizard, then code is the wand: A survey on how code empowers large language models to serve as intelligent agents\", \"A Survey on Large Language Model Based Autonomous Agents\", \"Prioritizing safeguarding over autonomy: Risks of llm agents for science\", \"Describing machine learning models via python execution for reasoning\", \"Coauthor: Designing a human-ai collaborative writing dataset for exploring language model capabilities\", \"Api-bank: A benchmark for tool-augmented llms\", \"Hotpotqa: A dataset for diverse, explainable multi-hop question answering\", \"Measuring the performance of large language models in retrieval-augmented generation\"], \"Citation\":[\"Chain of Tools: Large Language Model is an Automatic Multi-tool Learner\", \"Do as i can and not as i say: Grounding language in robotic affordances\", \"Webgpt: Browser-assisted question-answering with human feedback\", \"A Survey on Large Language Model Based Autonomous Agents\", \"Code as policies: Language model programs for embodied control\", \"Intercode: Standardizing and benchmarking interactive coding with execution feedback\", \"Tool learning with foundation models\", \"Toolllm: Facilitating large language models to master 16000+ real-world apis\", \"Training verifiers to solve math word problems\", \"Measuring massive multitask language understanding\", \"Measuring coding challenge competence with apps\", \"Measuring mathematical problem solving with the math dataset\", \"Fireact: Toward language agent fine-tuning\", \"Agentinstruct: Instruction tuning for language agent\", \"Agenttuning: Enabling generalized agent abilities for llms\", \"Gorilla: Large language model connected with massive apis\", \"Llama 2: Open foundation and fine-tuned chat models\", \"Evaluating large language models trained on code\", \"If llm is the wizard, then code is the wand: A survey on how code empowers large language models to serve as intelligent agents\", \"A survey on large language model based autonomous agents\", \"Prioritizing safeguarding over autonomy: Risks of llm agents for science\", \"Towards unified alignment between agents, humans, and environment\"]}\n[The End of Assistant's Predicted Answer]\n\n[System]\nWe would like to request your feedback on the performance of the AI assistant in response to the user question displayed above according to the gold answer. Please use the following listed aspects and their descriptions as evaluation criteria:\n    - Accuracy and Hallucinations: The assistant's answer is semantically consistent with the gold answer; The numerical value and order need to be accurate, and there should be no hallucinations.\n    - Completeness: Referring to the reference answers, the assistant's answer should contain all the key points needed to answer the user's question; further elaboration on these key points can be omitted.\nPlease rate whether this answer is suitable for the question. Please note that the gold answer can be considered as a correct answer to the question.\n\nThe assistant receives an overall score on a scale of 1 to 100, where a higher score indicates better overall performance.\nPlease note that if the assistant's answer and the gold answer fully meet the above criteria, its overall rating should be the full marks (100).\nPlease first provide a comprehensive explanation of your evaluation, avoiding any potential bias.\nThen, output a line indicating the score of the Assistant.\n\nPLEASE OUTPUT WITH THE FOLLOWING FORMAT, WHERE THE SCORE IS A SCALE OF 1 TO 100 BY STRICTLY FOLLOWING THIS FORMAT: \"[[score]]\", FOR EXAMPLE \"Rating: [[100]]\":\n<start output>\nEvaluation evidence: your evluation explanation here, no more than 100 words\nRating: [[score]]\n<end output>\n\nNow, start your evaluation:",
        "generate_response": "{\"Reference\":[\"A Survey on Evaluation of Large Language Models\", \"Do as i can and not as i say: Grounding language in robotic affordances\", \"Webgpt: Browser-assisted question-answering with human feedback\", \"Augmented language models: a survey\", \"Tool learning with foundation models\", \"Toolllm: Facilitating large language models to master 16000+ real-world apis\", \"Toolformer: Language models can teach themselves to use tools\", \"Progprompt: Generating situated robot task plans using large language models\", \"Leti: Learning to generate from textual interactions\", \"Mint: Evaluating llms in multi-turn interaction with tools and language feedback\", \"React: Synergizing reasoning and acting in language models\", \"Retroformer: Retrospective large language agents with policy gradient optimization\", \"Agenttuning: Enabling generalized agent abilities for llms\", \"Opencodeinterpreter: Integrating code generation with execution and refinement\", \"Ghost in the minecraft: Generally capable agents for open-world enviroments via large language models\", \"Chain-of-thought prompting elicits reasoning in large language models\", \"Self-consistency improves chain of thought reasoning in language models\", \"On the tool manipulation capability of open-source large language models\", \"Evaluating large language models trained on code\", \"Code as policies: Language model programs for embodied control\", \"Intercode: Standardizing and benchmarking interactive coding with execution feedback\", \"Voyager: An openended embodied agent with large language models\", \"Eureka: Human-level reward design via coding large language models\", \"Code4struct: Code generation for few-shot event structure prediction\", \"Hugginggpt: Solving ai tasks with chatgpt and its friends in huggingface\", \"Scaling instruction-finetuned language models\", \"Training verifiers to solve math word problems\", \"Measuring massive multitask language understanding\", \"Measuring coding challenge competence with apps\", \"Measuring mathematical problem solving with the math dataset\", \"Metagpt: Meta programming for multi-agent collaborative framework\", \"Data interpreter: An llm agent for data science\", \"Voxposer: Composable 3d value maps for robotic manipulation with language models\", \"Towards unified alignment between agents, humans, and environment\", \"Webshop: Towards scalable real-world web interaction with grounded language agents\", \"Tree of thoughts: Deliberating problem solving with large language models\", \"Pre-train, prompt, and predict: A systematic survey of prompting methods in natural language processing\", \"Training socially aligned language models in simulated human society\", \"Communicative agents for software development\", \"If llm is the wizard, then code is the wand: A survey on how code empowers large language models to serve as intelligent agents\", \"A Survey on Large Language Model Based Autonomous Agents\", \"Prioritizing safeguarding over autonomy: Risks of llm agents for science\", \"Describing machine learning models via python execution for reasoning\", \"Coauthor: Designing a human-ai collaborative writing dataset for exploring language model capabilities\", \"Api-bank: A benchmark for tool-augmented llms\", \"Hotpotqa: A dataset for diverse, explainable multi-hop question answering\", \"Measuring the performance of large language models in retrieval-augmented generation\"], \"Citation\":[\"Chain of Tools: Large Language Model is an Automatic Multi-tool Learner\", \"Do as i can and not as i say: Grounding language in robotic affordances\", \"Webgpt: Browser-assisted question-answering with human feedback\", \"A Survey on Large Language Model Based Autonomous Agents\", \"Code as policies: Language model programs for embodied control\", \"Intercode: Standardizing and benchmarking interactive coding with execution feedback\", \"Tool learning with foundation models\", \"Toolllm: Facilitating large language models to master 16000+ real-world apis\", \"Training verifiers to solve math word problems\", \"Measuring massive multitask language understanding\", \"Measuring coding challenge competence with apps\", \"Measuring mathematical problem solving with the math dataset\", \"Fireact: Toward language agent fine-tuning\", \"Agentinstruct: Instruction tuning for language agent\", \"Agenttuning: Enabling generalized agent abilities for llms\", \"Gorilla: Large language model connected with massive apis\", \"Llama 2: Open foundation and fine-tuned chat models\", \"Evaluating large language models trained on code\", \"If llm is the wizard, then code is the wand: A survey on how code empowers large language models to serve as intelligent agents\", \"A survey on large language model based autonomous agents\", \"Prioritizing safeguarding over autonomy: Risks of llm agents for science\", \"Towards unified alignment between agents, humans, and environment\"]}",
        "eval_response": "Evaluation evidence: The assistant's answer significantly deviates from the gold answer in terms of accuracy and completeness. The gold answer lists specific references and citations, while the assistant's answer includes an extensive and unrelated list of titles that do not match the gold answer. This indicates hallucinations and a lack of focus on the provided task requirements. The assistant failed to accurately identify the correct references and citations for the specific paper in question.\n\nRating: [[10]]",
        "score": 10
    },
    {
        "level": 3,
        "set": 2,
        "length": 66355,
        "type": "paper",
        "language": "en",
        "question": "Soaring from 4K to 400K: Extending LLM's Context with Activation Beacon",
        "instruction": "We hope you will carefully study the provided papers and determine the citation relationships between them. Please follow the instructions below strictly to complete the task:\n\n#Specific Requirements:\n1. Reference: When a given paper mentions other provided papers, those other papers are considered as \"references\" for the given paper. To summarize in this specific context, references are about what the given paper is using.\n2. Citation: Conversely, when other provided papers mention the given paper in their works, the given paper is being \"cited\" by those other papers. To summarize in this specific context, citations are about who is using the given paper.\n3. Given a paper, you need to determine the citation or reference relationship between this paper and the other papers. Do not consider papers that are not provided.\n3. Please present the paper titles in a json format as follows: {{\"Reference\":[\"Reference Title 1\", \"Reference Title 2\", ..., \"Reference Title n\"], \"Citation\":[\"Citation Title 1\", \"Citation Title 2\", ..., \"Citation Title n\"]}}.\n4. If a paper does not have any references or citations, please leave the corresponding list empty, e.g.{{\"Refernce\":[]}}, {{\"Citation\":[]}}.",
        "prompt_template": "#Papers:\n{docs}\n\n{instruction}\n\n#The paper you need to analyze:\n{question}",
        "doc": [
            "2401.03462.md",
            "2404.11912.md",
            "2405.17915.md",
            "2309.17453.md"
        ],
        "answer": {
            "Reference": [
                "# EFFICIENT STREAMING LANGUAGE MoDELS WITH ATTENTION SINKS "
            ],
            "Citation": [
                "# A TriForce: Lossless Acceleration of Long Sequence Generation with Hierarchical Speculative Decoding ",
                "# Long Context is Not Long at All: A Prospector of Long-Dependency Data for Large Language Models "
            ]
        },
        "shuffle_doc": true,
        "id": "88a21d99-4193-4939-aeb6-81b0b2554f7a",
        "prompt": "[Question]\n#Papers:\n{docs}\n\nWe hope you will carefully study the provided papers and determine the citation relationships between them. Please follow the instructions below strictly to complete the task:\n\n#Specific Requirements:\n1. Reference: When a given paper mentions other provided papers, those other papers are considered as \"references\" for the given paper. To summarize in this specific context, references are about what the given paper is using.\n2. Citation: Conversely, when other provided papers mention the given paper in their works, the given paper is being \"cited\" by those other papers. To summarize in this specific context, citations are about who is using the given paper.\n3. Given a paper, you need to determine the citation or reference relationship between this paper and the other papers. Do not consider papers that are not provided.\n3. Please present the paper titles in a json format as follows: {{\"Reference\":[\"Reference Title 1\", \"Reference Title 2\", ..., \"Reference Title n\"], \"Citation\":[\"Citation Title 1\", \"Citation Title 2\", ..., \"Citation Title n\"]}}.\n4. If a paper does not have any references or citations, please leave the corresponding list empty, e.g.{{\"Refernce\":[]}}, {{\"Citation\":[]}}.\n\n#The paper you need to analyze:\nSoaring from 4K to 400K: Extending LLM's Context with Activation Beacon\n\n[Gold Answer]\n{'Reference': ['# EFFICIENT STREAMING LANGUAGE MoDELS WITH ATTENTION SINKS '], 'Citation': ['# A TriForce: Lossless Acceleration of Long Sequence Generation with Hierarchical Speculative Decoding ', '# Long Context is Not Long at All: A Prospector of Long-Dependency Data for Large Language Models ']}\n\n[The Start of Assistant's Predicted Answer]\n{\"Reference\": [\"ALiBi\", \"Position Interpolation\", \"NTK-Aware scaled rope\", \"ReRoPE\", \"Longformer\", \"RMT\", \"Extending context window of large language models via positional interpolation\", \"LongLoRa: Efficient fine-tuning of long-context large language models\", \"Adapting language models to compress contexts\", \"Generating long sequences with sparse transformers\", \"Rethinking attention with performers\", \"Redpajama: An open source recipe to reproduce llama training dataset\", \"Flashattention-2: Faster attention with better parallelism and work partitioning\", \"LongNet: Scaling transformers to 1,000,000,000 tokens\", \"LM-Infinite: Simple on-the-fly length generalization for large language models\", \"Long-range language modeling with selective cache\", \"Reformer: The efficient transformer\", \"How long can open-source llms truly promise on context length?\", \"Train short, test long: Attention with linear biases enables input length extrapolation\", \"Compressive transformers for long-range sequence modelling\", \"Combiner: Full attention transformer with sparse computation cost\", \"Code llama: Open foundation models for code\", \"Long-range language modeling with self-retrieval\", \"Rectified rotary position embeddings\", \"Roformer: Enhanced transformer with rotary position embedding\", \"A length-extrapolatable transformer\", \"Llama: Open and efficient foundation language models\", \"Llama 2: Open foundation and fine-tuned chat models\", \"Natural language processing with transformers\", \"Focused transformer: Contrastive training for context scaling\", \"Linformer: Self-attention with linear complexity\", \"Augmenting language models with long-term memory\", \"Memorizing transformers\", \"Efficient streaming language models with attention sinks\", \"Retrieval meets long context large language models\", \"Big bird: Transformers for longer sequences\", \"Retrieve anything to augment large language models\", \"Proof-pile\", \"Pose: Efficient context window extension of llms via positional skip-wise training\"], \"Citation\": []}\n[The End of Assistant's Predicted Answer]\n\n[System]\nWe would like to request your feedback on the performance of the AI assistant in response to the user question displayed above according to the gold answer. Please use the following listed aspects and their descriptions as evaluation criteria:\n    - Accuracy and Hallucinations: The assistant's answer is semantically consistent with the gold answer; The numerical value and order need to be accurate, and there should be no hallucinations.\n    - Completeness: Referring to the reference answers, the assistant's answer should contain all the key points needed to answer the user's question; further elaboration on these key points can be omitted.\nPlease rate whether this answer is suitable for the question. Please note that the gold answer can be considered as a correct answer to the question.\n\nThe assistant receives an overall score on a scale of 1 to 100, where a higher score indicates better overall performance.\nPlease note that if the assistant's answer and the gold answer fully meet the above criteria, its overall rating should be the full marks (100).\nPlease first provide a comprehensive explanation of your evaluation, avoiding any potential bias.\nThen, output a line indicating the score of the Assistant.\n\nPLEASE OUTPUT WITH THE FOLLOWING FORMAT, WHERE THE SCORE IS A SCALE OF 1 TO 100 BY STRICTLY FOLLOWING THIS FORMAT: \"[[score]]\", FOR EXAMPLE \"Rating: [[100]]\":\n<start output>\nEvaluation evidence: your evluation explanation here, no more than 100 words\nRating: [[score]]\n<end output>\n\nNow, start your evaluation:",
        "generate_response": "{\"Reference\": [\"ALiBi\", \"Position Interpolation\", \"NTK-Aware scaled rope\", \"ReRoPE\", \"Longformer\", \"RMT\", \"Extending context window of large language models via positional interpolation\", \"LongLoRa: Efficient fine-tuning of long-context large language models\", \"Adapting language models to compress contexts\", \"Generating long sequences with sparse transformers\", \"Rethinking attention with performers\", \"Redpajama: An open source recipe to reproduce llama training dataset\", \"Flashattention-2: Faster attention with better parallelism and work partitioning\", \"LongNet: Scaling transformers to 1,000,000,000 tokens\", \"LM-Infinite: Simple on-the-fly length generalization for large language models\", \"Long-range language modeling with selective cache\", \"Reformer: The efficient transformer\", \"How long can open-source llms truly promise on context length?\", \"Train short, test long: Attention with linear biases enables input length extrapolation\", \"Compressive transformers for long-range sequence modelling\", \"Combiner: Full attention transformer with sparse computation cost\", \"Code llama: Open foundation models for code\", \"Long-range language modeling with self-retrieval\", \"Rectified rotary position embeddings\", \"Roformer: Enhanced transformer with rotary position embedding\", \"A length-extrapolatable transformer\", \"Llama: Open and efficient foundation language models\", \"Llama 2: Open foundation and fine-tuned chat models\", \"Natural language processing with transformers\", \"Focused transformer: Contrastive training for context scaling\", \"Linformer: Self-attention with linear complexity\", \"Augmenting language models with long-term memory\", \"Memorizing transformers\", \"Efficient streaming language models with attention sinks\", \"Retrieval meets long context large language models\", \"Big bird: Transformers for longer sequences\", \"Retrieve anything to augment large language models\", \"Proof-pile\", \"Pose: Efficient context window extension of llms via positional skip-wise training\"], \"Citation\": []}",
        "eval_response": "Evaluation evidence: The assistant's answer inaccurately lists numerous references that are not mentioned in the gold answer, indicating a significant hallucination issue. The gold answer specifies one reference, which the assistant fails to include correctly, and two citations, which are entirely missing from the assistant's response. This shows both inaccuracy and incompleteness in addressing the task requirements.\n\nRating: [[10]]",
        "score": 10
    },
    {
        "level": 3,
        "set": 1,
        "length": 48607,
        "type": "paper",
        "language": "en",
        "question": "AUtoAcT: Automatic Agent Learning from Scratch for QA via Self-Planning",
        "instruction": "We hope you will carefully study the provided papers and determine the citation relationships between them. Please follow the instructions below strictly to complete the task:\n\n#Specific Requirements:\n1. Reference: When a given paper mentions other provided papers, those other papers are considered as \"references\" for the given paper. To summarize in this specific context, references are about what the given paper is using.\n2. Citation: Conversely, when other provided papers mention the given paper in their works, the given paper is being \"cited\" by those other papers. To summarize in this specific context, citations are about who is using the given paper.\n3. Given a paper, you need to determine the citation or reference relationship between this paper and the other papers. Do not consider papers that are not provided.\n3. Please present the paper titles in a json format as follows: {{\"Reference\":[\"Reference Title 1\", \"Reference Title 2\", ..., \"Reference Title n\"], \"Citation\":[\"Citation Title 1\", \"Citation Title 2\", ..., \"Citation Title n\"]}}.\n4. If a paper does not have any references or citations, please leave the corresponding list empty, e.g.{{\"Refernce\":[]}}, {{\"Citation\":[]}}.",
        "prompt_template": "#Papers:\n{docs}\n\n{instruction}\n\n#The paper you need to analyze:\n{question}",
        "doc": [
            "2401.05268.md",
            "2402.02716.md",
            "2403.08140.md"
        ],
        "answer": {
            "Reference": [
                "# Understanding the planning of LLM agents: A survey ",
                "# BAGEL: Bootstrapping Agents by Guiding Exploration with Language "
            ],
            "Citation": []
        },
        "shuffle_doc": true,
        "id": "2d30c308-8267-4453-b27e-1eb8f4330e25",
        "prompt": "[Question]\n#Papers:\n{docs}\n\nWe hope you will carefully study the provided papers and determine the citation relationships between them. Please follow the instructions below strictly to complete the task:\n\n#Specific Requirements:\n1. Reference: When a given paper mentions other provided papers, those other papers are considered as \"references\" for the given paper. To summarize in this specific context, references are about what the given paper is using.\n2. Citation: Conversely, when other provided papers mention the given paper in their works, the given paper is being \"cited\" by those other papers. To summarize in this specific context, citations are about who is using the given paper.\n3. Given a paper, you need to determine the citation or reference relationship between this paper and the other papers. Do not consider papers that are not provided.\n3. Please present the paper titles in a json format as follows: {{\"Reference\":[\"Reference Title 1\", \"Reference Title 2\", ..., \"Reference Title n\"], \"Citation\":[\"Citation Title 1\", \"Citation Title 2\", ..., \"Citation Title n\"]}}.\n4. If a paper does not have any references or citations, please leave the corresponding list empty, e.g.{{\"Refernce\":[]}}, {{\"Citation\":[]}}.\n\n#The paper you need to analyze:\nAUtoAcT: Automatic Agent Learning from Scratch for QA via Self-Planning\n\n[Gold Answer]\n{'Reference': ['# Understanding the planning of LLM agents: A survey ', '# BAGEL: Bootstrapping Agents by Guiding Exploration with Language '], 'Citation': []}\n\n[The Start of Assistant's Predicted Answer]\n{\"Reference\": [\"Autoagents: A framework for automatic agent generation\", \"Fireact: Toward language agent fine-tuning\", \"Rest meets react: Self-improvement for multistep reasoning llm agent\", \"Hugginggpt: Solving AI tasks with chatgpt and its friends in huggingface\", \"Making language models better tool learners with execution feedback\", \"Chain-of-thought prompting elicits reasoning in large language models\", \"Toolllm: Facilitating large language models to master 16000+ real-world apis\", \"Medagents: Large language models as collaborators for zero-shot medical reasoning\", \"Stanford alpaca: An instruction-following llama model\", \"Xagent: An autonomous agent for complex task solving\", \"Large language model based multi-agents: A survey of progress and challenges\", \"Lora: Low-rank adaptation of large language models\", \"Large language models can self-improve\", \"Large language models cannot self-correct reasoning yet\", \"Understanding the planning of llm agents: A survey\", \"Babyagi\", \"Chatgpt: Optimizing language models for dialogue\", \"Gpt-4 technical report\", \"Gpt-engineer\", \"Reasoning with language model prompting: A survey\", \"Multiagent collaboration: Harnessing the power of intelligent LLM agents\", \"Llama 2: Open foundation and fine-tuned chat models\", \"A survey on large language model based autonomous agents\", \"Self-instruct: Aligning language models with self-generated instructions\", \"Gorilla: Large language model connected with massive apis\", \"The rise and potential of large language model based agents: A survey\", \"Rewoo: Decoupling reasoning from observations for efficient augmented language models\", \"Hotpotqa: A dataset for diverse, explainable multi-hop question answering\", \"Webshop: Towards scalable realworld web interaction with grounded language agents\", \"React: Synergizing reasoning and acting in language models\", \"Lumos: Learning agents with unified data, modular design, and open-source llms\", \"Agenttuning: Enabling generalized agent abilities for llms\", \"Igniting language intelligence: The hitchhiker's guide from chain-of-thought reasoning to language agents\", \"Judging llm-as-a-judge with mt-bench and chatbot arena\", \"Webarena: A realistic web environment for building autonomous agents\", \"Agents: An open-source framework for autonomous language agents\"], \"Citation\": []}\n[The End of Assistant's Predicted Answer]\n\n[System]\nWe would like to request your feedback on the performance of the AI assistant in response to the user question displayed above according to the gold answer. Please use the following listed aspects and their descriptions as evaluation criteria:\n    - Accuracy and Hallucinations: The assistant's answer is semantically consistent with the gold answer; The numerical value and order need to be accurate, and there should be no hallucinations.\n    - Completeness: Referring to the reference answers, the assistant's answer should contain all the key points needed to answer the user's question; further elaboration on these key points can be omitted.\nPlease rate whether this answer is suitable for the question. Please note that the gold answer can be considered as a correct answer to the question.\n\nThe assistant receives an overall score on a scale of 1 to 100, where a higher score indicates better overall performance.\nPlease note that if the assistant's answer and the gold answer fully meet the above criteria, its overall rating should be the full marks (100).\nPlease first provide a comprehensive explanation of your evaluation, avoiding any potential bias.\nThen, output a line indicating the score of the Assistant.\n\nPLEASE OUTPUT WITH THE FOLLOWING FORMAT, WHERE THE SCORE IS A SCALE OF 1 TO 100 BY STRICTLY FOLLOWING THIS FORMAT: \"[[score]]\", FOR EXAMPLE \"Rating: [[100]]\":\n<start output>\nEvaluation evidence: your evluation explanation here, no more than 100 words\nRating: [[score]]\n<end output>\n\nNow, start your evaluation:",
        "generate_response": "{\"Reference\": [\"Autoagents: A framework for automatic agent generation\", \"Fireact: Toward language agent fine-tuning\", \"Rest meets react: Self-improvement for multistep reasoning llm agent\", \"Hugginggpt: Solving AI tasks with chatgpt and its friends in huggingface\", \"Making language models better tool learners with execution feedback\", \"Chain-of-thought prompting elicits reasoning in large language models\", \"Toolllm: Facilitating large language models to master 16000+ real-world apis\", \"Medagents: Large language models as collaborators for zero-shot medical reasoning\", \"Stanford alpaca: An instruction-following llama model\", \"Xagent: An autonomous agent for complex task solving\", \"Large language model based multi-agents: A survey of progress and challenges\", \"Lora: Low-rank adaptation of large language models\", \"Large language models can self-improve\", \"Large language models cannot self-correct reasoning yet\", \"Understanding the planning of llm agents: A survey\", \"Babyagi\", \"Chatgpt: Optimizing language models for dialogue\", \"Gpt-4 technical report\", \"Gpt-engineer\", \"Reasoning with language model prompting: A survey\", \"Multiagent collaboration: Harnessing the power of intelligent LLM agents\", \"Llama 2: Open foundation and fine-tuned chat models\", \"A survey on large language model based autonomous agents\", \"Self-instruct: Aligning language models with self-generated instructions\", \"Gorilla: Large language model connected with massive apis\", \"The rise and potential of large language model based agents: A survey\", \"Rewoo: Decoupling reasoning from observations for efficient augmented language models\", \"Hotpotqa: A dataset for diverse, explainable multi-hop question answering\", \"Webshop: Towards scalable realworld web interaction with grounded language agents\", \"React: Synergizing reasoning and acting in language models\", \"Lumos: Learning agents with unified data, modular design, and open-source llms\", \"Agenttuning: Enabling generalized agent abilities for llms\", \"Igniting language intelligence: The hitchhiker's guide from chain-of-thought reasoning to language agents\", \"Judging llm-as-a-judge with mt-bench and chatbot arena\", \"Webarena: A realistic web environment for building autonomous agents\", \"Agents: An open-source framework for autonomous language agents\"], \"Citation\": []}",
        "eval_response": "Evaluation evidence: The assistant's answer does not match the gold answer in terms of accuracy. The assistant listed numerous references that are not mentioned in the gold answer, indicating a significant hallucination issue. The gold answer specifies two references, which are not included in the assistant's response. Additionally, the assistant's answer includes an extensive list of references that are irrelevant to the given task, showing a lack of focus and precision. There are no citations listed in both the gold answer and the assistant's response, which is the only point of agreement.\n\nRating: [[10]]",
        "score": 10
    },
    {
        "level": 3,
        "set": 1,
        "length": 32435,
        "type": "paper",
        "language": "en",
        "question": "Seven Failure Points When Engineering a Retrieval Augmented Generation System",
        "instruction": "We hope you will carefully study the provided papers and determine the citation relationships between them. Please follow the instructions below strictly to complete the task:\n\n#Specific Requirements:\n1. Reference: When a given paper mentions other provided papers, those other papers are considered as \"references\" for the given paper. To summarize in this specific context, references are about what the given paper is using.\n2. Citation: Conversely, when other provided papers mention the given paper in their works, the given paper is being \"cited\" by those other papers. To summarize in this specific context, citations are about who is using the given paper.\n3. Given a paper, you need to determine the citation or reference relationship between this paper and the other papers. Do not consider papers that are not provided.\n3. Please present the paper titles in a json format as follows: {{\"Reference\":[\"Reference Title 1\", \"Reference Title 2\", ..., \"Reference Title n\"], \"Citation\":[\"Citation Title 1\", \"Citation Title 2\", ..., \"Citation Title n\"]}}.\n4. If a paper does not have any references or citations, please leave the corresponding list empty, e.g.{{\"Refernce\":[]}}, {{\"Citation\":[]}}.",
        "prompt_template": "#Papers:\n{docs}\n\n{instruction}\n\n#The paper you need to analyze:\n{question}",
        "doc": [
            "2401.05856.md",
            "2404.19705.md",
            "2309.01431.md"
        ],
        "answer": {
            "Reference": [
                "# Benchmarking Large Language Models in Retrieval-Augmented Generation "
            ],
            "Citation": [
                "# When to Retrieve: Teaching LLMs to Utilize Information Retrieval Effectively "
            ]
        },
        "shuffle_doc": true,
        "id": "b7405d8e-13f0-4750-a25d-df3a67ffd39a",
        "prompt": "[Question]\n#Papers:\n{docs}\n\nWe hope you will carefully study the provided papers and determine the citation relationships between them. Please follow the instructions below strictly to complete the task:\n\n#Specific Requirements:\n1. Reference: When a given paper mentions other provided papers, those other papers are considered as \"references\" for the given paper. To summarize in this specific context, references are about what the given paper is using.\n2. Citation: Conversely, when other provided papers mention the given paper in their works, the given paper is being \"cited\" by those other papers. To summarize in this specific context, citations are about who is using the given paper.\n3. Given a paper, you need to determine the citation or reference relationship between this paper and the other papers. Do not consider papers that are not provided.\n3. Please present the paper titles in a json format as follows: {{\"Reference\":[\"Reference Title 1\", \"Reference Title 2\", ..., \"Reference Title n\"], \"Citation\":[\"Citation Title 1\", \"Citation Title 2\", ..., \"Citation Title n\"]}}.\n4. If a paper does not have any references or citations, please leave the corresponding list empty, e.g.{{\"Refernce\":[]}}, {{\"Citation\":[]}}.\n\n#The paper you need to analyze:\nSeven Failure Points When Engineering a Retrieval Augmented Generation System\n\n[Gold Answer]\n{'Reference': ['# Benchmarking Large Language Models in Retrieval-Augmented Generation '], 'Citation': ['# When to Retrieve: Teaching LLMs to Utilize Information Retrieval Effectively ']}\n\n[The Start of Assistant's Predicted Answer]\n{\"Reference\": [\"GPTCache: An Open-Source Semantic Cache for LLM Applications Enabling Faster Answers and Cost Savings\", \"Self-adaptive Machine Learning Systems: Research Challenges and Opportunities\", \"Benchmarking Large Language Models in Retrieval-Augmented Generation\", \"Efficient Open Domain Multi-Hop Question Answering with Few-Shot Data Synthesis\", \"Threshy: Supporting safe usage of intelligent web services\", \"Beware the evolving 'intelligent'web service! An integration architecture tactic to guard AI-first components\", \"Retrieval augmented language model pre-training\", \"Fidlight: Efficient and effective retrieval-augmented text generation\", \"Leveraging passage retrieval with generative models for open domain question answering\", \"BioASQ-QA: A manually curated corpus for biomedical question answering\", \"LoRA Fine-tuning Efficiently Undoes Safety Training in Llama 2-Chat 70B\", \"Retrieval-augmented generation for knowledge-intensive nlp tasks\", \"Lost in the middle: How language models use long contexts\", \"G-eval: Nlg evaluation using gpt-4 with better human alignment\", \"Retrieval-based prompt selection for code-related few-shot learning\", \"GPT-4 Technical Report\", \"Robust speech recognition via large-scale weak supervision\", \"Improving the domain adaptation of retrieval augmented generation (RAG) models for open domain question answering\", \"Large language models for information retrieval: A survey\"], \"Citation\": [\"Gpt-4 technical report\", \"RAG makes LLMs better and equal\", \"Bridging the lexical chasm: statistical approaches to answer-finding\", \"Improving language models by retrieving from trillions of tokens\", \"Language models are few-shot learners\", \"QuAC: Question Answering in Context\", \"Navigating uncertainty: Optimizing api dependency for hallucination reduction in closedbook question answering\", \"Simcse: Simple contrastive learning of sentence embeddings\", \"Retrieval-Augmented Generation for Large Language Models: A Survey\", \"Unsupervised dense information retrieval with contrastive learning\", \"REALM: retrieval-augmented language model pre-training\", \"Leveraging Passage Retrieval with Generative Models for Open Domain Question Answering\", \"Survey of hallucination in natural language generation\", \"Mixtral of experts\", \"Scaling laws for neural language models\", \"Dense Passage Retrieval for Open-Domain Question Answering\", \"Natural questions: a benchmark for question answering research\", \"Retrieval-augmented generation for knowledge-intensive NLP tasks\", \"Holistic evaluation of language models\", \"TruthfulQA: Measuring How Models Mimic Human Falsehoods\", \"When Not to Trust Language Models: Investigating Effectiveness of Parametric and NonParametric Memories\", \"Webgpt: Browser-assisted question-answering with human feedback\", \"Ms marco: A human-generated machine reading comprehension dataset\", \"SQuAD: 100,000+ Questions for Machine Comprehension of Text\", \"In-context retrieval-augmented language models\", \"Gemini 1.5: Unlocking multimodal understanding across millions of tokens of context\", \"Sentence-bert: Sentence embeddings using siamese bert-networks\", \"The probabilistic relevance framework: Bm 25 and beyond\", \"Toolformer: Language models can teach themselves to use tools\", \"Two-Step Question Retrieval for Open-Domain QA\", \"Beir: A heterogeneous benchmark for zero-shot evaluation of information retrieval models\", \"Llama: Open and efficient foundation language models\", \"Llama 2: Open foundation and fine-tuned chat models\", \"Retrieving and reading: A comprehensive survey on open-domain question answering\"]}\n[The End of Assistant's Predicted Answer]\n\n[System]\nWe would like to request your feedback on the performance of the AI assistant in response to the user question displayed above according to the gold answer. Please use the following listed aspects and their descriptions as evaluation criteria:\n    - Accuracy and Hallucinations: The assistant's answer is semantically consistent with the gold answer; The numerical value and order need to be accurate, and there should be no hallucinations.\n    - Completeness: Referring to the reference answers, the assistant's answer should contain all the key points needed to answer the user's question; further elaboration on these key points can be omitted.\nPlease rate whether this answer is suitable for the question. Please note that the gold answer can be considered as a correct answer to the question.\n\nThe assistant receives an overall score on a scale of 1 to 100, where a higher score indicates better overall performance.\nPlease note that if the assistant's answer and the gold answer fully meet the above criteria, its overall rating should be the full marks (100).\nPlease first provide a comprehensive explanation of your evaluation, avoiding any potential bias.\nThen, output a line indicating the score of the Assistant.\n\nPLEASE OUTPUT WITH THE FOLLOWING FORMAT, WHERE THE SCORE IS A SCALE OF 1 TO 100 BY STRICTLY FOLLOWING THIS FORMAT: \"[[score]]\", FOR EXAMPLE \"Rating: [[100]]\":\n<start output>\nEvaluation evidence: your evluation explanation here, no more than 100 words\nRating: [[score]]\n<end output>\n\nNow, start your evaluation:",
        "generate_response": "{\"Reference\": [\"GPTCache: An Open-Source Semantic Cache for LLM Applications Enabling Faster Answers and Cost Savings\", \"Self-adaptive Machine Learning Systems: Research Challenges and Opportunities\", \"Benchmarking Large Language Models in Retrieval-Augmented Generation\", \"Efficient Open Domain Multi-Hop Question Answering with Few-Shot Data Synthesis\", \"Threshy: Supporting safe usage of intelligent web services\", \"Beware the evolving 'intelligent'web service! An integration architecture tactic to guard AI-first components\", \"Retrieval augmented language model pre-training\", \"Fidlight: Efficient and effective retrieval-augmented text generation\", \"Leveraging passage retrieval with generative models for open domain question answering\", \"BioASQ-QA: A manually curated corpus for biomedical question answering\", \"LoRA Fine-tuning Efficiently Undoes Safety Training in Llama 2-Chat 70B\", \"Retrieval-augmented generation for knowledge-intensive nlp tasks\", \"Lost in the middle: How language models use long contexts\", \"G-eval: Nlg evaluation using gpt-4 with better human alignment\", \"Retrieval-based prompt selection for code-related few-shot learning\", \"GPT-4 Technical Report\", \"Robust speech recognition via large-scale weak supervision\", \"Improving the domain adaptation of retrieval augmented generation (RAG) models for open domain question answering\", \"Large language models for information retrieval: A survey\"], \"Citation\": [\"Gpt-4 technical report\", \"RAG makes LLMs better and equal\", \"Bridging the lexical chasm: statistical approaches to answer-finding\", \"Improving language models by retrieving from trillions of tokens\", \"Language models are few-shot learners\", \"QuAC: Question Answering in Context\", \"Navigating uncertainty: Optimizing api dependency for hallucination reduction in closedbook question answering\", \"Simcse: Simple contrastive learning of sentence embeddings\", \"Retrieval-Augmented Generation for Large Language Models: A Survey\", \"Unsupervised dense information retrieval with contrastive learning\", \"REALM: retrieval-augmented language model pre-training\", \"Leveraging Passage Retrieval with Generative Models for Open Domain Question Answering\", \"Survey of hallucination in natural language generation\", \"Mixtral of experts\", \"Scaling laws for neural language models\", \"Dense Passage Retrieval for Open-Domain Question Answering\", \"Natural questions: a benchmark for question answering research\", \"Retrieval-augmented generation for knowledge-intensive NLP tasks\", \"Holistic evaluation of language models\", \"TruthfulQA: Measuring How Models Mimic Human Falsehoods\", \"When Not to Trust Language Models: Investigating Effectiveness of Parametric and NonParametric Memories\", \"Webgpt: Browser-assisted question-answering with human feedback\", \"Ms marco: A human-generated machine reading comprehension dataset\", \"SQuAD: 100,000+ Questions for Machine Comprehension of Text\", \"In-context retrieval-augmented language models\", \"Gemini 1.5: Unlocking multimodal understanding across millions of tokens of context\", \"Sentence-bert: Sentence embeddings using siamese bert-networks\", \"The probabilistic relevance framework: Bm 25 and beyond\", \"Toolformer: Language models can teach themselves to use tools\", \"Two-Step Question Retrieval for Open-Domain QA\", \"Beir: A heterogeneous benchmark for zero-shot evaluation of information retrieval models\", \"Llama: Open and efficient foundation language models\", \"Llama 2: Open foundation and fine-tuned chat models\", \"Retrieving and reading: A comprehensive survey on open-domain question answering\"]}",
        "eval_response": "Evaluation evidence: The assistant's answer is inaccurate and contains hallucinations. The provided answer lists numerous references and citations that are not mentioned in the gold answer, indicating a failure to accurately identify the specific relationships for the paper in question. The gold answer specifies one reference and one citation, which the assistant's answer does not correctly reflect, instead providing an extensive and unrelated list. This demonstrates a lack of accuracy and introduces irrelevant information, which does not meet the requirements of the task.\n\nRating: [[10]]",
        "score": 10
    },
    {
        "level": 3,
        "set": 3,
        "length": 105104,
        "type": "paper",
        "language": "en",
        "question": "OpenMoE: An Early Effort on Open Mixture-of-Experts Language Models",
        "instruction": "We hope you will carefully study the provided papers and determine the citation relationships between them. Please follow the instructions below strictly to complete the task:\n\n#Specific Requirements:\n1. Reference: When a given paper mentions other provided papers, those other papers are considered as \"references\" for the given paper. To summarize in this specific context, references are about what the given paper is using.\n2. Citation: Conversely, when other provided papers mention the given paper in their works, the given paper is being \"cited\" by those other papers. To summarize in this specific context, citations are about who is using the given paper.\n3. Given a paper, you need to determine the citation or reference relationship between this paper and the other papers. Do not consider papers that are not provided.\n3. Please present the paper titles in a json format as follows: {{\"Reference\":[\"Reference Title 1\", \"Reference Title 2\", ..., \"Reference Title n\"], \"Citation\":[\"Citation Title 1\", \"Citation Title 2\", ..., \"Citation Title n\"]}}.\n4. If a paper does not have any references or citations, please leave the corresponding list empty, e.g.{{\"Refernce\":[]}}, {{\"Citation\":[]}}.",
        "prompt_template": "#Papers:\n{docs}\n\n{instruction}\n\n#The paper you need to analyze:\n{question}",
        "doc": [
            "2402.01739.md",
            "2404.15381.md",
            "2405.01029.md"
        ],
        "answer": {
            "Reference": [],
            "Citation": [
                "# Advances and Open Challenges in Federated Learning with Foundation Models ",
                "# MVMoE: Multi-Task Vehicle Routing Solver with Mixture-of-Experts "
            ]
        },
        "shuffle_doc": true,
        "id": "993e5286-0023-44cb-8259-c0d4129318c4",
        "prompt": "[Question]\n#Papers:\n{docs}\n\nWe hope you will carefully study the provided papers and determine the citation relationships between them. Please follow the instructions below strictly to complete the task:\n\n#Specific Requirements:\n1. Reference: When a given paper mentions other provided papers, those other papers are considered as \"references\" for the given paper. To summarize in this specific context, references are about what the given paper is using.\n2. Citation: Conversely, when other provided papers mention the given paper in their works, the given paper is being \"cited\" by those other papers. To summarize in this specific context, citations are about who is using the given paper.\n3. Given a paper, you need to determine the citation or reference relationship between this paper and the other papers. Do not consider papers that are not provided.\n3. Please present the paper titles in a json format as follows: {{\"Reference\":[\"Reference Title 1\", \"Reference Title 2\", ..., \"Reference Title n\"], \"Citation\":[\"Citation Title 1\", \"Citation Title 2\", ..., \"Citation Title n\"]}}.\n4. If a paper does not have any references or citations, please leave the corresponding list empty, e.g.{{\"Refernce\":[]}}, {{\"Citation\":[]}}.\n\n#The paper you need to analyze:\nOpenMoE: An Early Effort on Open Mixture-of-Experts Language Models\n\n[Gold Answer]\n{'Reference': [], 'Citation': ['# Advances and Open Challenges in Federated Learning with Foundation Models ', '# MVMoE: Multi-Task Vehicle Routing Solver with Mixture-of-Experts ']}\n\n[The Start of Assistant's Predicted Answer]\n{\"Reference\": [\"Palm 2 technical report\", \"(inthe)wildchat: 570k chatGPT interaction logs in the wild\", \"Beyond the imitation game: Quantifying and extrapolating the capabilities of language models\", \"Efficient training of language models to fill in the middle\", \"Findings of the 2016 conference on machine translation\", \"Language models are few-shot learners\", \"Evaluating large language models trained on code\", \"Palm: Scaling language modeling with pathways\", \"Switch transformers: Scaling to trillion parameter models with simple and efficient sparsity\", \"How does gpt obtain its ability? tracing emergent abilities of language models to their sources\", \"Megablocks: Efficient sparse training with mixture-of-experts\", \"A framework for few-shot language model evaluation\", \"Openllama: An open reproduction of llama\", \"Measuring massive multitask language understanding\", \"Training compute-optimal large language models\", \"Mixtral of experts\", \"Base layers: Simplifying training of large, sparse models\", \"Self-prompting large language models for open-domain qa\", \"Starcoder: May the source be with you!\", \"Roberta: A robustly optimized bert pretraining approach\", \"Cross-token modeling with conditional computation\", \"Multimodal contrastive learning with limoe: The language-image mixture of experts\", \"Xgen-7b technical report\", \"From sparse to soft mixtures of experts\", \"Scaling language models: Methods, analysis \\& insights from training gopher\", \"Exploring the limits of transfer learning with a unified text-to-text transformer\", \"Scaling vision with sparse mixture of experts\", \"Hash layers for large sparse models\", \"Code llama: Open foundation models for code\", \"Glu variants improve transformer\", \"Outrageously large neural networks: The sparsely-gated mixture-of-experts layer\", \"Megatron-lm: Training multi-billion parameter language models using model parallelism\", \"Dolma: An Open Corpus of Three Trillion Tokens for Language Model Pretraining Research\", \"Roformer: Enhanced transformer with rotary position embedding\", \"U12: Unifying language learning paradigms\", \"Unifying language learning paradigms\", \"Llama: Open and efficient foundation language models\", \"GPT-J-6B: A 6 Billion Parameter Autoregressive Language Model\", \"CCNet: Extracting high quality monolingual datasets from web crawl data\", \"Gspmd: General and scalable parallelization for ml computation graphs\", \"One student knows all experts know: From sparse to dense\", \"Go wider instead of deeper\", \"Efficient language modeling with sparse all-mlp\", \"Tinyllama: An open-source small language model\", \"Deep long-tailed learning: A survey\", \"Judging llm-as-a-judge with mt-bench and chatbot arena\", \"Brainformers: Trading simplicity for efficiency\", \"Mixture-of-experts with expert choice routing\", \"St-moe: Designing stable and transferable sparse expert models\"], \"Citation\": []}\n[The End of Assistant's Predicted Answer]\n\n[System]\nWe would like to request your feedback on the performance of the AI assistant in response to the user question displayed above according to the gold answer. Please use the following listed aspects and their descriptions as evaluation criteria:\n    - Accuracy and Hallucinations: The assistant's answer is semantically consistent with the gold answer; The numerical value and order need to be accurate, and there should be no hallucinations.\n    - Completeness: Referring to the reference answers, the assistant's answer should contain all the key points needed to answer the user's question; further elaboration on these key points can be omitted.\nPlease rate whether this answer is suitable for the question. Please note that the gold answer can be considered as a correct answer to the question.\n\nThe assistant receives an overall score on a scale of 1 to 100, where a higher score indicates better overall performance.\nPlease note that if the assistant's answer and the gold answer fully meet the above criteria, its overall rating should be the full marks (100).\nPlease first provide a comprehensive explanation of your evaluation, avoiding any potential bias.\nThen, output a line indicating the score of the Assistant.\n\nPLEASE OUTPUT WITH THE FOLLOWING FORMAT, WHERE THE SCORE IS A SCALE OF 1 TO 100 BY STRICTLY FOLLOWING THIS FORMAT: \"[[score]]\", FOR EXAMPLE \"Rating: [[100]]\":\n<start output>\nEvaluation evidence: your evluation explanation here, no more than 100 words\nRating: [[score]]\n<end output>\n\nNow, start your evaluation:",
        "generate_response": "{\"Reference\": [\"Palm 2 technical report\", \"(inthe)wildchat: 570k chatGPT interaction logs in the wild\", \"Beyond the imitation game: Quantifying and extrapolating the capabilities of language models\", \"Efficient training of language models to fill in the middle\", \"Findings of the 2016 conference on machine translation\", \"Language models are few-shot learners\", \"Evaluating large language models trained on code\", \"Palm: Scaling language modeling with pathways\", \"Switch transformers: Scaling to trillion parameter models with simple and efficient sparsity\", \"How does gpt obtain its ability? tracing emergent abilities of language models to their sources\", \"Megablocks: Efficient sparse training with mixture-of-experts\", \"A framework for few-shot language model evaluation\", \"Openllama: An open reproduction of llama\", \"Measuring massive multitask language understanding\", \"Training compute-optimal large language models\", \"Mixtral of experts\", \"Base layers: Simplifying training of large, sparse models\", \"Self-prompting large language models for open-domain qa\", \"Starcoder: May the source be with you!\", \"Roberta: A robustly optimized bert pretraining approach\", \"Cross-token modeling with conditional computation\", \"Multimodal contrastive learning with limoe: The language-image mixture of experts\", \"Xgen-7b technical report\", \"From sparse to soft mixtures of experts\", \"Scaling language models: Methods, analysis \\& insights from training gopher\", \"Exploring the limits of transfer learning with a unified text-to-text transformer\", \"Scaling vision with sparse mixture of experts\", \"Hash layers for large sparse models\", \"Code llama: Open foundation models for code\", \"Glu variants improve transformer\", \"Outrageously large neural networks: The sparsely-gated mixture-of-experts layer\", \"Megatron-lm: Training multi-billion parameter language models using model parallelism\", \"Dolma: An Open Corpus of Three Trillion Tokens for Language Model Pretraining Research\", \"Roformer: Enhanced transformer with rotary position embedding\", \"U12: Unifying language learning paradigms\", \"Unifying language learning paradigms\", \"Llama: Open and efficient foundation language models\", \"GPT-J-6B: A 6 Billion Parameter Autoregressive Language Model\", \"CCNet: Extracting high quality monolingual datasets from web crawl data\", \"Gspmd: General and scalable parallelization for ml computation graphs\", \"One student knows all experts know: From sparse to dense\", \"Go wider instead of deeper\", \"Efficient language modeling with sparse all-mlp\", \"Tinyllama: An open-source small language model\", \"Deep long-tailed learning: A survey\", \"Judging llm-as-a-judge with mt-bench and chatbot arena\", \"Brainformers: Trading simplicity for efficiency\", \"Mixture-of-experts with expert choice routing\", \"St-moe: Designing stable and transferable sparse expert models\"], \"Citation\": []}",
        "eval_response": "Evaluation evidence: The assistant's answer is inaccurate and contains hallucinations. The provided answer lists numerous references that are not mentioned in the gold answer, indicating a misunderstanding or misinterpretation of the task. The gold answer specifies two citations, which are completely missing from the assistant's response. This shows a lack of completeness and accuracy in identifying the citation relationships as required.\n\nRating: [[10]]",
        "score": 10
    },
    {
        "level": 4,
        "set": 3,
        "length": 135738,
        "type": "paper",
        "language": "en",
        "question": "",
        "instruction": "We kindly ask you to thoroughly review the provided papers and construct a citation chain from them. Please adhere to the following instructions strictly while completing the task:\n\n#Task Instructions:\nGiven several papers, you are required to identify and list the longest citation chain, which demonstrates the citation relationship among the provided papers.\n\n#Specific Requirements:\n1.Please present the titles of the papers in the form of a list, as follows: [\"Title of Paper 1\", \"Title of Paper 2\", ..., \"Title of Paper n\"].\n2.Ensure that the citation chain in the list is linear and continuous, meaning that the first paper title in the list (Paper 1) should not cite any other works. Instead, it should be cited by the next paper in the list (Paper 2); subsequently, each paper should then be cited by the next one in the list, continuing up to the last paper (Paper n).\n3.Consider only the citation relationships within the supplied collection of papers, and ensure that the citation chain accurately reflects the sequential citation order among these documents.\n4.Do not take into account any articles not provided, and disregard other non-linear citation relationships.",
        "prompt_template": "{instruction}\n\n#Papers Provided:\n{docs}",
        "doc": [
            "2306.02707.md",
            "2306.11644.md",
            "2309.05463.md",
            "2311.16867.md",
            "2405.08514.md"
        ],
        "answer": [
            "# Q Orca: Progressive Learning from Complex Explanation Traces of GPT-4 ",
            "# Textbooks Are All You Need ",
            "# Textbooks Are All You Need II: phi-1.5 technical report ",
            "# The Falcon Series of Open Language Models ",
            "# Falcon 7b for Software Mention Detection in Scholarly Documents "
        ],
        "shuffle_doc": true,
        "id": "401e789c-61bb-49e1-b160-2d3d4426a0e7",
        "prompt": "[Question]\nWe kindly ask you to thoroughly review the provided papers and construct a citation chain from them. Please adhere to the following instructions strictly while completing the task:\n\n#Task Instructions:\nGiven several papers, you are required to identify and list the longest citation chain, which demonstrates the citation relationship among the provided papers.\n\n#Specific Requirements:\n1.Please present the titles of the papers in the form of a list, as follows: [\"Title of Paper 1\", \"Title of Paper 2\", ..., \"Title of Paper n\"].\n2.Ensure that the citation chain in the list is linear and continuous, meaning that the first paper title in the list (Paper 1) should not cite any other works. Instead, it should be cited by the next paper in the list (Paper 2); subsequently, each paper should then be cited by the next one in the list, continuing up to the last paper (Paper n).\n3.Consider only the citation relationships within the supplied collection of papers, and ensure that the citation chain accurately reflects the sequential citation order among these documents.\n4.Do not take into account any articles not provided, and disregard other non-linear citation relationships.\n\n#Papers Provided:\n{docs}\n\n[Gold Answer]\n['# Q Orca: Progressive Learning from Complex Explanation Traces of GPT-4 ', '# Textbooks Are All You Need ', '# Textbooks Are All You Need II: phi-1.5 technical report ', '# The Falcon Series of Open Language Models ', '# Falcon 7b for Software Mention Detection in Scholarly Documents ']\n\n[The Start of Assistant's Predicted Answer]\n[\"The Falcon Series of Open Language Models\"]\n[The End of Assistant's Predicted Answer]\n\n[System]\nWe would like to request your feedback on the performance of the AI assistant in response to the user question displayed above according to the gold answer. Please use the following listed aspects and their descriptions as evaluation criteria:\n    - Accuracy and Hallucinations: The assistant's answer is semantically consistent with the gold answer; The numerical value and order need to be accurate, and there should be no hallucinations.\n    - Completeness: Referring to the reference answers, the assistant's answer should contain all the key points needed to answer the user's question; further elaboration on these key points can be omitted.\nPlease rate whether this answer is suitable for the question. Please note that the gold answer can be considered as a correct answer to the question.\n\nThe assistant receives an overall score on a scale of 1 to 100, where a higher score indicates better overall performance.\nPlease note that if the assistant's answer and the gold answer fully meet the above criteria, its overall rating should be the full marks (100).\nPlease first provide a comprehensive explanation of your evaluation, avoiding any potential bias.\nThen, output a line indicating the score of the Assistant.\n\nPLEASE OUTPUT WITH THE FOLLOWING FORMAT, WHERE THE SCORE IS A SCALE OF 1 TO 100 BY STRICTLY FOLLOWING THIS FORMAT: \"[[score]]\", FOR EXAMPLE \"Rating: [[100]]\":\n<start output>\nEvaluation evidence: your evluation explanation here, no more than 100 words\nRating: [[score]]\n<end output>\n\nNow, start your evaluation:",
        "generate_response": "[\"The Falcon Series of Open Language Models\"]",
        "eval_response": "Evaluation evidence: The assistant's answer is incomplete and inaccurate. It only lists one paper title, \"The Falcon Series of Open Language Models,\" whereas the gold answer provides a complete citation chain with five titles. The assistant's response fails to meet the requirements of identifying and listing the longest citation chain among the provided papers. This shows a lack of completeness and accuracy in constructing the required citation chain.\n\nRating: [[10]]",
        "score": 10
    },
    {
        "level": 3,
        "set": 2,
        "length": 83497,
        "type": "paper",
        "language": "en",
        "question": "RAG VS Fine-TUNing: PiPELINES, TRADEOFFS, AND A CASE STUDY ON AGRICULTURE",
        "instruction": "We hope you will carefully study the provided papers and determine the citation relationships between them. Please follow the instructions below strictly to complete the task:\n\n#Specific Requirements:\n1. Reference: When a given paper mentions other provided papers, those other papers are considered as \"references\" for the given paper. To summarize in this specific context, references are about what the given paper is using.\n2. Citation: Conversely, when other provided papers mention the given paper in their works, the given paper is being \"cited\" by those other papers. To summarize in this specific context, citations are about who is using the given paper.\n3. Given a paper, you need to determine the citation or reference relationship between this paper and the other papers. Do not consider papers that are not provided.\n3. Please present the paper titles in a json format as follows: {{\"Reference\":[\"Reference Title 1\", \"Reference Title 2\", ..., \"Reference Title n\"], \"Citation\":[\"Citation Title 1\", \"Citation Title 2\", ..., \"Citation Title n\"]}}.\n4. If a paper does not have any references or citations, please leave the corresponding list empty, e.g.{{\"Refernce\":[]}}, {{\"Citation\":[]}}.",
        "prompt_template": "#Papers:\n{docs}\n\n{instruction}\n\n#The paper you need to analyze:\n{question}",
        "doc": [
            "2401.08406.md",
            "2405.17147.md",
            "2405.20347.md",
            "2306.05685.md",
            "2310.06225.md"
        ],
        "answer": {
            "Reference": [
                "# Judging LLM-as-a-Judge with MT-Bench and Chatbot Arena ",
                "# GPT-4 AS AN AGRONOMIST ASSISTANT? ANSWERING AGRICULTURE QUESTIONS USING LARGE LANGUAGE MODELS "
            ],
            "Citation": [
                "# Large Language Models (LLMs): Deployment, Tokenomics and Sustainability ",
                "# Small Language Models for Application Interactions: A Case Study "
            ]
        },
        "shuffle_doc": true,
        "id": "58d8f1bc-ce8f-4ee4-80c5-873f234aa231",
        "prompt": "[Question]\n#Papers:\n{docs}\n\nWe hope you will carefully study the provided papers and determine the citation relationships between them. Please follow the instructions below strictly to complete the task:\n\n#Specific Requirements:\n1. Reference: When a given paper mentions other provided papers, those other papers are considered as \"references\" for the given paper. To summarize in this specific context, references are about what the given paper is using.\n2. Citation: Conversely, when other provided papers mention the given paper in their works, the given paper is being \"cited\" by those other papers. To summarize in this specific context, citations are about who is using the given paper.\n3. Given a paper, you need to determine the citation or reference relationship between this paper and the other papers. Do not consider papers that are not provided.\n3. Please present the paper titles in a json format as follows: {{\"Reference\":[\"Reference Title 1\", \"Reference Title 2\", ..., \"Reference Title n\"], \"Citation\":[\"Citation Title 1\", \"Citation Title 2\", ..., \"Citation Title n\"]}}.\n4. If a paper does not have any references or citations, please leave the corresponding list empty, e.g.{{\"Refernce\":[]}}, {{\"Citation\":[]}}.\n\n#The paper you need to analyze:\nRAG VS Fine-TUNing: PiPELINES, TRADEOFFS, AND A CASE STUDY ON AGRICULTURE\n\n[Gold Answer]\n{'Reference': ['# Judging LLM-as-a-Judge with MT-Bench and Chatbot Arena ', '# GPT-4 AS AN AGRONOMIST ASSISTANT? ANSWERING AGRICULTURE QUESTIONS USING LARGE LANGUAGE MODELS '], 'Citation': ['# Large Language Models (LLMs): Deployment, Tokenomics and Sustainability ', '# Small Language Models for Application Interactions: A Case Study ']}\n\n[The Start of Assistant's Predicted Answer]\n{\"Reference\":[\"ROUGE: A package for automatic evaluation of summaries\", \"Bleu: a method for automatic evaluation of machine translation\", \"Word Mover's Distance\", \"Kullback-Leibler divergence\", \"Evaluation with Guideline\", \"Succinctness\", \"Relevance\", \"Overlap\", \"Diversity\", \"Fluency\", \"Coverage\"], \"Citation\":[\"A Comparison of Statistical Downscaling Methods Suited for Wildfire Applications\", \"500 Questions 500 Answers - Embrapa/SCT\", \"2009 Cost Estimates of Establishing and Producing Gala Apples in Washington\", \"Vikaspedia\", \"Sparks of artificial general intelligence: Early experiments with gpt-4\", \"Democratizing data-driven agriculture using affordable hardware\", \"Farmvibes: Precision agriculture with ai\", \"Pdf2text\", \"Pypdf\", \"Training Deep Nets with Sublinear Memory Cost\", \"Can Large Language Models Be an Alternative to Human Evaluations?\", \"Tei p5: Guidelines for electronic text encoding and interchange\", \"Evaluating Open-Domain Question Answering in the Era of Large Language Models\", \"Adam: A Method for Stochastic Optimization\", \"Lora: Low-rank adaptation of large language models\", \"Billion-scale similarity search with GPUs\", \"Retrieval-augmented generation for knowledge-intensive nlp tasks\", \"Large language models for supply chain optimization\", \"SGDR: Stochastic Gradient Descent with Warm Restarts\", \"Decoupled Weight Decay Regularization\", \"Mixed Precision Training\", \"Azureml metrics python package\", \"FActScore: Fine-grained Atomic Evaluation of Factual Precision in Long Form Text Generation\", \"Large Dual Encoders Are Generalizable Retrievers\", \"Capabilities of gpt-4 on medical challenge problems\", \"Gpt-4 technical report\", \"Training language models to follow instructions with human feedback\", \"PyTorch: An Imperative Style, High-Performance Deep Learning Library\", \"Sentence-BERT: Sentence Embeddings using Siamese BERT-Networks\", \"Knowledge guided representation learning and causal structure learning in soil science\", \"GPT-4 as an agronomist assistant? answering agriculture exams using large language models\", \"Microsoft copilot impact on finance\", \"Large language models in medicine\", \"LLaMA: Open and Efficient Foundation Language Models\", \"Llama 2: Open Foundation and Fine-Tuned Chat Models\", \"How llm applications are revolutionizing the manufacturing industry\", \"FarmBeats: An IoT platform for data-driven agriculture\", \"Seeing through clouds in satellite images\", \"Judging LLM-as-a-Judge with MT-Bench and Chatbot Arena\", \"JudgeLM: Fine-tuned Large Language Models are Scalable Judges\", \"Scrapy\"]}\n[The End of Assistant's Predicted Answer]\n\n[System]\nWe would like to request your feedback on the performance of the AI assistant in response to the user question displayed above according to the gold answer. Please use the following listed aspects and their descriptions as evaluation criteria:\n    - Accuracy and Hallucinations: The assistant's answer is semantically consistent with the gold answer; The numerical value and order need to be accurate, and there should be no hallucinations.\n    - Completeness: Referring to the reference answers, the assistant's answer should contain all the key points needed to answer the user's question; further elaboration on these key points can be omitted.\nPlease rate whether this answer is suitable for the question. Please note that the gold answer can be considered as a correct answer to the question.\n\nThe assistant receives an overall score on a scale of 1 to 100, where a higher score indicates better overall performance.\nPlease note that if the assistant's answer and the gold answer fully meet the above criteria, its overall rating should be the full marks (100).\nPlease first provide a comprehensive explanation of your evaluation, avoiding any potential bias.\nThen, output a line indicating the score of the Assistant.\n\nPLEASE OUTPUT WITH THE FOLLOWING FORMAT, WHERE THE SCORE IS A SCALE OF 1 TO 100 BY STRICTLY FOLLOWING THIS FORMAT: \"[[score]]\", FOR EXAMPLE \"Rating: [[100]]\":\n<start output>\nEvaluation evidence: your evluation explanation here, no more than 100 words\nRating: [[score]]\n<end output>\n\nNow, start your evaluation:",
        "generate_response": "{\"Reference\":[\"ROUGE: A package for automatic evaluation of summaries\", \"Bleu: a method for automatic evaluation of machine translation\", \"Word Mover's Distance\", \"Kullback-Leibler divergence\", \"Evaluation with Guideline\", \"Succinctness\", \"Relevance\", \"Overlap\", \"Diversity\", \"Fluency\", \"Coverage\"], \"Citation\":[\"A Comparison of Statistical Downscaling Methods Suited for Wildfire Applications\", \"500 Questions 500 Answers - Embrapa/SCT\", \"2009 Cost Estimates of Establishing and Producing Gala Apples in Washington\", \"Vikaspedia\", \"Sparks of artificial general intelligence: Early experiments with gpt-4\", \"Democratizing data-driven agriculture using affordable hardware\", \"Farmvibes: Precision agriculture with ai\", \"Pdf2text\", \"Pypdf\", \"Training Deep Nets with Sublinear Memory Cost\", \"Can Large Language Models Be an Alternative to Human Evaluations?\", \"Tei p5: Guidelines for electronic text encoding and interchange\", \"Evaluating Open-Domain Question Answering in the Era of Large Language Models\", \"Adam: A Method for Stochastic Optimization\", \"Lora: Low-rank adaptation of large language models\", \"Billion-scale similarity search with GPUs\", \"Retrieval-augmented generation for knowledge-intensive nlp tasks\", \"Large language models for supply chain optimization\", \"SGDR: Stochastic Gradient Descent with Warm Restarts\", \"Decoupled Weight Decay Regularization\", \"Mixed Precision Training\", \"Azureml metrics python package\", \"FActScore: Fine-grained Atomic Evaluation of Factual Precision in Long Form Text Generation\", \"Large Dual Encoders Are Generalizable Retrievers\", \"Capabilities of gpt-4 on medical challenge problems\", \"Gpt-4 technical report\", \"Training language models to follow instructions with human feedback\", \"PyTorch: An Imperative Style, High-Performance Deep Learning Library\", \"Sentence-BERT: Sentence Embeddings using Siamese BERT-Networks\", \"Knowledge guided representation learning and causal structure learning in soil science\", \"GPT-4 as an agronomist assistant? answering agriculture exams using large language models\", \"Microsoft copilot impact on finance\", \"Large language models in medicine\", \"LLaMA: Open and Efficient Foundation Language Models\", \"Llama 2: Open Foundation and Fine-Tuned Chat Models\", \"How llm applications are revolutionizing the manufacturing industry\", \"FarmBeats: An IoT platform for data-driven agriculture\", \"Seeing through clouds in satellite images\", \"Judging LLM-as-a-Judge with MT-Bench and Chatbot Arena\", \"JudgeLM: Fine-tuned Large Language Models are Scalable Judges\", \"Scrapy\"]}",
        "eval_response": "Evaluation evidence: The assistant's answer does not match the gold answer in terms of accuracy and completeness. The references and citations listed by the assistant are entirely different from those in the gold answer, indicating a significant deviation and hallucination. The assistant's response includes a large number of unrelated papers, which were not mentioned in the gold answer, showing a lack of focus and accuracy in identifying the correct citation and reference relationships for the given paper.\n\nRating: [[10]]",
        "score": 10
    },
    {
        "level": 3,
        "set": 1,
        "length": 44986,
        "type": "paper",
        "language": "en",
        "question": "On Prompt-Driven Safeguarding for Large Language Models",
        "instruction": "We hope you will carefully study the provided papers and determine the citation relationships between them. Please follow the instructions below strictly to complete the task:\n\n#Specific Requirements:\n1. Reference: When a given paper mentions other provided papers, those other papers are considered as \"references\" for the given paper. To summarize in this specific context, references are about what the given paper is using.\n2. Citation: Conversely, when other provided papers mention the given paper in their works, the given paper is being \"cited\" by those other papers. To summarize in this specific context, citations are about who is using the given paper.\n3. Given a paper, you need to determine the citation or reference relationship between this paper and the other papers. Do not consider papers that are not provided.\n3. Please present the paper titles in a json format as follows: {{\"Reference\":[\"Reference Title 1\", \"Reference Title 2\", ..., \"Reference Title n\"], \"Citation\":[\"Citation Title 1\", \"Citation Title 2\", ..., \"Citation Title n\"]}}.\n4. If a paper does not have any references or citations, please leave the corresponding list empty, e.g.{{\"Refernce\":[]}}, {{\"Citation\":[]}}.",
        "prompt_template": "#Papers:\n{docs}\n\n{instruction}\n\n#The paper you need to analyze:\n{question}",
        "doc": [
            "2401.18018.md",
            "2405.17374.md",
            "2312.04724.md"
        ],
        "answer": {
            "Reference": [
                "# Purple Llama CYBERSECEvAL: A Secure Coding Benchmark for Language Models "
            ],
            "Citation": [
                "# Navigating the Safety Landscape: Measuring Risks in Finetuning Large Language Models "
            ]
        },
        "shuffle_doc": true,
        "id": "3e4ee654-dfd8-4ffd-a825-79e0997961c9",
        "prompt": "[Question]\n#Papers:\n{docs}\n\nWe hope you will carefully study the provided papers and determine the citation relationships between them. Please follow the instructions below strictly to complete the task:\n\n#Specific Requirements:\n1. Reference: When a given paper mentions other provided papers, those other papers are considered as \"references\" for the given paper. To summarize in this specific context, references are about what the given paper is using.\n2. Citation: Conversely, when other provided papers mention the given paper in their works, the given paper is being \"cited\" by those other papers. To summarize in this specific context, citations are about who is using the given paper.\n3. Given a paper, you need to determine the citation or reference relationship between this paper and the other papers. Do not consider papers that are not provided.\n3. Please present the paper titles in a json format as follows: {{\"Reference\":[\"Reference Title 1\", \"Reference Title 2\", ..., \"Reference Title n\"], \"Citation\":[\"Citation Title 1\", \"Citation Title 2\", ..., \"Citation Title n\"]}}.\n4. If a paper does not have any references or citations, please leave the corresponding list empty, e.g.{{\"Refernce\":[]}}, {{\"Citation\":[]}}.\n\n#The paper you need to analyze:\nOn Prompt-Driven Safeguarding for Large Language Models\n\n[Gold Answer]\n{'Reference': ['# Purple Llama CYBERSECEvAL: A Secure Coding Benchmark for Language Models '], 'Citation': ['# Navigating the Safety Landscape: Measuring Risks in Finetuning Large Language Models ']}\n\n[The Start of Assistant's Predicted Answer]\n{\"Reference\": [\"The CRINGE loss: Learning what language not to model\", \"Catastrophic jailbreak of open-source LLMs via exploiting generation\", \"Camels in a changing climate: Enhancing lm adaptation with tulu 2\", \"Mistral 7b\", \"The power of scale for parameter-efficient prompt tuning\", \"Alpacaeval: An automatic evaluator of instruction-following models\", \"Self-alignment with instruction backtranslation\", \"Prefix-tuning: Optimizing continuous prompts for generation\", \"AutoDAN: Generating stealthy jailbreak prompts on aligned large language models\", \"Code llama: Open foundation models for code\", \"Gpt-4 technical report\", \"On second thought, let's not think step by step! bias and toxicity in zero-shot reasoning\", \"Towards Controllable Biases in Language Generation\", \"AutoPrompt: Eliciting Knowledge from Language Models with Automatically Generated Prompts\", \"On the safety of conversational models: Taxonomy, dataset, and benchmark\", \"Llama 2: Open foundation and finetuned chat models\", \"Openchat: Advancing open-source language models with mixed-quality data\", \"Jailbroken: How does LLM safety training fail?\", \"Ethical and social risks of harm from language models\", \"Neural text generation with unlikelihood training\", \"Recipes for safety in open-domain chatbots\", \"Large language models as optimizers\", \"How johnny can persuade llms to jailbreak them: Rethinking persuasion to challenge ai safety by humanizing llms\", \"Ethical-advice taker: Do language models understand natural language interventions?\", \"Chat templates for huggingface large language models\", \"Exploring prompt-based few-shot learning for grounded dialog generation\", \"Click: Controllable text generation with sequence likelihood contrastive learning\", \"Weak-to-strong extrapolation expedites alignment\", \"Large language models are not robust multiple choice selectors\", \"Large language models are humanlevel prompt engineers\", \"Universal and transferable adversarial attacks on aligned language models\"], \"Citation\": []}\n[The End of Assistant's Predicted Answer]\n\n[System]\nWe would like to request your feedback on the performance of the AI assistant in response to the user question displayed above according to the gold answer. Please use the following listed aspects and their descriptions as evaluation criteria:\n    - Accuracy and Hallucinations: The assistant's answer is semantically consistent with the gold answer; The numerical value and order need to be accurate, and there should be no hallucinations.\n    - Completeness: Referring to the reference answers, the assistant's answer should contain all the key points needed to answer the user's question; further elaboration on these key points can be omitted.\nPlease rate whether this answer is suitable for the question. Please note that the gold answer can be considered as a correct answer to the question.\n\nThe assistant receives an overall score on a scale of 1 to 100, where a higher score indicates better overall performance.\nPlease note that if the assistant's answer and the gold answer fully meet the above criteria, its overall rating should be the full marks (100).\nPlease first provide a comprehensive explanation of your evaluation, avoiding any potential bias.\nThen, output a line indicating the score of the Assistant.\n\nPLEASE OUTPUT WITH THE FOLLOWING FORMAT, WHERE THE SCORE IS A SCALE OF 1 TO 100 BY STRICTLY FOLLOWING THIS FORMAT: \"[[score]]\", FOR EXAMPLE \"Rating: [[100]]\":\n<start output>\nEvaluation evidence: your evluation explanation here, no more than 100 words\nRating: [[score]]\n<end output>\n\nNow, start your evaluation:",
        "generate_response": "{\"Reference\": [\"The CRINGE loss: Learning what language not to model\", \"Catastrophic jailbreak of open-source LLMs via exploiting generation\", \"Camels in a changing climate: Enhancing lm adaptation with tulu 2\", \"Mistral 7b\", \"The power of scale for parameter-efficient prompt tuning\", \"Alpacaeval: An automatic evaluator of instruction-following models\", \"Self-alignment with instruction backtranslation\", \"Prefix-tuning: Optimizing continuous prompts for generation\", \"AutoDAN: Generating stealthy jailbreak prompts on aligned large language models\", \"Code llama: Open foundation models for code\", \"Gpt-4 technical report\", \"On second thought, let's not think step by step! bias and toxicity in zero-shot reasoning\", \"Towards Controllable Biases in Language Generation\", \"AutoPrompt: Eliciting Knowledge from Language Models with Automatically Generated Prompts\", \"On the safety of conversational models: Taxonomy, dataset, and benchmark\", \"Llama 2: Open foundation and finetuned chat models\", \"Openchat: Advancing open-source language models with mixed-quality data\", \"Jailbroken: How does LLM safety training fail?\", \"Ethical and social risks of harm from language models\", \"Neural text generation with unlikelihood training\", \"Recipes for safety in open-domain chatbots\", \"Large language models as optimizers\", \"How johnny can persuade llms to jailbreak them: Rethinking persuasion to challenge ai safety by humanizing llms\", \"Ethical-advice taker: Do language models understand natural language interventions?\", \"Chat templates for huggingface large language models\", \"Exploring prompt-based few-shot learning for grounded dialog generation\", \"Click: Controllable text generation with sequence likelihood contrastive learning\", \"Weak-to-strong extrapolation expedites alignment\", \"Large language models are not robust multiple choice selectors\", \"Large language models are humanlevel prompt engineers\", \"Universal and transferable adversarial attacks on aligned language models\"], \"Citation\": []}",
        "eval_response": "Evaluation evidence: The assistant's answer does not match the gold answer in terms of accuracy. The reference paper listed in the assistant's answer does not match the one in the gold answer, and it includes multiple hallucinated titles that are not mentioned in the gold answer. Additionally, the assistant's answer fails to include the citation mentioned in the gold answer, leading to incompleteness in addressing the task requirements. The assistant's response contains significant inaccuracies and hallucinations, and it does not capture the complete citation relationship as specified.\n\nRating: [[10]]",
        "score": 10
    },
    {
        "level": 3,
        "set": 2,
        "length": 65716,
        "type": "paper",
        "language": "en",
        "question": "GPT-4V(ision) is a Generalist Web Agent, if Grounded",
        "instruction": "We hope you will carefully study the provided papers and determine the citation relationships between them. Please follow the instructions below strictly to complete the task:\n\n#Specific Requirements:\n1. Reference: When a given paper mentions other provided papers, those other papers are considered as \"references\" for the given paper. To summarize in this specific context, references are about what the given paper is using.\n2. Citation: Conversely, when other provided papers mention the given paper in their works, the given paper is being \"cited\" by those other papers. To summarize in this specific context, citations are about who is using the given paper.\n3. Given a paper, you need to determine the citation or reference relationship between this paper and the other papers. Do not consider papers that are not provided.\n3. Please present the paper titles in a json format as follows: {{\"Reference\":[\"Reference Title 1\", \"Reference Title 2\", ..., \"Reference Title n\"], \"Citation\":[\"Citation Title 1\", \"Citation Title 2\", ..., \"Citation Title n\"]}}.\n4. If a paper does not have any references or citations, please leave the corresponding list empty, e.g.{{\"Refernce\":[]}}, {{\"Citation\":[]}}.",
        "prompt_template": "#Papers:\n{docs}\n\n{instruction}\n\n#The paper you need to analyze:\n{question}",
        "doc": [
            "2401.01614.md",
            "2405.03710.md",
            "2405.04497.md",
            "2405.11120.md"
        ],
        "answer": {
            "Reference": [],
            "Citation": [
                "# Automating the Enterprise with Foundation Models ",
                "# Unveiling Disparities in Web Task Handling Between Human and Web Agent ",
                "# Latent State Estimation Helps UI Agents to Reason "
            ]
        },
        "shuffle_doc": true,
        "id": "91e61e88-ffcb-4f35-a7e4-55dd5e4a938e",
        "prompt": "[Question]\n#Papers:\n{docs}\n\nWe hope you will carefully study the provided papers and determine the citation relationships between them. Please follow the instructions below strictly to complete the task:\n\n#Specific Requirements:\n1. Reference: When a given paper mentions other provided papers, those other papers are considered as \"references\" for the given paper. To summarize in this specific context, references are about what the given paper is using.\n2. Citation: Conversely, when other provided papers mention the given paper in their works, the given paper is being \"cited\" by those other papers. To summarize in this specific context, citations are about who is using the given paper.\n3. Given a paper, you need to determine the citation or reference relationship between this paper and the other papers. Do not consider papers that are not provided.\n3. Please present the paper titles in a json format as follows: {{\"Reference\":[\"Reference Title 1\", \"Reference Title 2\", ..., \"Reference Title n\"], \"Citation\":[\"Citation Title 1\", \"Citation Title 2\", ..., \"Citation Title n\"]}}.\n4. If a paper does not have any references or citations, please leave the corresponding list empty, e.g.{{\"Refernce\":[]}}, {{\"Citation\":[]}}.\n\n#The paper you need to analyze:\nGPT-4V(ision) is a Generalist Web Agent, if Grounded\n\n[Gold Answer]\n{'Reference': [], 'Citation': ['# Automating the Enterprise with Foundation Models ', '# Unveiling Disparities in Web Task Handling Between Human and Web Agent ', '# Latent State Estimation Helps UI Agents to Reason ']}\n\n[The Start of Assistant's Predicted Answer]\n{\"Reference\": [\"A Survey on Evaluation of Large Language Models\", \"Vicuna: An Open-Source Chatbot Impressing GPT-4 with $90 \\% *$ ChatGPT Quality\", \"ChatLaw: Open-Source Legal Large Language Model with Integrated External Knowledge Bases\", \"Compositional Semantic Parsing with Large Language Models\", \"Open LLM Leaderboard\", \"How Close is ChatGPT to Human Experts? Comparison Corpus, Evaluation, and Detection\", \"REALM: Retrieval-Augmented Language Model PreTraining\", \"Rethinking with Retrieval: Faithful Large Language Model Inference\", \"Measuring Massive Multitask Language Understanding\", \"Atlas: Few-shot Learning with Retrieval Augmented Language Models\", \"GPT-4 Technical Report\", \"An in-depth look at Gemini's language abilities\", \"Microsoft COCO: Common Objects in Context\", \"VisualWebArena: Evaluating Multimodal Agents on Realistic Visual Web Tasks\", \"Pix2Struct: Screenshot Parsing as Pretraining for Visual Language Understanding\", \"BLIP2: Bootstrapping Language-Image Pre-training with Frozen Image Encoders and Large Language Models\", \"Flin: A Flexible Natural Language Interface for Web Navigation\", \"GEMINI: A Family of Highly Capable Multimodal Models\", \"Grounding 'Grounding' in NLP\", \"Don't Generate, Discriminate: A Proposal for Grounding Language Models to Real-World Environments\", \"HallusionBench: An Advanced Diagnostic Suite for Entangled Language Hallusion\\\\&Visual Illusion in Large Vision-Language Models\", \"Understanding HTML with Large Language Models\", \"A Real-World WebAgent with Planning, Long Context Understanding, and Program Synthesis\", \"ReferItGame: Referring to Objects in Photographs of Natural Scenes\", \"Language Models Can Solve Computer Tasks\", \"Improved Baselines with Visual Instruction Tuning\", \"Visual Instruction Tuning\", \"Visual Instruction Tuning\", \"AgentBench: Evaluating LLMs as Agents\", \"Learn to Explain: Multimodal Reasoning via Thought Chains for Science Question Answering\", \"GOSMOS-2: Grounding Multimodal Large Language Models to the World\", \"Flickr300 Entities: Collecting Region-to-Phrase Correspondences for Richer Image-to-Sentence Models\", \"Learning Transferable Visual Models from Natural Language Supervision\", \"ScienceQA: A Novel Resource for Question Answering on Scholarly Articles\", \"From Pixels to UI Actions: Learning to Follow Instructions via Graphical User Interfaces\", \"How Much Can CLIP Benefit Vision-and-Language Tasks?\", \"World of Bits: An Open-Domain Platform for Web-Based Agents\", \"What Does CLIP Know About a Red Circle? Visual Prompt Engineering for VLMS\", \"Hierarchical Prompting Assists Large Language Model on Web Navigation\", \"GPT-4V in Wonderland: Large Multimodal Models for Zero-Shot Smartphone GUI Navigation\", \"Set-of-Mark Prompting Unleashes Extraordinary Visual Grounding in GPT-4V\", \"Fine-Grained Visual Prompting\", \"The Dawn of LMMS: Preliminary Explorations with GPT-4V(ision)\", \"WebShop: Towards Scalable Real-World Web Interaction with Grounded Language Agents\", \"FERRET: Refer and Ground Anything Anywhere at Any Granularity\", \"MMMU: A Massive Multi-Discipline Multimodal Understanding and Reasoning Benchmark for Expert AGI\", \"GPT-4V(ision) as a Generalist Evaluator for Vision-Language Tasks\", \"BUBO-GPT: Enabling Visual Grounding in Multi-Modal LLMs\", \"AGIEval: A Human-Centric Benchmark for Evaluating Foundation Models\", \"Scene Parsing Through ADE20K Dataset\"], \"Citation\": []}\n[The End of Assistant's Predicted Answer]\n\n[System]\nWe would like to request your feedback on the performance of the AI assistant in response to the user question displayed above according to the gold answer. Please use the following listed aspects and their descriptions as evaluation criteria:\n    - Accuracy and Hallucinations: The assistant's answer is semantically consistent with the gold answer; The numerical value and order need to be accurate, and there should be no hallucinations.\n    - Completeness: Referring to the reference answers, the assistant's answer should contain all the key points needed to answer the user's question; further elaboration on these key points can be omitted.\nPlease rate whether this answer is suitable for the question. Please note that the gold answer can be considered as a correct answer to the question.\n\nThe assistant receives an overall score on a scale of 1 to 100, where a higher score indicates better overall performance.\nPlease note that if the assistant's answer and the gold answer fully meet the above criteria, its overall rating should be the full marks (100).\nPlease first provide a comprehensive explanation of your evaluation, avoiding any potential bias.\nThen, output a line indicating the score of the Assistant.\n\nPLEASE OUTPUT WITH THE FOLLOWING FORMAT, WHERE THE SCORE IS A SCALE OF 1 TO 100 BY STRICTLY FOLLOWING THIS FORMAT: \"[[score]]\", FOR EXAMPLE \"Rating: [[100]]\":\n<start output>\nEvaluation evidence: your evluation explanation here, no more than 100 words\nRating: [[score]]\n<end output>\n\nNow, start your evaluation:",
        "generate_response": "{\"Reference\": [\"A Survey on Evaluation of Large Language Models\", \"Vicuna: An Open-Source Chatbot Impressing GPT-4 with $90 \\% *$ ChatGPT Quality\", \"ChatLaw: Open-Source Legal Large Language Model with Integrated External Knowledge Bases\", \"Compositional Semantic Parsing with Large Language Models\", \"Open LLM Leaderboard\", \"How Close is ChatGPT to Human Experts? Comparison Corpus, Evaluation, and Detection\", \"REALM: Retrieval-Augmented Language Model PreTraining\", \"Rethinking with Retrieval: Faithful Large Language Model Inference\", \"Measuring Massive Multitask Language Understanding\", \"Atlas: Few-shot Learning with Retrieval Augmented Language Models\", \"GPT-4 Technical Report\", \"An in-depth look at Gemini's language abilities\", \"Microsoft COCO: Common Objects in Context\", \"VisualWebArena: Evaluating Multimodal Agents on Realistic Visual Web Tasks\", \"Pix2Struct: Screenshot Parsing as Pretraining for Visual Language Understanding\", \"BLIP2: Bootstrapping Language-Image Pre-training with Frozen Image Encoders and Large Language Models\", \"Flin: A Flexible Natural Language Interface for Web Navigation\", \"GEMINI: A Family of Highly Capable Multimodal Models\", \"Grounding 'Grounding' in NLP\", \"Don't Generate, Discriminate: A Proposal for Grounding Language Models to Real-World Environments\", \"HallusionBench: An Advanced Diagnostic Suite for Entangled Language Hallusion\\\\&Visual Illusion in Large Vision-Language Models\", \"Understanding HTML with Large Language Models\", \"A Real-World WebAgent with Planning, Long Context Understanding, and Program Synthesis\", \"ReferItGame: Referring to Objects in Photographs of Natural Scenes\", \"Language Models Can Solve Computer Tasks\", \"Improved Baselines with Visual Instruction Tuning\", \"Visual Instruction Tuning\", \"Visual Instruction Tuning\", \"AgentBench: Evaluating LLMs as Agents\", \"Learn to Explain: Multimodal Reasoning via Thought Chains for Science Question Answering\", \"GOSMOS-2: Grounding Multimodal Large Language Models to the World\", \"Flickr300 Entities: Collecting Region-to-Phrase Correspondences for Richer Image-to-Sentence Models\", \"Learning Transferable Visual Models from Natural Language Supervision\", \"ScienceQA: A Novel Resource for Question Answering on Scholarly Articles\", \"From Pixels to UI Actions: Learning to Follow Instructions via Graphical User Interfaces\", \"How Much Can CLIP Benefit Vision-and-Language Tasks?\", \"World of Bits: An Open-Domain Platform for Web-Based Agents\", \"What Does CLIP Know About a Red Circle? Visual Prompt Engineering for VLMS\", \"Hierarchical Prompting Assists Large Language Model on Web Navigation\", \"GPT-4V in Wonderland: Large Multimodal Models for Zero-Shot Smartphone GUI Navigation\", \"Set-of-Mark Prompting Unleashes Extraordinary Visual Grounding in GPT-4V\", \"Fine-Grained Visual Prompting\", \"The Dawn of LMMS: Preliminary Explorations with GPT-4V(ision)\", \"WebShop: Towards Scalable Real-World Web Interaction with Grounded Language Agents\", \"FERRET: Refer and Ground Anything Anywhere at Any Granularity\", \"MMMU: A Massive Multi-Discipline Multimodal Understanding and Reasoning Benchmark for Expert AGI\", \"GPT-4V(ision) as a Generalist Evaluator for Vision-Language Tasks\", \"BUBO-GPT: Enabling Visual Grounding in Multi-Modal LLMs\", \"AGIEval: A Human-Centric Benchmark for Evaluating Foundation Models\", \"Scene Parsing Through ADE20K Dataset\"], \"Citation\": []}",
        "eval_response": "Evaluation evidence: The assistant's answer is inaccurate and contains hallucinations. The provided answer lists numerous references that are not mentioned in the gold answer, indicating a misunderstanding of the task. The gold answer specifies three citations, which are completely missing from the assistant's response. This shows a lack of completeness and accuracy in understanding the citation relationships for the paper \"GPT-4V(ision) is a Generalist Web Agent, if Grounded.\"\n\nRating: [[10]]",
        "score": 10
    },
    {
        "level": 4,
        "set": 3,
        "length": 134802,
        "type": "paper",
        "language": "en",
        "question": "",
        "instruction": "We kindly ask you to thoroughly review the provided papers and construct a citation chain from them. Please adhere to the following instructions strictly while completing the task:\n\n#Task Instructions:\nGiven several papers, you are required to identify and list the longest citation chain, which demonstrates the citation relationship among the provided papers.\n\n#Specific Requirements:\n1.Please present the titles of the papers in the form of a list, as follows: [\"Title of Paper 1\", \"Title of Paper 2\", ..., \"Title of Paper n\"].\n2.Ensure that the citation chain in the list is linear and continuous, meaning that the first paper title in the list (Paper 1) should not cite any other works. Instead, it should be cited by the next paper in the list (Paper 2); subsequently, each paper should then be cited by the next one in the list, continuing up to the last paper (Paper n).\n3.Consider only the citation relationships within the supplied collection of papers, and ensure that the citation chain accurately reflects the sequential citation order among these documents.\n4.Do not take into account any articles not provided, and disregard other non-linear citation relationships.",
        "prompt_template": "{instruction}\n\n#Papers Provided:\n{docs}",
        "doc": [
            "2310.08541.md",
            "2310.15144.md",
            "2403.12806.md",
            "2405.08745.md"
        ],
        "answer": [
            "# Idea2Img: Iterative Self-Refinement with GPT-4V(ision) for Automatic Image Design and Generation ",
            "# DEsignBench: Exploring and Benchmarking DALL-E 3 for Imagining Visual Design ",
            "# VisualCritic: Making LMMs Perceive Visual Quality Like Humans ",
            "# Enhancing Blind Video Quality Assessment with Rich Quality-aware Features "
        ],
        "shuffle_doc": true,
        "id": "c8c68dd7-6665-4e82-a01b-80b05459e0ab",
        "prompt": "[Question]\nWe kindly ask you to thoroughly review the provided papers and construct a citation chain from them. Please adhere to the following instructions strictly while completing the task:\n\n#Task Instructions:\nGiven several papers, you are required to identify and list the longest citation chain, which demonstrates the citation relationship among the provided papers.\n\n#Specific Requirements:\n1.Please present the titles of the papers in the form of a list, as follows: [\"Title of Paper 1\", \"Title of Paper 2\", ..., \"Title of Paper n\"].\n2.Ensure that the citation chain in the list is linear and continuous, meaning that the first paper title in the list (Paper 1) should not cite any other works. Instead, it should be cited by the next paper in the list (Paper 2); subsequently, each paper should then be cited by the next one in the list, continuing up to the last paper (Paper n).\n3.Consider only the citation relationships within the supplied collection of papers, and ensure that the citation chain accurately reflects the sequential citation order among these documents.\n4.Do not take into account any articles not provided, and disregard other non-linear citation relationships.\n\n#Papers Provided:\n{docs}\n\n[Gold Answer]\n['# Idea2Img: Iterative Self-Refinement with GPT-4V(ision) for Automatic Image Design and Generation ', '# DEsignBench: Exploring and Benchmarking DALL-E 3 for Imagining Visual Design ', '# VisualCritic: Making LMMs Perceive Visual Quality Like Humans ', '# Enhancing Blind Video Quality Assessment with Rich Quality-aware Features ']\n\n[The Start of Assistant's Predicted Answer]\n[\"VisualCritic: Making LMMs Perceive Visual Quality Like Humans\", \"Kosmos-2.5\", \"Kosmos-g\", \"Kosmos2\"]\n[The End of Assistant's Predicted Answer]\n\n[System]\nWe would like to request your feedback on the performance of the AI assistant in response to the user question displayed above according to the gold answer. Please use the following listed aspects and their descriptions as evaluation criteria:\n    - Accuracy and Hallucinations: The assistant's answer is semantically consistent with the gold answer; The numerical value and order need to be accurate, and there should be no hallucinations.\n    - Completeness: Referring to the reference answers, the assistant's answer should contain all the key points needed to answer the user's question; further elaboration on these key points can be omitted.\nPlease rate whether this answer is suitable for the question. Please note that the gold answer can be considered as a correct answer to the question.\n\nThe assistant receives an overall score on a scale of 1 to 100, where a higher score indicates better overall performance.\nPlease note that if the assistant's answer and the gold answer fully meet the above criteria, its overall rating should be the full marks (100).\nPlease first provide a comprehensive explanation of your evaluation, avoiding any potential bias.\nThen, output a line indicating the score of the Assistant.\n\nPLEASE OUTPUT WITH THE FOLLOWING FORMAT, WHERE THE SCORE IS A SCALE OF 1 TO 100 BY STRICTLY FOLLOWING THIS FORMAT: \"[[score]]\", FOR EXAMPLE \"Rating: [[100]]\":\n<start output>\nEvaluation evidence: your evluation explanation here, no more than 100 words\nRating: [[score]]\n<end output>\n\nNow, start your evaluation:",
        "generate_response": "[\"VisualCritic: Making LMMs Perceive Visual Quality Like Humans\", \"Kosmos-2.5\", \"Kosmos-g\", \"Kosmos2\"]",
        "eval_response": "Evaluation evidence: The assistant's answer does not match the gold answer in terms of the titles of the papers listed. The titles in the assistant's response do not appear in the gold answer, indicating a significant deviation and a failure to identify the correct citation chain among the provided papers. This results in both inaccuracies and hallucinations as the titles mentioned do not correspond to those in the gold answer. The assistant's response is incomplete as it does not include any of the correct paper titles from the gold answer.\n\nRating: [[10]]",
        "score": 10
    },
    {
        "level": 4,
        "set": 1,
        "length": 46420,
        "type": "paper",
        "language": "en",
        "question": "",
        "instruction": "We kindly ask you to thoroughly review the provided papers and construct a citation chain from them. Please adhere to the following instructions strictly while completing the task:\n\n#Task Instructions:\nGiven several papers, you are required to identify and list the longest citation chain, which demonstrates the citation relationship among the provided papers.\n\n#Specific Requirements:\n1.Please present the titles of the papers in the form of a list, as follows: [\"Title of Paper 1\", \"Title of Paper 2\", ..., \"Title of Paper n\"].\n2.Ensure that the citation chain in the list is linear and continuous, meaning that the first paper title in the list (Paper 1) should not cite any other works. Instead, it should be cited by the next paper in the list (Paper 2); subsequently, each paper should then be cited by the next one in the list, continuing up to the last paper (Paper n).\n3.Consider only the citation relationships within the supplied collection of papers, and ensure that the citation chain accurately reflects the sequential citation order among these documents.\n4.Do not take into account any articles not provided, and disregard other non-linear citation relationships.",
        "prompt_template": "{instruction}\n\n#Papers Provided:\n{docs}",
        "doc": [
            "2105.01051.md",
            "2106.04624.md",
            "2405.09142.md"
        ],
        "answer": [
            "# SUPERB: Speech processing Universal PERformance Benchmark ",
            "# SpeechBrain: A General-Purpose Speech Toolkit ",
            "# Speaker Embeddings With Weakly Supervised Voice Activity Detection For Efficient Speaker Diarization "
        ],
        "shuffle_doc": true,
        "id": "7bfef6af-b337-4b5d-aada-41f500b375d3",
        "prompt": "[Question]\nWe kindly ask you to thoroughly review the provided papers and construct a citation chain from them. Please adhere to the following instructions strictly while completing the task:\n\n#Task Instructions:\nGiven several papers, you are required to identify and list the longest citation chain, which demonstrates the citation relationship among the provided papers.\n\n#Specific Requirements:\n1.Please present the titles of the papers in the form of a list, as follows: [\"Title of Paper 1\", \"Title of Paper 2\", ..., \"Title of Paper n\"].\n2.Ensure that the citation chain in the list is linear and continuous, meaning that the first paper title in the list (Paper 1) should not cite any other works. Instead, it should be cited by the next paper in the list (Paper 2); subsequently, each paper should then be cited by the next one in the list, continuing up to the last paper (Paper n).\n3.Consider only the citation relationships within the supplied collection of papers, and ensure that the citation chain accurately reflects the sequential citation order among these documents.\n4.Do not take into account any articles not provided, and disregard other non-linear citation relationships.\n\n#Papers Provided:\n{docs}\n\n[Gold Answer]\n['# SUPERB: Speech processing Universal PERformance Benchmark ', '# SpeechBrain: A General-Purpose Speech Toolkit ', '# Speaker Embeddings With Weakly Supervised Voice Activity Detection For Efficient Speaker Diarization ']\n\n[The Start of Assistant's Predicted Answer]\n[\"Speaker Embeddings With Weakly Supervised Voice Activity Detection For Efficient Speaker Diarization\", \"X-vectors: Robust dnn embeddings for speaker recognition\", \"ECAPA-TDNN: Emphasized channel attention, propagation and aggregation in TDNN based speaker verification\"]\n[The End of Assistant's Predicted Answer]\n\n[System]\nWe would like to request your feedback on the performance of the AI assistant in response to the user question displayed above according to the gold answer. Please use the following listed aspects and their descriptions as evaluation criteria:\n    - Accuracy and Hallucinations: The assistant's answer is semantically consistent with the gold answer; The numerical value and order need to be accurate, and there should be no hallucinations.\n    - Completeness: Referring to the reference answers, the assistant's answer should contain all the key points needed to answer the user's question; further elaboration on these key points can be omitted.\nPlease rate whether this answer is suitable for the question. Please note that the gold answer can be considered as a correct answer to the question.\n\nThe assistant receives an overall score on a scale of 1 to 100, where a higher score indicates better overall performance.\nPlease note that if the assistant's answer and the gold answer fully meet the above criteria, its overall rating should be the full marks (100).\nPlease first provide a comprehensive explanation of your evaluation, avoiding any potential bias.\nThen, output a line indicating the score of the Assistant.\n\nPLEASE OUTPUT WITH THE FOLLOWING FORMAT, WHERE THE SCORE IS A SCALE OF 1 TO 100 BY STRICTLY FOLLOWING THIS FORMAT: \"[[score]]\", FOR EXAMPLE \"Rating: [[100]]\":\n<start output>\nEvaluation evidence: your evluation explanation here, no more than 100 words\nRating: [[score]]\n<end output>\n\nNow, start your evaluation:",
        "generate_response": "[\"Speaker Embeddings With Weakly Supervised Voice Activity Detection For Efficient Speaker Diarization\", \"X-vectors: Robust dnn embeddings for speaker recognition\", \"ECAPA-TDNN: Emphasized channel attention, propagation and aggregation in TDNN based speaker verification\"]",
        "eval_response": "Evaluation evidence: The assistant's answer does not match the gold answer in terms of the titles of the papers listed in the citation chain. The titles provided by the assistant do not appear in the gold answer, indicating a significant deviation in accuracy. There is also a hallucination issue as the titles mentioned by the assistant are not found in the provided papers list in the task description. This results in a failure to meet the accuracy and completeness criteria.\n\nRating: [[10]]",
        "score": 10
    },
    {
        "level": 3,
        "set": 2,
        "length": 92126,
        "type": "paper",
        "language": "en",
        "question": "DistServe: Disaggregating Prefill and Decoding for Goodput-optimized Large Language Model Serving",
        "instruction": "We hope you will carefully study the provided papers and determine the citation relationships between them. Please follow the instructions below strictly to complete the task:\n\n#Specific Requirements:\n1. Reference: When a given paper mentions other provided papers, those other papers are considered as \"references\" for the given paper. To summarize in this specific context, references are about what the given paper is using.\n2. Citation: Conversely, when other provided papers mention the given paper in their works, the given paper is being \"cited\" by those other papers. To summarize in this specific context, citations are about who is using the given paper.\n3. Given a paper, you need to determine the citation or reference relationship between this paper and the other papers. Do not consider papers that are not provided.\n3. Please present the paper titles in a json format as follows: {{\"Reference\":[\"Reference Title 1\", \"Reference Title 2\", ..., \"Reference Title n\"], \"Citation\":[\"Citation Title 1\", \"Citation Title 2\", ..., \"Citation Title n\"]}}.\n4. If a paper does not have any references or citations, please leave the corresponding list empty, e.g.{{\"Refernce\":[]}}, {{\"Citation\":[]}}.",
        "prompt_template": "#Papers:\n{docs}\n\n{instruction}\n\n#The paper you need to analyze:\n{question}",
        "doc": [
            "2401.09670.md",
            "2405.05465.md",
            "2405.06856.md",
            "2311.18677.md",
            "2401.11181.md"
        ],
        "answer": {
            "Reference": [
                "# Splitwise: Efficient Generative LLM Inference Using Phase Splitting ",
                "# Inference without Interference: Disaggregate LLM Inference for Mixed Downstream Workloads "
            ],
            "Citation": [
                "# VIDUR: A LARGE-SCALE SIMULATION FRAMEWORK FOR LLM INFERENCE ",
                "# Aladdin: Joint Placement and Scaling for SLO-Aware LLM Serving "
            ]
        },
        "shuffle_doc": true,
        "id": "6dbe3730-c72a-4ac3-9cf0-82f2bfe7def2",
        "prompt": "[Question]\n#Papers:\n{docs}\n\nWe hope you will carefully study the provided papers and determine the citation relationships between them. Please follow the instructions below strictly to complete the task:\n\n#Specific Requirements:\n1. Reference: When a given paper mentions other provided papers, those other papers are considered as \"references\" for the given paper. To summarize in this specific context, references are about what the given paper is using.\n2. Citation: Conversely, when other provided papers mention the given paper in their works, the given paper is being \"cited\" by those other papers. To summarize in this specific context, citations are about who is using the given paper.\n3. Given a paper, you need to determine the citation or reference relationship between this paper and the other papers. Do not consider papers that are not provided.\n3. Please present the paper titles in a json format as follows: {{\"Reference\":[\"Reference Title 1\", \"Reference Title 2\", ..., \"Reference Title n\"], \"Citation\":[\"Citation Title 1\", \"Citation Title 2\", ..., \"Citation Title n\"]}}.\n4. If a paper does not have any references or citations, please leave the corresponding list empty, e.g.{{\"Refernce\":[]}}, {{\"Citation\":[]}}.\n\n#The paper you need to analyze:\nDistServe: Disaggregating Prefill and Decoding for Goodput-optimized Large Language Model Serving\n\n[Gold Answer]\n{'Reference': ['# Splitwise: Efficient Generative LLM Inference Using Phase Splitting ', '# Inference without Interference: Disaggregate LLM Inference for Mixed Downstream Workloads '], 'Citation': ['# VIDUR: A LARGE-SCALE SIMULATION FRAMEWORK FOR LLM INFERENCE ', '# Aladdin: Joint Placement and Scaling for SLO-Aware LLM Serving ']}\n\n[The Start of Assistant's Predicted Answer]\n{\"Reference\":[\"A Survey on Evaluation of Large Language Models\", \"Vicuna: An Open-Source Chatbot Impressing GPT-4 with $90 \\% *$ ChatGPT Quality\", \"ChatLaw: Open-Source Legal Large Language Model with Integrated External Knowledge Bases\", \"Compositional Semantic Parsing with Large Language Models\", \"Open LLM Leaderboard\", \"How Close is ChatGPT to Human Experts? Comparison Corpus, Evaluation, and Detection\", \"REALM: Retrieval-Augmented Language Model PreTraining\", \"Rethinking with Retrieval: Faithful Large Language Model Inference\", \"Measuring Massive Multitask Language Understanding\", \"Atlas: Few-shot Learning with Retrieval Augmented Language Models\", \"Introducing chatgpt\", \"Bard, an experiment by google\", \"Inflection tech memo\", \"Lanchain usecase: Summarization\", \"NVIDIA Collective Communications Library (NCCL)\", \"Serve, Optimize and Scale PyTorch Models in Production\", \"ShareGPT Teams\", \"Evaluating Large Language Models Trained on Code\", \"Evaluating Large Language Models Trained on Code\", \"Vicuna: An open-source chatbot impressing gpt-4 with $90 \\% *$ chatgpt quality\", \"Compute Express Link Consortium\", \"NVIDIA Corporation\", \"NVIDIA Corporation\", \"FlashAttention: Fast and Memory-efficient Exact Attention with IO-awareness\", \"TurboTransformers: An Efficient GPU Serving System for Transformer Models\", \"Serving DNNs like Clockwork: Performance Predictability from the Bottom up\", \"Serving DNNs like Clockwork: Performance Predictability from the Bottom up\", \"MIRA: A Program-Behavior-Guided Far Memory System\", \"Microsecond-scale Preemption for Concurrent GPU-accelerated DNN Inferences\", \"Inference without Interference: Disaggregate 11 m inference for mixed downstream workloads\", \"GPipe: Efficient Training of Giant Neural Networks Using Pipeline Parallelism\", \"SIA: Heterogeneity-aware, Goodput-optimized ML-cluster Scheduling\", \"Efficient Memory Management for Large Language Model Serving with PagedAttention\", \"Efficient Memory Management for Large Language Model Serving with PagedAttention\", \"AlpaServe: Statistical Multiplexing with Model Parallelism for Deep Learning Serving\", \"Ray: A Distributed Framework for Emerging AI Applications\", \"PipeDream: Generalized Pipeline Parallelism for DNN Training\", \"GPT-4 Technical Report\", \"Splitwise: Efficient Generative LLM Inference Using Phase Splitting\", \"Pollux: Co-adaptive Cluster Scheduling for Goodput-optimized Deep Learning\", \"Zero: Memory Optimizations Toward Training Trillion Parameter Models\", \"Reuters, 2023\", \"Code Llama: Open Foundation Models for Code\", \"LEGOOS : A Disseminated, Distributed OS for Hardware Resource Disaggregation\", \"Megatron-LM: Training Multi-billion Parameter Language Models\", \"Fundamentals of Queueing Theory\", \"Djvu: KV-cache Streaming for Fast, Fault-tolerant Generative LLM Serving\", \"HotGPT: How to Make Software Documentation More Useful with a Large Language Model?\", \"LLAMA: Open and Efficient Foundation Language Models\", \"Fast Distributed Inference Serving for Large Language Models\", \"ORCA: A Distributed Serving System for Transformer-Based Generative Models\", \"Shepherd: Serving DNNs in the Wild\", \"OPT: Open Pre-trained Transformer Language Models\", \"Make it Real: An End-to-End Implementation of a Physically Disaggregated Data Center\", \"FT-CNN: Algorithm-based Fault Tolerance for Convolutional Neural Networks\", \"Alpa: Automating Inter- and Intra-Operator Parallelism for Distributed Deep Learning\", \"PetS: A Unified Framework for Parameter-Efficient Transformers Serving\"], \"Citation\":[]}\n[The End of Assistant's Predicted Answer]\n\n[System]\nWe would like to request your feedback on the performance of the AI assistant in response to the user question displayed above according to the gold answer. Please use the following listed aspects and their descriptions as evaluation criteria:\n    - Accuracy and Hallucinations: The assistant's answer is semantically consistent with the gold answer; The numerical value and order need to be accurate, and there should be no hallucinations.\n    - Completeness: Referring to the reference answers, the assistant's answer should contain all the key points needed to answer the user's question; further elaboration on these key points can be omitted.\nPlease rate whether this answer is suitable for the question. Please note that the gold answer can be considered as a correct answer to the question.\n\nThe assistant receives an overall score on a scale of 1 to 100, where a higher score indicates better overall performance.\nPlease note that if the assistant's answer and the gold answer fully meet the above criteria, its overall rating should be the full marks (100).\nPlease first provide a comprehensive explanation of your evaluation, avoiding any potential bias.\nThen, output a line indicating the score of the Assistant.\n\nPLEASE OUTPUT WITH THE FOLLOWING FORMAT, WHERE THE SCORE IS A SCALE OF 1 TO 100 BY STRICTLY FOLLOWING THIS FORMAT: \"[[score]]\", FOR EXAMPLE \"Rating: [[100]]\":\n<start output>\nEvaluation evidence: your evluation explanation here, no more than 100 words\nRating: [[score]]\n<end output>\n\nNow, start your evaluation:",
        "generate_response": "{\"Reference\":[\"A Survey on Evaluation of Large Language Models\", \"Vicuna: An Open-Source Chatbot Impressing GPT-4 with $90 \\% *$ ChatGPT Quality\", \"ChatLaw: Open-Source Legal Large Language Model with Integrated External Knowledge Bases\", \"Compositional Semantic Parsing with Large Language Models\", \"Open LLM Leaderboard\", \"How Close is ChatGPT to Human Experts? Comparison Corpus, Evaluation, and Detection\", \"REALM: Retrieval-Augmented Language Model PreTraining\", \"Rethinking with Retrieval: Faithful Large Language Model Inference\", \"Measuring Massive Multitask Language Understanding\", \"Atlas: Few-shot Learning with Retrieval Augmented Language Models\", \"Introducing chatgpt\", \"Bard, an experiment by google\", \"Inflection tech memo\", \"Lanchain usecase: Summarization\", \"NVIDIA Collective Communications Library (NCCL)\", \"Serve, Optimize and Scale PyTorch Models in Production\", \"ShareGPT Teams\", \"Evaluating Large Language Models Trained on Code\", \"Evaluating Large Language Models Trained on Code\", \"Vicuna: An open-source chatbot impressing gpt-4 with $90 \\% *$ chatgpt quality\", \"Compute Express Link Consortium\", \"NVIDIA Corporation\", \"NVIDIA Corporation\", \"FlashAttention: Fast and Memory-efficient Exact Attention with IO-awareness\", \"TurboTransformers: An Efficient GPU Serving System for Transformer Models\", \"Serving DNNs like Clockwork: Performance Predictability from the Bottom up\", \"Serving DNNs like Clockwork: Performance Predictability from the Bottom up\", \"MIRA: A Program-Behavior-Guided Far Memory System\", \"Microsecond-scale Preemption for Concurrent GPU-accelerated DNN Inferences\", \"Inference without Interference: Disaggregate 11 m inference for mixed downstream workloads\", \"GPipe: Efficient Training of Giant Neural Networks Using Pipeline Parallelism\", \"SIA: Heterogeneity-aware, Goodput-optimized ML-cluster Scheduling\", \"Efficient Memory Management for Large Language Model Serving with PagedAttention\", \"Efficient Memory Management for Large Language Model Serving with PagedAttention\", \"AlpaServe: Statistical Multiplexing with Model Parallelism for Deep Learning Serving\", \"Ray: A Distributed Framework for Emerging AI Applications\", \"PipeDream: Generalized Pipeline Parallelism for DNN Training\", \"GPT-4 Technical Report\", \"Splitwise: Efficient Generative LLM Inference Using Phase Splitting\", \"Pollux: Co-adaptive Cluster Scheduling for Goodput-optimized Deep Learning\", \"Zero: Memory Optimizations Toward Training Trillion Parameter Models\", \"Reuters, 2023\", \"Code Llama: Open Foundation Models for Code\", \"LEGOOS : A Disseminated, Distributed OS for Hardware Resource Disaggregation\", \"Megatron-LM: Training Multi-billion Parameter Language Models\", \"Fundamentals of Queueing Theory\", \"Djvu: KV-cache Streaming for Fast, Fault-tolerant Generative LLM Serving\", \"HotGPT: How to Make Software Documentation More Useful with a Large Language Model?\", \"LLAMA: Open and Efficient Foundation Language Models\", \"Fast Distributed Inference Serving for Large Language Models\", \"ORCA: A Distributed Serving System for Transformer-Based Generative Models\", \"Shepherd: Serving DNNs in the Wild\", \"OPT: Open Pre-trained Transformer Language Models\", \"Make it Real: An End-to-End Implementation of a Physically Disaggregated Data Center\", \"FT-CNN: Algorithm-based Fault Tolerance for Convolutional Neural Networks\", \"Alpa: Automating Inter- and Intra-Operator Parallelism for Distributed Deep Learning\", \"PetS: A Unified Framework for Parameter-Efficient Transformers Serving\"], \"Citation\":[]}",
        "eval_response": "Evaluation evidence: The assistant's answer does not match the gold answer in terms of accuracy and completeness. The references and citations listed in the assistant's response do not align with those in the gold answer, indicating a significant deviation and hallucination of content. The assistant's answer includes numerous unrelated titles, which are not mentioned in the gold answer, leading to inaccuracies and a lack of focus on the required task.\n\nRating: [[10]]",
        "score": 10
    },
    {
        "level": 3,
        "set": 2,
        "length": 99639,
        "type": "paper",
        "language": "en",
        "question": "Extreme Compression of Large Language Models via Additive Quantization",
        "instruction": "We hope you will carefully study the provided papers and determine the citation relationships between them. Please follow the instructions below strictly to complete the task:\n\n#Specific Requirements:\n1. Reference: When a given paper mentions other provided papers, those other papers are considered as \"references\" for the given paper. To summarize in this specific context, references are about what the given paper is using.\n2. Citation: Conversely, when other provided papers mention the given paper in their works, the given paper is being \"cited\" by those other papers. To summarize in this specific context, citations are about who is using the given paper.\n3. Given a paper, you need to determine the citation or reference relationship between this paper and the other papers. Do not consider papers that are not provided.\n3. Please present the paper titles in a json format as follows: {{\"Reference\":[\"Reference Title 1\", \"Reference Title 2\", ..., \"Reference Title n\"], \"Citation\":[\"Citation Title 1\", \"Citation Title 2\", ..., \"Citation Title n\"]}}.\n4. If a paper does not have any references or citations, please leave the corresponding list empty, e.g.{{\"Refernce\":[]}}, {{\"Citation\":[]}}.",
        "prompt_template": "#Papers:\n{docs}\n\n{instruction}\n\n#The paper you need to analyze:\n{question}",
        "doc": [
            "2401.06118.md",
            "2405.18137.md",
            "2405.18886.md",
            "2310.06825.md"
        ],
        "answer": {
            "Reference": [
                "# Mistral 7B "
            ],
            "Citation": [
                "# Exploiting LLM Quantization ",
                "# Compressing Large Language Models using Low Rank and Low Precision Decomposition "
            ]
        },
        "shuffle_doc": true,
        "id": "7df5cf53-1601-43c2-9688-6dc6667ef7b3",
        "prompt": "[Question]\n#Papers:\n{docs}\n\nWe hope you will carefully study the provided papers and determine the citation relationships between them. Please follow the instructions below strictly to complete the task:\n\n#Specific Requirements:\n1. Reference: When a given paper mentions other provided papers, those other papers are considered as \"references\" for the given paper. To summarize in this specific context, references are about what the given paper is using.\n2. Citation: Conversely, when other provided papers mention the given paper in their works, the given paper is being \"cited\" by those other papers. To summarize in this specific context, citations are about who is using the given paper.\n3. Given a paper, you need to determine the citation or reference relationship between this paper and the other papers. Do not consider papers that are not provided.\n3. Please present the paper titles in a json format as follows: {{\"Reference\":[\"Reference Title 1\", \"Reference Title 2\", ..., \"Reference Title n\"], \"Citation\":[\"Citation Title 1\", \"Citation Title 2\", ..., \"Citation Title n\"]}}.\n4. If a paper does not have any references or citations, please leave the corresponding list empty, e.g.{{\"Refernce\":[]}}, {{\"Citation\":[]}}.\n\n#The paper you need to analyze:\nExtreme Compression of Large Language Models via Additive Quantization\n\n[Gold Answer]\n{'Reference': ['# Mistral 7B '], 'Citation': ['# Exploiting LLM Quantization ', '# Compressing Large Language Models using Low Rank and Low Precision Decomposition ']}\n\n[The Start of Assistant's Predicted Answer]\n{\"Reference\": [\"Additive quantization for extreme vector compression\", \"On the statistical analysis of dirty pictures\", \"Pythia: A suite for analyzing large language models across training and scaling\", \"Multiplying matrices without multiplying\", \"A generalization of isolated word recognition using vector quantization\", \"Quip: 2-bit quantization of large language models with guarantees\", \"Approximate nearest neighbor search by residual vector quantization\", \"Think you have solved question answering? try arc, the ai 2 reasoning challenge\", \"Redpajama: an open dataset for training large language models\", \"The case for 4-bit precision: k-bit inference scaling laws\", \"LLM.int8(): 8-bit matrix multiplication for transformers at scale\", \"QLoRA: Efficient finetuning of quantized llms\", \"SPQR: A sparse-quantized representation for near-lossless 11 m weight compression\", \"Are we there yet? product quantization and its hardware acceleration\", \"A survey of quantization methods for efficient neural network inference\", \"Vector quantization\", \"Quantization based fast inner product search\", \"Distilling the knowledge in a neural network\", \"Product quantization for nearest neighbor search\", \"Mistral 7b\", \"Mixtral of experts\", \"SqueezeLLM: Dense-and-sparse quantization\", \"Adam: A method for stochastic optimization\", \"Sparse fine-tuning for inference acceleration of large language models\", \"Performance guaranteed network acceleration via high-order residual quantization\", \"AWQ: Activation-aware weight quantization for llm compression and acceleration\", \"Revisiting additive quantization\", \"LSQ++: Lower running time and higher recall in multi-codebook quantization\", \"Look-ups are not (yet) all you need for deep learning inference\", \"Pointer sentinel mixture models\", \"Up or down? Adaptive rounding for post-training quantization\", \"Cartesian k-means\", \"Competitive quantization for approximate nearest neighbor search\", \"nuQMM: Quantized matmul for efficient inference of large-scale generative language models\", \"PyTorch: An imperative style, high-performance deep learning library\", \"Exploring the limits of transfer learning with a unified text-to-text transformer\", \"Winogrande: An adversarial winograd schema challenge at scale\", \"BLOOM: A 176b-parameter open-access multilingual language model\", \"Patient knowledge distillation for BERT model compression\", \"PIQA: An algebra for querying protein data sets\", \"The Falcon family of large language models\", \"Llama: Open and efficient foundation language models\", \"Quip#: Quip with lattice codebooks\", \"Attention is all you need\", \"SmoothQuant: Accurate and efficient post-training quantization for large language models\", \"ZeroQuant: Efficient and affordable post-training quantization for large-scale transformers\", \"HellaSwag: Can a machine really finish your sentence?\", \"OPT: Open pre-trained transformer language models\", \"Composite quantization for approximate nearest neighbor search\", \"Balanced quantization: An effective and efficient approach to quantized neural networks\"], \"Citation\": []}\n[The End of Assistant's Predicted Answer]\n\n[System]\nWe would like to request your feedback on the performance of the AI assistant in response to the user question displayed above according to the gold answer. Please use the following listed aspects and their descriptions as evaluation criteria:\n    - Accuracy and Hallucinations: The assistant's answer is semantically consistent with the gold answer; The numerical value and order need to be accurate, and there should be no hallucinations.\n    - Completeness: Referring to the reference answers, the assistant's answer should contain all the key points needed to answer the user's question; further elaboration on these key points can be omitted.\nPlease rate whether this answer is suitable for the question. Please note that the gold answer can be considered as a correct answer to the question.\n\nThe assistant receives an overall score on a scale of 1 to 100, where a higher score indicates better overall performance.\nPlease note that if the assistant's answer and the gold answer fully meet the above criteria, its overall rating should be the full marks (100).\nPlease first provide a comprehensive explanation of your evaluation, avoiding any potential bias.\nThen, output a line indicating the score of the Assistant.\n\nPLEASE OUTPUT WITH THE FOLLOWING FORMAT, WHERE THE SCORE IS A SCALE OF 1 TO 100 BY STRICTLY FOLLOWING THIS FORMAT: \"[[score]]\", FOR EXAMPLE \"Rating: [[100]]\":\n<start output>\nEvaluation evidence: your evluation explanation here, no more than 100 words\nRating: [[score]]\n<end output>\n\nNow, start your evaluation:",
        "generate_response": "{\"Reference\": [\"Additive quantization for extreme vector compression\", \"On the statistical analysis of dirty pictures\", \"Pythia: A suite for analyzing large language models across training and scaling\", \"Multiplying matrices without multiplying\", \"A generalization of isolated word recognition using vector quantization\", \"Quip: 2-bit quantization of large language models with guarantees\", \"Approximate nearest neighbor search by residual vector quantization\", \"Think you have solved question answering? try arc, the ai 2 reasoning challenge\", \"Redpajama: an open dataset for training large language models\", \"The case for 4-bit precision: k-bit inference scaling laws\", \"LLM.int8(): 8-bit matrix multiplication for transformers at scale\", \"QLoRA: Efficient finetuning of quantized llms\", \"SPQR: A sparse-quantized representation for near-lossless 11 m weight compression\", \"Are we there yet? product quantization and its hardware acceleration\", \"A survey of quantization methods for efficient neural network inference\", \"Vector quantization\", \"Quantization based fast inner product search\", \"Distilling the knowledge in a neural network\", \"Product quantization for nearest neighbor search\", \"Mistral 7b\", \"Mixtral of experts\", \"SqueezeLLM: Dense-and-sparse quantization\", \"Adam: A method for stochastic optimization\", \"Sparse fine-tuning for inference acceleration of large language models\", \"Performance guaranteed network acceleration via high-order residual quantization\", \"AWQ: Activation-aware weight quantization for llm compression and acceleration\", \"Revisiting additive quantization\", \"LSQ++: Lower running time and higher recall in multi-codebook quantization\", \"Look-ups are not (yet) all you need for deep learning inference\", \"Pointer sentinel mixture models\", \"Up or down? Adaptive rounding for post-training quantization\", \"Cartesian k-means\", \"Competitive quantization for approximate nearest neighbor search\", \"nuQMM: Quantized matmul for efficient inference of large-scale generative language models\", \"PyTorch: An imperative style, high-performance deep learning library\", \"Exploring the limits of transfer learning with a unified text-to-text transformer\", \"Winogrande: An adversarial winograd schema challenge at scale\", \"BLOOM: A 176b-parameter open-access multilingual language model\", \"Patient knowledge distillation for BERT model compression\", \"PIQA: An algebra for querying protein data sets\", \"The Falcon family of large language models\", \"Llama: Open and efficient foundation language models\", \"Quip#: Quip with lattice codebooks\", \"Attention is all you need\", \"SmoothQuant: Accurate and efficient post-training quantization for large language models\", \"ZeroQuant: Efficient and affordable post-training quantization for large-scale transformers\", \"HellaSwag: Can a machine really finish your sentence?\", \"OPT: Open pre-trained transformer language models\", \"Composite quantization for approximate nearest neighbor search\", \"Balanced quantization: An effective and efficient approach to quantized neural networks\"], \"Citation\": []}",
        "eval_response": "Evaluation evidence: The assistant's answer is inaccurate and contains hallucinations. The provided answer lists numerous references that are not mentioned in the gold answer, indicating a misunderstanding or misinterpretation of the task. The gold answer specifies only one reference and two citations, none of which appear in the assistant's response. This discrepancy shows a failure to accurately identify the correct relationships between the papers, leading to a complete mismatch with the gold answer.\n\nRating: [[10]]",
        "score": 10
    },
    {
        "level": 3,
        "set": 2,
        "length": 83093,
        "type": "paper",
        "language": "en",
        "question": "Adapting Large Language Models for Document-Level Machine Translation",
        "instruction": "We hope you will carefully study the provided papers and determine the citation relationships between them. Please follow the instructions below strictly to complete the task:\n\n#Specific Requirements:\n1. Reference: When a given paper mentions other provided papers, those other papers are considered as \"references\" for the given paper. To summarize in this specific context, references are about what the given paper is using.\n2. Citation: Conversely, when other provided papers mention the given paper in their works, the given paper is being \"cited\" by those other papers. To summarize in this specific context, citations are about who is using the given paper.\n3. Given a paper, you need to determine the citation or reference relationship between this paper and the other papers. Do not consider papers that are not provided.\n3. Please present the paper titles in a json format as follows: {{\"Reference\":[\"Reference Title 1\", \"Reference Title 2\", ..., \"Reference Title n\"], \"Citation\":[\"Citation Title 1\", \"Citation Title 2\", ..., \"Citation Title n\"]}}.\n4. If a paper does not have any references or citations, please leave the corresponding list empty, e.g.{{\"Refernce\":[]}}, {{\"Citation\":[]}}.",
        "prompt_template": "#Papers:\n{docs}\n\n{instruction}\n\n#The paper you need to analyze:\n{question}",
        "doc": [
            "2401.06468.md",
            "2404.14122.md",
            "2405.18922.md",
            "2309.08958.md",
            "2310.13988.md"
        ],
        "answer": {
            "Reference": [
                "# Monolingual or Multilingual Instruction Tuning: Which Makes a Better Alpaca ",
                "# GEMBA-MQM: Detecting Translation Quality Error Spans with GPT-4 "
            ],
            "Citation": [
                "# Fine-Tuning Large Language Models to Translate: Will a Touch of Noisy Data in Misaligned Languages Suffice? ",
                "# Understanding and Addressing the Under-Translation Problem from the Perspective of Decoding Objective "
            ]
        },
        "shuffle_doc": true,
        "id": "8106e102-781d-4ed9-9437-314fd5a960e8",
        "prompt": "[Question]\n#Papers:\n{docs}\n\nWe hope you will carefully study the provided papers and determine the citation relationships between them. Please follow the instructions below strictly to complete the task:\n\n#Specific Requirements:\n1. Reference: When a given paper mentions other provided papers, those other papers are considered as \"references\" for the given paper. To summarize in this specific context, references are about what the given paper is using.\n2. Citation: Conversely, when other provided papers mention the given paper in their works, the given paper is being \"cited\" by those other papers. To summarize in this specific context, citations are about who is using the given paper.\n3. Given a paper, you need to determine the citation or reference relationship between this paper and the other papers. Do not consider papers that are not provided.\n3. Please present the paper titles in a json format as follows: {{\"Reference\":[\"Reference Title 1\", \"Reference Title 2\", ..., \"Reference Title n\"], \"Citation\":[\"Citation Title 1\", \"Citation Title 2\", ..., \"Citation Title n\"]}}.\n4. If a paper does not have any references or citations, please leave the corresponding list empty, e.g.{{\"Refernce\":[]}}, {{\"Citation\":[]}}.\n\n#The paper you need to analyze:\nAdapting Large Language Models for Document-Level Machine Translation\n\n[Gold Answer]\n{'Reference': ['# Monolingual or Multilingual Instruction Tuning: Which Makes a Better Alpaca ', '# GEMBA-MQM: Detecting Translation Quality Error Spans with GPT-4 '], 'Citation': ['# Fine-Tuning Large Language Models to Translate: Will a Touch of Noisy Data in Misaligned Languages Suffice? ', '# Understanding and Addressing the Under-Translation Problem from the Perspective of Decoding Objective ']}\n\n[The Start of Assistant's Predicted Answer]\n{ \\\"Reference\\\": [\\\"Language models are few-shot learners\\\", \\\"Multidimensional quality metrics: a flexible system for assessing translation quality\\\", \\\"Overview of the IWSLT 2017 evaluation campaign\\\", \\\"Monolingual or multilingual instruction tuning: Which makes a better alpaca\\\", \\\"Diving deep into context-aware neural machine translation\\\", \\\"MADLAD400: A multilingual and document-level large audited dataset\\\", \\\"Results of WMT22 metrics shared task: Stop using BLEU - neural metrics are better and more robust\\\", \\\"Textbooks are all you need\\\", \\\"How good are GPT models at machine translation? A comprehensive evaluation\\\", \\\"Lora: Low-rank adaptation of large language models\\\", \\\"Diving deep into context-aware neural machine translation\\\", \\\"Chain-of-dictionary prompting elicits translation in large language models\\\", \\\"Wizardcoder: Empowering code large language models with evolinstruct\\\", \\\"A simple and effective unified encoder for documentlevel machine translation\\\", \\\"Using whole document context in neural machine translation\\\", \\\"Adaptive machine translation with large language models\\\", \\\"A large-scale test set for the evaluation of context-aware pronoun translation in neural machine translation\\\", \\\"Culturax: A cleaned, enormous, and multilingual dataset for large language models in 167 languages\\\", \\\"GPT-4 technical report\\\", \\\"Training language models to follow instructions with human feedback\\\", \\\"Bleu: a method for automatic evaluation of machine translation\\\", \\\"A call for clarity in reporting BLEU scores\\\", \\\"Palm: Scaling language modeling with pathways\\\", \\\"Scaling instruction-finetuned language models\\\", \\\"No language left behind: Scaling human-centered machine translation\\\", \\\"Learn to remember: Transformer with recurrent memory for document-level machine translation\\\", \\\"Chain-of-thought prompting elicits reasoning in large language models\\\", \\\"Bactrian-x : A multilingual replicable instruction-following model with low-rank adaptation\\\", \\\"Multilingual denoising pretraining for neural machine translation\\\", \\\"Importance-aware data augmentation for document-level neural machine translation\\\", \\\"A paradigm shift in machine translation: Boosting translation performance of large language models\\\", \\\"mT5: A massively multilingual pre-trained text-to-text transformer\\\", \\\"Bigtrans: Augmenting large language models with multilingual translation capability over 100 languages\\\", \\\"Multilingual document-level translation enables zero-shot transfer from sentences to documents\\\", \\\"Improving the transformer translation model with document-level context\\\", \\\"Long-short term masking transformer: A simple but effective baseline for document-level neural machine translation\\\", \\\"Bayling: Bridging cross-lingual alignment and instruction following through interactive translation for large language models\\\", \\\"Multilingual machine translation with large language models: Empirical results and analysis\\\"], \\\"Citation\\\": [\\\"Fine-Tuning Large Language Models to Translate: Will a Touch of Noisy Data in Misaligned Languages Suffice?\\\"] }\"\n[The End of Assistant's Predicted Answer]\n\n[System]\nWe would like to request your feedback on the performance of the AI assistant in response to the user question displayed above according to the gold answer. Please use the following listed aspects and their descriptions as evaluation criteria:\n    - Accuracy and Hallucinations: The assistant's answer is semantically consistent with the gold answer; The numerical value and order need to be accurate, and there should be no hallucinations.\n    - Completeness: Referring to the reference answers, the assistant's answer should contain all the key points needed to answer the user's question; further elaboration on these key points can be omitted.\nPlease rate whether this answer is suitable for the question. Please note that the gold answer can be considered as a correct answer to the question.\n\nThe assistant receives an overall score on a scale of 1 to 100, where a higher score indicates better overall performance.\nPlease note that if the assistant's answer and the gold answer fully meet the above criteria, its overall rating should be the full marks (100).\nPlease first provide a comprehensive explanation of your evaluation, avoiding any potential bias.\nThen, output a line indicating the score of the Assistant.\n\nPLEASE OUTPUT WITH THE FOLLOWING FORMAT, WHERE THE SCORE IS A SCALE OF 1 TO 100 BY STRICTLY FOLLOWING THIS FORMAT: \"[[score]]\", FOR EXAMPLE \"Rating: [[100]]\":\n<start output>\nEvaluation evidence: your evluation explanation here, no more than 100 words\nRating: [[score]]\n<end output>\n\nNow, start your evaluation:",
        "generate_response": "{ \\\"Reference\\\": [\\\"Language models are few-shot learners\\\", \\\"Multidimensional quality metrics: a flexible system for assessing translation quality\\\", \\\"Overview of the IWSLT 2017 evaluation campaign\\\", \\\"Monolingual or multilingual instruction tuning: Which makes a better alpaca\\\", \\\"Diving deep into context-aware neural machine translation\\\", \\\"MADLAD400: A multilingual and document-level large audited dataset\\\", \\\"Results of WMT22 metrics shared task: Stop using BLEU - neural metrics are better and more robust\\\", \\\"Textbooks are all you need\\\", \\\"How good are GPT models at machine translation? A comprehensive evaluation\\\", \\\"Lora: Low-rank adaptation of large language models\\\", \\\"Diving deep into context-aware neural machine translation\\\", \\\"Chain-of-dictionary prompting elicits translation in large language models\\\", \\\"Wizardcoder: Empowering code large language models with evolinstruct\\\", \\\"A simple and effective unified encoder for documentlevel machine translation\\\", \\\"Using whole document context in neural machine translation\\\", \\\"Adaptive machine translation with large language models\\\", \\\"A large-scale test set for the evaluation of context-aware pronoun translation in neural machine translation\\\", \\\"Culturax: A cleaned, enormous, and multilingual dataset for large language models in 167 languages\\\", \\\"GPT-4 technical report\\\", \\\"Training language models to follow instructions with human feedback\\\", \\\"Bleu: a method for automatic evaluation of machine translation\\\", \\\"A call for clarity in reporting BLEU scores\\\", \\\"Palm: Scaling language modeling with pathways\\\", \\\"Scaling instruction-finetuned language models\\\", \\\"No language left behind: Scaling human-centered machine translation\\\", \\\"Learn to remember: Transformer with recurrent memory for document-level machine translation\\\", \\\"Chain-of-thought prompting elicits reasoning in large language models\\\", \\\"Bactrian-x : A multilingual replicable instruction-following model with low-rank adaptation\\\", \\\"Multilingual denoising pretraining for neural machine translation\\\", \\\"Importance-aware data augmentation for document-level neural machine translation\\\", \\\"A paradigm shift in machine translation: Boosting translation performance of large language models\\\", \\\"mT5: A massively multilingual pre-trained text-to-text transformer\\\", \\\"Bigtrans: Augmenting large language models with multilingual translation capability over 100 languages\\\", \\\"Multilingual document-level translation enables zero-shot transfer from sentences to documents\\\", \\\"Improving the transformer translation model with document-level context\\\", \\\"Long-short term masking transformer: A simple but effective baseline for document-level neural machine translation\\\", \\\"Bayling: Bridging cross-lingual alignment and instruction following through interactive translation for large language models\\\", \\\"Multilingual machine translation with large language models: Empirical results and analysis\\\"], \\\"Citation\\\": [\\\"Fine-Tuning Large Language Models to Translate: Will a Touch of Noisy Data in Misaligned Languages Suffice?\\\"] }\"",
        "eval_response": "Evaluation evidence: The assistant's answer includes a significant number of hallucinated references that are not mentioned in the gold answer, indicating a lack of accuracy and the presence of hallucinations. The citation part of the answer correctly identifies one of the two citations from the gold answer but misses another, affecting completeness. The reference part of the answer does not match the gold answer at all, further impacting accuracy and completeness.\n\nRating: [[20]]",
        "score": 20
    },
    {
        "level": 3,
        "set": 3,
        "length": 183865,
        "type": "paper",
        "language": "en",
        "question": "Seven Failure Points When Engineering a Retrieval Augmented Generation System",
        "instruction": "We hope you will carefully study the provided papers and determine the citation relationships between them. Please follow the instructions below strictly to complete the task:\n\n#Specific Requirements:\n1. Reference: When a given paper mentions other provided papers, those other papers are considered as \"references\" for the given paper. To summarize in this specific context, references are about what the given paper is using.\n2. Citation: Conversely, when other provided papers mention the given paper in their works, the given paper is being \"cited\" by those other papers. To summarize in this specific context, citations are about who is using the given paper.\n3. Given a paper, you need to determine the citation or reference relationship between this paper and the other papers. Do not consider papers that are not provided.\n3. Please present the paper titles in a json format as follows: {{\"Reference\":[\"Reference Title 1\", \"Reference Title 2\", ..., \"Reference Title n\"], \"Citation\":[\"Citation Title 1\", \"Citation Title 2\", ..., \"Citation Title n\"]}}.\n4. If a paper does not have any references or citations, please leave the corresponding list empty, e.g.{{\"Refernce\":[]}}, {{\"Citation\":[]}}.",
        "prompt_template": "#Papers:\n{docs}\n\n{instruction}\n\n#The paper you need to analyze:\n{question}",
        "doc": [
            "2401.05856.md",
            "2404.19705.md",
            "2405.01868.md",
            "2405.07437.md",
            "2405.15007.md",
            "2405.20362.md",
            "2212.04356.md",
            "2305.13691.md",
            "2307.03172.md",
            "2308.07107.md",
            "2309.01431.md"
        ],
        "answer": {
            "Reference": [
                "# Robust Speech Recognition via Large-Scale Weak Supervision ",
                "# Few-Shot Data Synthesis for Open Domain Multi-Hop Question Answering ",
                "# Lost in the Middle: How Language Models Use Long Contexts ",
                "# Benchmarking Large Language Models in Retrieval-Augmented Generation "
            ],
            "Citation": [
                "# When to Retrieve: Teaching LLMs to Utilize Information Retrieval Effectively ",
                "# Incorporating External Knowledge and Goal Guidance for LLM-based Conversational Recommender Systems ",
                "# RE-Adapt: Reverse Engineered Adaptation of Large Language Models ",
                "# Hallucination-Free? Assessing the Reliability of Leading AI Legal Research Tools "
            ]
        },
        "shuffle_doc": true,
        "id": "7eee3db2-a67b-438d-bb44-805bf71273fc",
        "prompt": "[Question]\n#Papers:\n{docs}\n\nWe hope you will carefully study the provided papers and determine the citation relationships between them. Please follow the instructions below strictly to complete the task:\n\n#Specific Requirements:\n1. Reference: When a given paper mentions other provided papers, those other papers are considered as \"references\" for the given paper. To summarize in this specific context, references are about what the given paper is using.\n2. Citation: Conversely, when other provided papers mention the given paper in their works, the given paper is being \"cited\" by those other papers. To summarize in this specific context, citations are about who is using the given paper.\n3. Given a paper, you need to determine the citation or reference relationship between this paper and the other papers. Do not consider papers that are not provided.\n3. Please present the paper titles in a json format as follows: {{\"Reference\":[\"Reference Title 1\", \"Reference Title 2\", ..., \"Reference Title n\"], \"Citation\":[\"Citation Title 1\", \"Citation Title 2\", ..., \"Citation Title n\"]}}.\n4. If a paper does not have any references or citations, please leave the corresponding list empty, e.g.{{\"Refernce\":[]}}, {{\"Citation\":[]}}.\n\n#The paper you need to analyze:\nSeven Failure Points When Engineering a Retrieval Augmented Generation System\n\n[Gold Answer]\n{'Reference': ['# Robust Speech Recognition via Large-Scale Weak Supervision ', '# Few-Shot Data Synthesis for Open Domain Multi-Hop Question Answering ', '# Lost in the Middle: How Language Models Use Long Contexts ', '# Benchmarking Large Language Models in Retrieval-Augmented Generation '], 'Citation': ['# When to Retrieve: Teaching LLMs to Utilize Information Retrieval Effectively ', '# Incorporating External Knowledge and Goal Guidance for LLM-based Conversational Recommender Systems ', '# RE-Adapt: Reverse Engineered Adaptation of Large Language Models ', '# Hallucination-Free? Assessing the Reliability of Leading AI Legal Research Tools ']}\n\n[The Start of Assistant's Predicted Answer]\n{\"Reference\":[\"GPTCache: An Open-Source Semantic Cache for LLM Applications Enabling Faster Answers and Cost Savings\", \"Self-adaptive Machine Learning Systems: Research Challenges and Opportunities\", \"Benchmarking Large Language Models in Retrieval-Augmented Generation\", \"Efficient Open Domain Multi-Hop Question Answering with Few-Shot Data Synthesis\", \"Threshy: Supporting safe usage of intelligent web services\", \"Beware the evolving 'intelligent'web service! An integration architecture tactic to guard AI-first components\", \"Retrieval augmented language model pre-training\", \"Fidlight: Efficient and effective retrieval-augmented text generation\", \"Leveraging Passage Retrieval with Generative Models for Open Domain Question Answering\", \"BioASQ-QA: A manually curated corpus for biomedical question answering\", \"LoRA Fine-tuning Efficiently Undoes Safety Training in Llama 2-Chat 70B\", \"Retrieval-augmented generation for knowledge-intensive NLP tasks\", \"Lost in the middle: How language models use long contexts\", \"G-eval: Nlg evaluation using gpt-4 with better human alignment\", \"Retrieval-based prompt selection for code-related few-shot learning\", \"GPT-4 Technical Report\", \"Robust speech recognition via large-scale weak supervision\", \"Improving the domain adaptation of retrieval augmented generation (RAG) models for open domain question answering\", \"Large language models for information retrieval: A survey\"], \"Citation\":[]}\n[The End of Assistant's Predicted Answer]\n\n[System]\nWe would like to request your feedback on the performance of the AI assistant in response to the user question displayed above according to the gold answer. Please use the following listed aspects and their descriptions as evaluation criteria:\n    - Accuracy and Hallucinations: The assistant's answer is semantically consistent with the gold answer; The numerical value and order need to be accurate, and there should be no hallucinations.\n    - Completeness: Referring to the reference answers, the assistant's answer should contain all the key points needed to answer the user's question; further elaboration on these key points can be omitted.\nPlease rate whether this answer is suitable for the question. Please note that the gold answer can be considered as a correct answer to the question.\n\nThe assistant receives an overall score on a scale of 1 to 100, where a higher score indicates better overall performance.\nPlease note that if the assistant's answer and the gold answer fully meet the above criteria, its overall rating should be the full marks (100).\nPlease first provide a comprehensive explanation of your evaluation, avoiding any potential bias.\nThen, output a line indicating the score of the Assistant.\n\nPLEASE OUTPUT WITH THE FOLLOWING FORMAT, WHERE THE SCORE IS A SCALE OF 1 TO 100 BY STRICTLY FOLLOWING THIS FORMAT: \"[[score]]\", FOR EXAMPLE \"Rating: [[100]]\":\n<start output>\nEvaluation evidence: your evluation explanation here, no more than 100 words\nRating: [[score]]\n<end output>\n\nNow, start your evaluation:",
        "generate_response": "{\"Reference\":[\"GPTCache: An Open-Source Semantic Cache for LLM Applications Enabling Faster Answers and Cost Savings\", \"Self-adaptive Machine Learning Systems: Research Challenges and Opportunities\", \"Benchmarking Large Language Models in Retrieval-Augmented Generation\", \"Efficient Open Domain Multi-Hop Question Answering with Few-Shot Data Synthesis\", \"Threshy: Supporting safe usage of intelligent web services\", \"Beware the evolving 'intelligent'web service! An integration architecture tactic to guard AI-first components\", \"Retrieval augmented language model pre-training\", \"Fidlight: Efficient and effective retrieval-augmented text generation\", \"Leveraging Passage Retrieval with Generative Models for Open Domain Question Answering\", \"BioASQ-QA: A manually curated corpus for biomedical question answering\", \"LoRA Fine-tuning Efficiently Undoes Safety Training in Llama 2-Chat 70B\", \"Retrieval-augmented generation for knowledge-intensive NLP tasks\", \"Lost in the middle: How language models use long contexts\", \"G-eval: Nlg evaluation using gpt-4 with better human alignment\", \"Retrieval-based prompt selection for code-related few-shot learning\", \"GPT-4 Technical Report\", \"Robust speech recognition via large-scale weak supervision\", \"Improving the domain adaptation of retrieval augmented generation (RAG) models for open domain question answering\", \"Large language models for information retrieval: A survey\"], \"Citation\":[]}",
        "eval_response": "Evaluation evidence: The assistant's answer does not match the gold answer in terms of accuracy and completeness. The references listed in the assistant's answer do not align with those in the gold answer, and it includes several titles not mentioned in the gold answer, indicating hallucinations. Additionally, the assistant's answer fails to list any citations, whereas the gold answer specifies four papers that cite the analyzed paper. This discrepancy shows a lack of completeness and accuracy in the assistant's response.\n\nRating: [[20]]",
        "score": 20
    },
    {
        "level": 3,
        "set": 3,
        "length": 141978,
        "type": "paper",
        "language": "en",
        "question": "Risk Taxonomy, Mitigation, and Assessment Benchmarks of Large Language Model Systems",
        "instruction": "We hope you will carefully study the provided papers and determine the citation relationships between them. Please follow the instructions below strictly to complete the task:\n\n#Specific Requirements:\n1. Reference: When a given paper mentions other provided papers, those other papers are considered as \"references\" for the given paper. To summarize in this specific context, references are about what the given paper is using.\n2. Citation: Conversely, when other provided papers mention the given paper in their works, the given paper is being \"cited\" by those other papers. To summarize in this specific context, citations are about who is using the given paper.\n3. Given a paper, you need to determine the citation or reference relationship between this paper and the other papers. Do not consider papers that are not provided.\n3. Please present the paper titles in a json format as follows: {{\"Reference\":[\"Reference Title 1\", \"Reference Title 2\", ..., \"Reference Title n\"], \"Citation\":[\"Citation Title 1\", \"Citation Title 2\", ..., \"Citation Title n\"]}}.\n4. If a paper does not have any references or citations, please leave the corresponding list empty, e.g.{{\"Refernce\":[]}}, {{\"Citation\":[]}}.",
        "prompt_template": "#Papers:\n{docs}\n\n{instruction}\n\n#The paper you need to analyze:\n{question}",
        "doc": [
            "2401.05778.md",
            "2404.05264.md",
            "2404.12736.md",
            "2310.13548.md",
            "2312.07930.md"
        ],
        "answer": {
            "Reference": [
                "# TOWARDS UNDERSTANDING SYCOPHANCY IN LANGUAGE MODELS ",
                "# Towards Optimal Statistical Watermarking "
            ],
            "Citation": [
                "# Unbridled Icarus: A Survey of the Potential Perils of Image Inputs in Multimodal Large Language Model Security ",
                "# Large Language Model Supply Chain: A Research Agenda "
            ]
        },
        "shuffle_doc": true,
        "id": "39b03fab-e31e-4159-b7ea-7314b361f0d9",
        "prompt": "[Question]\n#Papers:\n{docs}\n\nWe hope you will carefully study the provided papers and determine the citation relationships between them. Please follow the instructions below strictly to complete the task:\n\n#Specific Requirements:\n1. Reference: When a given paper mentions other provided papers, those other papers are considered as \"references\" for the given paper. To summarize in this specific context, references are about what the given paper is using.\n2. Citation: Conversely, when other provided papers mention the given paper in their works, the given paper is being \"cited\" by those other papers. To summarize in this specific context, citations are about who is using the given paper.\n3. Given a paper, you need to determine the citation or reference relationship between this paper and the other papers. Do not consider papers that are not provided.\n3. Please present the paper titles in a json format as follows: {{\"Reference\":[\"Reference Title 1\", \"Reference Title 2\", ..., \"Reference Title n\"], \"Citation\":[\"Citation Title 1\", \"Citation Title 2\", ..., \"Citation Title n\"]}}.\n4. If a paper does not have any references or citations, please leave the corresponding list empty, e.g.{{\"Refernce\":[]}}, {{\"Citation\":[]}}.\n\n#The paper you need to analyze:\nRisk Taxonomy, Mitigation, and Assessment Benchmarks of Large Language Model Systems\n\n[Gold Answer]\n{'Reference': ['# TOWARDS UNDERSTANDING SYCOPHANCY IN LANGUAGE MODELS ', '# Towards Optimal Statistical Watermarking '], 'Citation': ['# Unbridled Icarus: A Survey of the Potential Perils of Image Inputs in Multimodal Large Language Model Security ', '# Large Language Model Supply Chain: A Research Agenda ']}\n\n[The Start of Assistant's Predicted Answer]\n{\"Reference\":[\"AI safety via debate\", \"Discovering language model behaviors with model-written evaluations\", \"WebGPT: Browser-assisted question-answering with human feedback\", \"An MTurk crisis? Shifts in data quality and the impact on study results\", \"Why AI alignment could be hard with modern deep learning\", \"The false promise of imitating proprietary LLMs\", \"Scaling laws for reward model overoptimization\", \"Improving alignment of dialogue agents via targeted human judgements\", \"On the sensitivity of reward inference to misspecified human models\", \"Humans are not Boltzmann Distributions: Challenges and opportunities for modelling human feedback and interaction in reinforcement learning\", \"Question decomposition improves the faithfulness of model-generated reasoning\", \"Simple synthetic data reduces sycophancy in large language models\", \"Chain-of-thought prompting elicits reasoning in large language models\", \"Llama 2: Open foundation and fine-tuned chat models\", \"GLM-130B: an open bilingual pretrained model\", \"Codet5+: Open code large language models for code understanding and generation\", \"In-context instruction learning\", \"Tree of thoughts: Deliberate problem solving with large language models\", \"Graph of thoughts: Solving elaborate problems with large language models\", \"Training language models to follow instructions with human feedback\", \"Introducing chatgpt\", \"March 20 chatgpt outage: Here's what happened\", \"do anything now\": Characterizing and evaluating in-the-wild jailbreak prompts on large language models\", \"A survey of large language models\", \"Speech Understanding Systems\", \"Deep Learning\", \"Hierarchical Neural Story Generation\", \"The Curious Case of Neural Text Degeneration\", \"GPT3.5 Turbo Fine-Tuning and API Updates\", \"Model Index for Researchers\", \"Scaling Laws for Neural Language Models\", \"Attention is All You Need\", \"A Survey of Large Language Models\", \"Direct Preference Optimization: Your Language Model is Secretly a Reward Model\", \"Preference Ranking Optimization for Human Alignment\", \"RRHF: Rank Responses to Align Language Models with Human Feedback without Tears\", \"Calibrating Sequence Likelihood Improves Conditional Language Generation\", \"A Simple Method for Commonsense Reasoning\", \"Harnessing the Power of LLMs in Practice: A Survey on ChatGPT and Beyond\", \"An Empirical Analysis of Memorization in Fine-Tuned Autoregressive Language Models\", \"Not What You've Signed Up For: Compromising Real-World LLM-Integrated Applications with Indirect Prompt Injection\", \"Prompt Injection Attack Against LLM-Integrated Applications\", \"From Prompt Injections to SQL Injection Attacks: How Protected is Your LLM-Integrated Web Application?\", \"Bypass OpenAI's ChatGPT Alignment Efforts with This One Weird Trick\", \"Exploiting Programmatic Behavior of LLMs: Dual-Use Through Standard Security Attacks\", \"GPT-4 is Too Smart to be Safe: Stealthy Chat with LLMs via Cipher\", \"Multi-Step Jailbreaking Privacy Attacks on ChatGPT\", \"Jailbreaker: Automated Jailbreak Across Multiple Large Language Model Chatbots\", \"Ignore Previous Prompt: Attack Techniques for Language Models\", \"Not What You've Signed Up For: Compromising Real-World LLM-Integrated Applications with Indirect Prompt Injection\", \"Prompt Injection Attack Against LLM-Integrated Applications\", \"From Prompt Injections to SQL Injection Attacks: How Protected is Your LLM-Integrated Web Application?\", \"Exploiting Programmatic Behavior of LLMs: Dual-Use Through Standard Security Attacks\", \"Extracting Training Data from Large Language Models\", \"Are Large Pre-Trained Language Models Leaking Your Personal Information?\", \"Propile: Probing Privacy Leakage in Large Language Models\", \"TrustGPT: A Benchmark for Trustworthy and Responsible Large Language Models\", \"Toxicity Detection with Generative Prompt-Based Inference\", \"TextShield: Robust Text Classification Based on Multimodal Embedding and Neural Machine Translation\", \"Toxicity in ChatGPT: Analyzing Persona-Assigned Language Models\", \"'I'm Sorry to Hear That\": Finding New Biases in Language Models with a Holistic Descriptor Dataset\", \"MISGENDERED: Limits of Large Language Models in Understanding Pronouns\", \"Stereoset: Measuring Stereotypical Bias in Pretrained Language Models\", \"HolisticBias: A Dataset for Evaluating Gender, Profession, Race, and Religion Biases in Language Models\", \"CDAIL-Bias: A Large-Scale Chinese Bias Dataset Based on Social Dialog for Identifying Bias Issues in Dialog Systems\", \"A Survey of Hallucination in Large Language Models: Principles, Taxonomy, Challenges, and Open Questions\", \"LLMs as Factual Reasoners: Insights from Existing Benchmarks and Beyond\", \"Evaluating the Factual Consistency of Large Language Models Through News Summarization\", \"Evaluating Factual Consistency of Texts with Semantic Role Labeling\", \"TruthfulQA: Measuring How Models Mimic Human Falsehoods\", \"Methods for Measuring, Updating, and Visualizing Factual Beliefs in Language Models\", \"Factuality Enhanced Language Models for Open-Ended Text Generation\", \"Retrieval Augmentation Reduces Hallucination in Conversation\", \"Check Your Facts and Try Again: Improving Large Language Models with External Knowledge and Automated Feedback\", \"Automatic Evaluation of Attribution by Large Language Models\", \"Adaptive Chameleon or Stubborn Sloth: Unraveling the Behavior of Large Language Models in Knowledge Clashes\", \"The RefinedWeb Dataset for Falcon LLM: Outperforming Curated Corpora with Web Data, and Web Data Only\", \"Large Language Models with Controllable Working Memory\", \"When Not to Trust Language Models: Investigating Effectiveness of Parametric and Non-Parametric Memories\", \"Head-to-Tail: How Knowledgeable Are Large Language Models (1LM)? A.K.A. Will LLMs Replace Knowledge Graphs?\", \"Why Does ChatGPT Fall Short in Answering Questions Faithfully?\", \"Impact of Co-occurrence on Factual Knowledge of Large Language Models\", \"How Pre-Trained Language Models Capture Factual Knowledge? A Causal-Inspired Analysis\", \"Deduplicating Training Data Makes Language Models Better\", \"Large Language Models Struggle to Learn Long-Tail Knowledge\", \"Scaling Laws and Interpretability of Learning from Repeated Data\", \"Sources of Hallucination by Large Language Models on Inference Tasks\", \"Simple Synthetic Data Reduces Sycophancy in Large Language Models\", \"Towards Understanding Sycophancy in Language Models\", \"How Language Model Hallucinations Can Snowball\", \"The Internal State of an LLM Knows When Its Lying\", \"Overthinking the Truth: Understanding How Language Models Process False Demonstrations\", \"A Survey on In-Context Learning\", \"In-Context Learning and Induction Heads\", \"Neural Path Hunter: Reducing Hallucination in Dialogue Systems via Path Grounding\", \"D-DAE: Defense-Penetrating Model Extraction Attacks\", \"Model Stealing Attacks Against Inductive Graph Neural Networks\", \"Membership Inference Attacks Against Language Models via Neighborhood Comparison\", \"Property Inference Attacks Against GANs\", \"Using Highly Compressed Gradients in Federated Learning for Data Reconstruction Attacks\", \"Model Inversion Attacks that Exploit Confidence Information and Basic Countermeasures\", \"Poisoning Attacks in Federated Learning: A Survey\", \"Towards Backdoor Attacks and Defense in Robust Machine Learning Models\", \"Explaining and Harnessing Adversarial Examples\", \"Sponge Examples: Energy-Latency Attacks on Neural Networks\", \"Mondrian: Prompt Abstraction Attack Against Large Language Models for Cheaper API Pricing\", \"BadGPT: Exploring Security Vulnerabilities of ChatGPT via Backdoor Attacks to InstructGPT\", \"ChatGPT as an Attack Tool: Stealthy Textual Backdoor Attack via Blackbox Generative Model Trigger\", \"Tradeoffs in Continuous Integration: Assurance, Security, and Flexibility\", \"Characterizing the Security of GitHub CI Workflows\", \"Montage: A Neural Network Language Model-Guided JavaScript Engine Fuzzer\", \"ATP: In-Network Aggregation for Multi-Tenant Learning\", \"Seeing is Not Believing: Camouflage Attacks on Image Scaling Algorithms\", \"Can One Hear the Shape of a Neural Network?: Snooping the GPU via Magnetic Side Channel\", \"Spechammer: Combining Spectre and Rowhammer for New Speculative Attacks\", \"On a New Class of Pulsing Denial-of-Service Attacks and the Defense\", \"Adversarial Preprocessing: Understanding and Preventing Image-Scaling Attacks in Machine Learning\", \"Graphics Peeping Unit: Exploiting EM Side-Channel Information of GPUs to Eavesdrop on Your Neighbors\", \"Honeycomb: Secure and Efficient GPU Executions via Static Validation\", \"Strongbox: A GPU TEE on ARM Endpoints\", \"CryptGPU: Fast Privacy-Preserving Machine Learning on the GPU\", \"Bit-Flip Attack: Crushing Neural Network with Progressive Bit Search\", \"Deephammer: Depleting the Intelligence of Deep Neural Networks through Targeted Chain of Bit Flips\", \"Aegis: Mitigating Targeted Bit-Flip Attacks against Deep Neural Networks\", \"Neuropots: Realtime Proactive Defense against Bit-Flip Attacks in Neural Networks\", \"A Generic Communication Scheduler for Distributed DNN Training Acceleration\", \"A Unified Architecture for Accelerating Distributed DNN Training in Heterogeneous GPU/CPU Clusters\", \"Free Lunch for Testing: Fuzzing Deep-Learning Libraries from Open Source\", \"Is ChatGPT Biased Against Conservatives? An Empirical Study\", \"Exploring AI Ethics of ChatGPT: A Diagnostic Analysis\", \"Should ChatGPT Be Biased? Challenges and Risks of Bias in Large Language Models\", \"The Risks of Using ChatGPT to Obtain Common Safety-Related Information and Advice\", \"Chat-GPT: Opportunities and Challenges in Child Mental Healthcare\", \"OPC to Investigate ChatGPT Jointly with Provincial Privacy Authorities\", \"Samsung Bans Staff's AI Use After Spotting ChatGPT Data Leak\", \"Companies are Struggling to Keep Corporate Secrets Out of ChatGPT\", \"Measuring Causal Effects of Data Statistics on Language Model's 'Factual' Predictions\", \"Artificial Hallucinations in ChatGPT: Implications in Scientific Writing\", \"A Multitask, Multilingual, Multimodal Evaluation of ChatGPT on Reasoning, Hallucination, and Interactivity\", \"Google's AI Chatbot Bard Makes Factual Error in First Demo\", \"Release Strategies and the Social Impacts of Language Models\", \"AI-Generated Content (AIGC): A Survey\", \"NYC Education Department Blocks ChatGPT on School Devices, Networks\", \"College Instructor Put on Blast for Accusing Students of Using ChatGPT on Final Assignments\", \"Do Language Models Plagiarize?\", \"How Large Language Models are Transforming Machine-Paraphrased Plagiarism\", \"Impact of Big Data Analytics and ChatGPT on Cybersecurity\", \"From Text to MITRE Techniques: Exploring the Malicious Use of Large Language Models for Generating Cyber Attack Payloads\", \"Is GitHub's Copilot as Bad as Humans at Introducing Vulnerabilities in Code?\", \"Europol Warns That Hackers Use ChatGPT to Conduct Cyber Attacks\", \"ChatGPT Successfully Built Malware but Failed to Analyze the Complex Malware\", \"A Watermark for Large Language Models\", \"CoSyWa: Enhancing Semantic Integrity in Watermarking Natural Language Generation\", \"Natural Language Watermarking: Design, Analysis, and a Proof-of-Concept Implementation\", \"A Review of Digital Watermarking Techniques for Text Documents\", \"The Hiding Virtues of Ambiguity: Quantifiably Resilient Watermarking of Natural Language Text through Synonym Substitutions\", \"Electronic Marking and Identification Techniques to Discourage Document Copying\", \"Adversarial Watermarking Transformer: Towards Tracing Text Provenance with Data Hiding\", \"Can AI-Generated Text Be Reliably Detected?\", \"Warfare: Breaking the Watermark Protection of AI-Generated Content\", \"Towards Optimal Statistical Watermarking\", \"Towards Reliable Utilization of AIGC: Blockchain-Empowered Ownership Verification Mechanism\", \"Is ChatGPT Fair for Recommendation? Evaluating Fairness in Large Language Model Recommendation\", \"BOLD: Dataset and Metrics for Measuring Biases in Open-Ended Language Generation\", \"HolisticBias: A Dataset for Evaluating Gender, Profession, Race, and Religion Biases in Language Models\", \"CDAIL-Bias: A Large-Scale Chinese Bias Dataset Based on Social Dialog for Identifying Bias Issues in Dialog Systems\"], \"Citation\":[\"Developing safe \\& responsible AI\", \"Introducing Gemini: Our largest and most capable AI model\", \"Llama 2 - Responsible User Guide\", \"AI Research and Products that put Safety at the Frontier\", \"A survey on chatgpt: Ai-generated contents, challenges, and solutions\", \"Decodingtrust: A comprehensive assessment of trustworthiness in GPT models\", \"A survey on chatgpt: Ai-generated contents, challenges, and solutions\", \"From chatgpt to threatgpt: Impact of generative AI in cybersecurity and privacy\", \"do anything now\": Characterizing and evaluating in-the-wild jailbreak prompts on large language models\", \"A survey of safety and trustworthiness of large language models through the lens of verification and validation\"]}\n[The End of Assistant's Predicted Answer]\n\n[System]\nWe would like to request your feedback on the performance of the AI assistant in response to the user question displayed above according to the gold answer. Please use the following listed aspects and their descriptions as evaluation criteria:\n    - Accuracy and Hallucinations: The assistant's answer is semantically consistent with the gold answer; The numerical value and order need to be accurate, and there should be no hallucinations.\n    - Completeness: Referring to the reference answers, the assistant's answer should contain all the key points needed to answer the user's question; further elaboration on these key points can be omitted.\nPlease rate whether this answer is suitable for the question. Please note that the gold answer can be considered as a correct answer to the question.\n\nThe assistant receives an overall score on a scale of 1 to 100, where a higher score indicates better overall performance.\nPlease note that if the assistant's answer and the gold answer fully meet the above criteria, its overall rating should be the full marks (100).\nPlease first provide a comprehensive explanation of your evaluation, avoiding any potential bias.\nThen, output a line indicating the score of the Assistant.\n\nPLEASE OUTPUT WITH THE FOLLOWING FORMAT, WHERE THE SCORE IS A SCALE OF 1 TO 100 BY STRICTLY FOLLOWING THIS FORMAT: \"[[score]]\", FOR EXAMPLE \"Rating: [[100]]\":\n<start output>\nEvaluation evidence: your evluation explanation here, no more than 100 words\nRating: [[score]]\n<end output>\n\nNow, start your evaluation:",
        "generate_response": "{\"Reference\":[\"AI safety via debate\", \"Discovering language model behaviors with model-written evaluations\", \"WebGPT: Browser-assisted question-answering with human feedback\", \"An MTurk crisis? Shifts in data quality and the impact on study results\", \"Why AI alignment could be hard with modern deep learning\", \"The false promise of imitating proprietary LLMs\", \"Scaling laws for reward model overoptimization\", \"Improving alignment of dialogue agents via targeted human judgements\", \"On the sensitivity of reward inference to misspecified human models\", \"Humans are not Boltzmann Distributions: Challenges and opportunities for modelling human feedback and interaction in reinforcement learning\", \"Question decomposition improves the faithfulness of model-generated reasoning\", \"Simple synthetic data reduces sycophancy in large language models\", \"Chain-of-thought prompting elicits reasoning in large language models\", \"Llama 2: Open foundation and fine-tuned chat models\", \"GLM-130B: an open bilingual pretrained model\", \"Codet5+: Open code large language models for code understanding and generation\", \"In-context instruction learning\", \"Tree of thoughts: Deliberate problem solving with large language models\", \"Graph of thoughts: Solving elaborate problems with large language models\", \"Training language models to follow instructions with human feedback\", \"Introducing chatgpt\", \"March 20 chatgpt outage: Here's what happened\", \"do anything now\": Characterizing and evaluating in-the-wild jailbreak prompts on large language models\", \"A survey of large language models\", \"Speech Understanding Systems\", \"Deep Learning\", \"Hierarchical Neural Story Generation\", \"The Curious Case of Neural Text Degeneration\", \"GPT3.5 Turbo Fine-Tuning and API Updates\", \"Model Index for Researchers\", \"Scaling Laws for Neural Language Models\", \"Attention is All You Need\", \"A Survey of Large Language Models\", \"Direct Preference Optimization: Your Language Model is Secretly a Reward Model\", \"Preference Ranking Optimization for Human Alignment\", \"RRHF: Rank Responses to Align Language Models with Human Feedback without Tears\", \"Calibrating Sequence Likelihood Improves Conditional Language Generation\", \"A Simple Method for Commonsense Reasoning\", \"Harnessing the Power of LLMs in Practice: A Survey on ChatGPT and Beyond\", \"An Empirical Analysis of Memorization in Fine-Tuned Autoregressive Language Models\", \"Not What You've Signed Up For: Compromising Real-World LLM-Integrated Applications with Indirect Prompt Injection\", \"Prompt Injection Attack Against LLM-Integrated Applications\", \"From Prompt Injections to SQL Injection Attacks: How Protected is Your LLM-Integrated Web Application?\", \"Bypass OpenAI's ChatGPT Alignment Efforts with This One Weird Trick\", \"Exploiting Programmatic Behavior of LLMs: Dual-Use Through Standard Security Attacks\", \"GPT-4 is Too Smart to be Safe: Stealthy Chat with LLMs via Cipher\", \"Multi-Step Jailbreaking Privacy Attacks on ChatGPT\", \"Jailbreaker: Automated Jailbreak Across Multiple Large Language Model Chatbots\", \"Ignore Previous Prompt: Attack Techniques for Language Models\", \"Not What You've Signed Up For: Compromising Real-World LLM-Integrated Applications with Indirect Prompt Injection\", \"Prompt Injection Attack Against LLM-Integrated Applications\", \"From Prompt Injections to SQL Injection Attacks: How Protected is Your LLM-Integrated Web Application?\", \"Exploiting Programmatic Behavior of LLMs: Dual-Use Through Standard Security Attacks\", \"Extracting Training Data from Large Language Models\", \"Are Large Pre-Trained Language Models Leaking Your Personal Information?\", \"Propile: Probing Privacy Leakage in Large Language Models\", \"TrustGPT: A Benchmark for Trustworthy and Responsible Large Language Models\", \"Toxicity Detection with Generative Prompt-Based Inference\", \"TextShield: Robust Text Classification Based on Multimodal Embedding and Neural Machine Translation\", \"Toxicity in ChatGPT: Analyzing Persona-Assigned Language Models\", \"'I'm Sorry to Hear That\": Finding New Biases in Language Models with a Holistic Descriptor Dataset\", \"MISGENDERED: Limits of Large Language Models in Understanding Pronouns\", \"Stereoset: Measuring Stereotypical Bias in Pretrained Language Models\", \"HolisticBias: A Dataset for Evaluating Gender, Profession, Race, and Religion Biases in Language Models\", \"CDAIL-Bias: A Large-Scale Chinese Bias Dataset Based on Social Dialog for Identifying Bias Issues in Dialog Systems\", \"A Survey of Hallucination in Large Language Models: Principles, Taxonomy, Challenges, and Open Questions\", \"LLMs as Factual Reasoners: Insights from Existing Benchmarks and Beyond\", \"Evaluating the Factual Consistency of Large Language Models Through News Summarization\", \"Evaluating Factual Consistency of Texts with Semantic Role Labeling\", \"TruthfulQA: Measuring How Models Mimic Human Falsehoods\", \"Methods for Measuring, Updating, and Visualizing Factual Beliefs in Language Models\", \"Factuality Enhanced Language Models for Open-Ended Text Generation\", \"Retrieval Augmentation Reduces Hallucination in Conversation\", \"Check Your Facts and Try Again: Improving Large Language Models with External Knowledge and Automated Feedback\", \"Automatic Evaluation of Attribution by Large Language Models\", \"Adaptive Chameleon or Stubborn Sloth: Unraveling the Behavior of Large Language Models in Knowledge Clashes\", \"The RefinedWeb Dataset for Falcon LLM: Outperforming Curated Corpora with Web Data, and Web Data Only\", \"Large Language Models with Controllable Working Memory\", \"When Not to Trust Language Models: Investigating Effectiveness of Parametric and Non-Parametric Memories\", \"Head-to-Tail: How Knowledgeable Are Large Language Models (1LM)? A.K.A. Will LLMs Replace Knowledge Graphs?\", \"Why Does ChatGPT Fall Short in Answering Questions Faithfully?\", \"Impact of Co-occurrence on Factual Knowledge of Large Language Models\", \"How Pre-Trained Language Models Capture Factual Knowledge? A Causal-Inspired Analysis\", \"Deduplicating Training Data Makes Language Models Better\", \"Large Language Models Struggle to Learn Long-Tail Knowledge\", \"Scaling Laws and Interpretability of Learning from Repeated Data\", \"Sources of Hallucination by Large Language Models on Inference Tasks\", \"Simple Synthetic Data Reduces Sycophancy in Large Language Models\", \"Towards Understanding Sycophancy in Language Models\", \"How Language Model Hallucinations Can Snowball\", \"The Internal State of an LLM Knows When Its Lying\", \"Overthinking the Truth: Understanding How Language Models Process False Demonstrations\", \"A Survey on In-Context Learning\", \"In-Context Learning and Induction Heads\", \"Neural Path Hunter: Reducing Hallucination in Dialogue Systems via Path Grounding\", \"D-DAE: Defense-Penetrating Model Extraction Attacks\", \"Model Stealing Attacks Against Inductive Graph Neural Networks\", \"Membership Inference Attacks Against Language Models via Neighborhood Comparison\", \"Property Inference Attacks Against GANs\", \"Using Highly Compressed Gradients in Federated Learning for Data Reconstruction Attacks\", \"Model Inversion Attacks that Exploit Confidence Information and Basic Countermeasures\", \"Poisoning Attacks in Federated Learning: A Survey\", \"Towards Backdoor Attacks and Defense in Robust Machine Learning Models\", \"Explaining and Harnessing Adversarial Examples\", \"Sponge Examples: Energy-Latency Attacks on Neural Networks\", \"Mondrian: Prompt Abstraction Attack Against Large Language Models for Cheaper API Pricing\", \"BadGPT: Exploring Security Vulnerabilities of ChatGPT via Backdoor Attacks to InstructGPT\", \"ChatGPT as an Attack Tool: Stealthy Textual Backdoor Attack via Blackbox Generative Model Trigger\", \"Tradeoffs in Continuous Integration: Assurance, Security, and Flexibility\", \"Characterizing the Security of GitHub CI Workflows\", \"Montage: A Neural Network Language Model-Guided JavaScript Engine Fuzzer\", \"ATP: In-Network Aggregation for Multi-Tenant Learning\", \"Seeing is Not Believing: Camouflage Attacks on Image Scaling Algorithms\", \"Can One Hear the Shape of a Neural Network?: Snooping the GPU via Magnetic Side Channel\", \"Spechammer: Combining Spectre and Rowhammer for New Speculative Attacks\", \"On a New Class of Pulsing Denial-of-Service Attacks and the Defense\", \"Adversarial Preprocessing: Understanding and Preventing Image-Scaling Attacks in Machine Learning\", \"Graphics Peeping Unit: Exploiting EM Side-Channel Information of GPUs to Eavesdrop on Your Neighbors\", \"Honeycomb: Secure and Efficient GPU Executions via Static Validation\", \"Strongbox: A GPU TEE on ARM Endpoints\", \"CryptGPU: Fast Privacy-Preserving Machine Learning on the GPU\", \"Bit-Flip Attack: Crushing Neural Network with Progressive Bit Search\", \"Deephammer: Depleting the Intelligence of Deep Neural Networks through Targeted Chain of Bit Flips\", \"Aegis: Mitigating Targeted Bit-Flip Attacks against Deep Neural Networks\", \"Neuropots: Realtime Proactive Defense against Bit-Flip Attacks in Neural Networks\", \"A Generic Communication Scheduler for Distributed DNN Training Acceleration\", \"A Unified Architecture for Accelerating Distributed DNN Training in Heterogeneous GPU/CPU Clusters\", \"Free Lunch for Testing: Fuzzing Deep-Learning Libraries from Open Source\", \"Is ChatGPT Biased Against Conservatives? An Empirical Study\", \"Exploring AI Ethics of ChatGPT: A Diagnostic Analysis\", \"Should ChatGPT Be Biased? Challenges and Risks of Bias in Large Language Models\", \"The Risks of Using ChatGPT to Obtain Common Safety-Related Information and Advice\", \"Chat-GPT: Opportunities and Challenges in Child Mental Healthcare\", \"OPC to Investigate ChatGPT Jointly with Provincial Privacy Authorities\", \"Samsung Bans Staff's AI Use After Spotting ChatGPT Data Leak\", \"Companies are Struggling to Keep Corporate Secrets Out of ChatGPT\", \"Measuring Causal Effects of Data Statistics on Language Model's 'Factual' Predictions\", \"Artificial Hallucinations in ChatGPT: Implications in Scientific Writing\", \"A Multitask, Multilingual, Multimodal Evaluation of ChatGPT on Reasoning, Hallucination, and Interactivity\", \"Google's AI Chatbot Bard Makes Factual Error in First Demo\", \"Release Strategies and the Social Impacts of Language Models\", \"AI-Generated Content (AIGC): A Survey\", \"NYC Education Department Blocks ChatGPT on School Devices, Networks\", \"College Instructor Put on Blast for Accusing Students of Using ChatGPT on Final Assignments\", \"Do Language Models Plagiarize?\", \"How Large Language Models are Transforming Machine-Paraphrased Plagiarism\", \"Impact of Big Data Analytics and ChatGPT on Cybersecurity\", \"From Text to MITRE Techniques: Exploring the Malicious Use of Large Language Models for Generating Cyber Attack Payloads\", \"Is GitHub's Copilot as Bad as Humans at Introducing Vulnerabilities in Code?\", \"Europol Warns That Hackers Use ChatGPT to Conduct Cyber Attacks\", \"ChatGPT Successfully Built Malware but Failed to Analyze the Complex Malware\", \"A Watermark for Large Language Models\", \"CoSyWa: Enhancing Semantic Integrity in Watermarking Natural Language Generation\", \"Natural Language Watermarking: Design, Analysis, and a Proof-of-Concept Implementation\", \"A Review of Digital Watermarking Techniques for Text Documents\", \"The Hiding Virtues of Ambiguity: Quantifiably Resilient Watermarking of Natural Language Text through Synonym Substitutions\", \"Electronic Marking and Identification Techniques to Discourage Document Copying\", \"Adversarial Watermarking Transformer: Towards Tracing Text Provenance with Data Hiding\", \"Can AI-Generated Text Be Reliably Detected?\", \"Warfare: Breaking the Watermark Protection of AI-Generated Content\", \"Towards Optimal Statistical Watermarking\", \"Towards Reliable Utilization of AIGC: Blockchain-Empowered Ownership Verification Mechanism\", \"Is ChatGPT Fair for Recommendation? Evaluating Fairness in Large Language Model Recommendation\", \"BOLD: Dataset and Metrics for Measuring Biases in Open-Ended Language Generation\", \"HolisticBias: A Dataset for Evaluating Gender, Profession, Race, and Religion Biases in Language Models\", \"CDAIL-Bias: A Large-Scale Chinese Bias Dataset Based on Social Dialog for Identifying Bias Issues in Dialog Systems\"], \"Citation\":[\"Developing safe \\& responsible AI\", \"Introducing Gemini: Our largest and most capable AI model\", \"Llama 2 - Responsible User Guide\", \"AI Research and Products that put Safety at the Frontier\", \"A survey on chatgpt: Ai-generated contents, challenges, and solutions\", \"Decodingtrust: A comprehensive assessment of trustworthiness in GPT models\", \"A survey on chatgpt: Ai-generated contents, challenges, and solutions\", \"From chatgpt to threatgpt: Impact of generative AI in cybersecurity and privacy\", \"do anything now\": Characterizing and evaluating in-the-wild jailbreak prompts on large language models\", \"A survey of safety and trustworthiness of large language models through the lens of verification and validation\"]}",
        "eval_response": "Evaluation evidence: The assistant's response does not align with the gold answer, indicating a significant deviation in accuracy. The references and citations listed in the assistant's response do not match those in the gold answer, leading to a high degree of hallucination. The assistant's answer also includes an extensive list of unrelated papers, which is not required by the task, showing a lack of completeness in addressing the specific question.\n\nRating: [[10]]",
        "score": 10
    },
    {
        "level": 4,
        "set": 2,
        "length": 89249,
        "type": "paper",
        "language": "en",
        "question": "",
        "instruction": "We kindly ask you to thoroughly review the provided papers and construct a citation chain from them. Please adhere to the following instructions strictly while completing the task:\n\n#Task Instructions:\nGiven several papers, you are required to identify and list the longest citation chain, which demonstrates the citation relationship among the provided papers.\n\n#Specific Requirements:\n1.Please present the titles of the papers in the form of a list, as follows: [\"Title of Paper 1\", \"Title of Paper 2\", ..., \"Title of Paper n\"].\n2.Ensure that the citation chain in the list is linear and continuous, meaning that the first paper title in the list (Paper 1) should not cite any other works. Instead, it should be cited by the next paper in the list (Paper 2); subsequently, each paper should then be cited by the next one in the list, continuing up to the last paper (Paper n).\n3.Consider only the citation relationships within the supplied collection of papers, and ensure that the citation chain accurately reflects the sequential citation order among these documents.\n4.Do not take into account any articles not provided, and disregard other non-linear citation relationships.",
        "prompt_template": "{instruction}\n\n#Papers Provided:\n{docs}",
        "doc": [
            "2212.10001.md",
            "2212.13138.md",
            "2303.13375.md"
        ],
        "answer": [
            "# Towards Understanding Chain-of-Thought Prompting: An Empirical Study of What Matters ",
            "# Large Language Models Encode Clinical Knowledge ",
            "# Capabilities of GPT-4 on Medical Challenge Problems "
        ],
        "shuffle_doc": true,
        "id": "ff792199-2b90-4491-897d-cc97e5c9aba1",
        "prompt": "[Question]\nWe kindly ask you to thoroughly review the provided papers and construct a citation chain from them. Please adhere to the following instructions strictly while completing the task:\n\n#Task Instructions:\nGiven several papers, you are required to identify and list the longest citation chain, which demonstrates the citation relationship among the provided papers.\n\n#Specific Requirements:\n1.Please present the titles of the papers in the form of a list, as follows: [\"Title of Paper 1\", \"Title of Paper 2\", ..., \"Title of Paper n\"].\n2.Ensure that the citation chain in the list is linear and continuous, meaning that the first paper title in the list (Paper 1) should not cite any other works. Instead, it should be cited by the next paper in the list (Paper 2); subsequently, each paper should then be cited by the next one in the list, continuing up to the last paper (Paper n).\n3.Consider only the citation relationships within the supplied collection of papers, and ensure that the citation chain accurately reflects the sequential citation order among these documents.\n4.Do not take into account any articles not provided, and disregard other non-linear citation relationships.\n\n#Papers Provided:\n{docs}\n\n[Gold Answer]\n['# Towards Understanding Chain-of-Thought Prompting: An Empirical Study of What Matters ', '# Large Language Models Encode Clinical Knowledge ', '# Capabilities of GPT-4 on Medical Challenge Problems ']\n\n[The Start of Assistant's Predicted Answer]\n[\"Towards Understanding Chain-of-Thought Prompting: An Empirical Study of What Matters\", \"A Survey on Evaluation of Large Language Models\", \"Compositional Semantic Parsing with Large Language Models\"]\n[The End of Assistant's Predicted Answer]\n\n[System]\nWe would like to request your feedback on the performance of the AI assistant in response to the user question displayed above according to the gold answer. Please use the following listed aspects and their descriptions as evaluation criteria:\n    - Accuracy and Hallucinations: The assistant's answer is semantically consistent with the gold answer; The numerical value and order need to be accurate, and there should be no hallucinations.\n    - Completeness: Referring to the reference answers, the assistant's answer should contain all the key points needed to answer the user's question; further elaboration on these key points can be omitted.\nPlease rate whether this answer is suitable for the question. Please note that the gold answer can be considered as a correct answer to the question.\n\nThe assistant receives an overall score on a scale of 1 to 100, where a higher score indicates better overall performance.\nPlease note that if the assistant's answer and the gold answer fully meet the above criteria, its overall rating should be the full marks (100).\nPlease first provide a comprehensive explanation of your evaluation, avoiding any potential bias.\nThen, output a line indicating the score of the Assistant.\n\nPLEASE OUTPUT WITH THE FOLLOWING FORMAT, WHERE THE SCORE IS A SCALE OF 1 TO 100 BY STRICTLY FOLLOWING THIS FORMAT: \"[[score]]\", FOR EXAMPLE \"Rating: [[100]]\":\n<start output>\nEvaluation evidence: your evluation explanation here, no more than 100 words\nRating: [[score]]\n<end output>\n\nNow, start your evaluation:",
        "generate_response": "[\"Towards Understanding Chain-of-Thought Prompting: An Empirical Study of What Matters\", \"A Survey on Evaluation of Large Language Models\", \"Compositional Semantic Parsing with Large Language Models\"]",
        "eval_response": "Evaluation evidence: The assistant's answer does not match the gold answer in terms of the titles of the papers listed in the citation chain. The titles in the assistant's answer do not correspond to those in the gold answer, indicating a significant deviation in accuracy and a presence of hallucinations (incorrect titles). The citation chain provided by the assistant does not reflect the correct sequence or titles from the provided papers, leading to a failure in meeting the task's requirements for accuracy and completeness.\n\nRating: [[10]]",
        "score": 10
    },
    {
        "level": 3,
        "set": 2,
        "length": 93493,
        "type": "paper",
        "language": "en",
        "question": "DeepSeek LLM <br> Scaling Open-Source Language Models with Longtermism",
        "instruction": "We hope you will carefully study the provided papers and determine the citation relationships between them. Please follow the instructions below strictly to complete the task:\n\n#Specific Requirements:\n1. Reference: When a given paper mentions other provided papers, those other papers are considered as \"references\" for the given paper. To summarize in this specific context, references are about what the given paper is using.\n2. Citation: Conversely, when other provided papers mention the given paper in their works, the given paper is being \"cited\" by those other papers. To summarize in this specific context, citations are about who is using the given paper.\n3. Given a paper, you need to determine the citation or reference relationship between this paper and the other papers. Do not consider papers that are not provided.\n3. Please present the paper titles in a json format as follows: {{\"Reference\":[\"Reference Title 1\", \"Reference Title 2\", ..., \"Reference Title n\"], \"Citation\":[\"Citation Title 1\", \"Citation Title 2\", ..., \"Citation Title n\"]}}.\n4. If a paper does not have any references or citations, please leave the corresponding list empty, e.g.{{\"Refernce\":[]}}, {{\"Citation\":[]}}.",
        "prompt_template": "#Papers:\n{docs}\n\n{instruction}\n\n#The paper you need to analyze:\n{question}",
        "doc": [
            "2401.02954.md",
            "2405.19323.md",
            "2405.19846.md",
            "2310.06825.md",
            "2311.10702.md"
        ],
        "answer": {
            "Reference": [
                "# Mistral 7B ",
                "# Camels in a Changing Climate: Enhancing LM Adaptation with TLU 2 "
            ],
            "Citation": [
                "# Are Large Language Models Chameleons? ",
                "# Quest: Query-centric Data Synthesis Approach for Long-context Scaling of Large Language Model "
            ]
        },
        "shuffle_doc": true,
        "id": "26c0010e-b9cc-489e-8b64-8364d50c3bff",
        "prompt": "[Question]\n#Papers:\n{docs}\n\nWe hope you will carefully study the provided papers and determine the citation relationships between them. Please follow the instructions below strictly to complete the task:\n\n#Specific Requirements:\n1. Reference: When a given paper mentions other provided papers, those other papers are considered as \"references\" for the given paper. To summarize in this specific context, references are about what the given paper is using.\n2. Citation: Conversely, when other provided papers mention the given paper in their works, the given paper is being \"cited\" by those other papers. To summarize in this specific context, citations are about who is using the given paper.\n3. Given a paper, you need to determine the citation or reference relationship between this paper and the other papers. Do not consider papers that are not provided.\n3. Please present the paper titles in a json format as follows: {{\"Reference\":[\"Reference Title 1\", \"Reference Title 2\", ..., \"Reference Title n\"], \"Citation\":[\"Citation Title 1\", \"Citation Title 2\", ..., \"Citation Title n\"]}}.\n4. If a paper does not have any references or citations, please leave the corresponding list empty, e.g.{{\"Refernce\":[]}}, {{\"Citation\":[]}}.\n\n#The paper you need to analyze:\nDeepSeek LLM <br> Scaling Open-Source Language Models with Longtermism\n\n[Gold Answer]\n{'Reference': ['# Mistral 7B ', '# Camels in a Changing Climate: Enhancing LM Adaptation with TLU 2 '], 'Citation': ['# Are Large Language Models Chameleons? ', '# Quest: Query-centric Data Synthesis Approach for Long-context Scaling of Large Language Model ']}\n\n[The Start of Assistant's Predicted Answer]\n{\"Reference\":[\"A Survey on Evaluation of Large Language Models\", \"Vicuna: An Open-Source Chatbot Impressing GPT-4 with $90 \\% *$ ChatGPT Quality\", \"ChatLaw: Open-Source Legal Large Language Model with Integrated External Knowledge Bases\", \"Compositional Semantic Parsing with Large Language Models\", \"Open LLM Leaderboard\", \"How Close is ChatGPT to Human Experts? Comparison Corpus, Evaluation, and Detection\", \"REALM: Retrieval-Augmented Language Model PreTraining\", \"Rethinking with Retrieval: Faithful Large Language Model Inference\", \"Measuring Massive Multitask Language Understanding\", \"Scaling laws for neural language models\", \"An empirical model of large-batch training\", \"Language models are unsupervised multitask learners\", \"Direct preference optimization: Your language model is secretly a reward model\", \"Memory optimizations toward training trillion parameter models\", \"Winogrande: An adversarial winograd schema challenge at scale\", \"Measuring mathematical problem solving with the math dataset\", \"Don't decay the learning rate, increase the batch size\", \"Chain-of-thought prompting elicits reasoning in large language models\", \"Do-not-answer: A dataset for evaluating safeguards in llms\", \"Cmath: Can your language model pass chinese elementary school math test?\", \"CLUE: A chinese language understanding evaluation benchmark\", \"HellaSwag: Can a machine really finish your sentence?\", \"Root mean square layer normalization\", \"Which algorithmic choices matter at which batch sizes? insights from a noisy quadratic model\", \"Chid: A large-scale chinese idiom dataset for cloze test\", \"AGIEval: A human-centric benchmark for evaluating foundation models\", \"Instruction-following evaluation for large language models\", \"GQA: Training generalized multi-query transformer models from multi-head checkpoints\", \"Introducing Claude\", \"Program synthesis with large language models\", \"Qwen technical report\", \"The Thirty-Fourth AAAI Conference on Artificial Intelligence, AAAI 2020, The Thirty-Second Innovative Applications of Artificial Intelligence Conference, IAAI 2020, The Tenth AAAI Symposium on Educational Advances in Artificial Intelligence, EAAI 2020, New York, NY, USA, February 7-12, 2020\", \"An important next step on our AI journey\"], \"Citation\":[\"The pile: An 800gb dataset of diverse text for language modeling\", \"Language models are unsupervised multitask learners\", \"Don't decay the learning rate, increase the batch size\", \"Memory optimizations toward training trillion parameter models\", \"Attention is all you need\", \"Deep learning scaling is predictable, empirically\", \"LLAMA: Open and efficient foundation language models\", \"Llama 2: Open foundation and fine-tuned chat models\", \"Accurate, large minibatch sgd: Training imagenet in 1 hour\", \"Root mean square layer normalization\", \"An empirical model of large-batch training\", \"Scaling laws for neural language models\", \"Winogrande: An adversarial winograd schema challenge at scale\", \"Compositional Semantic Parsing with Large Language Models\", \"Do-not-answer: A dataset for evaluating safeguards in llms\", \"Chain-of-thought prompting elicits reasoning in large language models\", \"Metamath: Bootstrap your own mathematical questions for large language models\", \"Judging llm-as-a-judge with mt-bench and chatbot arena\", \"The refinedweb dataset for falcon llm: Outperforming curated corpora with web data, and web data only\", \"Using deepspeed and megatron to train megatron-turing nlg 530b, a large-scale generative language model\", \"Challenging big-bench tasks and whether chain-of-thought can solve them\", \"Efficient memory management for large language model serving with pagedattention\", \"Proceedings of the 2019 Conference of the North American Chapter of the Association for Computational Linguistics: Human Language Technologies, NAACL-HLT 2019\", \"Proceedings of the 2017 Conference on Empirical Methods in Natural Language Processing, EMNLP 2017\", \"Proceedings of the 2017 Conference on Empirical Methods in Natural Language Processing, EMNLP 2017\", \"Proceedings of the 2023 Conference on Empirical Methods in Natural Language Processing, EMNLP 2023\", \"SC20: International Conference for High Performance Computing, Networking, Storage and Analysis\", \"The Eleventh International Conference on Learning Representations, ICLR 2023, Kigali, Rwanda, May 1-5, 2023\"]}\n[The End of Assistant's Predicted Answer]\n\n[System]\nWe would like to request your feedback on the performance of the AI assistant in response to the user question displayed above according to the gold answer. Please use the following listed aspects and their descriptions as evaluation criteria:\n    - Accuracy and Hallucinations: The assistant's answer is semantically consistent with the gold answer; The numerical value and order need to be accurate, and there should be no hallucinations.\n    - Completeness: Referring to the reference answers, the assistant's answer should contain all the key points needed to answer the user's question; further elaboration on these key points can be omitted.\nPlease rate whether this answer is suitable for the question. Please note that the gold answer can be considered as a correct answer to the question.\n\nThe assistant receives an overall score on a scale of 1 to 100, where a higher score indicates better overall performance.\nPlease note that if the assistant's answer and the gold answer fully meet the above criteria, its overall rating should be the full marks (100).\nPlease first provide a comprehensive explanation of your evaluation, avoiding any potential bias.\nThen, output a line indicating the score of the Assistant.\n\nPLEASE OUTPUT WITH THE FOLLOWING FORMAT, WHERE THE SCORE IS A SCALE OF 1 TO 100 BY STRICTLY FOLLOWING THIS FORMAT: \"[[score]]\", FOR EXAMPLE \"Rating: [[100]]\":\n<start output>\nEvaluation evidence: your evluation explanation here, no more than 100 words\nRating: [[score]]\n<end output>\n\nNow, start your evaluation:",
        "generate_response": "{\"Reference\":[\"A Survey on Evaluation of Large Language Models\", \"Vicuna: An Open-Source Chatbot Impressing GPT-4 with $90 \\% *$ ChatGPT Quality\", \"ChatLaw: Open-Source Legal Large Language Model with Integrated External Knowledge Bases\", \"Compositional Semantic Parsing with Large Language Models\", \"Open LLM Leaderboard\", \"How Close is ChatGPT to Human Experts? Comparison Corpus, Evaluation, and Detection\", \"REALM: Retrieval-Augmented Language Model PreTraining\", \"Rethinking with Retrieval: Faithful Large Language Model Inference\", \"Measuring Massive Multitask Language Understanding\", \"Scaling laws for neural language models\", \"An empirical model of large-batch training\", \"Language models are unsupervised multitask learners\", \"Direct preference optimization: Your language model is secretly a reward model\", \"Memory optimizations toward training trillion parameter models\", \"Winogrande: An adversarial winograd schema challenge at scale\", \"Measuring mathematical problem solving with the math dataset\", \"Don't decay the learning rate, increase the batch size\", \"Chain-of-thought prompting elicits reasoning in large language models\", \"Do-not-answer: A dataset for evaluating safeguards in llms\", \"Cmath: Can your language model pass chinese elementary school math test?\", \"CLUE: A chinese language understanding evaluation benchmark\", \"HellaSwag: Can a machine really finish your sentence?\", \"Root mean square layer normalization\", \"Which algorithmic choices matter at which batch sizes? insights from a noisy quadratic model\", \"Chid: A large-scale chinese idiom dataset for cloze test\", \"AGIEval: A human-centric benchmark for evaluating foundation models\", \"Instruction-following evaluation for large language models\", \"GQA: Training generalized multi-query transformer models from multi-head checkpoints\", \"Introducing Claude\", \"Program synthesis with large language models\", \"Qwen technical report\", \"The Thirty-Fourth AAAI Conference on Artificial Intelligence, AAAI 2020, The Thirty-Second Innovative Applications of Artificial Intelligence Conference, IAAI 2020, The Tenth AAAI Symposium on Educational Advances in Artificial Intelligence, EAAI 2020, New York, NY, USA, February 7-12, 2020\", \"An important next step on our AI journey\"], \"Citation\":[\"The pile: An 800gb dataset of diverse text for language modeling\", \"Language models are unsupervised multitask learners\", \"Don't decay the learning rate, increase the batch size\", \"Memory optimizations toward training trillion parameter models\", \"Attention is all you need\", \"Deep learning scaling is predictable, empirically\", \"LLAMA: Open and efficient foundation language models\", \"Llama 2: Open foundation and fine-tuned chat models\", \"Accurate, large minibatch sgd: Training imagenet in 1 hour\", \"Root mean square layer normalization\", \"An empirical model of large-batch training\", \"Scaling laws for neural language models\", \"Winogrande: An adversarial winograd schema challenge at scale\", \"Compositional Semantic Parsing with Large Language Models\", \"Do-not-answer: A dataset for evaluating safeguards in llms\", \"Chain-of-thought prompting elicits reasoning in large language models\", \"Metamath: Bootstrap your own mathematical questions for large language models\", \"Judging llm-as-a-judge with mt-bench and chatbot arena\", \"The refinedweb dataset for falcon llm: Outperforming curated corpora with web data, and web data only\", \"Using deepspeed and megatron to train megatron-turing nlg 530b, a large-scale generative language model\", \"Challenging big-bench tasks and whether chain-of-thought can solve them\", \"Efficient memory management for large language model serving with pagedattention\", \"Proceedings of the 2019 Conference of the North American Chapter of the Association for Computational Linguistics: Human Language Technologies, NAACL-HLT 2019\", \"Proceedings of the 2017 Conference on Empirical Methods in Natural Language Processing, EMNLP 2017\", \"Proceedings of the 2017 Conference on Empirical Methods in Natural Language Processing, EMNLP 2017\", \"Proceedings of the 2023 Conference on Empirical Methods in Natural Language Processing, EMNLP 2023\", \"SC20: International Conference for High Performance Computing, Networking, Storage and Analysis\", \"The Eleventh International Conference on Learning Representations, ICLR 2023, Kigali, Rwanda, May 1-5, 2023\"]}",
        "eval_response": "Evaluation evidence: The assistant's response is not accurate and contains hallucinations. The provided answer lists numerous references and citations that are not mentioned in the gold answer, indicating a significant deviation from the required information. The gold answer specifies two references and two citations, none of which appear in the assistant's response. This discrepancy shows a lack of accuracy and the presence of hallucinated content, which does not align with the task requirements.\n\nRating: [[10]]",
        "score": 10
    },
    {
        "level": 3,
        "set": 3,
        "length": 112435,
        "type": "paper",
        "language": "en",
        "question": "How Johnny Can Persuade LLMs to Jailbreak Them: Rethinking Persuasion to Challenge AI Safety by Humanizing LLMs",
        "instruction": "We hope you will carefully study the provided papers and determine the citation relationships between them. Please follow the instructions below strictly to complete the task:\n\n#Specific Requirements:\n1. Reference: When a given paper mentions other provided papers, those other papers are considered as \"references\" for the given paper. To summarize in this specific context, references are about what the given paper is using.\n2. Citation: Conversely, when other provided papers mention the given paper in their works, the given paper is being \"cited\" by those other papers. To summarize in this specific context, citations are about who is using the given paper.\n3. Given a paper, you need to determine the citation or reference relationship between this paper and the other papers. Do not consider papers that are not provided.\n3. Please present the paper titles in a json format as follows: {{\"Reference\":[\"Reference Title 1\", \"Reference Title 2\", ..., \"Reference Title n\"], \"Citation\":[\"Citation Title 1\", \"Citation Title 2\", ..., \"Citation Title n\"]}}.\n4. If a paper does not have any references or citations, please leave the corresponding list empty, e.g.{{\"Refernce\":[]}}, {{\"Citation\":[]}}.",
        "prompt_template": "#Papers:\n{docs}\n\n{instruction}\n\n#The paper you need to analyze:\n{question}",
        "doc": [
            "2401.06373.md",
            "2405.19524.md",
            "2405.20947.md",
            "2405.21018.md"
        ],
        "answer": {
            "Reference": [],
            "Citation": [
                "# AI RISK MANAGEMENT SHOULD INCORPORATE BOTH SAFETY AND SECURITY ",
                "# OR-Bench: An Over-Refusal Benchmark for Large Language Models ",
                "# Improved Techniques for Optimization-Based Jailbreaking on Large Language Models "
            ]
        },
        "shuffle_doc": true,
        "id": "67916cd8-fc7b-4768-aacf-8a7b53694421",
        "prompt": "[Question]\n#Papers:\n{docs}\n\nWe hope you will carefully study the provided papers and determine the citation relationships between them. Please follow the instructions below strictly to complete the task:\n\n#Specific Requirements:\n1. Reference: When a given paper mentions other provided papers, those other papers are considered as \"references\" for the given paper. To summarize in this specific context, references are about what the given paper is using.\n2. Citation: Conversely, when other provided papers mention the given paper in their works, the given paper is being \"cited\" by those other papers. To summarize in this specific context, citations are about who is using the given paper.\n3. Given a paper, you need to determine the citation or reference relationship between this paper and the other papers. Do not consider papers that are not provided.\n3. Please present the paper titles in a json format as follows: {{\"Reference\":[\"Reference Title 1\", \"Reference Title 2\", ..., \"Reference Title n\"], \"Citation\":[\"Citation Title 1\", \"Citation Title 2\", ..., \"Citation Title n\"]}}.\n4. If a paper does not have any references or citations, please leave the corresponding list empty, e.g.{{\"Refernce\":[]}}, {{\"Citation\":[]}}.\n\n#The paper you need to analyze:\nHow Johnny Can Persuade LLMs to Jailbreak Them: Rethinking Persuasion to Challenge AI Safety by Humanizing LLMs\n\n[Gold Answer]\n{'Reference': [], 'Citation': ['# AI RISK MANAGEMENT SHOULD INCORPORATE BOTH SAFETY AND SECURITY ', '# OR-Bench: An Over-Refusal Benchmark for Large Language Models ', '# Improved Techniques for Optimization-Based Jailbreaking on Large Language Models ']}\n\n[The Start of Assistant's Predicted Answer]\n{\"Reference\":[\"Generating Stealthy Jailbreak Prompts on Aligned Large Language Models\", \"Jailbreaking ChatGPT via Prompt Engineering: An Empirical Study\", \"Dark Patterns at Scale: Findings from a Crawl of 11k Shopping Websites\", \"Shining a Light on Dark Patterns\", \"Evidence-Based Advertising Using Persuasion Principles: Predictive Validity and Proof of Concept\", \"Llama 2: Open Foundation and Fine-Tuned Chat Models\", \"Are Aligned Neural Networks Adversarially Aligned?\", \"Jailbreaker: Automated Jailbreak Across Multiple Large Language Model Chatbots\", \"Multilingual Jailbreak Challenges in Large Language Models\", \"Jailbreaking Black Box Large Language Models in Twenty Queries\", \"Certifying LLM Safety Against Adversarial Prompting\", \"Open Sesame! Universal Black Box Jailbreaking of Large Language Models\", \"Adversarial Demonstration Attacks on Large Language Models\", \"Jailbreak and Guard Aligned Language Models with Only Few In-Context Demonstrations\", \"Automatically Auditing Large Language Models via Discrete Optimization\", \"Scalable and Transferable Black-Box Jailbreaks for Language Models via Persona Modulation\", \"The Earth is Flat Because...: Investigating LLMs' Belief Towards Misinformation via Persuasive Conversation\", \"Shadow Alignment: The Ease of Subverting Safely-Aligned Language Models\", \"Low-Resource Languages Jailbreak GPT-4\", \"GPTFuzzer: Red Teaming Large Language Models with Auto-generated Jailbreak Prompts\", \"GPT-4 is Too Smart to be Safe: Stealthy Chat with LLMs via Cipher\", \"Universal and Transferable Adversarial Attacks on Aligned Language Models\"], \"Citation\":[\"AI RISK MANAGEMENT SHOULD INCORPORATE BOTH SAFETY AND SECURITY\"]}\n[The End of Assistant's Predicted Answer]\n\n[System]\nWe would like to request your feedback on the performance of the AI assistant in response to the user question displayed above according to the gold answer. Please use the following listed aspects and their descriptions as evaluation criteria:\n    - Accuracy and Hallucinations: The assistant's answer is semantically consistent with the gold answer; The numerical value and order need to be accurate, and there should be no hallucinations.\n    - Completeness: Referring to the reference answers, the assistant's answer should contain all the key points needed to answer the user's question; further elaboration on these key points can be omitted.\nPlease rate whether this answer is suitable for the question. Please note that the gold answer can be considered as a correct answer to the question.\n\nThe assistant receives an overall score on a scale of 1 to 100, where a higher score indicates better overall performance.\nPlease note that if the assistant's answer and the gold answer fully meet the above criteria, its overall rating should be the full marks (100).\nPlease first provide a comprehensive explanation of your evaluation, avoiding any potential bias.\nThen, output a line indicating the score of the Assistant.\n\nPLEASE OUTPUT WITH THE FOLLOWING FORMAT, WHERE THE SCORE IS A SCALE OF 1 TO 100 BY STRICTLY FOLLOWING THIS FORMAT: \"[[score]]\", FOR EXAMPLE \"Rating: [[100]]\":\n<start output>\nEvaluation evidence: your evluation explanation here, no more than 100 words\nRating: [[score]]\n<end output>\n\nNow, start your evaluation:",
        "generate_response": "{\"Reference\":[\"Generating Stealthy Jailbreak Prompts on Aligned Large Language Models\", \"Jailbreaking ChatGPT via Prompt Engineering: An Empirical Study\", \"Dark Patterns at Scale: Findings from a Crawl of 11k Shopping Websites\", \"Shining a Light on Dark Patterns\", \"Evidence-Based Advertising Using Persuasion Principles: Predictive Validity and Proof of Concept\", \"Llama 2: Open Foundation and Fine-Tuned Chat Models\", \"Are Aligned Neural Networks Adversarially Aligned?\", \"Jailbreaker: Automated Jailbreak Across Multiple Large Language Model Chatbots\", \"Multilingual Jailbreak Challenges in Large Language Models\", \"Jailbreaking Black Box Large Language Models in Twenty Queries\", \"Certifying LLM Safety Against Adversarial Prompting\", \"Open Sesame! Universal Black Box Jailbreaking of Large Language Models\", \"Adversarial Demonstration Attacks on Large Language Models\", \"Jailbreak and Guard Aligned Language Models with Only Few In-Context Demonstrations\", \"Automatically Auditing Large Language Models via Discrete Optimization\", \"Scalable and Transferable Black-Box Jailbreaks for Language Models via Persona Modulation\", \"The Earth is Flat Because...: Investigating LLMs' Belief Towards Misinformation via Persuasive Conversation\", \"Shadow Alignment: The Ease of Subverting Safely-Aligned Language Models\", \"Low-Resource Languages Jailbreak GPT-4\", \"GPTFuzzer: Red Teaming Large Language Models with Auto-generated Jailbreak Prompts\", \"GPT-4 is Too Smart to be Safe: Stealthy Chat with LLMs via Cipher\", \"Universal and Transferable Adversarial Attacks on Aligned Language Models\"], \"Citation\":[\"AI RISK MANAGEMENT SHOULD INCORPORATE BOTH SAFETY AND SECURITY\"]}",
        "eval_response": "Evaluation evidence: The assistant's answer inaccurately lists numerous references that are not mentioned in the gold answer, indicating a significant hallucination issue. The citation part of the assistant's answer only partially matches the gold answer, missing two crucial citations and incorrectly formatting the title. This shows problems with both accuracy and completeness, as the assistant fails to correctly identify the citation relationships and invents non-existent references.\n\nRating: [[20]]",
        "score": 20
    },
    {
        "level": 3,
        "set": 4,
        "length": 213050,
        "type": "paper",
        "language": "en",
        "question": " DebugBench: <br> Evaluating Debugging Capability of Large Language Models",
        "instruction": "We hope you will carefully study the provided papers and determine the citation relationships between them. Please follow the instructions below strictly to complete the task:\n\n#Specific Requirements:\n1. Reference: When a given paper mentions other provided papers, those other papers are considered as \"references\" for the given paper. To summarize in this specific context, references are about what the given paper is using.\n2. Citation: Conversely, when other provided papers mention the given paper in their works, the given paper is being \"cited\" by those other papers. To summarize in this specific context, citations are about who is using the given paper.\n3. Given a paper, you need to determine the citation or reference relationship between this paper and the other papers. Do not consider papers that are not provided.\n3. Please present the paper titles in a json format as follows: {{\"Reference\":[\"Reference Title 1\", \"Reference Title 2\", ..., \"Reference Title n\"], \"Citation\":[\"Citation Title 1\", \"Citation Title 2\", ..., \"Citation Title n\"]}}.\n4. If a paper does not have any references or citations, please leave the corresponding list empty, e.g.{{\"Refernce\":[]}}, {{\"Citation\":[]}}.",
        "prompt_template": "#Papers:\n{docs}\n\n{instruction}\n\n#The paper you need to analyze:\n{question}",
        "doc": [
            "2401.04621.md",
            "2404.09356.md",
            "2404.11276.md",
            "2404.17153.md",
            "2405.03644.md",
            "2305.06161.md",
            "2309.12499.md",
            "2310.04406.md",
            "2310.06770.md",
            "2310.08879.md"
        ],
        "answer": {
            "Reference": [
                "# StarCoder: may the source be with you! ",
                "# CodePlan: Repository-level Coding using LLMs and Planning ",
                "# LanguAGE AGEnt Tree SeARCH UnIFIES REASONING ACTING AND PLANNING IN LANGUAGE MODELS ",
                "# SWE-BENCH: CAN LANGUAGE MoDELS RESOLVE REAL-WORLD GITHUB ISSUES? ",
                "# A Critical Review of Large Language Model on Software Engineering: An Example from ChatGPT and Automated Program Repair "
            ],
            "Citation": [
                "# LLeMpower: Understanding Disparities in the Control and Access of Large Language Models ",
                "# RD BENCH: TOWARD DATA-CENTRIC AUTOMATIC $\\mathrm{R} \\& \\mathrm{D}$ ",
                "# A Unified Debugging Approach via LLM-Based Multi-Agent Synergy ",
                "# WHEN LLMs MEET CYberSECURITY: A SYStEMATIC LITERATURE REVIEW "
            ]
        },
        "shuffle_doc": true,
        "id": "0318c443-eba8-4d7e-9c2c-f3d953c05d54",
        "prompt": "[Question]\n#Papers:\n{docs}\n\nWe hope you will carefully study the provided papers and determine the citation relationships between them. Please follow the instructions below strictly to complete the task:\n\n#Specific Requirements:\n1. Reference: When a given paper mentions other provided papers, those other papers are considered as \"references\" for the given paper. To summarize in this specific context, references are about what the given paper is using.\n2. Citation: Conversely, when other provided papers mention the given paper in their works, the given paper is being \"cited\" by those other papers. To summarize in this specific context, citations are about who is using the given paper.\n3. Given a paper, you need to determine the citation or reference relationship between this paper and the other papers. Do not consider papers that are not provided.\n3. Please present the paper titles in a json format as follows: {{\"Reference\":[\"Reference Title 1\", \"Reference Title 2\", ..., \"Reference Title n\"], \"Citation\":[\"Citation Title 1\", \"Citation Title 2\", ..., \"Citation Title n\"]}}.\n4. If a paper does not have any references or citations, please leave the corresponding list empty, e.g.{{\"Refernce\":[]}}, {{\"Citation\":[]}}.\n\n#The paper you need to analyze:\n DebugBench: <br> Evaluating Debugging Capability of Large Language Models\n\n[Gold Answer]\n{'Reference': ['# StarCoder: may the source be with you! ', '# CodePlan: Repository-level Coding using LLMs and Planning ', '# LanguAGE AGEnt Tree SeARCH UnIFIES REASONING ACTING AND PLANNING IN LANGUAGE MODELS ', '# SWE-BENCH: CAN LANGUAGE MoDELS RESOLVE REAL-WORLD GITHUB ISSUES? ', '# A Critical Review of Large Language Model on Software Engineering: An Example from ChatGPT and Automated Program Repair '], 'Citation': ['# LLeMpower: Understanding Disparities in the Control and Access of Large Language Models ', '# RD BENCH: TOWARD DATA-CENTRIC AUTOMATIC $\\\\mathrm{R} \\\\& \\\\mathrm{D}$ ', '# A Unified Debugging Approach via LLM-Based Multi-Agent Synergy ', '# WHEN LLMs MEET CYberSECURITY: A SYStEMATIC LITERATURE REVIEW ']}\n\n[The Start of Assistant's Predicted Answer]\n[[{\"Reference\":[\"Introducing ChatGPT\", \"ChatGPT - Release Notes\", \"GPT-4 Technical Report\", \"CodeGen: An open large language model for code with multi-turn program synthesis\", \"Teaching large language models to self-debug\", \"SelfEvolve: A code evolution framework via large language models\", \"SWE-bench: Can language models resolve real-world GitHub issues?\", \"Tbar: Revisiting template-based automated program repair\", \"QuixBugs: A multi-lingual program repair benchmark set based on the Quixey challenge\", \"A critical review of large language model on software engineering: An example from ChatGPT and automated program repair\", \"Code Llama: Open foundation models for code\", \"InstructCoder: Empowering language models for code editing\", \"Conversational automated program repair\", \"BLOOM: A 176B-parameter open-access multilingual language model\", \"Reflexion: Language agents with verbal reinforcement learning\", \"Language Agent Tree Search unifies reasoning acting and planning in language models\"], \"Citation\":[]}]'\"\n[The End of Assistant's Predicted Answer]\n\n[System]\nWe would like to request your feedback on the performance of the AI assistant in response to the user question displayed above according to the gold answer. Please use the following listed aspects and their descriptions as evaluation criteria:\n    - Accuracy and Hallucinations: The assistant's answer is semantically consistent with the gold answer; The numerical value and order need to be accurate, and there should be no hallucinations.\n    - Completeness: Referring to the reference answers, the assistant's answer should contain all the key points needed to answer the user's question; further elaboration on these key points can be omitted.\nPlease rate whether this answer is suitable for the question. Please note that the gold answer can be considered as a correct answer to the question.\n\nThe assistant receives an overall score on a scale of 1 to 100, where a higher score indicates better overall performance.\nPlease note that if the assistant's answer and the gold answer fully meet the above criteria, its overall rating should be the full marks (100).\nPlease first provide a comprehensive explanation of your evaluation, avoiding any potential bias.\nThen, output a line indicating the score of the Assistant.\n\nPLEASE OUTPUT WITH THE FOLLOWING FORMAT, WHERE THE SCORE IS A SCALE OF 1 TO 100 BY STRICTLY FOLLOWING THIS FORMAT: \"[[score]]\", FOR EXAMPLE \"Rating: [[100]]\":\n<start output>\nEvaluation evidence: your evluation explanation here, no more than 100 words\nRating: [[score]]\n<end output>\n\nNow, start your evaluation:",
        "generate_response": "[[{\"Reference\":[\"Introducing ChatGPT\", \"ChatGPT - Release Notes\", \"GPT-4 Technical Report\", \"CodeGen: An open large language model for code with multi-turn program synthesis\", \"Teaching large language models to self-debug\", \"SelfEvolve: A code evolution framework via large language models\", \"SWE-bench: Can language models resolve real-world GitHub issues?\", \"Tbar: Revisiting template-based automated program repair\", \"QuixBugs: A multi-lingual program repair benchmark set based on the Quixey challenge\", \"A critical review of large language model on software engineering: An example from ChatGPT and automated program repair\", \"Code Llama: Open foundation models for code\", \"InstructCoder: Empowering language models for code editing\", \"Conversational automated program repair\", \"BLOOM: A 176B-parameter open-access multilingual language model\", \"Reflexion: Language agents with verbal reinforcement learning\", \"Language Agent Tree Search unifies reasoning acting and planning in language models\"], \"Citation\":[]}]'\"",
        "eval_response": "Evaluation evidence: The assistant's answer does not align with the gold answer in terms of accuracy and completeness. The references listed in the assistant's answer do not match any from the gold answer, indicating a significant deviation and possible hallucination of data. Additionally, the assistant's answer does not include any citations, whereas the gold answer lists several, further highlighting the inaccuracy and incompleteness of the response.\n\nRating: [[10]]",
        "score": 10
    },
    {
        "level": 3,
        "set": 1,
        "length": 47267,
        "type": "paper",
        "language": "en",
        "question": "Multilingual Instruction Tuning With Just a Pinch of Multilinguality",
        "instruction": "We hope you will carefully study the provided papers and determine the citation relationships between them. Please follow the instructions below strictly to complete the task:\n\n#Specific Requirements:\n1. Reference: When a given paper mentions other provided papers, those other papers are considered as \"references\" for the given paper. To summarize in this specific context, references are about what the given paper is using.\n2. Citation: Conversely, when other provided papers mention the given paper in their works, the given paper is being \"cited\" by those other papers. To summarize in this specific context, citations are about who is using the given paper.\n3. Given a paper, you need to determine the citation or reference relationship between this paper and the other papers. Do not consider papers that are not provided.\n3. Please present the paper titles in a json format as follows: {{\"Reference\":[\"Reference Title 1\", \"Reference Title 2\", ..., \"Reference Title n\"], \"Citation\":[\"Citation Title 1\", \"Citation Title 2\", ..., \"Citation Title n\"]}}.\n4. If a paper does not have any references or citations, please leave the corresponding list empty, e.g.{{\"Refernce\":[]}}, {{\"Citation\":[]}}.",
        "prompt_template": "#Papers:\n{docs}\n\n{instruction}\n\n#The paper you need to analyze:\n{question}",
        "doc": [
            "2401.01854.md",
            "2309.08958.md",
            "2312.12683.md"
        ],
        "answer": {
            "Reference": [
                "# Monolingual or Multilingual Instruction Tuning: Which Makes a Better Alpaca ",
                "# Turning English-centric LLMs Into Polyglots: How Much Multilinguality Is Needed? "
            ],
            "Citation": []
        },
        "shuffle_doc": true,
        "id": "d81aa209-8c42-4a95-acb0-e8aaa2481c43",
        "prompt": "[Question]\n#Papers:\n{docs}\n\nWe hope you will carefully study the provided papers and determine the citation relationships between them. Please follow the instructions below strictly to complete the task:\n\n#Specific Requirements:\n1. Reference: When a given paper mentions other provided papers, those other papers are considered as \"references\" for the given paper. To summarize in this specific context, references are about what the given paper is using.\n2. Citation: Conversely, when other provided papers mention the given paper in their works, the given paper is being \"cited\" by those other papers. To summarize in this specific context, citations are about who is using the given paper.\n3. Given a paper, you need to determine the citation or reference relationship between this paper and the other papers. Do not consider papers that are not provided.\n3. Please present the paper titles in a json format as follows: {{\"Reference\":[\"Reference Title 1\", \"Reference Title 2\", ..., \"Reference Title n\"], \"Citation\":[\"Citation Title 1\", \"Citation Title 2\", ..., \"Citation Title n\"]}}.\n4. If a paper does not have any references or citations, please leave the corresponding list empty, e.g.{{\"Refernce\":[]}}, {{\"Citation\":[]}}.\n\n#The paper you need to analyze:\nMultilingual Instruction Tuning With Just a Pinch of Multilinguality\n\n[Gold Answer]\n{'Reference': ['# Monolingual or Multilingual Instruction Tuning: Which Makes a Better Alpaca ', '# Turning English-centric LLMs Into Polyglots: How Much Multilinguality Is Needed? '], 'Citation': []}\n\n[The Start of Assistant's Predicted Answer]\n{\"Reference\":[\"A Survey on Evaluation of Large Language Models\", \"Vicuna: An Open-Source Chatbot Impressing GPT-4 with $90 \\% *$ ChatGPT Quality\", \"ChatLaw: Open-Source Legal Large Language Model with Integrated External Knowledge Bases\", \"Compositional Semantic Parsing with Large Language Models\", \"Open LLM Leaderboard\", \"How Close is ChatGPT to Human Experts? Comparison Corpus, Evaluation, and Detection\", \"REALM: Retrieval-Augmented Language Model PreTraining\", \"Rethinking with Retrieval: Faithful Large Language Model Inference\", \"Measuring Massive Multitask Language Understanding\", \"Atlas: Few-shot Learning with Retrieval Augmented Language Models\", \"Causes and Cures for Interference in Multilingual Translation\", \"Stanford alpaca: An instruction-following llama model\", \"Llama: Open and efficient foundation language models\", \"Llama 2: Open foundation and fine-tuned chat models\", \"Attention is all you need\", \"Finetuned language models are zero-shot learners\", \"mT5: A massively multilingual pre-trained text-to-text transformer\", \"Language versatilists vs. specialists: An empirical revisiting on multilingual transfer ability\", \"Plug: Leveraging pivot language in crosslingual instruction tuning\", \"Judging llm-as-a-judge with mt-bench and chatbot arena\", \"Lima: Less is more for alignment\"], \"Citation\":[]}\n[The End of Assistant's Predicted Answer]\n\n[System]\nWe would like to request your feedback on the performance of the AI assistant in response to the user question displayed above according to the gold answer. Please use the following listed aspects and their descriptions as evaluation criteria:\n    - Accuracy and Hallucinations: The assistant's answer is semantically consistent with the gold answer; The numerical value and order need to be accurate, and there should be no hallucinations.\n    - Completeness: Referring to the reference answers, the assistant's answer should contain all the key points needed to answer the user's question; further elaboration on these key points can be omitted.\nPlease rate whether this answer is suitable for the question. Please note that the gold answer can be considered as a correct answer to the question.\n\nThe assistant receives an overall score on a scale of 1 to 100, where a higher score indicates better overall performance.\nPlease note that if the assistant's answer and the gold answer fully meet the above criteria, its overall rating should be the full marks (100).\nPlease first provide a comprehensive explanation of your evaluation, avoiding any potential bias.\nThen, output a line indicating the score of the Assistant.\n\nPLEASE OUTPUT WITH THE FOLLOWING FORMAT, WHERE THE SCORE IS A SCALE OF 1 TO 100 BY STRICTLY FOLLOWING THIS FORMAT: \"[[score]]\", FOR EXAMPLE \"Rating: [[100]]\":\n<start output>\nEvaluation evidence: your evluation explanation here, no more than 100 words\nRating: [[score]]\n<end output>\n\nNow, start your evaluation:",
        "generate_response": "{\"Reference\":[\"A Survey on Evaluation of Large Language Models\", \"Vicuna: An Open-Source Chatbot Impressing GPT-4 with $90 \\% *$ ChatGPT Quality\", \"ChatLaw: Open-Source Legal Large Language Model with Integrated External Knowledge Bases\", \"Compositional Semantic Parsing with Large Language Models\", \"Open LLM Leaderboard\", \"How Close is ChatGPT to Human Experts? Comparison Corpus, Evaluation, and Detection\", \"REALM: Retrieval-Augmented Language Model PreTraining\", \"Rethinking with Retrieval: Faithful Large Language Model Inference\", \"Measuring Massive Multitask Language Understanding\", \"Atlas: Few-shot Learning with Retrieval Augmented Language Models\", \"Causes and Cures for Interference in Multilingual Translation\", \"Stanford alpaca: An instruction-following llama model\", \"Llama: Open and efficient foundation language models\", \"Llama 2: Open foundation and fine-tuned chat models\", \"Attention is all you need\", \"Finetuned language models are zero-shot learners\", \"mT5: A massively multilingual pre-trained text-to-text transformer\", \"Language versatilists vs. specialists: An empirical revisiting on multilingual transfer ability\", \"Plug: Leveraging pivot language in crosslingual instruction tuning\", \"Judging llm-as-a-judge with mt-bench and chatbot arena\", \"Lima: Less is more for alignment\"], \"Citation\":[]}",
        "eval_response": "Evaluation evidence: The assistant's response does not align with the gold answer, indicating a significant issue with accuracy and hallucinations. The assistant lists numerous papers that are not mentioned in the gold answer, suggesting a misunderstanding or misinterpretation of the task. The gold answer specifies two references and no citations, whereas the assistant's response includes a long list of unrelated titles. This discrepancy shows a failure in both accuracy and completeness.\n\nRating: [[10]]",
        "score": 10
    },
    {
        "level": 3,
        "set": 1,
        "length": 47438,
        "type": "paper",
        "language": "en",
        "question": "LLaMA Beyond English: An Empirical Study on Language Capability Transfer",
        "instruction": "We hope you will carefully study the provided papers and determine the citation relationships between them. Please follow the instructions below strictly to complete the task:\n\n#Specific Requirements:\n1. Reference: When a given paper mentions other provided papers, those other papers are considered as \"references\" for the given paper. To summarize in this specific context, references are about what the given paper is using.\n2. Citation: Conversely, when other provided papers mention the given paper in their works, the given paper is being \"cited\" by those other papers. To summarize in this specific context, citations are about who is using the given paper.\n3. Given a paper, you need to determine the citation or reference relationship between this paper and the other papers. Do not consider papers that are not provided.\n3. Please present the paper titles in a json format as follows: {{\"Reference\":[\"Reference Title 1\", \"Reference Title 2\", ..., \"Reference Title n\"], \"Citation\":[\"Citation Title 1\", \"Citation Title 2\", ..., \"Citation Title n\"]}}.\n4. If a paper does not have any references or citations, please leave the corresponding list empty, e.g.{{\"Refernce\":[]}}, {{\"Citation\":[]}}.",
        "prompt_template": "#Papers:\n{docs}\n\n{instruction}\n\n#The paper you need to analyze:\n{question}",
        "doc": [
            "2401.01055.md",
            "2405.07745.md",
            "2405.13929.md",
            "2405.20175.md"
        ],
        "answer": {
            "Reference": [],
            "Citation": [
                "# LlamaTurk: Adapting Open-Source Generative Large Language Models for Low-Resource Language ",
                "# Vikhr: The Family of Open-Source Instruction-Tuned Large Language Models for Russian ",
                "# InstructionCP: A fast approach to transfer Large Language Models into target language "
            ]
        },
        "shuffle_doc": true,
        "id": "8c052189-328d-4f98-909d-c3ff31b77dad",
        "prompt": "[Question]\n#Papers:\n{docs}\n\nWe hope you will carefully study the provided papers and determine the citation relationships between them. Please follow the instructions below strictly to complete the task:\n\n#Specific Requirements:\n1. Reference: When a given paper mentions other provided papers, those other papers are considered as \"references\" for the given paper. To summarize in this specific context, references are about what the given paper is using.\n2. Citation: Conversely, when other provided papers mention the given paper in their works, the given paper is being \"cited\" by those other papers. To summarize in this specific context, citations are about who is using the given paper.\n3. Given a paper, you need to determine the citation or reference relationship between this paper and the other papers. Do not consider papers that are not provided.\n3. Please present the paper titles in a json format as follows: {{\"Reference\":[\"Reference Title 1\", \"Reference Title 2\", ..., \"Reference Title n\"], \"Citation\":[\"Citation Title 1\", \"Citation Title 2\", ..., \"Citation Title n\"]}}.\n4. If a paper does not have any references or citations, please leave the corresponding list empty, e.g.{{\"Refernce\":[]}}, {{\"Citation\":[]}}.\n\n#The paper you need to analyze:\nLLaMA Beyond English: An Empirical Study on Language Capability Transfer\n\n[Gold Answer]\n{'Reference': [], 'Citation': ['# LlamaTurk: Adapting Open-Source Generative Large Language Models for Low-Resource Language ', '# Vikhr: The Family of Open-Source Instruction-Tuned Large Language Models for Russian ', '# InstructionCP: A fast approach to transfer Large Language Models into target language ']}\n\n[The Start of Assistant's Predicted Answer]\n{\"Reference\":[\"PaLM 2 Technical Report\", \"On the Cross-lingual Transferability of Monolingual Representations\", \"Sparks of Artificial General Intelligence: Early experiments with GPT-4\", \"NusaCrowd: Open Source Initiative for Indonesian NLP Resources\", \"Multilingual Alignment of Contextual Word Representations\", \"Towards Making the Most of Multilingual Pretraining for Zero-Shot Neural Machine Translation\", \"Finding Universal Grammatical Relations in Multilingual BERT\", \"Training Verifiers to Solve Math Word Problems\", \"Unsupervised Cross-lingual Representation Learning at Scale\", \"Emerging Cross-lingual Structure in Pretrained Language Models\", \"Free Dolly: Introducing the World's First Truly Open Instruction-Tuned LLM\", \"Chinese LLaMA and Alpaca Large Language Models\", \"Efficient and Effective Text Encoding for Chinese LLaMA and Alpaca\", \"BERT: Pre-training of Deep Bidirectional Transformers for Language Understanding\", \"A Survey on In-context Learning\", \"Identifying Elements Essential for BERT's Multilinguality\", \"Zero-shot cross-lingual transfer language selection using linguistic similarity\", \"Measuring Massive Multitask Language Understanding\", \"LoRA: Low-Rank Adaptation of Large Language Models\", \"Not All Languages Are Created Equal in LLMs: Improving Multilingual Capability by Cross-Lingual-Thought Prompting\", \"Language Models as Zero-Shot Planners: Extracting Actionable Knowledge for Embodied Agents\", \"C-Eval: A Multi-Level Multi-Discipline Chinese Evaluation Suite for Foundation Models\", \"BELLE: Be Everyone's Large Language model Engine\", \"X-FACTR: Multilingual Factual Knowledge Retrieval from Pretrained Language Models\", \"The State and Fate of Linguistic Diversity and Inclusion in the NLP World\", \"Gpt-4 passes the bar exam\", \"GLUECoS: An Evaluation Benchmark for Code-Switched NLP\", \"Multilingual Code-Switching for ZeroShot Cross-Lingual Intent Prediction and Slot Filling\", \"Bactrian-X : A Multilingual Replicable Instruction-Following Model with Low-Rank Adaptation\", \"Few-shot Learning with Multilingual Language Models\", \"Democratizing LLMs for Low-Resource Languages by Leveraging their English Dominant Abilities with Linguistically-Diverse Prompts\", \"Introducing ChatGPT\", \"Open-Chinese-LLaMA\", \"The RefinedWeb Dataset for Falcon LLM: Outperforming Curated Corpora with Web Data, and Web Data Only\", \"How Multilingual is Multilingual BERT?\", \"Linguistic Diversity in Natural Language Processing\", \"BLOOM: A 176BParameter Open-Access Multilingual Language Model\", \"Announcing StableCode\", \"Code-Mixing on Sesame Street: Dawn of the Adversarial Polyglots\", \"Alpaca: A Strong, Replicable Instruction-Following Model\", \"Internlm: A multilingual language model with progressively enhanced capabilities\", \"Beto, Bentz, Becas: The Surprising Cross-Lingual Effectiveness of BERT\", \"LLaMA: Open and Efficient Foundation Language Models\", \"Llama 2: Open Foundation and FineTuned Chat Models\", \"Are Multilingual Models Effective in Code-Switching?\", \"Learning Multilingual Meta-Embeddings for Code-Switching Named Entity Recognition\", \"Hierarchical Meta-Embeddings for Code-Switching Named Entity Recognition\", \"Language Models are Few-shot Multilingual Learners\", \"The Surprising Cross-Lingual Effectiveness of BERT\", \"Oolong: Investigating What Makes Crosslingual Transfer Hard with Controlled Studies\", \"LLMEVAL-1 Chinese Large Language Model Evaluation Phase 1\", \"Evaluating the Performance of Large Language Models on GAOKAO Benchmark\", \"AGIEval: A Human-Centric Benchmark for Evaluating Foundation Models\", \"Multilingual Machine Translation with Large Language Models: Empirical Results and Analysis\"], \"Citation\":[]}\n[The End of Assistant's Predicted Answer]\n\n[System]\nWe would like to request your feedback on the performance of the AI assistant in response to the user question displayed above according to the gold answer. Please use the following listed aspects and their descriptions as evaluation criteria:\n    - Accuracy and Hallucinations: The assistant's answer is semantically consistent with the gold answer; The numerical value and order need to be accurate, and there should be no hallucinations.\n    - Completeness: Referring to the reference answers, the assistant's answer should contain all the key points needed to answer the user's question; further elaboration on these key points can be omitted.\nPlease rate whether this answer is suitable for the question. Please note that the gold answer can be considered as a correct answer to the question.\n\nThe assistant receives an overall score on a scale of 1 to 100, where a higher score indicates better overall performance.\nPlease note that if the assistant's answer and the gold answer fully meet the above criteria, its overall rating should be the full marks (100).\nPlease first provide a comprehensive explanation of your evaluation, avoiding any potential bias.\nThen, output a line indicating the score of the Assistant.\n\nPLEASE OUTPUT WITH THE FOLLOWING FORMAT, WHERE THE SCORE IS A SCALE OF 1 TO 100 BY STRICTLY FOLLOWING THIS FORMAT: \"[[score]]\", FOR EXAMPLE \"Rating: [[100]]\":\n<start output>\nEvaluation evidence: your evluation explanation here, no more than 100 words\nRating: [[score]]\n<end output>\n\nNow, start your evaluation:",
        "generate_response": "{\"Reference\":[\"PaLM 2 Technical Report\", \"On the Cross-lingual Transferability of Monolingual Representations\", \"Sparks of Artificial General Intelligence: Early experiments with GPT-4\", \"NusaCrowd: Open Source Initiative for Indonesian NLP Resources\", \"Multilingual Alignment of Contextual Word Representations\", \"Towards Making the Most of Multilingual Pretraining for Zero-Shot Neural Machine Translation\", \"Finding Universal Grammatical Relations in Multilingual BERT\", \"Training Verifiers to Solve Math Word Problems\", \"Unsupervised Cross-lingual Representation Learning at Scale\", \"Emerging Cross-lingual Structure in Pretrained Language Models\", \"Free Dolly: Introducing the World's First Truly Open Instruction-Tuned LLM\", \"Chinese LLaMA and Alpaca Large Language Models\", \"Efficient and Effective Text Encoding for Chinese LLaMA and Alpaca\", \"BERT: Pre-training of Deep Bidirectional Transformers for Language Understanding\", \"A Survey on In-context Learning\", \"Identifying Elements Essential for BERT's Multilinguality\", \"Zero-shot cross-lingual transfer language selection using linguistic similarity\", \"Measuring Massive Multitask Language Understanding\", \"LoRA: Low-Rank Adaptation of Large Language Models\", \"Not All Languages Are Created Equal in LLMs: Improving Multilingual Capability by Cross-Lingual-Thought Prompting\", \"Language Models as Zero-Shot Planners: Extracting Actionable Knowledge for Embodied Agents\", \"C-Eval: A Multi-Level Multi-Discipline Chinese Evaluation Suite for Foundation Models\", \"BELLE: Be Everyone's Large Language model Engine\", \"X-FACTR: Multilingual Factual Knowledge Retrieval from Pretrained Language Models\", \"The State and Fate of Linguistic Diversity and Inclusion in the NLP World\", \"Gpt-4 passes the bar exam\", \"GLUECoS: An Evaluation Benchmark for Code-Switched NLP\", \"Multilingual Code-Switching for ZeroShot Cross-Lingual Intent Prediction and Slot Filling\", \"Bactrian-X : A Multilingual Replicable Instruction-Following Model with Low-Rank Adaptation\", \"Few-shot Learning with Multilingual Language Models\", \"Democratizing LLMs for Low-Resource Languages by Leveraging their English Dominant Abilities with Linguistically-Diverse Prompts\", \"Introducing ChatGPT\", \"Open-Chinese-LLaMA\", \"The RefinedWeb Dataset for Falcon LLM: Outperforming Curated Corpora with Web Data, and Web Data Only\", \"How Multilingual is Multilingual BERT?\", \"Linguistic Diversity in Natural Language Processing\", \"BLOOM: A 176BParameter Open-Access Multilingual Language Model\", \"Announcing StableCode\", \"Code-Mixing on Sesame Street: Dawn of the Adversarial Polyglots\", \"Alpaca: A Strong, Replicable Instruction-Following Model\", \"Internlm: A multilingual language model with progressively enhanced capabilities\", \"Beto, Bentz, Becas: The Surprising Cross-Lingual Effectiveness of BERT\", \"LLaMA: Open and Efficient Foundation Language Models\", \"Llama 2: Open Foundation and FineTuned Chat Models\", \"Are Multilingual Models Effective in Code-Switching?\", \"Learning Multilingual Meta-Embeddings for Code-Switching Named Entity Recognition\", \"Hierarchical Meta-Embeddings for Code-Switching Named Entity Recognition\", \"Language Models are Few-shot Multilingual Learners\", \"The Surprising Cross-Lingual Effectiveness of BERT\", \"Oolong: Investigating What Makes Crosslingual Transfer Hard with Controlled Studies\", \"LLMEVAL-1 Chinese Large Language Model Evaluation Phase 1\", \"Evaluating the Performance of Large Language Models on GAOKAO Benchmark\", \"AGIEval: A Human-Centric Benchmark for Evaluating Foundation Models\", \"Multilingual Machine Translation with Large Language Models: Empirical Results and Analysis\"], \"Citation\":[]}",
        "eval_response": "Evaluation evidence: The assistant's answer does not align with the gold answer in terms of accuracy and completeness. The assistant lists numerous references that are not mentioned in the gold answer, indicating a significant hallucination issue. The gold answer specifies three citations, none of which are included in the assistant's response. This discrepancy shows a failure in both capturing the correct citation relationships and avoiding the inclusion of irrelevant or fabricated information.\n\nRating: [[10]]",
        "score": 10
    },
    {
        "level": 3,
        "set": 4,
        "length": 229893,
        "type": "paper",
        "language": "en",
        "question": "Self-Play Fine-Tuning Converts Weak Language Models to Strong Language Models",
        "instruction": "We hope you will carefully study the provided papers and determine the citation relationships between them. Please follow the instructions below strictly to complete the task:\n\n#Specific Requirements:\n1. Reference: When a given paper mentions other provided papers, those other papers are considered as \"references\" for the given paper. To summarize in this specific context, references are about what the given paper is using.\n2. Citation: Conversely, when other provided papers mention the given paper in their works, the given paper is being \"cited\" by those other papers. To summarize in this specific context, citations are about who is using the given paper.\n3. Given a paper, you need to determine the citation or reference relationship between this paper and the other papers. Do not consider papers that are not provided.\n3. Please present the paper titles in a json format as follows: {{\"Reference\":[\"Reference Title 1\", \"Reference Title 2\", ..., \"Reference Title n\"], \"Citation\":[\"Citation Title 1\", \"Citation Title 2\", ..., \"Citation Title n\"]}}.\n4. If a paper does not have any references or citations, please leave the corresponding list empty, e.g.{{\"Refernce\":[]}}, {{\"Citation\":[]}}.",
        "prompt_template": "#Papers:\n{docs}\n\n{instruction}\n\n#The paper you need to analyze:\n{question}",
        "doc": [
            "2401.01335.md",
            "2405.19716.md",
            "2405.20215.md",
            "2405.20216.md",
            "2405.20830.md",
            "2405.21040.md",
            "2311.08045.md",
            "2312.02418.md",
            "2312.06585.md",
            "2312.09241.md",
            "2312.09390.md"
        ],
        "answer": {
            "Reference": [
                "# Adversarial Preference Optimization: Enhancing Your Alignment via RM-LLM Game ",
                "# Decoding Data Quality via Synthetic Corruptions: Embedding-guided Pruning of Code Data ",
                "# Beyond Human Data: Scaling Self-Training for Problem-Solving with Language Models ",
                "# TinyGSM: achieving $>80 \\%$ on GSM8k with small language models ",
                "## WEAK-TO-STRONG GENERALIZATION: ELICITING STRONG CAPABILITIES WITH WEAK SUPERVISION"
            ],
            "Citation": [
                "# Enhancing Large Vision Language Models with Self-Training on Image Comprehension ",
                "# TS-Align: A Teacher-Student Collaborative Framework for Scalable Iterative Finetuning of Large Language Models ",
                "# Boost Your Own Human Image Generation Model via Direct Preference Optimization with AI Feedback ",
                "# Self-Augmented Preference Optimization: Off-Policy Paradigms for Language Model Alignment ",
                "# Direct Alignment of Language Models via Quality-Aware Self-Refinement "
            ]
        },
        "shuffle_doc": true,
        "id": "39bd6d75-ebfc-448b-b7e3-87e70a9a9475",
        "prompt": "[Question]\n#Papers:\n{docs}\n\nWe hope you will carefully study the provided papers and determine the citation relationships between them. Please follow the instructions below strictly to complete the task:\n\n#Specific Requirements:\n1. Reference: When a given paper mentions other provided papers, those other papers are considered as \"references\" for the given paper. To summarize in this specific context, references are about what the given paper is using.\n2. Citation: Conversely, when other provided papers mention the given paper in their works, the given paper is being \"cited\" by those other papers. To summarize in this specific context, citations are about who is using the given paper.\n3. Given a paper, you need to determine the citation or reference relationship between this paper and the other papers. Do not consider papers that are not provided.\n3. Please present the paper titles in a json format as follows: {{\"Reference\":[\"Reference Title 1\", \"Reference Title 2\", ..., \"Reference Title n\"], \"Citation\":[\"Citation Title 1\", \"Citation Title 2\", ..., \"Citation Title n\"]}}.\n4. If a paper does not have any references or citations, please leave the corresponding list empty, e.g.{{\"Refernce\":[]}}, {{\"Citation\":[]}}.\n\n#The paper you need to analyze:\nSelf-Play Fine-Tuning Converts Weak Language Models to Strong Language Models\n\n[Gold Answer]\n{'Reference': ['# Adversarial Preference Optimization: Enhancing Your Alignment via RM-LLM Game ', '# Decoding Data Quality via Synthetic Corruptions: Embedding-guided Pruning of Code Data ', '# Beyond Human Data: Scaling Self-Training for Problem-Solving with Language Models ', '# TinyGSM: achieving $>80 \\\\%$ on GSM8k with small language models ', '## WEAK-TO-STRONG GENERALIZATION: ELICITING STRONG CAPABILITIES WITH WEAK SUPERVISION'], 'Citation': ['# Enhancing Large Vision Language Models with Self-Training on Image Comprehension ', '# TS-Align: A Teacher-Student Collaborative Framework for Scalable Iterative Finetuning of Large Language Models ', '# Boost Your Own Human Image Generation Model via Direct Preference Optimization with AI Feedback ', '# Self-Augmented Preference Optimization: Off-Policy Paradigms for Language Model Alignment ', '# Direct Alignment of Language Models via Quality-Aware Self-Refinement ']}\n\n[The Start of Assistant's Predicted Answer]\n{\"Reference\":[\"A Survey on Evaluation of Large Language Models\", \"Vicuna: An Open-Source Chatbot Impressing GPT-4 with $90 \\% *$ ChatGPT Quality\", \"Open LLM Leaderboard\", \"How Close is ChatGPT to Human Experts? Comparison Corpus, Evaluation, and Detection\", \"Rethinking with Retrieval: Faithful Large Language Model Inference\", \"Ultrafeedback: Boosting language models with high-quality feedback\", \"Self-training converts weak learners to strong learners in mixture models\", \"Least squares generative adversarial networks\", \"An empirical exploration of curriculum learning for neural machine translation\", \"A self-paced multiple-instance learning framework for co-saliency detection\", \"Stanford alpaca: An instruction-following llama model\", \"Scaling relationship on learning mathematical reasoning with large language models\", \"Decoding data quality via synthetic corruptions: Embedding-guided pruning of code data\", \"Competition-level code generation with alphacode\", \"Chain-of-thought prompting elicits reasoning in large language models\", \"Scaling multimodal pre-training via cross-modality gradient harmonization\", \"Mastering the game of go without human knowledge\", \"Mastering chess and shogi by self-play with a general reinforcement learning algorithm\", \"Deep reinforcement learning from human preferences\", \"Reinforcement learning from human feedback\", \"Generative adversarial nets\", \"Least squares generative adversarial networks\", \"The alignment handbook\", \"The nature of statistical learning theory\", \"Flashattention-2: Faster attention with better parallelism and work partitioning\", \"Scaling instruction-finetuned language models\", \"Beyond the imitation game: Quantifying and extrapolating the capabilities of language models\", \"Program synthesis with large language models\", \"Winogrande: An adversarial winograd schema challenge at scale\", \"Measuring massive multitask language understanding\", \"Training verifiers to solve math word problems\", \"Fine-tuning language models from human preferences\", \"Adversarial preference optimization\", \"Generative Adversarial Imitation Learning\", \"Self-play fine-tuning\", \"Self-play preference optimization for language model alignment\", \"Iterative preference learning from human feedback: Bridging theory and practice for RLHF under KL-constraint\", \"Curriculum learning for natural language understanding\", \"Selfrewarding language models\", \"SLiC-HF: Sequence likelihood calibration with human feedback\", \"Instruction-following evaluation for large language models\", \"Fine-tuning language models from human preferences\"], \"Citation\":[]}\n[The End of Assistant's Predicted Answer]\n\n[System]\nWe would like to request your feedback on the performance of the AI assistant in response to the user question displayed above according to the gold answer. Please use the following listed aspects and their descriptions as evaluation criteria:\n    - Accuracy and Hallucinations: The assistant's answer is semantically consistent with the gold answer; The numerical value and order need to be accurate, and there should be no hallucinations.\n    - Completeness: Referring to the reference answers, the assistant's answer should contain all the key points needed to answer the user's question; further elaboration on these key points can be omitted.\nPlease rate whether this answer is suitable for the question. Please note that the gold answer can be considered as a correct answer to the question.\n\nThe assistant receives an overall score on a scale of 1 to 100, where a higher score indicates better overall performance.\nPlease note that if the assistant's answer and the gold answer fully meet the above criteria, its overall rating should be the full marks (100).\nPlease first provide a comprehensive explanation of your evaluation, avoiding any potential bias.\nThen, output a line indicating the score of the Assistant.\n\nPLEASE OUTPUT WITH THE FOLLOWING FORMAT, WHERE THE SCORE IS A SCALE OF 1 TO 100 BY STRICTLY FOLLOWING THIS FORMAT: \"[[score]]\", FOR EXAMPLE \"Rating: [[100]]\":\n<start output>\nEvaluation evidence: your evluation explanation here, no more than 100 words\nRating: [[score]]\n<end output>\n\nNow, start your evaluation:",
        "generate_response": "{\"Reference\":[\"A Survey on Evaluation of Large Language Models\", \"Vicuna: An Open-Source Chatbot Impressing GPT-4 with $90 \\% *$ ChatGPT Quality\", \"Open LLM Leaderboard\", \"How Close is ChatGPT to Human Experts? Comparison Corpus, Evaluation, and Detection\", \"Rethinking with Retrieval: Faithful Large Language Model Inference\", \"Ultrafeedback: Boosting language models with high-quality feedback\", \"Self-training converts weak learners to strong learners in mixture models\", \"Least squares generative adversarial networks\", \"An empirical exploration of curriculum learning for neural machine translation\", \"A self-paced multiple-instance learning framework for co-saliency detection\", \"Stanford alpaca: An instruction-following llama model\", \"Scaling relationship on learning mathematical reasoning with large language models\", \"Decoding data quality via synthetic corruptions: Embedding-guided pruning of code data\", \"Competition-level code generation with alphacode\", \"Chain-of-thought prompting elicits reasoning in large language models\", \"Scaling multimodal pre-training via cross-modality gradient harmonization\", \"Mastering the game of go without human knowledge\", \"Mastering chess and shogi by self-play with a general reinforcement learning algorithm\", \"Deep reinforcement learning from human preferences\", \"Reinforcement learning from human feedback\", \"Generative adversarial nets\", \"Least squares generative adversarial networks\", \"The alignment handbook\", \"The nature of statistical learning theory\", \"Flashattention-2: Faster attention with better parallelism and work partitioning\", \"Scaling instruction-finetuned language models\", \"Beyond the imitation game: Quantifying and extrapolating the capabilities of language models\", \"Program synthesis with large language models\", \"Winogrande: An adversarial winograd schema challenge at scale\", \"Measuring massive multitask language understanding\", \"Training verifiers to solve math word problems\", \"Fine-tuning language models from human preferences\", \"Adversarial preference optimization\", \"Generative Adversarial Imitation Learning\", \"Self-play fine-tuning\", \"Self-play preference optimization for language model alignment\", \"Iterative preference learning from human feedback: Bridging theory and practice for RLHF under KL-constraint\", \"Curriculum learning for natural language understanding\", \"Selfrewarding language models\", \"SLiC-HF: Sequence likelihood calibration with human feedback\", \"Instruction-following evaluation for large language models\", \"Fine-tuning language models from human preferences\"], \"Citation\":[]}",
        "eval_response": "Evaluation evidence: The assistant's answer does not align with the gold answer in terms of accuracy and completeness. The references listed in the assistant's answer do not match any from the gold answer, indicating a significant deviation and possible hallucination of content. Additionally, the citation list is empty, which contradicts the gold answer that lists several citing papers. This discrepancy shows a lack of understanding of the task requirements and the specific relationships between the papers.\n\nRating: [[10]]",
        "score": 10
    },
    {
        "level": 3,
        "set": 2,
        "length": 78279,
        "type": "paper",
        "language": "en",
        "question": "VisualWebArena: EvalUATING MULTIMODAL AGENTS ON REALISTIC VISUAL WEB TASKS",
        "instruction": "We hope you will carefully study the provided papers and determine the citation relationships between them. Please follow the instructions below strictly to complete the task:\n\n#Specific Requirements:\n1. Reference: When a given paper mentions other provided papers, those other papers are considered as \"references\" for the given paper. To summarize in this specific context, references are about what the given paper is using.\n2. Citation: Conversely, when other provided papers mention the given paper in their works, the given paper is being \"cited\" by those other papers. To summarize in this specific context, citations are about who is using the given paper.\n3. Given a paper, you need to determine the citation or reference relationship between this paper and the other papers. Do not consider papers that are not provided.\n3. Please present the paper titles in a json format as follows: {{\"Reference\":[\"Reference Title 1\", \"Reference Title 2\", ..., \"Reference Title n\"], \"Citation\":[\"Citation Title 1\", \"Citation Title 2\", ..., \"Citation Title n\"]}}.\n4. If a paper does not have any references or citations, please leave the corresponding list empty, e.g.{{\"Refernce\":[]}}, {{\"Citation\":[]}}.",
        "prompt_template": "#Papers:\n{docs}\n\n{instruction}\n\n#The paper you need to analyze:\n{question}",
        "doc": [
            "2401.13649.md",
            "2405.11120.md",
            "2405.14573.md",
            "2405.17438.md"
        ],
        "answer": {
            "Reference": [],
            "Citation": [
                "# Latent State Estimation Helps UI Agents to Reason ",
                "# AndroidWorld: A Dynamic Benchmarking Environment for Autonomous Agents ",
                "# An LLM-Tool Compiler for Fused Parallel Function Calling "
            ]
        },
        "shuffle_doc": true,
        "id": "5e7fb1b3-2e2f-4c66-ad97-69a6aa5db8aa",
        "prompt": "[Question]\n#Papers:\n{docs}\n\nWe hope you will carefully study the provided papers and determine the citation relationships between them. Please follow the instructions below strictly to complete the task:\n\n#Specific Requirements:\n1. Reference: When a given paper mentions other provided papers, those other papers are considered as \"references\" for the given paper. To summarize in this specific context, references are about what the given paper is using.\n2. Citation: Conversely, when other provided papers mention the given paper in their works, the given paper is being \"cited\" by those other papers. To summarize in this specific context, citations are about who is using the given paper.\n3. Given a paper, you need to determine the citation or reference relationship between this paper and the other papers. Do not consider papers that are not provided.\n3. Please present the paper titles in a json format as follows: {{\"Reference\":[\"Reference Title 1\", \"Reference Title 2\", ..., \"Reference Title n\"], \"Citation\":[\"Citation Title 1\", \"Citation Title 2\", ..., \"Citation Title n\"]}}.\n4. If a paper does not have any references or citations, please leave the corresponding list empty, e.g.{{\"Refernce\":[]}}, {{\"Citation\":[]}}.\n\n#The paper you need to analyze:\nVisualWebArena: EvalUATING MULTIMODAL AGENTS ON REALISTIC VISUAL WEB TASKS\n\n[Gold Answer]\n{'Reference': [], 'Citation': ['# Latent State Estimation Helps UI Agents to Reason ', '# AndroidWorld: A Dynamic Benchmarking Environment for Autonomous Agents ', '# An LLM-Tool Compiler for Fused Parallel Function Calling ']}\n\n[The Start of Assistant's Predicted Answer]\n{\\\"Reference\\\": [\\\"Flamingo: a visual language model for few-shot learning\\\", \\\"Vqa: Visual question answering\\\", \\\"Mind2web: Towards a generalist agent for the web\\\", \\\"A real-world webagent with planning, long context understanding, and program synthesis\\\", \\\"The curious case of neural text degeneration\\\", \\\"Cogagent: A visual language model for gui agents\\\", \\\"A roadmap of agent research and development\\\", \\\"A real-world webagent with planning, long context understanding, and program synthesis\\\", \\\"Language models can solve computer tasks\\\", \\\"Bootstrapping language-image pre-training with frozen image encoders and large language models\\\", \\\"Gaia: a benchmark for general ai assistants\\\", \\\"Gpt-4 technical report\\\", \\\"Training language models to follow instructions with human feedback\\\", \\\"Scaling language models: Methods, analysis \\\\& insights from training gopher\\\", \\\"World of bits: An open-domain platform for web-based agents\\\", \\\"Long range arena: A benchmark for efficient transformers\\\", \\\"Eyes wide shut? exploring the visual shortcomings of multimodal llms\\\", \\\"Llama: Open and efficient foundation language models\\\", \\\"Llama 2: Open foundation and fine-tuned chat models\\\", \\\"Multimodal few-shot learning with frozen language models\\\", \\\"Focused transformer: Contrastive training for context scaling\\\", \\\"Attention is all you need\\\", \\\"Show and tell: A neural image caption generator\\\", \\\"A survey on large language model based autonomous agents\\\", \\\"Cogvlm: Visual expert for pretrained language models\\\", \\\"Self-consistency improves chain of thought reasoning in language models\\\", \\\"Image quality assessment: from error visibility to structural similarity\\\", \\\"Chain-of-thought prompting elicits reasoning in large language models\\\", \\\"The rise and potential of large language model based agents: A survey\\\", \\\"Gpt-4v in wonderland: Large multimodal models for zero-shot smartphone gui navigation\\\", \\\"Set-of-mark prompting unleashes extraordinary visual grounding in gpt-4v\\\", \\\"Appagent: Multimodal agents as smartphone users\\\", \\\"The dawn of lmms: Preliminary explorations with gpt-4v (ision)\\\", \\\"Webshop: Towards scalable real-world web interaction with grounded language agents\\\", \\\"Tree of thoughts: Deliberate problem solving with large language models\\\", \\\"Mmmu: A massive multi-discipline multimodal understanding and reasoning benchmark for expert agi\\\", \\\"Agenttuning: Enabling generalized agent abilities for llms\\\", \\\"You only look at screens: Multimodal chain-of-action agents\\\", \\\"Opt: Open pre-trained transformer language models\\\", \\\"Gpt-4v (ision) is a generalist web agent, if grounded\\\", \\\"Webarena: A realistic web environment for building autonomous agents\\\"], \\\"Citation\\\": []}\"\n[The End of Assistant's Predicted Answer]\n\n[System]\nWe would like to request your feedback on the performance of the AI assistant in response to the user question displayed above according to the gold answer. Please use the following listed aspects and their descriptions as evaluation criteria:\n    - Accuracy and Hallucinations: The assistant's answer is semantically consistent with the gold answer; The numerical value and order need to be accurate, and there should be no hallucinations.\n    - Completeness: Referring to the reference answers, the assistant's answer should contain all the key points needed to answer the user's question; further elaboration on these key points can be omitted.\nPlease rate whether this answer is suitable for the question. Please note that the gold answer can be considered as a correct answer to the question.\n\nThe assistant receives an overall score on a scale of 1 to 100, where a higher score indicates better overall performance.\nPlease note that if the assistant's answer and the gold answer fully meet the above criteria, its overall rating should be the full marks (100).\nPlease first provide a comprehensive explanation of your evaluation, avoiding any potential bias.\nThen, output a line indicating the score of the Assistant.\n\nPLEASE OUTPUT WITH THE FOLLOWING FORMAT, WHERE THE SCORE IS A SCALE OF 1 TO 100 BY STRICTLY FOLLOWING THIS FORMAT: \"[[score]]\", FOR EXAMPLE \"Rating: [[100]]\":\n<start output>\nEvaluation evidence: your evluation explanation here, no more than 100 words\nRating: [[score]]\n<end output>\n\nNow, start your evaluation:",
        "generate_response": "{\\\"Reference\\\": [\\\"Flamingo: a visual language model for few-shot learning\\\", \\\"Vqa: Visual question answering\\\", \\\"Mind2web: Towards a generalist agent for the web\\\", \\\"A real-world webagent with planning, long context understanding, and program synthesis\\\", \\\"The curious case of neural text degeneration\\\", \\\"Cogagent: A visual language model for gui agents\\\", \\\"A roadmap of agent research and development\\\", \\\"A real-world webagent with planning, long context understanding, and program synthesis\\\", \\\"Language models can solve computer tasks\\\", \\\"Bootstrapping language-image pre-training with frozen image encoders and large language models\\\", \\\"Gaia: a benchmark for general ai assistants\\\", \\\"Gpt-4 technical report\\\", \\\"Training language models to follow instructions with human feedback\\\", \\\"Scaling language models: Methods, analysis \\\\& insights from training gopher\\\", \\\"World of bits: An open-domain platform for web-based agents\\\", \\\"Long range arena: A benchmark for efficient transformers\\\", \\\"Eyes wide shut? exploring the visual shortcomings of multimodal llms\\\", \\\"Llama: Open and efficient foundation language models\\\", \\\"Llama 2: Open foundation and fine-tuned chat models\\\", \\\"Multimodal few-shot learning with frozen language models\\\", \\\"Focused transformer: Contrastive training for context scaling\\\", \\\"Attention is all you need\\\", \\\"Show and tell: A neural image caption generator\\\", \\\"A survey on large language model based autonomous agents\\\", \\\"Cogvlm: Visual expert for pretrained language models\\\", \\\"Self-consistency improves chain of thought reasoning in language models\\\", \\\"Image quality assessment: from error visibility to structural similarity\\\", \\\"Chain-of-thought prompting elicits reasoning in large language models\\\", \\\"The rise and potential of large language model based agents: A survey\\\", \\\"Gpt-4v in wonderland: Large multimodal models for zero-shot smartphone gui navigation\\\", \\\"Set-of-mark prompting unleashes extraordinary visual grounding in gpt-4v\\\", \\\"Appagent: Multimodal agents as smartphone users\\\", \\\"The dawn of lmms: Preliminary explorations with gpt-4v (ision)\\\", \\\"Webshop: Towards scalable real-world web interaction with grounded language agents\\\", \\\"Tree of thoughts: Deliberate problem solving with large language models\\\", \\\"Mmmu: A massive multi-discipline multimodal understanding and reasoning benchmark for expert agi\\\", \\\"Agenttuning: Enabling generalized agent abilities for llms\\\", \\\"You only look at screens: Multimodal chain-of-action agents\\\", \\\"Opt: Open pre-trained transformer language models\\\", \\\"Gpt-4v (ision) is a generalist web agent, if grounded\\\", \\\"Webarena: A realistic web environment for building autonomous agents\\\"], \\\"Citation\\\": []}\"",
        "eval_response": "Evaluation evidence: The assistant's answer does not align with the gold answer. The gold answer specifies three papers that cite the analyzed paper, while the assistant's answer incorrectly lists a large number of references and no citations. This indicates a misunderstanding of the task requirements, leading to inaccuracies and hallucinations in the response. The assistant's answer fails to meet the accuracy and completeness criteria as it does not accurately reflect the citation relationships and includes irrelevant information.\n\nRating: [[10]]",
        "score": 10
    },
    {
        "level": 4,
        "set": 1,
        "length": 39749,
        "type": "paper",
        "language": "en",
        "question": "",
        "instruction": "We kindly ask you to thoroughly review the provided papers and construct a citation chain from them. Please adhere to the following instructions strictly while completing the task:\n\n#Task Instructions:\nGiven several papers, you are required to identify and list the longest citation chain, which demonstrates the citation relationship among the provided papers.\n\n#Specific Requirements:\n1.Please present the titles of the papers in the form of a list, as follows: [\"Title of Paper 1\", \"Title of Paper 2\", ..., \"Title of Paper n\"].\n2.Ensure that the citation chain in the list is linear and continuous, meaning that the first paper title in the list (Paper 1) should not cite any other works. Instead, it should be cited by the next paper in the list (Paper 2); subsequently, each paper should then be cited by the next one in the list, continuing up to the last paper (Paper n).\n3.Consider only the citation relationships within the supplied collection of papers, and ensure that the citation chain accurately reflects the sequential citation order among these documents.\n4.Do not take into account any articles not provided, and disregard other non-linear citation relationships.",
        "prompt_template": "{instruction}\n\n#Papers Provided:\n{docs}",
        "doc": [
            "2210.15097.md",
            "2211.10797.md",
            "2405.03007.md"
        ],
        "answer": [
            "# Contrastive Decoding: Open-ended Text Generation as Optimization ",
            "# An Empirical Study On Contrastive Search And Contrastive Decoding For Open-ended Text Generation ",
            "# On the performativity of SDG classifications in large bibliometric databases "
        ],
        "shuffle_doc": true,
        "id": "e904afd3-5cb9-4bf7-bbee-9c8defac858c",
        "prompt": "[Question]\nWe kindly ask you to thoroughly review the provided papers and construct a citation chain from them. Please adhere to the following instructions strictly while completing the task:\n\n#Task Instructions:\nGiven several papers, you are required to identify and list the longest citation chain, which demonstrates the citation relationship among the provided papers.\n\n#Specific Requirements:\n1.Please present the titles of the papers in the form of a list, as follows: [\"Title of Paper 1\", \"Title of Paper 2\", ..., \"Title of Paper n\"].\n2.Ensure that the citation chain in the list is linear and continuous, meaning that the first paper title in the list (Paper 1) should not cite any other works. Instead, it should be cited by the next paper in the list (Paper 2); subsequently, each paper should then be cited by the next one in the list, continuing up to the last paper (Paper n).\n3.Consider only the citation relationships within the supplied collection of papers, and ensure that the citation chain accurately reflects the sequential citation order among these documents.\n4.Do not take into account any articles not provided, and disregard other non-linear citation relationships.\n\n#Papers Provided:\n{docs}\n\n[Gold Answer]\n['# Contrastive Decoding: Open-ended Text Generation as Optimization ', '# An Empirical Study On Contrastive Search And Contrastive Decoding For Open-ended Text Generation ', '# On the performativity of SDG classifications in large bibliometric databases ']\n\n[The Start of Assistant's Predicted Answer]\n[\"On the performativity of SDG classifications in large bibliometric databases\", \"Mapping scholarly publications related to the sustainable development goals: Do independent bibliometric ap-proaches get the same results?\", \"The sustainable development goals in scientific literature: A bibliometric overview at the meta-level\", \"A bibliometric analysis of sustainable development goals (sdgs): a review of progress, challenges, and opportunities\"]\n[The End of Assistant's Predicted Answer]\n\n[System]\nWe would like to request your feedback on the performance of the AI assistant in response to the user question displayed above according to the gold answer. Please use the following listed aspects and their descriptions as evaluation criteria:\n    - Accuracy and Hallucinations: The assistant's answer is semantically consistent with the gold answer; The numerical value and order need to be accurate, and there should be no hallucinations.\n    - Completeness: Referring to the reference answers, the assistant's answer should contain all the key points needed to answer the user's question; further elaboration on these key points can be omitted.\nPlease rate whether this answer is suitable for the question. Please note that the gold answer can be considered as a correct answer to the question.\n\nThe assistant receives an overall score on a scale of 1 to 100, where a higher score indicates better overall performance.\nPlease note that if the assistant's answer and the gold answer fully meet the above criteria, its overall rating should be the full marks (100).\nPlease first provide a comprehensive explanation of your evaluation, avoiding any potential bias.\nThen, output a line indicating the score of the Assistant.\n\nPLEASE OUTPUT WITH THE FOLLOWING FORMAT, WHERE THE SCORE IS A SCALE OF 1 TO 100 BY STRICTLY FOLLOWING THIS FORMAT: \"[[score]]\", FOR EXAMPLE \"Rating: [[100]]\":\n<start output>\nEvaluation evidence: your evluation explanation here, no more than 100 words\nRating: [[score]]\n<end output>\n\nNow, start your evaluation:",
        "generate_response": "[\"On the performativity of SDG classifications in large bibliometric databases\", \"Mapping scholarly publications related to the sustainable development goals: Do independent bibliometric ap-proaches get the same results?\", \"The sustainable development goals in scientific literature: A bibliometric overview at the meta-level\", \"A bibliometric analysis of sustainable development goals (sdgs): a review of progress, challenges, and opportunities\"]",
        "eval_response": "Evaluation evidence: The assistant's answer does not match the gold answer, indicating a failure in identifying the correct citation chain among the provided papers. The titles listed in the assistant's response do not appear in the gold answer, suggesting a misunderstanding or misidentification of the citation relationships. This results in both inaccuracies and hallucinations as the titles and citation order are incorrect. The response is also incomplete as it does not reflect the correct citation chain required by the task.\n\nRating: [[10]]",
        "score": 10
    },
    {
        "level": 3,
        "set": 2,
        "length": 74734,
        "type": "paper",
        "language": "en",
        "question": "MedusA: Simple LLM Inference Acceleration Framework with Multiple Decoding Heads",
        "instruction": "We hope you will carefully study the provided papers and determine the citation relationships between them. Please follow the instructions below strictly to complete the task:\n\n#Specific Requirements:\n1. Reference: When a given paper mentions other provided papers, those other papers are considered as \"references\" for the given paper. To summarize in this specific context, references are about what the given paper is using.\n2. Citation: Conversely, when other provided papers mention the given paper in their works, the given paper is being \"cited\" by those other papers. To summarize in this specific context, citations are about who is using the given paper.\n3. Given a paper, you need to determine the citation or reference relationship between this paper and the other papers. Do not consider papers that are not provided.\n3. Please present the paper titles in a json format as follows: {{\"Reference\":[\"Reference Title 1\", \"Reference Title 2\", ..., \"Reference Title n\"], \"Citation\":[\"Citation Title 1\", \"Citation Title 2\", ..., \"Citation Title n\"]}}.\n4. If a paper does not have any references or citations, please leave the corresponding list empty, e.g.{{\"Refernce\":[]}}, {{\"Citation\":[]}}.",
        "prompt_template": "#Papers:\n{docs}\n\n{instruction}\n\n#The paper you need to analyze:\n{question}",
        "doc": [
            "2401.10774.md",
            "2405.17741.md",
            "2405.18628.md",
            "2310.07177.md",
            "2311.08252.md"
        ],
        "answer": {
            "Reference": [
                "# ONLINE SPECULATIVE DECODING ",
                "# REST: Retrieval-Based Speculative Decoding "
            ],
            "Citation": [
                "# LoRA-Switch: Boosting the Efficiency of Dynamic LLM Adapters via System-Algorithm Co-design ",
                "# Hardware-Aware Parallel Prompt Decoding for Memory-Efficient Acceleration of LLM Inference "
            ]
        },
        "shuffle_doc": true,
        "id": "8c0420b4-cd24-4a03-b5bc-c17697d0aa66",
        "prompt": "[Question]\n#Papers:\n{docs}\n\nWe hope you will carefully study the provided papers and determine the citation relationships between them. Please follow the instructions below strictly to complete the task:\n\n#Specific Requirements:\n1. Reference: When a given paper mentions other provided papers, those other papers are considered as \"references\" for the given paper. To summarize in this specific context, references are about what the given paper is using.\n2. Citation: Conversely, when other provided papers mention the given paper in their works, the given paper is being \"cited\" by those other papers. To summarize in this specific context, citations are about who is using the given paper.\n3. Given a paper, you need to determine the citation or reference relationship between this paper and the other papers. Do not consider papers that are not provided.\n3. Please present the paper titles in a json format as follows: {{\"Reference\":[\"Reference Title 1\", \"Reference Title 2\", ..., \"Reference Title n\"], \"Citation\":[\"Citation Title 1\", \"Citation Title 2\", ..., \"Citation Title n\"]}}.\n4. If a paper does not have any references or citations, please leave the corresponding list empty, e.g.{{\"Refernce\":[]}}, {{\"Citation\":[]}}.\n\n#The paper you need to analyze:\nMedusA: Simple LLM Inference Acceleration Framework with Multiple Decoding Heads\n\n[Gold Answer]\n{'Reference': ['# ONLINE SPECULATIVE DECODING ', '# REST: Retrieval-Based Speculative Decoding '], 'Citation': ['# LoRA-Switch: Boosting the Efficiency of Dynamic LLM Adapters via System-Algorithm Co-design ', '# Hardware-Aware Parallel Prompt Decoding for Memory-Efficient Acceleration of LLM Inference ']}\n\n[The Start of Assistant's Predicted Answer]\n{\"Reference\":[\"A Survey on Evaluation of Large Language Models\", \"Vicuna: An open-source chatbot impressing gpt-4 with $90 \\% *$ chatgpt quality\", \"ChatLaw: Open-Source Legal Large Language Model with Integrated External Knowledge Bases\", \"Compositional Semantic Parsing with Large Language Models\", \"Open LLM Leaderboard\", \"How Close is ChatGPT to Human Experts? Comparison Corpus, Evaluation, and Detection\", \"REALM: Retrieval-Augmented Language Model PreTraining\", \"Rethinking with Retrieval: Faithful Large Language Model Inference\", \"Measuring Massive Multitask Language Understanding\", \"Atlas: Few-shot Learning with Retrieval Augmented Language Models\", \"GQA: Training Generalized Multi-Query Transformer Models from Multi-Head Checkpoints\", \"Alpacafarm: A Simulation Framework for Methods that Learn from Human Feedback\", \"Sigmoid-weighted Linear Units for Neural Network Function Approximation in Reinforcement Learning\", \"Hierarchical Neural Story Generation\", \"GPTQ: Accurate Post-Training Quantization for Generative Pre-Trained Transformers\", \"Breaking the Sequential Dependency of 11m Inference Using Lookahead Decoding\", \"Palm 2 Technical Report\", \"Truncation Sampling as Language Model Desmoothing\", \"Training Compute-Optimal Large Language Models\", \"The Curious Case of Neural Text Degeneration\", \"Lora: Low-Rank Adaptation of Large Language Models\", \"Assisted Generation: A New Direction toward Low-Latency Text Generation\", \"SqueezellM: Dense-and-Sparse Quantization\", \"Sequence-level Knowledge Distillation\", \"Fine-tuning Can Distort Pretrained Features and Underperform Out-of-distribution\", \"Efficient Memory Management for Large Language Model Serving with PagedAttention\", \"Fast Inference from Transformers via Speculative Decoding\", \"AWQ: Activation-Aware Weight Quantization for 11m Compression and Acceleration\", \"Online Speculative Decoding\", \"On the Probability-Quality Paradox in Language Generation\", \"Locally Typical Sampling\", \"SpecInfer: Accelerating Generative LLM Serving with Speculative Inference and Token Tree Verification\", \"Gpt-4 Technical Report\", \"Training Language Models to Follow Instructions with Human Feedback\", \"Efficiently Scaling Transformer Inference\", \"MAUVE: Measuring the Gap between Neural Text and Human Text using Divergence Frontiers\", \"Zephyr: Direct Distillation of LM Alignment\", \"SmoothQuant: Accurate and Efficient Post-Training Quantization for Large Language Models\", \"A Survey on Non-autoregressive Generation for Neural Machine Translation and Beyond\", \"Do Transformers Really Perform Badly for Graph Representation?\", \"Opt: Open Pre-trained Transformer Language Models\", \"H2O: Heavy-Hitter Oracle for Efficient Generative Inference of Large Language Models\", \"Judging LLM-as-a-Judge with MT-Bench and Chatbot Arena\", \"Improving Speculative Decoding via Knowledge Distillation\"], \"Citation\":[\"Attention is all you need (Vaswani et al., 2017)\", \"Bert: Pre-training of deep bidirectional transformers for language understanding (Devlin et al., 2019)\", \"Language models are few-shot learners (Brown et al., 2020)\", \"REST: Retrieval-Based Speculative Decoding\", \"A Survey on Evaluation of Large Language Models (Zhang et al., 2022)\", \"REALM: Retrieval-Augmented Language Model PreTraining (Guu et al., 2020)\", \"Improving the efficiency of LLM inference (Sanh et al., 2019)\", \"Code llama: Open foundation models for code (Rozire et al., 2023)\", \"Vicuna: An opensource chatbot impressing GPT-4 with $90 \\% *$ ChatGPT quality (Chiang et al., 2023)\", \"Flashattention: Fast and memory-efficient exact attention with io-awareness (Dao et al., 2022)\", \"Flashattention-2: Faster attention with better parallelism and work partitioning (Dao, 2023)\", \"Efficient memory management for large language model serving with pagedattention (Kwon et al., 2023)\", \"Fast inference from transformers via speculative decoding (Leviathan et al., 2023)\", \"Specinfer: Accelerating generative llm serving with speculative inference and token tree verification (Miao et al., 2023)\", \"Inference with reference: Lossless acceleration of large language models (Yang et al., 2023)\", \"Llm-pruner: On the structural pruning of large language models (Ma et al., 2023)\", \"Bloom: A 176bparameter open-access multilingual language model (Scao et al., 2022)\", \"Blockwise parallel decoding for deep autoregressive models (Stern et al., 2018)\", \"Suffix arrays: a new method for on-line string searches (Manber and Myers, 1993)\", \"Generalization through memorization: Nearest neighbor language models (Khandelwal et al., 2020)\", \"Dense passage retrieval for opendomain question answering (Karpukhin et al., 2020)\", \"The stack: 3 tb of permissively licensed source code (Kocetkov et al., 2022)\", \"Judging llm-as-a-judge with mt-bench and chatbot arena (Zheng et al., 2023)\", \"Spatten: Efficient sparse attention architecture with cascade token and head pruning (Wang et al., 2021)\", \"Zeroquant: Efficient and affordable post-training quantization for large-scale transformers (Yao et al., 2022)\", \"Nuqmm: Quantized matmul for efficient inference of large-scale generative language models (Park et al., 2022)\", \"GPT3. int8 (): 8-bit matrix multiplication for transformers at scale (Dettmers et al., 2022)\", \"Llm-qat: Data-free quantization aware training for large language models (Liu et al., 2023)\", \"Smoothquant: Accurate and efficient post-training quantization for large language models (Xiao et al., 2023)\", \"Flexgen: High-throughput generative inference of large language models with a single gpu (Sheng et al., 2023)\", \"Llama 2: Open foundation and fine-tuned chat models (Touvron et al., 2023)\", \"Evaluating large language models trained on code (Chen et al., 2021)\"]}\n[The End of Assistant's Predicted Answer]\n\n[System]\nWe would like to request your feedback on the performance of the AI assistant in response to the user question displayed above according to the gold answer. Please use the following listed aspects and their descriptions as evaluation criteria:\n    - Accuracy and Hallucinations: The assistant's answer is semantically consistent with the gold answer; The numerical value and order need to be accurate, and there should be no hallucinations.\n    - Completeness: Referring to the reference answers, the assistant's answer should contain all the key points needed to answer the user's question; further elaboration on these key points can be omitted.\nPlease rate whether this answer is suitable for the question. Please note that the gold answer can be considered as a correct answer to the question.\n\nThe assistant receives an overall score on a scale of 1 to 100, where a higher score indicates better overall performance.\nPlease note that if the assistant's answer and the gold answer fully meet the above criteria, its overall rating should be the full marks (100).\nPlease first provide a comprehensive explanation of your evaluation, avoiding any potential bias.\nThen, output a line indicating the score of the Assistant.\n\nPLEASE OUTPUT WITH THE FOLLOWING FORMAT, WHERE THE SCORE IS A SCALE OF 1 TO 100 BY STRICTLY FOLLOWING THIS FORMAT: \"[[score]]\", FOR EXAMPLE \"Rating: [[100]]\":\n<start output>\nEvaluation evidence: your evluation explanation here, no more than 100 words\nRating: [[score]]\n<end output>\n\nNow, start your evaluation:",
        "generate_response": "{\"Reference\":[\"A Survey on Evaluation of Large Language Models\", \"Vicuna: An open-source chatbot impressing gpt-4 with $90 \\% *$ chatgpt quality\", \"ChatLaw: Open-Source Legal Large Language Model with Integrated External Knowledge Bases\", \"Compositional Semantic Parsing with Large Language Models\", \"Open LLM Leaderboard\", \"How Close is ChatGPT to Human Experts? Comparison Corpus, Evaluation, and Detection\", \"REALM: Retrieval-Augmented Language Model PreTraining\", \"Rethinking with Retrieval: Faithful Large Language Model Inference\", \"Measuring Massive Multitask Language Understanding\", \"Atlas: Few-shot Learning with Retrieval Augmented Language Models\", \"GQA: Training Generalized Multi-Query Transformer Models from Multi-Head Checkpoints\", \"Alpacafarm: A Simulation Framework for Methods that Learn from Human Feedback\", \"Sigmoid-weighted Linear Units for Neural Network Function Approximation in Reinforcement Learning\", \"Hierarchical Neural Story Generation\", \"GPTQ: Accurate Post-Training Quantization for Generative Pre-Trained Transformers\", \"Breaking the Sequential Dependency of 11m Inference Using Lookahead Decoding\", \"Palm 2 Technical Report\", \"Truncation Sampling as Language Model Desmoothing\", \"Training Compute-Optimal Large Language Models\", \"The Curious Case of Neural Text Degeneration\", \"Lora: Low-Rank Adaptation of Large Language Models\", \"Assisted Generation: A New Direction toward Low-Latency Text Generation\", \"SqueezellM: Dense-and-Sparse Quantization\", \"Sequence-level Knowledge Distillation\", \"Fine-tuning Can Distort Pretrained Features and Underperform Out-of-distribution\", \"Efficient Memory Management for Large Language Model Serving with PagedAttention\", \"Fast Inference from Transformers via Speculative Decoding\", \"AWQ: Activation-Aware Weight Quantization for 11m Compression and Acceleration\", \"Online Speculative Decoding\", \"On the Probability-Quality Paradox in Language Generation\", \"Locally Typical Sampling\", \"SpecInfer: Accelerating Generative LLM Serving with Speculative Inference and Token Tree Verification\", \"Gpt-4 Technical Report\", \"Training Language Models to Follow Instructions with Human Feedback\", \"Efficiently Scaling Transformer Inference\", \"MAUVE: Measuring the Gap between Neural Text and Human Text using Divergence Frontiers\", \"Zephyr: Direct Distillation of LM Alignment\", \"SmoothQuant: Accurate and Efficient Post-Training Quantization for Large Language Models\", \"A Survey on Non-autoregressive Generation for Neural Machine Translation and Beyond\", \"Do Transformers Really Perform Badly for Graph Representation?\", \"Opt: Open Pre-trained Transformer Language Models\", \"H2O: Heavy-Hitter Oracle for Efficient Generative Inference of Large Language Models\", \"Judging LLM-as-a-Judge with MT-Bench and Chatbot Arena\", \"Improving Speculative Decoding via Knowledge Distillation\"], \"Citation\":[\"Attention is all you need (Vaswani et al., 2017)\", \"Bert: Pre-training of deep bidirectional transformers for language understanding (Devlin et al., 2019)\", \"Language models are few-shot learners (Brown et al., 2020)\", \"REST: Retrieval-Based Speculative Decoding\", \"A Survey on Evaluation of Large Language Models (Zhang et al., 2022)\", \"REALM: Retrieval-Augmented Language Model PreTraining (Guu et al., 2020)\", \"Improving the efficiency of LLM inference (Sanh et al., 2019)\", \"Code llama: Open foundation models for code (Rozire et al., 2023)\", \"Vicuna: An opensource chatbot impressing GPT-4 with $90 \\% *$ ChatGPT quality (Chiang et al., 2023)\", \"Flashattention: Fast and memory-efficient exact attention with io-awareness (Dao et al., 2022)\", \"Flashattention-2: Faster attention with better parallelism and work partitioning (Dao, 2023)\", \"Efficient memory management for large language model serving with pagedattention (Kwon et al., 2023)\", \"Fast inference from transformers via speculative decoding (Leviathan et al., 2023)\", \"Specinfer: Accelerating generative llm serving with speculative inference and token tree verification (Miao et al., 2023)\", \"Inference with reference: Lossless acceleration of large language models (Yang et al., 2023)\", \"Llm-pruner: On the structural pruning of large language models (Ma et al., 2023)\", \"Bloom: A 176bparameter open-access multilingual language model (Scao et al., 2022)\", \"Blockwise parallel decoding for deep autoregressive models (Stern et al., 2018)\", \"Suffix arrays: a new method for on-line string searches (Manber and Myers, 1993)\", \"Generalization through memorization: Nearest neighbor language models (Khandelwal et al., 2020)\", \"Dense passage retrieval for opendomain question answering (Karpukhin et al., 2020)\", \"The stack: 3 tb of permissively licensed source code (Kocetkov et al., 2022)\", \"Judging llm-as-a-judge with mt-bench and chatbot arena (Zheng et al., 2023)\", \"Spatten: Efficient sparse attention architecture with cascade token and head pruning (Wang et al., 2021)\", \"Zeroquant: Efficient and affordable post-training quantization for large-scale transformers (Yao et al., 2022)\", \"Nuqmm: Quantized matmul for efficient inference of large-scale generative language models (Park et al., 2022)\", \"GPT3. int8 (): 8-bit matrix multiplication for transformers at scale (Dettmers et al., 2022)\", \"Llm-qat: Data-free quantization aware training for large language models (Liu et al., 2023)\", \"Smoothquant: Accurate and efficient post-training quantization for large language models (Xiao et al., 2023)\", \"Flexgen: High-throughput generative inference of large language models with a single gpu (Sheng et al., 2023)\", \"Llama 2: Open foundation and fine-tuned chat models (Touvron et al., 2023)\", \"Evaluating large language models trained on code (Chen et al., 2021)\"]}",
        "eval_response": "Evaluation evidence: The assistant's response does not match the gold answer in terms of accuracy and completeness. The references and citations listed in the assistant's response are entirely different from those in the gold answer, indicating a significant deviation and possible hallucination of content. The assistant's answer includes a vast array of unrelated titles, which are not mentioned in the gold answer, thus failing to meet the required accuracy and completeness.\n\nRating: [[10]]",
        "score": 10
    },
    {
        "level": 3,
        "set": 4,
        "length": 235607,
        "type": "paper",
        "language": "en",
        "question": "EasyTool: Enhancing LLM-based Agents with Concise Tool Instruction",
        "instruction": "We hope you will carefully study the provided papers and determine the citation relationships between them. Please follow the instructions below strictly to complete the task:\n\n#Specific Requirements:\n1. Reference: When a given paper mentions other provided papers, those other papers are considered as \"references\" for the given paper. To summarize in this specific context, references are about what the given paper is using.\n2. Citation: Conversely, when other provided papers mention the given paper in their works, the given paper is being \"cited\" by those other papers. To summarize in this specific context, citations are about who is using the given paper.\n3. Given a paper, you need to determine the citation or reference relationship between this paper and the other papers. Do not consider papers that are not provided.\n3. Please present the paper titles in a json format as follows: {{\"Reference\":[\"Reference Title 1\", \"Reference Title 2\", ..., \"Reference Title n\"], \"Citation\":[\"Citation Title 1\", \"Citation Title 2\", ..., \"Citation Title n\"]}}.\n4. If a paper does not have any references or citations, please leave the corresponding list empty, e.g.{{\"Refernce\":[]}}, {{\"Citation\":[]}}.",
        "prompt_template": "#Papers:\n{docs}\n\n{instruction}\n\n#The paper you need to analyze:\n{question}",
        "doc": [
            "2401.06201.md",
            "2405.05955.md",
            "2405.16510.md",
            "2405.16533.md",
            "2405.17935.md",
            "2405.18208.md",
            "2306.06624.md",
            "2307.16789.md",
            "2310.07075.md",
            "2310.13227.md",
            "2311.18760.md"
        ],
        "answer": {
            "Reference": [
                "# RestGPT: Connecting Large Language Models with Real-World RESTful APIs ",
                "# ToolLLM: Facilitating Large LanguAGE MODELS TO MASTER 16000+ REAL-WORLD APIS ",
                "# Don't Fine-Tune, Decode: Syntax Error-Free Tool Use via Constrained Decoding ",
                "# ToolChaIN*: EFFiCIENT ACtion SpaCe NAVIGATION in Large Language ModelS With A* Search ",
                "# TASKBENCH: BENCHMARKING LARGE "
            ],
            "Citation": [
                "# Smurfs: Leveraging Multiple Proficiency Agents with Context-Efficiency for Tool Planning ",
                "# Meta-Task Planning for Language Agents ",
                "# Chain of Tools: Large Language Model is an Automatic Multi-tool Learner ",
                "# Tool Learning with Large Language Models: A Survey ",
                "# A Human-Like Reasoning Framework for Multi-Phases Planning Task with Large Language Models "
            ]
        },
        "shuffle_doc": true,
        "id": "f7dafea8-ad54-4d17-8e46-db4a9c9d2ebe",
        "prompt": "[Question]\n#Papers:\n{docs}\n\nWe hope you will carefully study the provided papers and determine the citation relationships between them. Please follow the instructions below strictly to complete the task:\n\n#Specific Requirements:\n1. Reference: When a given paper mentions other provided papers, those other papers are considered as \"references\" for the given paper. To summarize in this specific context, references are about what the given paper is using.\n2. Citation: Conversely, when other provided papers mention the given paper in their works, the given paper is being \"cited\" by those other papers. To summarize in this specific context, citations are about who is using the given paper.\n3. Given a paper, you need to determine the citation or reference relationship between this paper and the other papers. Do not consider papers that are not provided.\n3. Please present the paper titles in a json format as follows: {{\"Reference\":[\"Reference Title 1\", \"Reference Title 2\", ..., \"Reference Title n\"], \"Citation\":[\"Citation Title 1\", \"Citation Title 2\", ..., \"Citation Title n\"]}}.\n4. If a paper does not have any references or citations, please leave the corresponding list empty, e.g.{{\"Refernce\":[]}}, {{\"Citation\":[]}}.\n\n#The paper you need to analyze:\nEasyTool: Enhancing LLM-based Agents with Concise Tool Instruction\n\n[Gold Answer]\n{'Reference': ['# RestGPT: Connecting Large Language Models with Real-World RESTful APIs ', '# ToolLLM: Facilitating Large LanguAGE MODELS TO MASTER 16000+ REAL-WORLD APIS ', \"# Don't Fine-Tune, Decode: Syntax Error-Free Tool Use via Constrained Decoding \", '# ToolChaIN*: EFFiCIENT ACtion SpaCe NAVIGATION in Large Language ModelS With A* Search ', '# TASKBENCH: BENCHMARKING LARGE '], 'Citation': ['# Smurfs: Leveraging Multiple Proficiency Agents with Context-Efficiency for Tool Planning ', '# Meta-Task Planning for Language Agents ', '# Chain of Tools: Large Language Model is an Automatic Multi-tool Learner ', '# Tool Learning with Large Language Models: A Survey ', '# A Human-Like Reasoning Framework for Multi-Phases Planning Task with Large Language Models ']}\n\n[The Start of Assistant's Predicted Answer]\n{\"Reference\":[\"A Survey on Evaluation of Large Language Models\", \"Vicuna: An open-source chatbot impressing GPT-4 with $90 \\% *$ ChatGPT quality\", \"ChatLaw: Open-Source Legal Large Language Model with Integrated External Knowledge Bases\", \"Compositional Semantic Parsing with Large Language Models\", \"Open LLM Leaderboard\", \"How Close is ChatGPT to Human Experts? Comparison Corpus, Evaluation, and Detection\", \"REALM: Retrieval-Augmented Language Model PreTraining\", \"Rethinking with Retrieval: Faithful Large Language Model Inference\", \"Measuring Massive Multitask Language Understanding\", \"Atlas: Few-shot Learning with Retrieval Augmented Language Models\", \"Language models are few-shot learners\", \"Evaluating large language models trained on code\", \"Program of thoughts prompting: Disentangling computation from reasoning for numerical reasoning tasks\", \"Retrieval augmented language model pre-training\", \"WebGPT: Browser-assisted question-answering with human feedback\", \"Training language models to follow instructions with human feedback\", \"Toolformer: Language models can teach themselves to use tools\", \"HuggingGPT: Solving AI tasks with ChatGPT and its friends in Hugging Face\", \"Chameleon: Plug-and-play compositional reasoning with large language models\", \"ToolLLM: Facilitating Large Language Models to Master $16000+$ Real-world APIs\", \"Toolkengpt: Augmenting frozen language models with massive tools via tool embeddings\", \"RestGPT: Connecting Large Language Models with Real-World RESTful APIs\"], \"Citation\":[\"Don't Fine-Tune, Decode: Syntax Error-Free Tool Use via Constrained Decoding\"]}\n[The End of Assistant's Predicted Answer]\n\n[System]\nWe would like to request your feedback on the performance of the AI assistant in response to the user question displayed above according to the gold answer. Please use the following listed aspects and their descriptions as evaluation criteria:\n    - Accuracy and Hallucinations: The assistant's answer is semantically consistent with the gold answer; The numerical value and order need to be accurate, and there should be no hallucinations.\n    - Completeness: Referring to the reference answers, the assistant's answer should contain all the key points needed to answer the user's question; further elaboration on these key points can be omitted.\nPlease rate whether this answer is suitable for the question. Please note that the gold answer can be considered as a correct answer to the question.\n\nThe assistant receives an overall score on a scale of 1 to 100, where a higher score indicates better overall performance.\nPlease note that if the assistant's answer and the gold answer fully meet the above criteria, its overall rating should be the full marks (100).\nPlease first provide a comprehensive explanation of your evaluation, avoiding any potential bias.\nThen, output a line indicating the score of the Assistant.\n\nPLEASE OUTPUT WITH THE FOLLOWING FORMAT, WHERE THE SCORE IS A SCALE OF 1 TO 100 BY STRICTLY FOLLOWING THIS FORMAT: \"[[score]]\", FOR EXAMPLE \"Rating: [[100]]\":\n<start output>\nEvaluation evidence: your evluation explanation here, no more than 100 words\nRating: [[score]]\n<end output>\n\nNow, start your evaluation:",
        "generate_response": "{\"Reference\":[\"A Survey on Evaluation of Large Language Models\", \"Vicuna: An open-source chatbot impressing GPT-4 with $90 \\% *$ ChatGPT quality\", \"ChatLaw: Open-Source Legal Large Language Model with Integrated External Knowledge Bases\", \"Compositional Semantic Parsing with Large Language Models\", \"Open LLM Leaderboard\", \"How Close is ChatGPT to Human Experts? Comparison Corpus, Evaluation, and Detection\", \"REALM: Retrieval-Augmented Language Model PreTraining\", \"Rethinking with Retrieval: Faithful Large Language Model Inference\", \"Measuring Massive Multitask Language Understanding\", \"Atlas: Few-shot Learning with Retrieval Augmented Language Models\", \"Language models are few-shot learners\", \"Evaluating large language models trained on code\", \"Program of thoughts prompting: Disentangling computation from reasoning for numerical reasoning tasks\", \"Retrieval augmented language model pre-training\", \"WebGPT: Browser-assisted question-answering with human feedback\", \"Training language models to follow instructions with human feedback\", \"Toolformer: Language models can teach themselves to use tools\", \"HuggingGPT: Solving AI tasks with ChatGPT and its friends in Hugging Face\", \"Chameleon: Plug-and-play compositional reasoning with large language models\", \"ToolLLM: Facilitating Large Language Models to Master $16000+$ Real-world APIs\", \"Toolkengpt: Augmenting frozen language models with massive tools via tool embeddings\", \"RestGPT: Connecting Large Language Models with Real-World RESTful APIs\"], \"Citation\":[\"Don't Fine-Tune, Decode: Syntax Error-Free Tool Use via Constrained Decoding\"]}",
        "eval_response": "Evaluation evidence: The assistant's answer shows significant discrepancies from the gold answer. The references and citations listed do not match those in the gold answer, indicating inaccuracies and hallucinations. The assistant's answer includes many titles not mentioned in the gold answer, suggesting a misunderstanding of the task or source data. This results in a failure to meet the accuracy and completeness criteria.\n\nRating: [[20]]",
        "score": 20
    },
    {
        "level": 3,
        "set": 4,
        "length": 247557,
        "type": "paper",
        "language": "en",
        "question": "Hallucination is Inevitable: <br> An Innate Limitation of Large Language Models",
        "instruction": "We hope you will carefully study the provided papers and determine the citation relationships between them. Please follow the instructions below strictly to complete the task:\n\n#Specific Requirements:\n1. Reference: When a given paper mentions other provided papers, those other papers are considered as \"references\" for the given paper. To summarize in this specific context, references are about what the given paper is using.\n2. Citation: Conversely, when other provided papers mention the given paper in their works, the given paper is being \"cited\" by those other papers. To summarize in this specific context, citations are about who is using the given paper.\n3. Given a paper, you need to determine the citation or reference relationship between this paper and the other papers. Do not consider papers that are not provided.\n3. Please present the paper titles in a json format as follows: {{\"Reference\":[\"Reference Title 1\", \"Reference Title 2\", ..., \"Reference Title n\"], \"Citation\":[\"Citation Title 1\", \"Citation Title 2\", ..., \"Citation Title n\"]}}.\n4. If a paper does not have any references or citations, please leave the corresponding list empty, e.g.{{\"Refernce\":[]}}, {{\"Citation\":[]}}.",
        "prompt_template": "#Papers:\n{docs}\n\n{instruction}\n\n#The paper you need to analyze:\n{question}",
        "doc": [
            "2401.11817.md",
            "2404.18852.md",
            "2405.12910.md",
            "2405.12999.md",
            "2405.15652.md",
            "2405.20404.md",
            "2310.10501.md",
            "2311.05232.md",
            "2401.07897.md",
            "2401.08358.md",
            "2401.09395.md"
        ],
        "answer": {
            "Reference": [
                "# NeMo Guardrails: A Toolkit for Controllable and Safe LLM Applications with Programmable Rails ",
                "# A Survey on Hallucination in Large Language Models: Principles, Taxonomy, Challenges, and Open Questions ",
                "# The Pitfalls of Defining Hallucination ",
                "# Hallucination Detection and Hallucination Mitigation: An Investigation ",
                "# Caught in the Quicksand of Reasoning, Far from AGI Summit: Evaluating LLMs' Mathematical and Coding Competency through Ontology-guided Interventions "
            ],
            "Citation": [
                "# VERT: Verified Equivalent Rust Transpilation with Large Language Models as Few-Shot Learners ",
                "# Topic Modelling Case Law Using a Large Language Model and a New Taxonomy for UK Law: AI Insights into Summary Judgment ",
                "# An ASSESSMENT OF MODEL-ON-MODEL DECEPTION ",
                "# $\\mathrm{L}^{2} \\cdot \\mathrm{M}=\\mathrm{C}^{2}$ <br> Large Language Models as Covert Channels... a Systematic Analysis ",
                "# XPrompt: Explaining Large Language Model's Generation via Joint Prompt Attribution "
            ]
        },
        "shuffle_doc": true,
        "id": "e4901743-e3bb-42d4-9e55-8cbcb578e0e0",
        "prompt": "[Question]\n#Papers:\n{docs}\n\nWe hope you will carefully study the provided papers and determine the citation relationships between them. Please follow the instructions below strictly to complete the task:\n\n#Specific Requirements:\n1. Reference: When a given paper mentions other provided papers, those other papers are considered as \"references\" for the given paper. To summarize in this specific context, references are about what the given paper is using.\n2. Citation: Conversely, when other provided papers mention the given paper in their works, the given paper is being \"cited\" by those other papers. To summarize in this specific context, citations are about who is using the given paper.\n3. Given a paper, you need to determine the citation or reference relationship between this paper and the other papers. Do not consider papers that are not provided.\n3. Please present the paper titles in a json format as follows: {{\"Reference\":[\"Reference Title 1\", \"Reference Title 2\", ..., \"Reference Title n\"], \"Citation\":[\"Citation Title 1\", \"Citation Title 2\", ..., \"Citation Title n\"]}}.\n4. If a paper does not have any references or citations, please leave the corresponding list empty, e.g.{{\"Refernce\":[]}}, {{\"Citation\":[]}}.\n\n#The paper you need to analyze:\nHallucination is Inevitable: <br> An Innate Limitation of Large Language Models\n\n[Gold Answer]\n{'Reference': ['# NeMo Guardrails: A Toolkit for Controllable and Safe LLM Applications with Programmable Rails ', '# A Survey on Hallucination in Large Language Models: Principles, Taxonomy, Challenges, and Open Questions ', '# The Pitfalls of Defining Hallucination ', '# Hallucination Detection and Hallucination Mitigation: An Investigation ', \"# Caught in the Quicksand of Reasoning, Far from AGI Summit: Evaluating LLMs' Mathematical and Coding Competency through Ontology-guided Interventions \"], 'Citation': ['# VERT: Verified Equivalent Rust Transpilation with Large Language Models as Few-Shot Learners ', '# Topic Modelling Case Law Using a Large Language Model and a New Taxonomy for UK Law: AI Insights into Summary Judgment ', '# An ASSESSMENT OF MODEL-ON-MODEL DECEPTION ', '# $\\\\mathrm{L}^{2} \\\\cdot \\\\mathrm{M}=\\\\mathrm{C}^{2}$ <br> Large Language Models as Covert Channels... a Systematic Analysis ', \"# XPrompt: Explaining Large Language Model's Generation via Joint Prompt Attribution \"]}\n\n[The Start of Assistant's Predicted Answer]\n{\"Reference\":[\"Language Models are Few-Shot Learners\", \"The Complexity of Theorem-Proving Procedures\", \"The Pitfalls of Defining Hallucination\", \"Chain-of-Verification Reduces Hallucination in Large Language Models\", \"Neural Path Hunter: Reducing Hallucination in Dialogue Systems via Path Grounding\", \"Overcoming a Theoretical Limitation of Self-Attention\", \"Softmax Bottleneck Makes Language Models Unable to Represent Multi-mode Word Distributions\", \"scheduled sampling for sequence prediction with recurrent neural networks\", \"Super-exponential complexity of Presburger arithmetic\", \"The Pile: An 800GB Dataset of Diverse Text for Language Modeling\", \"Language Identification in the Limit\", \"Assessing The Factual Accuracy of Generated Text\", \"Looking for a Needle in a Haystack: A Comprehensive Study of Hallucinations in Neural Machine Translation\", \"Textbooks Are All You Need\", \"Theoretical Limitations of Self-Attention in Neural Sequence Models\", \"Stuck in the Quicksand of Numeracy, Far from AGI Summit: Evaluating LLMs, Mathematical Competency through Ontology-guided Perturbations\", \"A Survey on Hallucination in Large Language Models: Principles, Taxonomy, Challenges, and Open Questions\", \"Large Language Models Struggle to Learn Long-Tail Knowledge\", \"Deduplicating Training Data Makes Language Models Better\", \"Factuality Enhanced Language Models for Open-Ended Text Generation\", \"Large Language Models with Controllable Working Memory\", \"HaluEval: A Large-Scale Hallucination Evaluation Benchmark for Large Language Models\", \"TruthfulQA: Measuring How Models Mimic Human Falsehoods\", \"Exposing Attention Glitches with Flip-Flop Language Modeling\", \"When Not to Trust Language Models: Investigating Effectiveness of Parametric and Non-Parametric Memories\", \"Knowledge Injection to Counter Large Language Model (LLM) Hallucination\", \"Looking Beyond Sentence-Level Natural Language Inference for Question Answering and Text Summarization\", \"Nationality Bias in Text Generation\", \"A Simple Recipe towards Reducing Hallucination in Neural Surface Realisation\", \"Entity Cloze By Date: What LMs Know About Unseen Entities\", \"ChatGPT Release Notes\", \"GPT-4 Technical Report\", \"Automatically Correcting Large Language Models: Surveying the landscape of diverse self-correction strategies\", \"Data and its (dis)contents: A survey of dataset development and use in machine learning research\", \"Check Your Facts and Try Again: Improving Large Language Models with External Knowledge and Automated Feedback\", \"Uber die vollstandigkeiteines gewissensystems der arithmetik ganzer zahlen, in welchen die addition als einzige operation hervortritt\", \"Sequence Level Training with Recurrent Neural Networks\", \"The Curious Case of Hallucinations in Neural Machine Translation\", \"The Troubling Emergence of Hallucination in Large Language Models - An Extensive Definition, Quantification, and Prescriptive Remediations\", \"NeMo Guardrails: A Toolkit for Controllable and Safe LLM Applications with Programmable Rails\", \"Identifying Untrustworthy Samples: Data Filtering for Open-Domain Dialogues with Bayesian Optimization\", \"Trusting Your Evidence: Hallucinate Less with Context-aware Decoding\", \"In-Context Pretraining: Language Modeling Beyond Document Boundaries\", \"Retrieval Augmentation Reduces Hallucination in Conversation\", \"LLaMA: Open and Efficient Foundation Language Models\", \"Llama 2: Open Foundation and Fine-Tuned Chat Models\", \"On Computable Numbers, with an Application to the Entscheidungsproblem\", \"A Theory of the Learnable\", \"Attention is All You Need\", \"FreshLLMs: Refreshing Large Language Models with Search Engine Augmentation\", \"Survey on Factuality in Large Language Models: Knowledge, Retrieval and Domain-Specificity\", \"SCOTT: Self-Consistent Chain-of-Thought Distillation\", \"Emergent Abilities of Large Language Models\", \"Chain-of-Thought Prompting Elicits Reasoning in Large Language Models\", \"Embedding Symbolic Knowledge into Deep Networks\", \"Don't Pour Cereal into Coffee: Differentiable Temporal Logic for Temporal Action Segmentation\", \"Breaking the Softmax Bottleneck: A High-Rank RNN Language Model\", \"Tree of Thoughts: Deliberate Problem Solving with Large Language Models\", \"Siren's Song in the AI Ocean: A Survey on Hallucination in Large Language Models\", \"Verify-andEdit: A Knowledge-Enhanced Chain-of-Thought Framework\"], \"Citation\":[\"Caught in the Quicksand of Reasoning, Far from AGI Summit: Evaluating LLMs' Mathematical and Coding Competency through Ontology-guided Interventions\", \"A Survey on Hallucination in Large Language Models: Principles, Taxonomy, Challenges, and Open Questions\", \"Topic Modelling Case Law Using a Large Language Model and a New Taxonomy for UK Law: AI Insights into Summary Judgment\", \"Hallucination Detection and Hallucination Mitigation: An Investigation\", \"VERT: Verified Equivalent Rust Transpilation with Large Language Models as Few-Shot Learners\", \"An ASSESSMENT OF MODEL-ON-MODEL DECEPTION\", \"XPrompt: Explaining Large Language Model's Generation via Joint Prompt Attribution\"]}\n[The End of Assistant's Predicted Answer]\n\n[System]\nWe would like to request your feedback on the performance of the AI assistant in response to the user question displayed above according to the gold answer. Please use the following listed aspects and their descriptions as evaluation criteria:\n    - Accuracy and Hallucinations: The assistant's answer is semantically consistent with the gold answer; The numerical value and order need to be accurate, and there should be no hallucinations.\n    - Completeness: Referring to the reference answers, the assistant's answer should contain all the key points needed to answer the user's question; further elaboration on these key points can be omitted.\nPlease rate whether this answer is suitable for the question. Please note that the gold answer can be considered as a correct answer to the question.\n\nThe assistant receives an overall score on a scale of 1 to 100, where a higher score indicates better overall performance.\nPlease note that if the assistant's answer and the gold answer fully meet the above criteria, its overall rating should be the full marks (100).\nPlease first provide a comprehensive explanation of your evaluation, avoiding any potential bias.\nThen, output a line indicating the score of the Assistant.\n\nPLEASE OUTPUT WITH THE FOLLOWING FORMAT, WHERE THE SCORE IS A SCALE OF 1 TO 100 BY STRICTLY FOLLOWING THIS FORMAT: \"[[score]]\", FOR EXAMPLE \"Rating: [[100]]\":\n<start output>\nEvaluation evidence: your evluation explanation here, no more than 100 words\nRating: [[score]]\n<end output>\n\nNow, start your evaluation:",
        "generate_response": "{\"Reference\":[\"Language Models are Few-Shot Learners\", \"The Complexity of Theorem-Proving Procedures\", \"The Pitfalls of Defining Hallucination\", \"Chain-of-Verification Reduces Hallucination in Large Language Models\", \"Neural Path Hunter: Reducing Hallucination in Dialogue Systems via Path Grounding\", \"Overcoming a Theoretical Limitation of Self-Attention\", \"Softmax Bottleneck Makes Language Models Unable to Represent Multi-mode Word Distributions\", \"scheduled sampling for sequence prediction with recurrent neural networks\", \"Super-exponential complexity of Presburger arithmetic\", \"The Pile: An 800GB Dataset of Diverse Text for Language Modeling\", \"Language Identification in the Limit\", \"Assessing The Factual Accuracy of Generated Text\", \"Looking for a Needle in a Haystack: A Comprehensive Study of Hallucinations in Neural Machine Translation\", \"Textbooks Are All You Need\", \"Theoretical Limitations of Self-Attention in Neural Sequence Models\", \"Stuck in the Quicksand of Numeracy, Far from AGI Summit: Evaluating LLMs, Mathematical Competency through Ontology-guided Perturbations\", \"A Survey on Hallucination in Large Language Models: Principles, Taxonomy, Challenges, and Open Questions\", \"Large Language Models Struggle to Learn Long-Tail Knowledge\", \"Deduplicating Training Data Makes Language Models Better\", \"Factuality Enhanced Language Models for Open-Ended Text Generation\", \"Large Language Models with Controllable Working Memory\", \"HaluEval: A Large-Scale Hallucination Evaluation Benchmark for Large Language Models\", \"TruthfulQA: Measuring How Models Mimic Human Falsehoods\", \"Exposing Attention Glitches with Flip-Flop Language Modeling\", \"When Not to Trust Language Models: Investigating Effectiveness of Parametric and Non-Parametric Memories\", \"Knowledge Injection to Counter Large Language Model (LLM) Hallucination\", \"Looking Beyond Sentence-Level Natural Language Inference for Question Answering and Text Summarization\", \"Nationality Bias in Text Generation\", \"A Simple Recipe towards Reducing Hallucination in Neural Surface Realisation\", \"Entity Cloze By Date: What LMs Know About Unseen Entities\", \"ChatGPT Release Notes\", \"GPT-4 Technical Report\", \"Automatically Correcting Large Language Models: Surveying the landscape of diverse self-correction strategies\", \"Data and its (dis)contents: A survey of dataset development and use in machine learning research\", \"Check Your Facts and Try Again: Improving Large Language Models with External Knowledge and Automated Feedback\", \"Uber die vollstandigkeiteines gewissensystems der arithmetik ganzer zahlen, in welchen die addition als einzige operation hervortritt\", \"Sequence Level Training with Recurrent Neural Networks\", \"The Curious Case of Hallucinations in Neural Machine Translation\", \"The Troubling Emergence of Hallucination in Large Language Models - An Extensive Definition, Quantification, and Prescriptive Remediations\", \"NeMo Guardrails: A Toolkit for Controllable and Safe LLM Applications with Programmable Rails\", \"Identifying Untrustworthy Samples: Data Filtering for Open-Domain Dialogues with Bayesian Optimization\", \"Trusting Your Evidence: Hallucinate Less with Context-aware Decoding\", \"In-Context Pretraining: Language Modeling Beyond Document Boundaries\", \"Retrieval Augmentation Reduces Hallucination in Conversation\", \"LLaMA: Open and Efficient Foundation Language Models\", \"Llama 2: Open Foundation and Fine-Tuned Chat Models\", \"On Computable Numbers, with an Application to the Entscheidungsproblem\", \"A Theory of the Learnable\", \"Attention is All You Need\", \"FreshLLMs: Refreshing Large Language Models with Search Engine Augmentation\", \"Survey on Factuality in Large Language Models: Knowledge, Retrieval and Domain-Specificity\", \"SCOTT: Self-Consistent Chain-of-Thought Distillation\", \"Emergent Abilities of Large Language Models\", \"Chain-of-Thought Prompting Elicits Reasoning in Large Language Models\", \"Embedding Symbolic Knowledge into Deep Networks\", \"Don't Pour Cereal into Coffee: Differentiable Temporal Logic for Temporal Action Segmentation\", \"Breaking the Softmax Bottleneck: A High-Rank RNN Language Model\", \"Tree of Thoughts: Deliberate Problem Solving with Large Language Models\", \"Siren's Song in the AI Ocean: A Survey on Hallucination in Large Language Models\", \"Verify-andEdit: A Knowledge-Enhanced Chain-of-Thought Framework\"], \"Citation\":[\"Caught in the Quicksand of Reasoning, Far from AGI Summit: Evaluating LLMs' Mathematical and Coding Competency through Ontology-guided Interventions\", \"A Survey on Hallucination in Large Language Models: Principles, Taxonomy, Challenges, and Open Questions\", \"Topic Modelling Case Law Using a Large Language Model and a New Taxonomy for UK Law: AI Insights into Summary Judgment\", \"Hallucination Detection and Hallucination Mitigation: An Investigation\", \"VERT: Verified Equivalent Rust Transpilation with Large Language Models as Few-Shot Learners\", \"An ASSESSMENT OF MODEL-ON-MODEL DECEPTION\", \"XPrompt: Explaining Large Language Model's Generation via Joint Prompt Attribution\"]}",
        "eval_response": "Evaluation evidence: The assistant's answer does not match the gold answer in terms of accuracy and completeness. The references and citations listed in the assistant's response include numerous titles that are not present in the gold answer, indicating a significant amount of hallucination. Additionally, the assistant's response omits several key references and citations that are present in the gold answer, demonstrating a lack of completeness. The format requested was also not followed as the response was not provided in JSON format.\n\nRating: [[20]]",
        "score": 20
    },
    {
        "level": 3,
        "set": 3,
        "length": 100170,
        "type": "paper",
        "language": "en",
        "question": "AUTORT: EMBODIED FOUNDATION MODELS FOR LARGE SCALE ORCHESTRATION OF ROBOTIC AGENTS",
        "instruction": "We hope you will carefully study the provided papers and determine the citation relationships between them. Please follow the instructions below strictly to complete the task:\n\n#Specific Requirements:\n1. Reference: When a given paper mentions other provided papers, those other papers are considered as \"references\" for the given paper. To summarize in this specific context, references are about what the given paper is using.\n2. Citation: Conversely, when other provided papers mention the given paper in their works, the given paper is being \"cited\" by those other papers. To summarize in this specific context, citations are about who is using the given paper.\n3. Given a paper, you need to determine the citation or reference relationship between this paper and the other papers. Do not consider papers that are not provided.\n3. Please present the paper titles in a json format as follows: {{\"Reference\":[\"Reference Title 1\", \"Reference Title 2\", ..., \"Reference Title n\"], \"Citation\":[\"Citation Title 1\", \"Citation Title 2\", ..., \"Citation Title n\"]}}.\n4. If a paper does not have any references or citations, please leave the corresponding list empty, e.g.{{\"Refernce\":[]}}, {{\"Citation\":[]}}.",
        "prompt_template": "#Papers:\n{docs}\n\n{instruction}\n\n#The paper you need to analyze:\n{question}",
        "doc": [
            "2401.12963.md",
            "2403.13438.md",
            "2304.03442.md",
            "2307.06135.md"
        ],
        "answer": {
            "Reference": [
                "# Generative Agents: Interactive Simulacra of Human Behavior ",
                "# SayPlan: Grounding Large Language Models using 3D Scene Graphs for Scalable Robot Task Planning "
            ],
            "Citation": [
                "# See, Imagine, Plan: Discovering and Hallucinating Tasks from a Single Image "
            ]
        },
        "shuffle_doc": true,
        "id": "35b1bae2-45b2-4e93-aab5-2abcf78e2f93",
        "prompt": "[Question]\n#Papers:\n{docs}\n\nWe hope you will carefully study the provided papers and determine the citation relationships between them. Please follow the instructions below strictly to complete the task:\n\n#Specific Requirements:\n1. Reference: When a given paper mentions other provided papers, those other papers are considered as \"references\" for the given paper. To summarize in this specific context, references are about what the given paper is using.\n2. Citation: Conversely, when other provided papers mention the given paper in their works, the given paper is being \"cited\" by those other papers. To summarize in this specific context, citations are about who is using the given paper.\n3. Given a paper, you need to determine the citation or reference relationship between this paper and the other papers. Do not consider papers that are not provided.\n3. Please present the paper titles in a json format as follows: {{\"Reference\":[\"Reference Title 1\", \"Reference Title 2\", ..., \"Reference Title n\"], \"Citation\":[\"Citation Title 1\", \"Citation Title 2\", ..., \"Citation Title n\"]}}.\n4. If a paper does not have any references or citations, please leave the corresponding list empty, e.g.{{\"Refernce\":[]}}, {{\"Citation\":[]}}.\n\n#The paper you need to analyze:\nAUTORT: EMBODIED FOUNDATION MODELS FOR LARGE SCALE ORCHESTRATION OF ROBOTIC AGENTS\n\n[Gold Answer]\n{'Reference': ['# Generative Agents: Interactive Simulacra of Human Behavior ', '# SayPlan: Grounding Large Language Models using 3D Scene Graphs for Scalable Robot Task Planning '], 'Citation': ['# See, Imagine, Plan: Discovering and Hallucinating Tasks from a Single Image ']}\n\n[The Start of Assistant's Predicted Answer]\n{\"Reference\": [\"Do as i can and not as i say: Grounding language in robotic affordances\", \"How to prompt your robot: A promptbook for manipulation skills with code as policies\", \"Runaround\", \"Constitutional ai: Harmlessness from ai feedback\", \"Robocat: A self-improving foundation agent for robotic manipulation\", \"RT-1: Robotics transformer for real-world control at scale\", \"RT2: Vision-language-action models transfer web knowledge to robotic control\", \"Universal sentence encoder\", \"Open-vocabulary queryable scene representations for real world planning\", \"Pali: A jointly-scaled multilingual language-image model\", \"Leveraging procedural generation to benchmark reinforcement learning\", \"Robonet: Large-scale multi-robot learning\", \"Physically grounded vision-language models for robotic manipulation\", \"Robot learning in homes: Improving generalization and reducing dataset bias\", \"Fleet-dagger: Interactive robot fleet learning with scalable human supervision\", \"BC-z: Zero-shot task generalization with robotic imitation learning\", \"Qt-opt: Scalable deep reinforcement learning for vision-based robotic manipulation\", \"Continuous multi-task robotic reinforcement learning at scale\", \"Interactive imitation learning with human experts\", \"Learning hand-eye coordination for robotic grasping with deep learning and large-scale data collection\", \"Code as policies: Language model programs for embodied control\", \"Interactive language: Talking to robots in real time\", \"Scaling robot supervision to hundreds of hours with roboturk: Robotic manipulation dataset through human reasoning and dexterity\", \"Grounding language with visual affordances over unstructured data\", \"Generative agents: Interactive simulacra of human behavior\", \"Supersizing self-supervision: Learning to grasp from 50k tries and 700 robot hours\", \"Grasp learning: Models, methods, and performance\", \"A reduction of imitation learning and structured prediction to no-regret online learning\", \"Multiple interactions made easy (mime): Large scale demonstrations data for imitation\", \"Reflexion: Language agents with verbal reinforcement learning\", \"Improving llm pretraining via document de-duplication and diversification\", \"Chatgpt for robotics: Design principles and model abilities\", \"Voyager: An open-ended embodied agent with large language models\", \"Towards a foundation model for generalist robots: Diverse skill learning at scale via automated task and scene generation\", \"Robotic skill acquistion via instruction augmentation with visionlanguage models\", \"React: Synergizing reasoning and acting in language models\", \"More than a million ways to be pushed: A high-fidelity experimental dataset of planar pushing\"], \"Citation\": [\"SayPlan: Grounding Large Language Models using 3D Scene Graphs for Scalable Robot Task Planning\"]}\n[The End of Assistant's Predicted Answer]\n\n[System]\nWe would like to request your feedback on the performance of the AI assistant in response to the user question displayed above according to the gold answer. Please use the following listed aspects and their descriptions as evaluation criteria:\n    - Accuracy and Hallucinations: The assistant's answer is semantically consistent with the gold answer; The numerical value and order need to be accurate, and there should be no hallucinations.\n    - Completeness: Referring to the reference answers, the assistant's answer should contain all the key points needed to answer the user's question; further elaboration on these key points can be omitted.\nPlease rate whether this answer is suitable for the question. Please note that the gold answer can be considered as a correct answer to the question.\n\nThe assistant receives an overall score on a scale of 1 to 100, where a higher score indicates better overall performance.\nPlease note that if the assistant's answer and the gold answer fully meet the above criteria, its overall rating should be the full marks (100).\nPlease first provide a comprehensive explanation of your evaluation, avoiding any potential bias.\nThen, output a line indicating the score of the Assistant.\n\nPLEASE OUTPUT WITH THE FOLLOWING FORMAT, WHERE THE SCORE IS A SCALE OF 1 TO 100 BY STRICTLY FOLLOWING THIS FORMAT: \"[[score]]\", FOR EXAMPLE \"Rating: [[100]]\":\n<start output>\nEvaluation evidence: your evluation explanation here, no more than 100 words\nRating: [[score]]\n<end output>\n\nNow, start your evaluation:",
        "generate_response": "{\"Reference\": [\"Do as i can and not as i say: Grounding language in robotic affordances\", \"How to prompt your robot: A promptbook for manipulation skills with code as policies\", \"Runaround\", \"Constitutional ai: Harmlessness from ai feedback\", \"Robocat: A self-improving foundation agent for robotic manipulation\", \"RT-1: Robotics transformer for real-world control at scale\", \"RT2: Vision-language-action models transfer web knowledge to robotic control\", \"Universal sentence encoder\", \"Open-vocabulary queryable scene representations for real world planning\", \"Pali: A jointly-scaled multilingual language-image model\", \"Leveraging procedural generation to benchmark reinforcement learning\", \"Robonet: Large-scale multi-robot learning\", \"Physically grounded vision-language models for robotic manipulation\", \"Robot learning in homes: Improving generalization and reducing dataset bias\", \"Fleet-dagger: Interactive robot fleet learning with scalable human supervision\", \"BC-z: Zero-shot task generalization with robotic imitation learning\", \"Qt-opt: Scalable deep reinforcement learning for vision-based robotic manipulation\", \"Continuous multi-task robotic reinforcement learning at scale\", \"Interactive imitation learning with human experts\", \"Learning hand-eye coordination for robotic grasping with deep learning and large-scale data collection\", \"Code as policies: Language model programs for embodied control\", \"Interactive language: Talking to robots in real time\", \"Scaling robot supervision to hundreds of hours with roboturk: Robotic manipulation dataset through human reasoning and dexterity\", \"Grounding language with visual affordances over unstructured data\", \"Generative agents: Interactive simulacra of human behavior\", \"Supersizing self-supervision: Learning to grasp from 50k tries and 700 robot hours\", \"Grasp learning: Models, methods, and performance\", \"A reduction of imitation learning and structured prediction to no-regret online learning\", \"Multiple interactions made easy (mime): Large scale demonstrations data for imitation\", \"Reflexion: Language agents with verbal reinforcement learning\", \"Improving llm pretraining via document de-duplication and diversification\", \"Chatgpt for robotics: Design principles and model abilities\", \"Voyager: An open-ended embodied agent with large language models\", \"Towards a foundation model for generalist robots: Diverse skill learning at scale via automated task and scene generation\", \"Robotic skill acquistion via instruction augmentation with visionlanguage models\", \"React: Synergizing reasoning and acting in language models\", \"More than a million ways to be pushed: A high-fidelity experimental dataset of planar pushing\"], \"Citation\": [\"SayPlan: Grounding Large Language Models using 3D Scene Graphs for Scalable Robot Task Planning\"]}",
        "eval_response": "Evaluation evidence: The assistant's answer contains significant hallucinations and inaccuracies. The references and citations listed do not match those in the gold answer, indicating a misunderstanding or misinterpretation of the source material. The gold answer specifies two references and one citation, whereas the assistant's answer lists numerous unrelated papers, showing a lack of accuracy and relevance. This deviation from the provided gold answer suggests a failure in correctly identifying the citation relationships for the specified paper.\n\nRating: [[10]]",
        "score": 10
    },
    {
        "level": 3,
        "set": 4,
        "length": 227455,
        "type": "paper",
        "language": "en",
        "question": "AUtoAcT: Automatic Agent Learning from Scratch for QA via Self-Planning",
        "instruction": "We hope you will carefully study the provided papers and determine the citation relationships between them. Please follow the instructions below strictly to complete the task:\n\n#Specific Requirements:\n1. Reference: When a given paper mentions other provided papers, those other papers are considered as \"references\" for the given paper. To summarize in this specific context, references are about what the given paper is using.\n2. Citation: Conversely, when other provided papers mention the given paper in their works, the given paper is being \"cited\" by those other papers. To summarize in this specific context, citations are about who is using the given paper.\n3. Given a paper, you need to determine the citation or reference relationship between this paper and the other papers. Do not consider papers that are not provided.\n3. Please present the paper titles in a json format as follows: {{\"Reference\":[\"Reference Title 1\", \"Reference Title 2\", ..., \"Reference Title n\"], \"Citation\":[\"Citation Title 1\", \"Citation Title 2\", ..., \"Citation Title n\"]}}.\n4. If a paper does not have any references or citations, please leave the corresponding list empty, e.g.{{\"Refernce\":[]}}, {{\"Citation\":[]}}.",
        "prompt_template": "#Papers:\n{docs}\n\n{instruction}\n\n#The paper you need to analyze:\n{question}",
        "doc": [
            "2401.05268.md",
            "2403.19154.md",
            "2403.19962.md",
            "2404.14387.md",
            "2405.14205.md",
            "2405.14751.md",
            "2311.11797.md",
            "2401.07324.md",
            "2402.01622.md",
            "2402.02716.md",
            "2403.08140.md"
        ],
        "answer": {
            "Reference": [
                "# Igniting Language Intelligence: The Hitchhiker's Guide From Chain-of-Thought Reasoning to Language Agents ",
                "# Small LLMs Are Weak Tool Learners: A Multi-LLM Agent ",
                "# TravelPlanner: A Benchmark for Real-World Planning with Language Agents ",
                "# Understanding the planning of LLM agents: A survey ",
                "# BAGEL: Bootstrapping Agents by Guiding Exploration with Language "
            ],
            "Citation": [
                "# STaR-GATE: <br> Teaching Language Models to Ask Clarifying Questions ",
                "# Enhancing the General Agent Capabilities of Low-Parameter LLMs through Tuning and Multi-Branch Reasoning ",
                "# A Survey on Self-Evolution of Large Language Models ",
                "# Agent Planning with World Knowledge Model ",
                "# AGILE: A Novel Framework of LLM Agents "
            ]
        },
        "shuffle_doc": true,
        "id": "eb919a13-c051-44ba-8c23-1e953664611a",
        "prompt": "[Question]\n#Papers:\n{docs}\n\nWe hope you will carefully study the provided papers and determine the citation relationships between them. Please follow the instructions below strictly to complete the task:\n\n#Specific Requirements:\n1. Reference: When a given paper mentions other provided papers, those other papers are considered as \"references\" for the given paper. To summarize in this specific context, references are about what the given paper is using.\n2. Citation: Conversely, when other provided papers mention the given paper in their works, the given paper is being \"cited\" by those other papers. To summarize in this specific context, citations are about who is using the given paper.\n3. Given a paper, you need to determine the citation or reference relationship between this paper and the other papers. Do not consider papers that are not provided.\n3. Please present the paper titles in a json format as follows: {{\"Reference\":[\"Reference Title 1\", \"Reference Title 2\", ..., \"Reference Title n\"], \"Citation\":[\"Citation Title 1\", \"Citation Title 2\", ..., \"Citation Title n\"]}}.\n4. If a paper does not have any references or citations, please leave the corresponding list empty, e.g.{{\"Refernce\":[]}}, {{\"Citation\":[]}}.\n\n#The paper you need to analyze:\nAUtoAcT: Automatic Agent Learning from Scratch for QA via Self-Planning\n\n[Gold Answer]\n{'Reference': [\"# Igniting Language Intelligence: The Hitchhiker's Guide From Chain-of-Thought Reasoning to Language Agents \", '# Small LLMs Are Weak Tool Learners: A Multi-LLM Agent ', '# TravelPlanner: A Benchmark for Real-World Planning with Language Agents ', '# Understanding the planning of LLM agents: A survey ', '# BAGEL: Bootstrapping Agents by Guiding Exploration with Language '], 'Citation': ['# STaR-GATE: <br> Teaching Language Models to Ask Clarifying Questions ', '# Enhancing the General Agent Capabilities of Low-Parameter LLMs through Tuning and Multi-Branch Reasoning ', '# A Survey on Self-Evolution of Large Language Models ', '# Agent Planning with World Knowledge Model ', '# AGILE: A Novel Framework of LLM Agents ']}\n\n[The Start of Assistant's Predicted Answer]\n{\"Reference\":[\"A Survey on Evaluation of Large Language Models\", \"Chain-of-thought prompting elicits reasoning in large language models\", \"REALM: Retrieval-Augmented Language Model PreTraining\", \"Rethinking with Retrieval: Faithful Large Language Model Inference\", \"Problems of Monetary Management: The UK Experience\", \"Large language model based multi-agents: A survey of progress and challenges\", \"Making language models better tool learners with execution feedback\", \"Reasoning with language model prompting: A survey\", \"Hugginggpt: Solving AI tasks with chatgpt and its friends in huggingface\", \"Reflexion: language agents with verbal reinforcement learning\", \"Multiagent collaboration: Harnessing the power of intelligent LLM agents\", \"Medagents: Large language models as collaborators for zero-shot medical reasoning\", \"Stanford alpaca: An instruction-following llama model\", \"Xagent: An autonomous agent for complex task solving\", \"Autogpt: build & use ai agents\", \"Llama 2: Open foundation and fine-tuned chat models\", \"A survey on large language model based autonomous agents\", \"Self-instruct: Aligning language models with self-generated instructions\", \"Describe, explain, plan and select: interactive planning with llms enables open-world multi-task agents\", \"Jarvis-1: Open-world multi-task agents with memoryaugmented multimodal language models\", \"Do as I can, not as I say: Grounding language in robotic affordances\", \"GPT-4 technical report\", \"GPT-engineer\", \"Gorilla: Large language model connected with massive apis\", \"Virtualhome: Simulating household activities via programs\", \"Making language models better tool learners with execution feedback\", \"Reasoning with language model prompting: A survey\", \"Toolllm: Facilitating large language models to master 16000+ real-world apis\", \"Deepspeed: System optimizations enable training deep learning models with over 100 billion parameters\", \"Small llms are weak tool learners: A multi-llm agent\", \"Multiagent collaboration: Harnessing the power of intelligent LLM agents\", \"LLM-planner: Few-shot grounded planning for embodied agents with large language models\", \"Language models meet world models: Embodied experiences enhance language models\", \"Learning agents with unified data, modular design, and open-source llms\", \"Star: Bootstrapping reasoning with reasoning\", \"Agenttuning: Enabling generalized agent abilities for llms\", \"Igniting language intelligence: The hitchhiker's guide from chain-of-thought reasoning to language agents\", \"Judging llm-as-a-judge with mt-bench and chatbot arena\", \"Webarena: A realistic web environment for building autonomous agents\", \"Agents: An open-source framework for autonomous language agents\"], \"Citation\":[]}\n[The End of Assistant's Predicted Answer]\n\n[System]\nWe would like to request your feedback on the performance of the AI assistant in response to the user question displayed above according to the gold answer. Please use the following listed aspects and their descriptions as evaluation criteria:\n    - Accuracy and Hallucinations: The assistant's answer is semantically consistent with the gold answer; The numerical value and order need to be accurate, and there should be no hallucinations.\n    - Completeness: Referring to the reference answers, the assistant's answer should contain all the key points needed to answer the user's question; further elaboration on these key points can be omitted.\nPlease rate whether this answer is suitable for the question. Please note that the gold answer can be considered as a correct answer to the question.\n\nThe assistant receives an overall score on a scale of 1 to 100, where a higher score indicates better overall performance.\nPlease note that if the assistant's answer and the gold answer fully meet the above criteria, its overall rating should be the full marks (100).\nPlease first provide a comprehensive explanation of your evaluation, avoiding any potential bias.\nThen, output a line indicating the score of the Assistant.\n\nPLEASE OUTPUT WITH THE FOLLOWING FORMAT, WHERE THE SCORE IS A SCALE OF 1 TO 100 BY STRICTLY FOLLOWING THIS FORMAT: \"[[score]]\", FOR EXAMPLE \"Rating: [[100]]\":\n<start output>\nEvaluation evidence: your evluation explanation here, no more than 100 words\nRating: [[score]]\n<end output>\n\nNow, start your evaluation:",
        "generate_response": "{\"Reference\":[\"A Survey on Evaluation of Large Language Models\", \"Chain-of-thought prompting elicits reasoning in large language models\", \"REALM: Retrieval-Augmented Language Model PreTraining\", \"Rethinking with Retrieval: Faithful Large Language Model Inference\", \"Problems of Monetary Management: The UK Experience\", \"Large language model based multi-agents: A survey of progress and challenges\", \"Making language models better tool learners with execution feedback\", \"Reasoning with language model prompting: A survey\", \"Hugginggpt: Solving AI tasks with chatgpt and its friends in huggingface\", \"Reflexion: language agents with verbal reinforcement learning\", \"Multiagent collaboration: Harnessing the power of intelligent LLM agents\", \"Medagents: Large language models as collaborators for zero-shot medical reasoning\", \"Stanford alpaca: An instruction-following llama model\", \"Xagent: An autonomous agent for complex task solving\", \"Autogpt: build & use ai agents\", \"Llama 2: Open foundation and fine-tuned chat models\", \"A survey on large language model based autonomous agents\", \"Self-instruct: Aligning language models with self-generated instructions\", \"Describe, explain, plan and select: interactive planning with llms enables open-world multi-task agents\", \"Jarvis-1: Open-world multi-task agents with memoryaugmented multimodal language models\", \"Do as I can, not as I say: Grounding language in robotic affordances\", \"GPT-4 technical report\", \"GPT-engineer\", \"Gorilla: Large language model connected with massive apis\", \"Virtualhome: Simulating household activities via programs\", \"Making language models better tool learners with execution feedback\", \"Reasoning with language model prompting: A survey\", \"Toolllm: Facilitating large language models to master 16000+ real-world apis\", \"Deepspeed: System optimizations enable training deep learning models with over 100 billion parameters\", \"Small llms are weak tool learners: A multi-llm agent\", \"Multiagent collaboration: Harnessing the power of intelligent LLM agents\", \"LLM-planner: Few-shot grounded planning for embodied agents with large language models\", \"Language models meet world models: Embodied experiences enhance language models\", \"Learning agents with unified data, modular design, and open-source llms\", \"Star: Bootstrapping reasoning with reasoning\", \"Agenttuning: Enabling generalized agent abilities for llms\", \"Igniting language intelligence: The hitchhiker's guide from chain-of-thought reasoning to language agents\", \"Judging llm-as-a-judge with mt-bench and chatbot arena\", \"Webarena: A realistic web environment for building autonomous agents\", \"Agents: An open-source framework for autonomous language agents\"], \"Citation\":[]}",
        "eval_response": "Evaluation evidence: The assistant's answer does not align with the gold answer in terms of accuracy and completeness. The references listed in the assistant's answer do not match any from the gold answer, indicating a significant deviation and hallucination of content. Additionally, the assistant failed to identify any citations, whereas the gold answer lists several. This discrepancy highlights issues in both accuracy and completeness, as the assistant neither correctly identified the references nor the citations for the given paper.\n\nRating: [[10]]",
        "score": 10
    },
    {
        "level": 3,
        "set": 3,
        "length": 100561,
        "type": "paper",
        "language": "en",
        "question": "Contrastive Preference Optimization: Pushing the Boundaries of LLM Performance in Machine Translation",
        "instruction": "We hope you will carefully study the provided papers and determine the citation relationships between them. Please follow the instructions below strictly to complete the task:\n\n#Specific Requirements:\n1. Reference: When a given paper mentions other provided papers, those other papers are considered as \"references\" for the given paper. To summarize in this specific context, references are about what the given paper is using.\n2. Citation: Conversely, when other provided papers mention the given paper in their works, the given paper is being \"cited\" by those other papers. To summarize in this specific context, citations are about who is using the given paper.\n3. Given a paper, you need to determine the citation or reference relationship between this paper and the other papers. Do not consider papers that are not provided.\n3. Please present the paper titles in a json format as follows: {{\"Reference\":[\"Reference Title 1\", \"Reference Title 2\", ..., \"Reference Title n\"], \"Citation\":[\"Citation Title 1\", \"Citation Title 2\", ..., \"Citation Title n\"]}}.\n4. If a paper does not have any references or citations, please leave the corresponding list empty, e.g.{{\"Refernce\":[]}}, {{\"Citation\":[]}}.",
        "prompt_template": "#Papers:\n{docs}\n\n{instruction}\n\n#The paper you need to analyze:\n{question}",
        "doc": [
            "2401.08417.md",
            "2405.14734.md",
            "2405.20512.md",
            "2310.06825.md",
            "2310.10482.md"
        ],
        "answer": {
            "Reference": [
                "# Mistral 7B ",
                "# xCOMET: Transparent Machine Translation Evaluation through Fine-grained Error Detection "
            ],
            "Citation": [
                "# SimPO: Simple Preference Optimization with a Reference-Free Reward ",
                "# How Multilingual Are Large Language Models Fine-Tuned for Translation? "
            ]
        },
        "shuffle_doc": true,
        "id": "c0e9301f-8c74-4a78-9642-07b6316c5955",
        "prompt": "[Question]\n#Papers:\n{docs}\n\nWe hope you will carefully study the provided papers and determine the citation relationships between them. Please follow the instructions below strictly to complete the task:\n\n#Specific Requirements:\n1. Reference: When a given paper mentions other provided papers, those other papers are considered as \"references\" for the given paper. To summarize in this specific context, references are about what the given paper is using.\n2. Citation: Conversely, when other provided papers mention the given paper in their works, the given paper is being \"cited\" by those other papers. To summarize in this specific context, citations are about who is using the given paper.\n3. Given a paper, you need to determine the citation or reference relationship between this paper and the other papers. Do not consider papers that are not provided.\n3. Please present the paper titles in a json format as follows: {{\"Reference\":[\"Reference Title 1\", \"Reference Title 2\", ..., \"Reference Title n\"], \"Citation\":[\"Citation Title 1\", \"Citation Title 2\", ..., \"Citation Title n\"]}}.\n4. If a paper does not have any references or citations, please leave the corresponding list empty, e.g.{{\"Refernce\":[]}}, {{\"Citation\":[]}}.\n\n#The paper you need to analyze:\nContrastive Preference Optimization: Pushing the Boundaries of LLM Performance in Machine Translation\n\n[Gold Answer]\n{'Reference': ['# Mistral 7B ', '# xCOMET: Transparent Machine Translation Evaluation through Fine-grained Error Detection '], 'Citation': ['# SimPO: Simple Preference Optimization with a Reference-Free Reward ', '# How Multilingual Are Large Language Models Fine-Tuned for Translation? ']}\n\n[The Start of Assistant's Predicted Answer]\n{\"Reference\":[\"A Survey on Evaluation of Large Language Models\", \"Vicuna: An Open-Source Chatbot Impressing GPT-4 with $90 \\% *$ ChatGPT Quality\", \"ChatLaw: Open-Source Legal Large Language Model with Integrated External Knowledge Bases\", \"Compositional Semantic Parsing with Large Language Models\", \"Open LLM Leaderboard\", \"How Close is ChatGPT to Human Experts? Comparison Corpus, Evaluation, and Detection\", \"Improving translation faithfulness of large language models via augmenting instructions\", \"Beyond english-centric multilingual machine translation\", \"Results of WMT23 metrics shared task: Metrics might be guilty but references are not innocent\", \"xcomet: Transparent machine translation evaluation through fine-grained error detection\", \"Momentum contrast for unsupervised visual representation learning\", \"Contrastive prefence learning: Learning from human feedback without rl\", \"How good are gpt models at machine translation? a comprehensive evaluation\", \"LoRA: Low-rank adaptation of large language models\", \"Mistral 7b\", \"Is chatgpt a good translator? a preliminary study\", \"Madlad-400: A multilingual and document-level large audited dataset\", \"Eliciting the translation ability of large language models via multilingual finetuning with translation instructions\", \"Small data, big impact: Leveraging minimal data for effective machine translation\", \"No language left behind: Scaling human-centered machine translation\", \"Representation learning with contrastive predictive coding\", \"Gpt-4 technical report\", \"Training language models to follow instructions with human feedback\", \"A call for clarity in reporting BLEU scores\", \"Direct preference optimization: Your language model is secretly a reward model\", \"Deepspeed: System optimizations enable training deep learning models with over 100 billion parameters\", \"T. COMET-22: Unbabel-IST 2022 submission for the metrics shared task\", \"Scaling up cometkiwi: Unbabel-ist 2023 submission for the quality estimation shared task\", \"Contrastive learning with hard negative samples\", \"Proximal policy optimization algorithms\", \"BLEURT: Learning robust metrics for text generation\", \"Multilingual representation distillation with contrastive learning\", \"Llama: Open and efficient foundation language models\", \"Llama 2: Open foundation and finetuned chat models\", \"Attention is all you need\", \"Exploring prompt engineering with GPT language models for document-level machine translation: Insights and findings\", \"BERT, mBERT, or BiBERT? a study on contextualized embeddings for neural machine translation\", \"mT5: A massively multilingual pre-trained text-to-text transformer\", \"Bigtrans: Augmenting large language models with multilingual translation capability over 100 languages\", \"Tim: Teaching large language models to translate with comparison\", \"Bayling: Bridging cross-lingual alignment and instruction following through interactive translation for large language models\", \"Multilingual machine translation with large language models: Empirical results and analysis\", \"Extrapolating large language models to non-english by aligning languages\", \"Fine-tuning language models from human preferences\"], \"Citation\":[]}\n[The End of Assistant's Predicted Answer]\n\n[System]\nWe would like to request your feedback on the performance of the AI assistant in response to the user question displayed above according to the gold answer. Please use the following listed aspects and their descriptions as evaluation criteria:\n    - Accuracy and Hallucinations: The assistant's answer is semantically consistent with the gold answer; The numerical value and order need to be accurate, and there should be no hallucinations.\n    - Completeness: Referring to the reference answers, the assistant's answer should contain all the key points needed to answer the user's question; further elaboration on these key points can be omitted.\nPlease rate whether this answer is suitable for the question. Please note that the gold answer can be considered as a correct answer to the question.\n\nThe assistant receives an overall score on a scale of 1 to 100, where a higher score indicates better overall performance.\nPlease note that if the assistant's answer and the gold answer fully meet the above criteria, its overall rating should be the full marks (100).\nPlease first provide a comprehensive explanation of your evaluation, avoiding any potential bias.\nThen, output a line indicating the score of the Assistant.\n\nPLEASE OUTPUT WITH THE FOLLOWING FORMAT, WHERE THE SCORE IS A SCALE OF 1 TO 100 BY STRICTLY FOLLOWING THIS FORMAT: \"[[score]]\", FOR EXAMPLE \"Rating: [[100]]\":\n<start output>\nEvaluation evidence: your evluation explanation here, no more than 100 words\nRating: [[score]]\n<end output>\n\nNow, start your evaluation:",
        "generate_response": "{\"Reference\":[\"A Survey on Evaluation of Large Language Models\", \"Vicuna: An Open-Source Chatbot Impressing GPT-4 with $90 \\% *$ ChatGPT Quality\", \"ChatLaw: Open-Source Legal Large Language Model with Integrated External Knowledge Bases\", \"Compositional Semantic Parsing with Large Language Models\", \"Open LLM Leaderboard\", \"How Close is ChatGPT to Human Experts? Comparison Corpus, Evaluation, and Detection\", \"Improving translation faithfulness of large language models via augmenting instructions\", \"Beyond english-centric multilingual machine translation\", \"Results of WMT23 metrics shared task: Metrics might be guilty but references are not innocent\", \"xcomet: Transparent machine translation evaluation through fine-grained error detection\", \"Momentum contrast for unsupervised visual representation learning\", \"Contrastive prefence learning: Learning from human feedback without rl\", \"How good are gpt models at machine translation? a comprehensive evaluation\", \"LoRA: Low-rank adaptation of large language models\", \"Mistral 7b\", \"Is chatgpt a good translator? a preliminary study\", \"Madlad-400: A multilingual and document-level large audited dataset\", \"Eliciting the translation ability of large language models via multilingual finetuning with translation instructions\", \"Small data, big impact: Leveraging minimal data for effective machine translation\", \"No language left behind: Scaling human-centered machine translation\", \"Representation learning with contrastive predictive coding\", \"Gpt-4 technical report\", \"Training language models to follow instructions with human feedback\", \"A call for clarity in reporting BLEU scores\", \"Direct preference optimization: Your language model is secretly a reward model\", \"Deepspeed: System optimizations enable training deep learning models with over 100 billion parameters\", \"T. COMET-22: Unbabel-IST 2022 submission for the metrics shared task\", \"Scaling up cometkiwi: Unbabel-ist 2023 submission for the quality estimation shared task\", \"Contrastive learning with hard negative samples\", \"Proximal policy optimization algorithms\", \"BLEURT: Learning robust metrics for text generation\", \"Multilingual representation distillation with contrastive learning\", \"Llama: Open and efficient foundation language models\", \"Llama 2: Open foundation and finetuned chat models\", \"Attention is all you need\", \"Exploring prompt engineering with GPT language models for document-level machine translation: Insights and findings\", \"BERT, mBERT, or BiBERT? a study on contextualized embeddings for neural machine translation\", \"mT5: A massively multilingual pre-trained text-to-text transformer\", \"Bigtrans: Augmenting large language models with multilingual translation capability over 100 languages\", \"Tim: Teaching large language models to translate with comparison\", \"Bayling: Bridging cross-lingual alignment and instruction following through interactive translation for large language models\", \"Multilingual machine translation with large language models: Empirical results and analysis\", \"Extrapolating large language models to non-english by aligning languages\", \"Fine-tuning language models from human preferences\"], \"Citation\":[]}",
        "eval_response": "Evaluation evidence: The assistant's answer does not match the gold answer in terms of accuracy. The references listed in the assistant's response are extensive and do not include the specific titles mentioned in the gold answer, which are '# Mistral 7B ' and '# xCOMET: Transparent Machine Translation Evaluation through Fine-grained Error Detection '. Additionally, the assistant's response incorrectly lists no citations, whereas the gold answer specifies two citations. This discrepancy indicates issues with both accuracy and completeness, as the assistant failed to identify the correct references and citations.\n\nRating: [[20]]",
        "score": 20
    },
    {
        "level": 3,
        "set": 3,
        "length": 102138,
        "type": "paper",
        "language": "en",
        "question": "MM-LLMs: Recent Advances in MultiModal Large Language Models",
        "instruction": "We hope you will carefully study the provided papers and determine the citation relationships between them. Please follow the instructions below strictly to complete the task:\n\n#Specific Requirements:\n1. Reference: When a given paper mentions other provided papers, those other papers are considered as \"references\" for the given paper. To summarize in this specific context, references are about what the given paper is using.\n2. Citation: Conversely, when other provided papers mention the given paper in their works, the given paper is being \"cited\" by those other papers. To summarize in this specific context, citations are about who is using the given paper.\n3. Given a paper, you need to determine the citation or reference relationship between this paper and the other papers. Do not consider papers that are not provided.\n3. Please present the paper titles in a json format as follows: {{\"Reference\":[\"Reference Title 1\", \"Reference Title 2\", ..., \"Reference Title n\"], \"Citation\":[\"Citation Title 1\", \"Citation Title 2\", ..., \"Citation Title n\"]}}.\n4. If a paper does not have any references or citations, please leave the corresponding list empty, e.g.{{\"Refernce\":[]}}, {{\"Citation\":[]}}.",
        "prompt_template": "#Papers:\n{docs}\n\n{instruction}\n\n#The paper you need to analyze:\n{question}",
        "doc": [
            "2401.13601.md",
            "2405.15341.md",
            "2403.19949.md",
            "2404.14688.md"
        ],
        "answer": {
            "Reference": [
                "# FairCLIP: Harnessing Fairness in Vision-Language Learning ",
                "# FMint: Bridging Human Designed and Data Pretrained Models for Differential Equation Foundation Model "
            ],
            "Citation": [
                "# V-Zen: Efficient GUI Understanding and Precise Grounding With A Novel Multimodal LLM "
            ]
        },
        "shuffle_doc": true,
        "id": "6cc497e2-0f84-418a-afa3-f315f9354e49",
        "prompt": "[Question]\n#Papers:\n{docs}\n\nWe hope you will carefully study the provided papers and determine the citation relationships between them. Please follow the instructions below strictly to complete the task:\n\n#Specific Requirements:\n1. Reference: When a given paper mentions other provided papers, those other papers are considered as \"references\" for the given paper. To summarize in this specific context, references are about what the given paper is using.\n2. Citation: Conversely, when other provided papers mention the given paper in their works, the given paper is being \"cited\" by those other papers. To summarize in this specific context, citations are about who is using the given paper.\n3. Given a paper, you need to determine the citation or reference relationship between this paper and the other papers. Do not consider papers that are not provided.\n3. Please present the paper titles in a json format as follows: {{\"Reference\":[\"Reference Title 1\", \"Reference Title 2\", ..., \"Reference Title n\"], \"Citation\":[\"Citation Title 1\", \"Citation Title 2\", ..., \"Citation Title n\"]}}.\n4. If a paper does not have any references or citations, please leave the corresponding list empty, e.g.{{\"Refernce\":[]}}, {{\"Citation\":[]}}.\n\n#The paper you need to analyze:\nMM-LLMs: Recent Advances in MultiModal Large Language Models\n\n[Gold Answer]\n{'Reference': ['# FairCLIP: Harnessing Fairness in Vision-Language Learning ', '# FMint: Bridging Human Designed and Data Pretrained Models for Differential Equation Foundation Model '], 'Citation': ['# V-Zen: Efficient GUI Understanding and Precise Grounding With A Novel Multimodal LLM ']}\n\n[The Start of Assistant's Predicted Answer]\n{\"Reference\":[\"Flamingo: a visual language model for few-shot learning\", \"BLIP-2: Bootstrapping Language-Image Pre-training with Frozen Image Encoders and Large Language Models\", \"LLaVA: Unleashing Multimodal LLM's Referential Dialogue Magic\", \"MiniGPT-4: Enhancing vision-language understanding with advanced large language models\", \"mPLUG-Owl: A Framework for aligning X-Modal instruction-aware representations to LLMs and Emergent Cross-modal Reasoning\", \"Otter: A multi-modal model with in-context instruction tuning\", \"X-LLM: Bootstrapping advanced large language models by treating multi-modalities as foreign languages\", \"VideoChat: Improving Large MultiModal Models with Better Captions\", \"InstructBLIP: Towards General-purpose Vision-Language Models with Instruction Tuning\", \"PandaGPT: One model to instruction-follow them all\", \"GILL: Generating images with multimodal language models\", \"PaLI-X: On Scaling up a Multilingual Vision and Language Model\", \"Video-LLaMA: Unified Video-Language Pre-training with Decoupled Visual-Motional Tokenization\", \"Video-ChatGPT: Towards Detailed Video Understanding via Large Vision and Language Models\", \"Shikra: Unleashing Multimodal LLM's Referential Dialogue Magic\", \"LLaVAR: Enhanced visual instruction tuning for text-rich image understanding\", \"mPLUG-DocOwl: Scientific Diagram Analysis with the Multimodal Large Language Model\", \"Lynx: Multimodal-gpt: A vision and language model for dialogue with humans\", \"Emu: Synergistic multimodal comprehension and creation\", \"DLP: Multimodal-gpt: A vision and language model for dialogue with humans\", \"BuboGPT: Understanding and generating speech, music, sound, and talking head\", \"ChatSpot: A simple method to reduce hallucination in large vision-language models\", \"IDEFICS-9B: Introducing IDEFICS: An Open Reproduction of State-of-the-Art Visual Language Model\", \"IDEFICS-80B: Introducing IDEFICS: An Open Reproduction of State-of-the-Art Visual Language Model\", \"Qwen-VL-(Chat): Qwen-VL: A Frontier Large Vision-Language Model with Versatile Abilities\", \"LaVIT : Planting a seed of vision in large language model\", \"NExT-GPT: Next-gpt: Any-to-any multimodal llm\", \"DreamLLM: Synergistic multimodal comprehension and creation\", \"AnyMAL: An efficient and scalable any-modality augmented language model\", \"MiniGPT-5: Interleaved vision-and-language generation via generative vokens\", \"LLaVA-1.5: LLaVA: An Instruction-tuned Audio-Visual Language Model for Video Understanding\", \"MiniGPT-v2: Large language model as a unified interface for vision-language multi-task learning\", \"MiniGPT-v2-Chat: Large language model as a unified interface for vision-language multi-task learning\", \"VILA-7B: On Pre-training for Visual Language Models\", \"VILA-13B: On Pre-training for Visual Language Models\", \"Qwen-Audio: Advancing universal audio understanding via unified large-scale audiolanguage models\", \"DRESS: Instructing large vision-language models to align and interact with humans via natural language feedback\", \"X-InstructBLIP: A Framework for aligning X-Modal instruction-aware representations to LLMs and Emergent Cross-modal Reasoning\", \"RLHF-V: Towards trustworthy mllms via behavior alignment from fine-grained correctional human feedback\", \"Silkie: Preference Distillation for Large Visual Language Models\", \"Lyrics: Boosting Fine-grained Language-Vision Alignment and Comprehension via Semantic-aware Visual Objects\", \"LLaVA-Phi: Efficient Multi-Modal Assistant with Small Language Model\", \"LLaVA-NeXT: Improved reasoning, OCR, and world knowledge\", \"LLaMA-VID: An Image is Worth 2 Tokens in Large Language Models\", \"MM-Interleaved: Interleaved Image-Text Generative Modeling via Multi-modal Feature Synchronizer\", \"MMCode: Evaluating Multi-Modal Code Large Language Models with Visually Rich Programming Problems\", \"WebVoyager: Building an End-to-End Web Agent with Large Multimodal Models\", \"Mobile-Agent: Autonomous Multi-Modal Mobile Device Agent with Visual Perception\", \"MoE-LLaVA: Mixture of Experts for Large Vision-Language Models\", \"MobileVLM V2: Faster and Stronger Baseline for Vision Language Model\", \"CogVLM: A fast, reproducible and strong vision language assistant for mobile devices\", \"Multimodal ArXiv: A Dataset for Improving Scientific Comprehension of Large Vision-Language Models\", \"VisLingInstruct: Elevating Zero-Shot Learning in Multi-Modal Language Models with Autonomous Instruction Optimization\", \"CLOVA: A Closed-Loop Visual Assistant with Tool Usage and Update\", \"Multimodal Large Language Models for Autonomous Driving\", \"Multimodal Large Language Model (LLM) AI Text Generation Detection based on Transformer Deep Learning Algorithm\", \"Multimodal Large Language Models: A survey\"], \"Citation\":[]}\n[The End of Assistant's Predicted Answer]\n\n[System]\nWe would like to request your feedback on the performance of the AI assistant in response to the user question displayed above according to the gold answer. Please use the following listed aspects and their descriptions as evaluation criteria:\n    - Accuracy and Hallucinations: The assistant's answer is semantically consistent with the gold answer; The numerical value and order need to be accurate, and there should be no hallucinations.\n    - Completeness: Referring to the reference answers, the assistant's answer should contain all the key points needed to answer the user's question; further elaboration on these key points can be omitted.\nPlease rate whether this answer is suitable for the question. Please note that the gold answer can be considered as a correct answer to the question.\n\nThe assistant receives an overall score on a scale of 1 to 100, where a higher score indicates better overall performance.\nPlease note that if the assistant's answer and the gold answer fully meet the above criteria, its overall rating should be the full marks (100).\nPlease first provide a comprehensive explanation of your evaluation, avoiding any potential bias.\nThen, output a line indicating the score of the Assistant.\n\nPLEASE OUTPUT WITH THE FOLLOWING FORMAT, WHERE THE SCORE IS A SCALE OF 1 TO 100 BY STRICTLY FOLLOWING THIS FORMAT: \"[[score]]\", FOR EXAMPLE \"Rating: [[100]]\":\n<start output>\nEvaluation evidence: your evluation explanation here, no more than 100 words\nRating: [[score]]\n<end output>\n\nNow, start your evaluation:",
        "generate_response": "{\"Reference\":[\"Flamingo: a visual language model for few-shot learning\", \"BLIP-2: Bootstrapping Language-Image Pre-training with Frozen Image Encoders and Large Language Models\", \"LLaVA: Unleashing Multimodal LLM's Referential Dialogue Magic\", \"MiniGPT-4: Enhancing vision-language understanding with advanced large language models\", \"mPLUG-Owl: A Framework for aligning X-Modal instruction-aware representations to LLMs and Emergent Cross-modal Reasoning\", \"Otter: A multi-modal model with in-context instruction tuning\", \"X-LLM: Bootstrapping advanced large language models by treating multi-modalities as foreign languages\", \"VideoChat: Improving Large MultiModal Models with Better Captions\", \"InstructBLIP: Towards General-purpose Vision-Language Models with Instruction Tuning\", \"PandaGPT: One model to instruction-follow them all\", \"GILL: Generating images with multimodal language models\", \"PaLI-X: On Scaling up a Multilingual Vision and Language Model\", \"Video-LLaMA: Unified Video-Language Pre-training with Decoupled Visual-Motional Tokenization\", \"Video-ChatGPT: Towards Detailed Video Understanding via Large Vision and Language Models\", \"Shikra: Unleashing Multimodal LLM's Referential Dialogue Magic\", \"LLaVAR: Enhanced visual instruction tuning for text-rich image understanding\", \"mPLUG-DocOwl: Scientific Diagram Analysis with the Multimodal Large Language Model\", \"Lynx: Multimodal-gpt: A vision and language model for dialogue with humans\", \"Emu: Synergistic multimodal comprehension and creation\", \"DLP: Multimodal-gpt: A vision and language model for dialogue with humans\", \"BuboGPT: Understanding and generating speech, music, sound, and talking head\", \"ChatSpot: A simple method to reduce hallucination in large vision-language models\", \"IDEFICS-9B: Introducing IDEFICS: An Open Reproduction of State-of-the-Art Visual Language Model\", \"IDEFICS-80B: Introducing IDEFICS: An Open Reproduction of State-of-the-Art Visual Language Model\", \"Qwen-VL-(Chat): Qwen-VL: A Frontier Large Vision-Language Model with Versatile Abilities\", \"LaVIT : Planting a seed of vision in large language model\", \"NExT-GPT: Next-gpt: Any-to-any multimodal llm\", \"DreamLLM: Synergistic multimodal comprehension and creation\", \"AnyMAL: An efficient and scalable any-modality augmented language model\", \"MiniGPT-5: Interleaved vision-and-language generation via generative vokens\", \"LLaVA-1.5: LLaVA: An Instruction-tuned Audio-Visual Language Model for Video Understanding\", \"MiniGPT-v2: Large language model as a unified interface for vision-language multi-task learning\", \"MiniGPT-v2-Chat: Large language model as a unified interface for vision-language multi-task learning\", \"VILA-7B: On Pre-training for Visual Language Models\", \"VILA-13B: On Pre-training for Visual Language Models\", \"Qwen-Audio: Advancing universal audio understanding via unified large-scale audiolanguage models\", \"DRESS: Instructing large vision-language models to align and interact with humans via natural language feedback\", \"X-InstructBLIP: A Framework for aligning X-Modal instruction-aware representations to LLMs and Emergent Cross-modal Reasoning\", \"RLHF-V: Towards trustworthy mllms via behavior alignment from fine-grained correctional human feedback\", \"Silkie: Preference Distillation for Large Visual Language Models\", \"Lyrics: Boosting Fine-grained Language-Vision Alignment and Comprehension via Semantic-aware Visual Objects\", \"LLaVA-Phi: Efficient Multi-Modal Assistant with Small Language Model\", \"LLaVA-NeXT: Improved reasoning, OCR, and world knowledge\", \"LLaMA-VID: An Image is Worth 2 Tokens in Large Language Models\", \"MM-Interleaved: Interleaved Image-Text Generative Modeling via Multi-modal Feature Synchronizer\", \"MMCode: Evaluating Multi-Modal Code Large Language Models with Visually Rich Programming Problems\", \"WebVoyager: Building an End-to-End Web Agent with Large Multimodal Models\", \"Mobile-Agent: Autonomous Multi-Modal Mobile Device Agent with Visual Perception\", \"MoE-LLaVA: Mixture of Experts for Large Vision-Language Models\", \"MobileVLM V2: Faster and Stronger Baseline for Vision Language Model\", \"CogVLM: A fast, reproducible and strong vision language assistant for mobile devices\", \"Multimodal ArXiv: A Dataset for Improving Scientific Comprehension of Large Vision-Language Models\", \"VisLingInstruct: Elevating Zero-Shot Learning in Multi-Modal Language Models with Autonomous Instruction Optimization\", \"CLOVA: A Closed-Loop Visual Assistant with Tool Usage and Update\", \"Multimodal Large Language Models for Autonomous Driving\", \"Multimodal Large Language Model (LLM) AI Text Generation Detection based on Transformer Deep Learning Algorithm\", \"Multimodal Large Language Models: A survey\"], \"Citation\":[]}",
        "eval_response": "Evaluation evidence: The assistant's answer does not match the gold answer in terms of accuracy and hallucinations. The references listed in the assistant's answer do not align with those in the gold answer, indicating a significant deviation and hallucination of content. Additionally, the assistant's answer does not list any citations, whereas the gold answer lists one, showing a lack of completeness in capturing the necessary citation relationship. The assistant's response is overly detailed and includes numerous unrelated titles, further evidencing inaccuracies and hallucinations.\n\nRating: [[10]]",
        "score": 10
    },
    {
        "level": 3,
        "set": 2,
        "length": 51661,
        "type": "paper",
        "language": "en",
        "question": "WebVoyager : Building an End-to-End Web Agent with Large Multimodal Models",
        "instruction": "We hope you will carefully study the provided papers and determine the citation relationships between them. Please follow the instructions below strictly to complete the task:\n\n#Specific Requirements:\n1. Reference: When a given paper mentions other provided papers, those other papers are considered as \"references\" for the given paper. To summarize in this specific context, references are about what the given paper is using.\n2. Citation: Conversely, when other provided papers mention the given paper in their works, the given paper is being \"cited\" by those other papers. To summarize in this specific context, citations are about who is using the given paper.\n3. Given a paper, you need to determine the citation or reference relationship between this paper and the other papers. Do not consider papers that are not provided.\n3. Please present the paper titles in a json format as follows: {{\"Reference\":[\"Reference Title 1\", \"Reference Title 2\", ..., \"Reference Title n\"], \"Citation\":[\"Citation Title 1\", \"Citation Title 2\", ..., \"Citation Title n\"]}}.\n4. If a paper does not have any references or citations, please leave the corresponding list empty, e.g.{{\"Refernce\":[]}}, {{\"Citation\":[]}}.",
        "prompt_template": "#Papers:\n{docs}\n\n{instruction}\n\n#The paper you need to analyze:\n{question}",
        "doc": [
            "2401.13919.md",
            "2404.05955.md",
            "2405.03710.md"
        ],
        "answer": {
            "Reference": [],
            "Citation": [
                "# VisualWebBench: How Far Have Multimodal LLMs Evolved in Web Page Understanding and Grounding? ",
                "# Automating the Enterprise with Foundation Models "
            ]
        },
        "shuffle_doc": true,
        "id": "ca7efd6e-31ea-4fa2-a819-724406e866db",
        "prompt": "[Question]\n#Papers:\n{docs}\n\nWe hope you will carefully study the provided papers and determine the citation relationships between them. Please follow the instructions below strictly to complete the task:\n\n#Specific Requirements:\n1. Reference: When a given paper mentions other provided papers, those other papers are considered as \"references\" for the given paper. To summarize in this specific context, references are about what the given paper is using.\n2. Citation: Conversely, when other provided papers mention the given paper in their works, the given paper is being \"cited\" by those other papers. To summarize in this specific context, citations are about who is using the given paper.\n3. Given a paper, you need to determine the citation or reference relationship between this paper and the other papers. Do not consider papers that are not provided.\n3. Please present the paper titles in a json format as follows: {{\"Reference\":[\"Reference Title 1\", \"Reference Title 2\", ..., \"Reference Title n\"], \"Citation\":[\"Citation Title 1\", \"Citation Title 2\", ..., \"Citation Title n\"]}}.\n4. If a paper does not have any references or citations, please leave the corresponding list empty, e.g.{{\"Refernce\":[]}}, {{\"Citation\":[]}}.\n\n#The paper you need to analyze:\nWebVoyager : Building an End-to-End Web Agent with Large Multimodal Models\n\n[Gold Answer]\n{'Reference': [], 'Citation': ['# VisualWebBench: How Far Have Multimodal LLMs Evolved in Web Page Understanding and Grounding? ', '# Automating the Enterprise with Foundation Models ']}\n\n[The Start of Assistant's Predicted Answer]\n{\"Reference\": [\"Cm3: A causal masked multimodal model of the internet\", \"AutoGPT\", \"Language models are few-shot learners\", \"Evaluating large language models trained on code\", \"Webvln: Vision-and-language navigation on websites\", \"Seeclick: Harnessing gui grounding for advanced visual gui agents\", \"Palm: Scaling language modeling with pathways\", \"A coefficient of agreement for nominal scales\", \"Mind2web: Towards a generalist agent for the web\", \"A survey for in-context learning\", \"An image is worth $16 \\times 16$ words: Transformers for image recognition at scale\", \"Measuring nominal scale agreement among many raters\", \"Multimodal web navigation with instruction-finetuned foundation models\", \"Making the v in vqa matter: Elevating the role of image understanding in visual question answering\", \"A real-world webagent with planning, long context understanding, and program synthesis\", \"The abduction of sherlock holmes: A dataset for visual abductive reasoning\", \"Visualwebarena: Evaluating multimodal agents on realistic visual web tasks\", \"Pix2struct: Screenshot parsing as pretraining for visual language understanding\", \"Visualbert: A simple and performant baseline for vision and language\", \"Learn to explain: Multimodal reasoning via thought chains for science question answering\", \"Chameleon: Plug-and-play compositional reasoning with large language models\", \"Laser: Llm agent with state-space exploration for web navigation\", \"Gaia: a benchmark for general ai assistants\", \"Webgpt: Browser-assisted questionanswering with human feedback\", \"Gpt-4 technical report\", \"Training language models to follow instructions with human feedback\", \"Toolllm: Facilitating large language models to master 16000+ real-world apis\", \"Exploring the limits of transfer learning with a unified text-to-text transformer\", \"Toolformer: Language models can teach themselves to use tools\", \"From pixels to ui actions: Learning to follow instructions via graphical user interfaces\", \"World of bits: An open-domain platform for web-based agents\", \"Reflexion: Language agents with verbal reinforcement learning\", \"Gemini: a family of highly capable multimodal models\", \"Self-instruct: Aligning language model with self generated instructions\", \"Simvlm: Simple visual language model pretraining with weak supervision\", \"Llm-powered autonomous agents\", \"Gpt-4v in wonderland: Large multimodal models for zero-shot smartphone gui navigation\", \"Set-of-mark prompting unleashes extraordinary visual grounding in gpt-4v\", \"The dawn of $1 mms$ : Preliminary explorations with gpt-4v (ision)\", \"Webshop: Towards scalable real-world web interaction with grounded language agents\", \"React: Synergizing reasoning and acting in language models\", \"Lumos: Learning agents with unified data, modular design, and open-source llms\", \"From recognition to cognition: Visual commonsense reasoning\", \"Appagent: Multimodal agents as smartphone users\", \"Gpt-4v (ision) is a generalist web agent, if grounded\", \"Webarena: A realistic web environment for building autonomous agents\", \"Object detection in 20 years: A survey\"], \"Citation\": [\"Cm3: A causal masked multimodal model of the internet\", \"AutoGPT\", \"Language models are few-shot learners\", \"Evaluating large language models trained on code\", \"Webvln: Vision-and-language navigation on websites\", \"Seeclick: Harnessing gui grounding for advanced visual gui agents\", \"Palm: Scaling language modeling with pathways\", \"A coefficient of agreement for nominal scales\", \"Mind2web: Towards a generalist agent for the web\", \"A survey for in-context learning\", \"An image is worth $16 \\times 16$ words: Transformers for image recognition at scale\", \"Measuring nominal scale agreement among many raters\", \"Multimodal web navigation with instruction-finetuned foundation models\", \"Making the v in vqa matter: Elevating the role of image understanding in visual question answering\", \"A real-world webagent with planning, long context understanding, and program synthesis\", \"The abduction of sherlock holmes: A dataset for visual abductive reasoning\", \"Visualwebarena: Evaluating multimodal agents on realistic visual web tasks\", \"Pix2struct: Screenshot parsing as pretraining for visual language understanding\", \"Visualbert: A simple and performant baseline for vision and language\", \"Learn to explain: Multimodal reasoning via thought chains for science question answering\", \"Chameleon: Plug-and-play compositional reasoning with large language models\", \"Laser: Llm agent with state-space exploration for web navigation\", \"Gaia: a benchmark for general ai assistants\", \"Webgpt: Browser-assisted questionanswering with human feedback\", \"Gpt-4 technical report\", \"Training language models to follow instructions with human feedback\", \"Toolllm: Facilitating large language models to master 16000+ real-world apis\", \"Exploring the limits of transfer learning with a unified text-to-text transformer\", \"Toolformer: Language models can teach themselves to use tools\", \"From pixels to ui actions: Learning to follow instructions via graphical user interfaces\", \"World of bits: An open-domain platform for web-based agents\", \"Reflexion: Language agents with verbal reinforcement learning\", \"Gemini: a family of highly capable multimodal models\", \"Self-instruct: Aligning language model with self generated instructions\", \"Simvlm: Simple visual language model pretraining with weak supervision\", \"Llm-powered autonomous agents\", \"Gpt-4v in wonderland: Large multimodal models for zero-shot smartphone gui navigation\", \"Set-of-mark prompting unleashes extraordinary visual grounding in gpt-4v\", \"The dawn of $1 mms$ : Preliminary explorations with gpt-4v (ision)\", \"Webshop: Towards scalable real-world web interaction with grounded language agents\", \"React: Synergizing reasoning and acting in language models\", \"Lumos: Learning agents with unified data, modular design, and open-source llms\", \"From recognition to cognition: Visual commonsense reasoning\", \"Appagent: Multimodal agents as smartphone users\", \"Gpt-4v (ision) is a generalist web agent, if grounded\", \"Webarena: A realistic web environment for building autonomous agents\", \"Object detection in 20 years: A survey\"]}\n[The End of Assistant's Predicted Answer]\n\n[System]\nWe would like to request your feedback on the performance of the AI assistant in response to the user question displayed above according to the gold answer. Please use the following listed aspects and their descriptions as evaluation criteria:\n    - Accuracy and Hallucinations: The assistant's answer is semantically consistent with the gold answer; The numerical value and order need to be accurate, and there should be no hallucinations.\n    - Completeness: Referring to the reference answers, the assistant's answer should contain all the key points needed to answer the user's question; further elaboration on these key points can be omitted.\nPlease rate whether this answer is suitable for the question. Please note that the gold answer can be considered as a correct answer to the question.\n\nThe assistant receives an overall score on a scale of 1 to 100, where a higher score indicates better overall performance.\nPlease note that if the assistant's answer and the gold answer fully meet the above criteria, its overall rating should be the full marks (100).\nPlease first provide a comprehensive explanation of your evaluation, avoiding any potential bias.\nThen, output a line indicating the score of the Assistant.\n\nPLEASE OUTPUT WITH THE FOLLOWING FORMAT, WHERE THE SCORE IS A SCALE OF 1 TO 100 BY STRICTLY FOLLOWING THIS FORMAT: \"[[score]]\", FOR EXAMPLE \"Rating: [[100]]\":\n<start output>\nEvaluation evidence: your evluation explanation here, no more than 100 words\nRating: [[score]]\n<end output>\n\nNow, start your evaluation:",
        "generate_response": "{\"Reference\": [\"Cm3: A causal masked multimodal model of the internet\", \"AutoGPT\", \"Language models are few-shot learners\", \"Evaluating large language models trained on code\", \"Webvln: Vision-and-language navigation on websites\", \"Seeclick: Harnessing gui grounding for advanced visual gui agents\", \"Palm: Scaling language modeling with pathways\", \"A coefficient of agreement for nominal scales\", \"Mind2web: Towards a generalist agent for the web\", \"A survey for in-context learning\", \"An image is worth $16 \\times 16$ words: Transformers for image recognition at scale\", \"Measuring nominal scale agreement among many raters\", \"Multimodal web navigation with instruction-finetuned foundation models\", \"Making the v in vqa matter: Elevating the role of image understanding in visual question answering\", \"A real-world webagent with planning, long context understanding, and program synthesis\", \"The abduction of sherlock holmes: A dataset for visual abductive reasoning\", \"Visualwebarena: Evaluating multimodal agents on realistic visual web tasks\", \"Pix2struct: Screenshot parsing as pretraining for visual language understanding\", \"Visualbert: A simple and performant baseline for vision and language\", \"Learn to explain: Multimodal reasoning via thought chains for science question answering\", \"Chameleon: Plug-and-play compositional reasoning with large language models\", \"Laser: Llm agent with state-space exploration for web navigation\", \"Gaia: a benchmark for general ai assistants\", \"Webgpt: Browser-assisted questionanswering with human feedback\", \"Gpt-4 technical report\", \"Training language models to follow instructions with human feedback\", \"Toolllm: Facilitating large language models to master 16000+ real-world apis\", \"Exploring the limits of transfer learning with a unified text-to-text transformer\", \"Toolformer: Language models can teach themselves to use tools\", \"From pixels to ui actions: Learning to follow instructions via graphical user interfaces\", \"World of bits: An open-domain platform for web-based agents\", \"Reflexion: Language agents with verbal reinforcement learning\", \"Gemini: a family of highly capable multimodal models\", \"Self-instruct: Aligning language model with self generated instructions\", \"Simvlm: Simple visual language model pretraining with weak supervision\", \"Llm-powered autonomous agents\", \"Gpt-4v in wonderland: Large multimodal models for zero-shot smartphone gui navigation\", \"Set-of-mark prompting unleashes extraordinary visual grounding in gpt-4v\", \"The dawn of $1 mms$ : Preliminary explorations with gpt-4v (ision)\", \"Webshop: Towards scalable real-world web interaction with grounded language agents\", \"React: Synergizing reasoning and acting in language models\", \"Lumos: Learning agents with unified data, modular design, and open-source llms\", \"From recognition to cognition: Visual commonsense reasoning\", \"Appagent: Multimodal agents as smartphone users\", \"Gpt-4v (ision) is a generalist web agent, if grounded\", \"Webarena: A realistic web environment for building autonomous agents\", \"Object detection in 20 years: A survey\"], \"Citation\": [\"Cm3: A causal masked multimodal model of the internet\", \"AutoGPT\", \"Language models are few-shot learners\", \"Evaluating large language models trained on code\", \"Webvln: Vision-and-language navigation on websites\", \"Seeclick: Harnessing gui grounding for advanced visual gui agents\", \"Palm: Scaling language modeling with pathways\", \"A coefficient of agreement for nominal scales\", \"Mind2web: Towards a generalist agent for the web\", \"A survey for in-context learning\", \"An image is worth $16 \\times 16$ words: Transformers for image recognition at scale\", \"Measuring nominal scale agreement among many raters\", \"Multimodal web navigation with instruction-finetuned foundation models\", \"Making the v in vqa matter: Elevating the role of image understanding in visual question answering\", \"A real-world webagent with planning, long context understanding, and program synthesis\", \"The abduction of sherlock holmes: A dataset for visual abductive reasoning\", \"Visualwebarena: Evaluating multimodal agents on realistic visual web tasks\", \"Pix2struct: Screenshot parsing as pretraining for visual language understanding\", \"Visualbert: A simple and performant baseline for vision and language\", \"Learn to explain: Multimodal reasoning via thought chains for science question answering\", \"Chameleon: Plug-and-play compositional reasoning with large language models\", \"Laser: Llm agent with state-space exploration for web navigation\", \"Gaia: a benchmark for general ai assistants\", \"Webgpt: Browser-assisted questionanswering with human feedback\", \"Gpt-4 technical report\", \"Training language models to follow instructions with human feedback\", \"Toolllm: Facilitating large language models to master 16000+ real-world apis\", \"Exploring the limits of transfer learning with a unified text-to-text transformer\", \"Toolformer: Language models can teach themselves to use tools\", \"From pixels to ui actions: Learning to follow instructions via graphical user interfaces\", \"World of bits: An open-domain platform for web-based agents\", \"Reflexion: Language agents with verbal reinforcement learning\", \"Gemini: a family of highly capable multimodal models\", \"Self-instruct: Aligning language model with self generated instructions\", \"Simvlm: Simple visual language model pretraining with weak supervision\", \"Llm-powered autonomous agents\", \"Gpt-4v in wonderland: Large multimodal models for zero-shot smartphone gui navigation\", \"Set-of-mark prompting unleashes extraordinary visual grounding in gpt-4v\", \"The dawn of $1 mms$ : Preliminary explorations with gpt-4v (ision)\", \"Webshop: Towards scalable real-world web interaction with grounded language agents\", \"React: Synergizing reasoning and acting in language models\", \"Lumos: Learning agents with unified data, modular design, and open-source llms\", \"From recognition to cognition: Visual commonsense reasoning\", \"Appagent: Multimodal agents as smartphone users\", \"Gpt-4v (ision) is a generalist web agent, if grounded\", \"Webarena: A realistic web environment for building autonomous agents\", \"Object detection in 20 years: A survey\"]}",
        "eval_response": "Evaluation evidence: The assistant's response does not align with the gold answer, indicating a significant error in identifying the correct citation relationships for the paper \"WebVoyager: Building an End-to-End Web Agent with Large Multimodal Models.\" The gold answer lists two citations, while the assistant's response incorrectly lists numerous references and citations that are not mentioned in the gold answer. This discrepancy suggests a misunderstanding of the task requirements or an error in processing the correct documents, leading to hallucinations and inaccuracies in the response.\n\nRating: [[10]]",
        "score": 10
    },
    {
        "level": 4,
        "set": 4,
        "length": 229769,
        "type": "paper",
        "language": "en",
        "question": "",
        "instruction": "We kindly ask you to thoroughly review the provided papers and construct a citation chain from them. Please adhere to the following instructions strictly while completing the task:\n\n#Task Instructions:\nGiven several papers, you are required to identify and list the longest citation chain, which demonstrates the citation relationship among the provided papers.\n\n#Specific Requirements:\n1.Please present the titles of the papers in the form of a list, as follows: [\"Title of Paper 1\", \"Title of Paper 2\", ..., \"Title of Paper n\"].\n2.Ensure that the citation chain in the list is linear and continuous, meaning that the first paper title in the list (Paper 1) should not cite any other works. Instead, it should be cited by the next paper in the list (Paper 2); subsequently, each paper should then be cited by the next one in the list, continuing up to the last paper (Paper n).\n3.Consider only the citation relationships within the supplied collection of papers, and ensure that the citation chain accurately reflects the sequential citation order among these documents.\n4.Do not take into account any articles not provided, and disregard other non-linear citation relationships.",
        "prompt_template": "{instruction}\n\n#Papers Provided:\n{docs}",
        "doc": [
            "2212.10001.md",
            "2212.13138.md",
            "2303.13375.md",
            "2303.12712.md",
            "2304.12898.md"
        ],
        "answer": [
            "# Towards Understanding Chain-of-Thought Prompting: An Empirical Study of What Matters ",
            "# Large Language Models Encode Clinical Knowledge ",
            "# Capabilities of GPT-4 on Medical Challenge Problems ",
            "# Sparks of Artificial General Intelligence: Early experiments with GPT-4 ",
            "# ChatGPT believes it is conscious "
        ],
        "shuffle_doc": true,
        "id": "3ce4bb4c-fd5e-4ffe-824f-492849931f0f",
        "prompt": "[Question]\nWe kindly ask you to thoroughly review the provided papers and construct a citation chain from them. Please adhere to the following instructions strictly while completing the task:\n\n#Task Instructions:\nGiven several papers, you are required to identify and list the longest citation chain, which demonstrates the citation relationship among the provided papers.\n\n#Specific Requirements:\n1.Please present the titles of the papers in the form of a list, as follows: [\"Title of Paper 1\", \"Title of Paper 2\", ..., \"Title of Paper n\"].\n2.Ensure that the citation chain in the list is linear and continuous, meaning that the first paper title in the list (Paper 1) should not cite any other works. Instead, it should be cited by the next paper in the list (Paper 2); subsequently, each paper should then be cited by the next one in the list, continuing up to the last paper (Paper n).\n3.Consider only the citation relationships within the supplied collection of papers, and ensure that the citation chain accurately reflects the sequential citation order among these documents.\n4.Do not take into account any articles not provided, and disregard other non-linear citation relationships.\n\n#Papers Provided:\n{docs}\n\n[Gold Answer]\n['# Towards Understanding Chain-of-Thought Prompting: An Empirical Study of What Matters ', '# Large Language Models Encode Clinical Knowledge ', '# Capabilities of GPT-4 on Medical Challenge Problems ', '# Sparks of Artificial General Intelligence: Early experiments with GPT-4 ', '# ChatGPT believes it is conscious ']\n\n[The Start of Assistant's Predicted Answer]\n[\"Capabilities of GPT-4 on Medical Challenge Problems\", \"Large language models encode clinical knowledge\", \"A Survey on Evaluation of Large Language Models\"]\n[The End of Assistant's Predicted Answer]\n\n[System]\nWe would like to request your feedback on the performance of the AI assistant in response to the user question displayed above according to the gold answer. Please use the following listed aspects and their descriptions as evaluation criteria:\n    - Accuracy and Hallucinations: The assistant's answer is semantically consistent with the gold answer; The numerical value and order need to be accurate, and there should be no hallucinations.\n    - Completeness: Referring to the reference answers, the assistant's answer should contain all the key points needed to answer the user's question; further elaboration on these key points can be omitted.\nPlease rate whether this answer is suitable for the question. Please note that the gold answer can be considered as a correct answer to the question.\n\nThe assistant receives an overall score on a scale of 1 to 100, where a higher score indicates better overall performance.\nPlease note that if the assistant's answer and the gold answer fully meet the above criteria, its overall rating should be the full marks (100).\nPlease first provide a comprehensive explanation of your evaluation, avoiding any potential bias.\nThen, output a line indicating the score of the Assistant.\n\nPLEASE OUTPUT WITH THE FOLLOWING FORMAT, WHERE THE SCORE IS A SCALE OF 1 TO 100 BY STRICTLY FOLLOWING THIS FORMAT: \"[[score]]\", FOR EXAMPLE \"Rating: [[100]]\":\n<start output>\nEvaluation evidence: your evluation explanation here, no more than 100 words\nRating: [[score]]\n<end output>\n\nNow, start your evaluation:",
        "generate_response": "[\"Capabilities of GPT-4 on Medical Challenge Problems\", \"Large language models encode clinical knowledge\", \"A Survey on Evaluation of Large Language Models\"]",
        "eval_response": "Evaluation evidence: The assistant's answer does not match the gold answer in terms of the titles and the sequence of the citation chain. The titles listed by the assistant are partially incorrect and do not reflect a continuous citation chain as required by the task instructions. The gold answer provides a different set of titles and a specific citation order that the assistant's response fails to capture. This indicates issues with both accuracy and completeness.\n\nRating: [[20]]",
        "score": 20
    },
    {
        "level": 3,
        "set": 2,
        "length": 89596,
        "type": "paper",
        "language": "en",
        "question": "Eyes Wide Shut? Exploring the Visual Shortcomings of Multimodal LLMs",
        "instruction": "We hope you will carefully study the provided papers and determine the citation relationships between them. Please follow the instructions below strictly to complete the task:\n\n#Specific Requirements:\n1. Reference: When a given paper mentions other provided papers, those other papers are considered as \"references\" for the given paper. To summarize in this specific context, references are about what the given paper is using.\n2. Citation: Conversely, when other provided papers mention the given paper in their works, the given paper is being \"cited\" by those other papers. To summarize in this specific context, citations are about who is using the given paper.\n3. Given a paper, you need to determine the citation or reference relationship between this paper and the other papers. Do not consider papers that are not provided.\n3. Please present the paper titles in a json format as follows: {{\"Reference\":[\"Reference Title 1\", \"Reference Title 2\", ..., \"Reference Title n\"], \"Citation\":[\"Citation Title 1\", \"Citation Title 2\", ..., \"Citation Title n\"]}}.\n4. If a paper does not have any references or citations, please leave the corresponding list empty, e.g.{{\"Refernce\":[]}}, {{\"Citation\":[]}}.",
        "prompt_template": "#Papers:\n{docs}\n\n{instruction}\n\n#The paper you need to analyze:\n{question}",
        "doc": [
            "2401.06209.md",
            "2405.15306.md",
            "2405.17820.md",
            "2405.18415.md"
        ],
        "answer": {
            "Reference": [],
            "Citation": [
                "# DeTikZify: Synthesizing Graphics Programs for Scientific Figures and Sketches with TikZ ",
                "# Don't Miss the Forest for the Trees: Attentional Vision Calibration for Large Vision Language Models ",
                "# Why are Visually-Grounded Language Models Bad at Image Classification? "
            ]
        },
        "shuffle_doc": true,
        "id": "be13c879-47d0-4d2d-83f0-c15446addd09",
        "prompt": "[Question]\n#Papers:\n{docs}\n\nWe hope you will carefully study the provided papers and determine the citation relationships between them. Please follow the instructions below strictly to complete the task:\n\n#Specific Requirements:\n1. Reference: When a given paper mentions other provided papers, those other papers are considered as \"references\" for the given paper. To summarize in this specific context, references are about what the given paper is using.\n2. Citation: Conversely, when other provided papers mention the given paper in their works, the given paper is being \"cited\" by those other papers. To summarize in this specific context, citations are about who is using the given paper.\n3. Given a paper, you need to determine the citation or reference relationship between this paper and the other papers. Do not consider papers that are not provided.\n3. Please present the paper titles in a json format as follows: {{\"Reference\":[\"Reference Title 1\", \"Reference Title 2\", ..., \"Reference Title n\"], \"Citation\":[\"Citation Title 1\", \"Citation Title 2\", ..., \"Citation Title n\"]}}.\n4. If a paper does not have any references or citations, please leave the corresponding list empty, e.g.{{\"Refernce\":[]}}, {{\"Citation\":[]}}.\n\n#The paper you need to analyze:\nEyes Wide Shut? Exploring the Visual Shortcomings of Multimodal LLMs\n\n[Gold Answer]\n{'Reference': [], 'Citation': ['# DeTikZify: Synthesizing Graphics Programs for Scientific Figures and Sketches with TikZ ', \"# Don't Miss the Forest for the Trees: Attentional Vision Calibration for Large Vision Language Models \", '# Why are Visually-Grounded Language Models Bad at Image Classification? ']}\n\n[The Start of Assistant's Predicted Answer]\n{\"Reference\":[\"GPT-4V\", \"Multimodal Large Language Models\", \"pretrained vision and language models\", \"pretrained Contrastive Language-Image PreTraining (CLIP) model\", \"DINOv2\", \"Tong et al.\", \"Yuksekgonul et al.\", \"textguided generative models\", \"OpenAI ViT-L-14\", \"SigL\", \"Met:\", \"M\", \"EVA02 ViT-bigE-14+\", \"LLaVA 1.5\", \"InstructBLIP\", \"GPT-4\", \"LLaVA\", \"DINOV2-ViT-L-14\", \"CLIP-ViT-L-14\", \"MAE\", \"MoCoV3\", \"MMBench\", \"GQA\", \"Multimodal LLMs\", \"pretrained Large Language Models\", \"MLPs\", \"Q-Former\", \"gated attention\", \"Radford et al.\", \"SLIP\", \"Thrush et al.\", \"Yuksekgonul et al.\", \"Hsieh et al.\", \"Tschannen et al.\"], \"Citation\":[\"Why are Visually-Grounded Language Models Bad at Image Classification?\"]}\n[The End of Assistant's Predicted Answer]\n\n[System]\nWe would like to request your feedback on the performance of the AI assistant in response to the user question displayed above according to the gold answer. Please use the following listed aspects and their descriptions as evaluation criteria:\n    - Accuracy and Hallucinations: The assistant's answer is semantically consistent with the gold answer; The numerical value and order need to be accurate, and there should be no hallucinations.\n    - Completeness: Referring to the reference answers, the assistant's answer should contain all the key points needed to answer the user's question; further elaboration on these key points can be omitted.\nPlease rate whether this answer is suitable for the question. Please note that the gold answer can be considered as a correct answer to the question.\n\nThe assistant receives an overall score on a scale of 1 to 100, where a higher score indicates better overall performance.\nPlease note that if the assistant's answer and the gold answer fully meet the above criteria, its overall rating should be the full marks (100).\nPlease first provide a comprehensive explanation of your evaluation, avoiding any potential bias.\nThen, output a line indicating the score of the Assistant.\n\nPLEASE OUTPUT WITH THE FOLLOWING FORMAT, WHERE THE SCORE IS A SCALE OF 1 TO 100 BY STRICTLY FOLLOWING THIS FORMAT: \"[[score]]\", FOR EXAMPLE \"Rating: [[100]]\":\n<start output>\nEvaluation evidence: your evluation explanation here, no more than 100 words\nRating: [[score]]\n<end output>\n\nNow, start your evaluation:",
        "generate_response": "{\"Reference\":[\"GPT-4V\", \"Multimodal Large Language Models\", \"pretrained vision and language models\", \"pretrained Contrastive Language-Image PreTraining (CLIP) model\", \"DINOv2\", \"Tong et al.\", \"Yuksekgonul et al.\", \"textguided generative models\", \"OpenAI ViT-L-14\", \"SigL\", \"Met:\", \"M\", \"EVA02 ViT-bigE-14+\", \"LLaVA 1.5\", \"InstructBLIP\", \"GPT-4\", \"LLaVA\", \"DINOV2-ViT-L-14\", \"CLIP-ViT-L-14\", \"MAE\", \"MoCoV3\", \"MMBench\", \"GQA\", \"Multimodal LLMs\", \"pretrained Large Language Models\", \"MLPs\", \"Q-Former\", \"gated attention\", \"Radford et al.\", \"SLIP\", \"Thrush et al.\", \"Yuksekgonul et al.\", \"Hsieh et al.\", \"Tschannen et al.\"], \"Citation\":[\"Why are Visually-Grounded Language Models Bad at Image Classification?\"]}",
        "eval_response": "Evaluation evidence: The assistant's answer inaccurately lists numerous references that are not mentioned in the gold answer, indicating a significant hallucination issue. The citation part of the assistant's answer partially matches the gold answer but omits two crucial citations. This results in both inaccuracies and incompleteness in addressing the task requirements. The assistant's response does not align well with the gold answer, leading to a low score.\n\nRating: [[20]]",
        "score": 20
    },
    {
        "level": 3,
        "set": 3,
        "length": 122849,
        "type": "paper",
        "language": "en",
        "question": "KTO: Model Alignment as Prospect Theoretic Optimization",
        "instruction": "We hope you will carefully study the provided papers and determine the citation relationships between them. Please follow the instructions below strictly to complete the task:\n\n#Specific Requirements:\n1. Reference: When a given paper mentions other provided papers, those other papers are considered as \"references\" for the given paper. To summarize in this specific context, references are about what the given paper is using.\n2. Citation: Conversely, when other provided papers mention the given paper in their works, the given paper is being \"cited\" by those other papers. To summarize in this specific context, citations are about who is using the given paper.\n3. Given a paper, you need to determine the citation or reference relationship between this paper and the other papers. Do not consider papers that are not provided.\n3. Please present the paper titles in a json format as follows: {{\"Reference\":[\"Reference Title 1\", \"Reference Title 2\", ..., \"Reference Title n\"], \"Citation\":[\"Citation Title 1\", \"Citation Title 2\", ..., \"Citation Title n\"]}}.\n4. If a paper does not have any references or citations, please leave the corresponding list empty, e.g.{{\"Refernce\":[]}}, {{\"Citation\":[]}}.",
        "prompt_template": "#Papers:\n{docs}\n\n{instruction}\n\n#The paper you need to analyze:\n{question}",
        "doc": [
            "2402.01306.md",
            "2405.21046.md",
            "2311.08401.md",
            "2401.01335.md"
        ],
        "answer": {
            "Reference": [
                "# Fine-tuning Language Models for Factuality ",
                "# Self-Play Fine-Tuning Converts Weak Language Models to Strong Language Models "
            ],
            "Citation": [
                "# Exploratory Preference Optimization: "
            ]
        },
        "shuffle_doc": true,
        "id": "7cb9fcc8-9c3f-4518-bff9-61e85238ab5c",
        "prompt": "[Question]\n#Papers:\n{docs}\n\nWe hope you will carefully study the provided papers and determine the citation relationships between them. Please follow the instructions below strictly to complete the task:\n\n#Specific Requirements:\n1. Reference: When a given paper mentions other provided papers, those other papers are considered as \"references\" for the given paper. To summarize in this specific context, references are about what the given paper is using.\n2. Citation: Conversely, when other provided papers mention the given paper in their works, the given paper is being \"cited\" by those other papers. To summarize in this specific context, citations are about who is using the given paper.\n3. Given a paper, you need to determine the citation or reference relationship between this paper and the other papers. Do not consider papers that are not provided.\n3. Please present the paper titles in a json format as follows: {{\"Reference\":[\"Reference Title 1\", \"Reference Title 2\", ..., \"Reference Title n\"], \"Citation\":[\"Citation Title 1\", \"Citation Title 2\", ..., \"Citation Title n\"]}}.\n4. If a paper does not have any references or citations, please leave the corresponding list empty, e.g.{{\"Refernce\":[]}}, {{\"Citation\":[]}}.\n\n#The paper you need to analyze:\nKTO: Model Alignment as Prospect Theoretic Optimization\n\n[Gold Answer]\n{'Reference': ['# Fine-tuning Language Models for Factuality ', '# Self-Play Fine-Tuning Converts Weak Language Models to Strong Language Models '], 'Citation': ['# Exploratory Preference Optimization: ']}\n\n[The Start of Assistant's Predicted Answer]\n{\"Reference\":[\"Benchmarking Large Language Models in Retrieval-Augmented Generation\", \"A Survey on Evaluation of Large Language Models\", \"Vicuna: An Open-Source Chatbot Impressing GPT-4 with $90 \\% *$ ChatGPT Quality\", \"ChatLaw: Open-Source Legal Large Language Model with Integrated External Knowledge Bases\", \"Compositional Semantic Parsing with Large Language Models\", \"Open LLM Leaderboard\", \"How Close is ChatGPT to Human Experts? Comparison Corpus, Evaluation, and Detection\", \"REALM: Retrieval-Augmented Language Model PreTraining\", \"Rethinking with Retrieval: Faithful Large Language Model Inference\", \"Measuring Massive Multitask Language Understanding\", \"Improving language models with advantage-based offline policy gradients\", \"Training a helpful and harmless assistant with reinforcement learning from human feedback\", \"Pythia: A suite for analyzing large language models across training and scaling\", \"Rank analysis of incomplete block designs: I. the method of paired comparisons\", \"Preference-based reinforcement learning: evolutionary direct policy search using a preference-based racing algorithm\", \"Open problems and fundamental limitations of reinforcement learning from human feedback\", \"Human irrationality: both bad and good for reward inference\", \"Evaluating large language models trained on code\", \"Self-play fine-tuning converts weak language models to strong language models\", \"Deep reinforcement learning from human preferences\", \"Training verifiers to solve math word problems\", \"Ultrafeedback: Boosting language models with high-quality feedback\", \"Understanding dataset difficulty with $\\mathcal{V}$-usable information\", \"Red teaming language models to reduce harms: Methods, scaling behaviors, and lessons learned\", \"Aligning language models with preferences through f-divergence minimization\", \"Decision-making under uncertainty-a field study of cumulative prospect theory\", \"Neural collaborative filtering\", \"Measuring massive multitask language understanding\", \"Constructing stable preferences: A look into dimensions of experience and their impact on preference stability\", \"A simple sequentially rejective multiple test procedure\", \"Learning trajectory preferences for manipulators via iterative improvement\", \"Mistral 7b\", \"Prospect theory: An analysis of decision under risk\", \"Openassistant conversationsdemocratizing large language model alignment\", \"Reliability and learnability of human bandit feedback for sequence-to-sequence reinforcement learning\", \"When humans aren't optimal: Robots that collaborate with risk-aware humans\", \"Alpacaeval: An automatic evaluator of instruction-following models\", \"Training language models to follow instructions with human feedback\", \"Advantage-weighted regression: Simple and scalable off-policy reinforcement learning\", \"Reinforcement learning by reward-weighted regression for operational space control\", \"Direct preference optimization: Your language model is secretly a reward model\", \"Proximal policy optimization algorithms\", \"Beyond the imitation game: Quantifying and extrapolating the capabilities of language models\", \"Learning to summarize with human feedback\", \"Interpretable modelling of driving behaviors in interactive driving scenarios based on cumulative prospect theory\", \"Fine-tuning language models for factuality\", \"Fine-tuning language models from human preferences\"], \"Citation\":[]}\n[The End of Assistant's Predicted Answer]\n\n[System]\nWe would like to request your feedback on the performance of the AI assistant in response to the user question displayed above according to the gold answer. Please use the following listed aspects and their descriptions as evaluation criteria:\n    - Accuracy and Hallucinations: The assistant's answer is semantically consistent with the gold answer; The numerical value and order need to be accurate, and there should be no hallucinations.\n    - Completeness: Referring to the reference answers, the assistant's answer should contain all the key points needed to answer the user's question; further elaboration on these key points can be omitted.\nPlease rate whether this answer is suitable for the question. Please note that the gold answer can be considered as a correct answer to the question.\n\nThe assistant receives an overall score on a scale of 1 to 100, where a higher score indicates better overall performance.\nPlease note that if the assistant's answer and the gold answer fully meet the above criteria, its overall rating should be the full marks (100).\nPlease first provide a comprehensive explanation of your evaluation, avoiding any potential bias.\nThen, output a line indicating the score of the Assistant.\n\nPLEASE OUTPUT WITH THE FOLLOWING FORMAT, WHERE THE SCORE IS A SCALE OF 1 TO 100 BY STRICTLY FOLLOWING THIS FORMAT: \"[[score]]\", FOR EXAMPLE \"Rating: [[100]]\":\n<start output>\nEvaluation evidence: your evluation explanation here, no more than 100 words\nRating: [[score]]\n<end output>\n\nNow, start your evaluation:",
        "generate_response": "{\"Reference\":[\"Benchmarking Large Language Models in Retrieval-Augmented Generation\", \"A Survey on Evaluation of Large Language Models\", \"Vicuna: An Open-Source Chatbot Impressing GPT-4 with $90 \\% *$ ChatGPT Quality\", \"ChatLaw: Open-Source Legal Large Language Model with Integrated External Knowledge Bases\", \"Compositional Semantic Parsing with Large Language Models\", \"Open LLM Leaderboard\", \"How Close is ChatGPT to Human Experts? Comparison Corpus, Evaluation, and Detection\", \"REALM: Retrieval-Augmented Language Model PreTraining\", \"Rethinking with Retrieval: Faithful Large Language Model Inference\", \"Measuring Massive Multitask Language Understanding\", \"Improving language models with advantage-based offline policy gradients\", \"Training a helpful and harmless assistant with reinforcement learning from human feedback\", \"Pythia: A suite for analyzing large language models across training and scaling\", \"Rank analysis of incomplete block designs: I. the method of paired comparisons\", \"Preference-based reinforcement learning: evolutionary direct policy search using a preference-based racing algorithm\", \"Open problems and fundamental limitations of reinforcement learning from human feedback\", \"Human irrationality: both bad and good for reward inference\", \"Evaluating large language models trained on code\", \"Self-play fine-tuning converts weak language models to strong language models\", \"Deep reinforcement learning from human preferences\", \"Training verifiers to solve math word problems\", \"Ultrafeedback: Boosting language models with high-quality feedback\", \"Understanding dataset difficulty with $\\mathcal{V}$-usable information\", \"Red teaming language models to reduce harms: Methods, scaling behaviors, and lessons learned\", \"Aligning language models with preferences through f-divergence minimization\", \"Decision-making under uncertainty-a field study of cumulative prospect theory\", \"Neural collaborative filtering\", \"Measuring massive multitask language understanding\", \"Constructing stable preferences: A look into dimensions of experience and their impact on preference stability\", \"A simple sequentially rejective multiple test procedure\", \"Learning trajectory preferences for manipulators via iterative improvement\", \"Mistral 7b\", \"Prospect theory: An analysis of decision under risk\", \"Openassistant conversationsdemocratizing large language model alignment\", \"Reliability and learnability of human bandit feedback for sequence-to-sequence reinforcement learning\", \"When humans aren't optimal: Robots that collaborate with risk-aware humans\", \"Alpacaeval: An automatic evaluator of instruction-following models\", \"Training language models to follow instructions with human feedback\", \"Advantage-weighted regression: Simple and scalable off-policy reinforcement learning\", \"Reinforcement learning by reward-weighted regression for operational space control\", \"Direct preference optimization: Your language model is secretly a reward model\", \"Proximal policy optimization algorithms\", \"Beyond the imitation game: Quantifying and extrapolating the capabilities of language models\", \"Learning to summarize with human feedback\", \"Interpretable modelling of driving behaviors in interactive driving scenarios based on cumulative prospect theory\", \"Fine-tuning language models for factuality\", \"Fine-tuning language models from human preferences\"], \"Citation\":[]}",
        "eval_response": "Evaluation evidence: The assistant's answer does not match the gold answer in terms of accuracy and completeness. The assistant listed numerous references that are not mentioned in the gold answer, indicating a significant hallucination issue. Additionally, the assistant failed to identify any citations, whereas the gold answer lists one citation. This discrepancy shows a lack of accuracy in identifying the correct citation relationships and an overgeneration of references that are not relevant or verified.\n\nRating: [[20]]",
        "score": 20
    },
    {
        "level": 3,
        "set": 3,
        "length": 123654,
        "type": "paper",
        "language": "en",
        "question": "OpenMoE: An Early Effort on Open Mixture-of-Experts Language Models",
        "instruction": "We hope you will carefully study the provided papers and determine the citation relationships between them. Please follow the instructions below strictly to complete the task:\n\n#Specific Requirements:\n1. Reference: When a given paper mentions other provided papers, those other papers are considered as \"references\" for the given paper. To summarize in this specific context, references are about what the given paper is using.\n2. Citation: Conversely, when other provided papers mention the given paper in their works, the given paper is being \"cited\" by those other papers. To summarize in this specific context, citations are about who is using the given paper.\n3. Given a paper, you need to determine the citation or reference relationship between this paper and the other papers. Do not consider papers that are not provided.\n3. Please present the paper titles in a json format as follows: {{\"Reference\":[\"Reference Title 1\", \"Reference Title 2\", ..., \"Reference Title n\"], \"Citation\":[\"Citation Title 1\", \"Citation Title 2\", ..., \"Citation Title n\"]}}.\n4. If a paper does not have any references or citations, please leave the corresponding list empty, e.g.{{\"Refernce\":[]}}, {{\"Citation\":[]}}.",
        "prompt_template": "#Papers:\n{docs}\n\n{instruction}\n\n#The paper you need to analyze:\n{question}",
        "doc": [
            "2402.01739.md",
            "2404.15381.md",
            "2405.01029.md",
            "2405.03133.md"
        ],
        "answer": {
            "Reference": [],
            "Citation": [
                "# Advances and Open Challenges in Federated Learning with Foundation Models ",
                "# MVMoE: Multi-Task Vehicle Routing Solver with Mixture-of-Experts ",
                "# Lory: Fully Differentiable Mixture-of-Experts for Autoregressive Language Model Pre-training "
            ]
        },
        "shuffle_doc": true,
        "id": "a102e82e-301c-4546-8184-e6c4a28ed511",
        "prompt": "[Question]\n#Papers:\n{docs}\n\nWe hope you will carefully study the provided papers and determine the citation relationships between them. Please follow the instructions below strictly to complete the task:\n\n#Specific Requirements:\n1. Reference: When a given paper mentions other provided papers, those other papers are considered as \"references\" for the given paper. To summarize in this specific context, references are about what the given paper is using.\n2. Citation: Conversely, when other provided papers mention the given paper in their works, the given paper is being \"cited\" by those other papers. To summarize in this specific context, citations are about who is using the given paper.\n3. Given a paper, you need to determine the citation or reference relationship between this paper and the other papers. Do not consider papers that are not provided.\n3. Please present the paper titles in a json format as follows: {{\"Reference\":[\"Reference Title 1\", \"Reference Title 2\", ..., \"Reference Title n\"], \"Citation\":[\"Citation Title 1\", \"Citation Title 2\", ..., \"Citation Title n\"]}}.\n4. If a paper does not have any references or citations, please leave the corresponding list empty, e.g.{{\"Refernce\":[]}}, {{\"Citation\":[]}}.\n\n#The paper you need to analyze:\nOpenMoE: An Early Effort on Open Mixture-of-Experts Language Models\n\n[Gold Answer]\n{'Reference': [], 'Citation': ['# Advances and Open Challenges in Federated Learning with Foundation Models ', '# MVMoE: Multi-Task Vehicle Routing Solver with Mixture-of-Experts ', '# Lory: Fully Differentiable Mixture-of-Experts for Autoregressive Language Model Pre-training ']}\n\n[The Start of Assistant's Predicted Answer]\n{\"Reference\":[\"Mixture-of-Experts (MoE) based large language models (LLMs)\", \"ChatGPT\", \"Bard\", \"Copilot\", \"REALM: Retrieval-Augmented Language Model PreTraining\", \"GLaM\", \"ST-MoE\", \"Mixtral\", \"DeepSeek-MoE\", \"Palm 2 technical report\", \"(inthe)wildchat: 570k chatGPT interaction logs in the wild\", \"Efficient large scale language modeling with mixtures of experts\", \"Palm: Scaling language modeling with pathways\", \"Unimax: Fairer and more effective language sampling for large-scale multilingual pretraining\", \"Megablocks: Efficient sparse training with mixture-of-experts\", \"A framework for few-shot language model evaluation\", \"Openllama: An open reproduction of llama\", \"Measuring massive multitask language understanding\", \"Training compute-optimal large language models\", \"Sparse upcycling: Training mixture-of-experts from dense checkpoints\", \"Gshard: Scaling giant models with conditional computation and automatic sharding\", \"Base layers: Simplifying training of large, sparse models\", \"Self-prompting large language models for open-domain qa\", \"Starcoder: May the source be with you!\", \"Roberta: A robustly optimized bert pretraining approach\", \"Xgen-7b technical report\", \"From sparse to soft mixtures of experts\", \"Scaling language models: Methods, analysis \\& insights from training gopher\", \"Exploring the limits of transfer learning with a unified text-to-text transformer\", \"Scaling vision with sparse mixture of experts\", \"Hash layers for large sparse models\", \"Code llama: Open foundation models for code\", \"Glu variants improve transformer\", \"Outrageously large neural networks: The sparsely-gated mixture-of-experts layer\", \"Megatron-lm: Training multi-billion parameter language models using model parallelism\", \"Dolma: An Open Corpus of Three Trillion Tokens for Language Model Pretraining Research\", \"Roformer: Enhanced transformer with rotary position embedding\", \"U12: Unifying language learning paradigms\", \"Unifying language learning paradigms\", \"Llama: Open and efficient foundation language models\", \"GPT-J-6B: A 6 Billion Parameter Autoregressive Language Model\", \"CCNet: Extracting high quality monolingual datasets from web crawl data\", \"Gspmd: General and scalable parallelization for ml computation graphs\", \"One student knows all experts know: From sparse to dense\", \"Go wider instead of deeper\", \"Efficient language modeling with sparse all-mlp\", \"Tinyllama: An open-source small language model\", \"Deep long-tailed learning: A survey\", \"Judging llm-as-a-judge with mt-bench and chatbot arena\", \"Brainformers: Trading simplicity for efficiency\", \"Mixture-of-experts with expert choice routing\", \"ST-MoE: Designing stable and transferable sparse expert models\"], \"Citation\":[\"MVMoE: Multi-Task Vehicle Routing Solver with Mixture-of-Experts\", \"Beyond the imitation game: Quantifying and extrapolating the capabilities of language models\", \"Efficient training of language models to fill in the middle\", \"Findings of the 2016 conference on machine translation\", \"Language models are few-shot learners\", \"Evaluating large language models trained on code\", \"Palm: Scaling language modeling with pathways\", \"Megablocks: Efficient sparse training with mixture-of-experts\", \"A framework for few-shot language model evaluation\", \"Openllama: An open reproduction of llama\", \"Measuring massive multitask language understanding\", \"Training compute-optimal large language models\", \"Mixtral of experts\", \"From sparse to soft mixtures of experts\", \"Hash layers for large sparse models\", \"Code llama: Open foundation models for code\", \"Glu variants improve transformer\", \"Outrageously large neural models: The sparsely-gated mixture-of-experts layer\", \"Megatron-lm: Training multi-billion parameter language models using model parallelism\", \"Dolma: An Open Corpus of Three Trillion Tokens for Language Model Pretraining Research\", \"Roformer: Enhanced transformer with rotary position embedding\", \"U12: Unifying language learning paradigms\", \"Unifying language learning paradigms\", \"Llama: Open and efficient foundation language models\", \"GPT-J-6B: A 6 Billion Parameter Autoregressive Language Model\", \"CCNet: Extracting high quality monolingual datasets from web crawl data\", \"Gspmd: General and scalable parallelization for ml computation graphs\", \"One student knows all experts know: From sparse to dense\", \"Go wider instead of deeper\", \"Efficient language modeling with sparse all-mlp\", \"Tinyllama: An open-source small language model\", \"Deep long-tailed learning: A survey\", \"Judging llm-as-a-judge with mt-bench and chatbot arena\", \"Brainformers: Trading simplicity for efficiency\", \"Mixture-of-experts with expert choice routing\", \"ST-MoE: Designing stable and transferable sparse expert models\"]}\n[The End of Assistant's Predicted Answer]\n\n[System]\nWe would like to request your feedback on the performance of the AI assistant in response to the user question displayed above according to the gold answer. Please use the following listed aspects and their descriptions as evaluation criteria:\n    - Accuracy and Hallucinations: The assistant's answer is semantically consistent with the gold answer; The numerical value and order need to be accurate, and there should be no hallucinations.\n    - Completeness: Referring to the reference answers, the assistant's answer should contain all the key points needed to answer the user's question; further elaboration on these key points can be omitted.\nPlease rate whether this answer is suitable for the question. Please note that the gold answer can be considered as a correct answer to the question.\n\nThe assistant receives an overall score on a scale of 1 to 100, where a higher score indicates better overall performance.\nPlease note that if the assistant's answer and the gold answer fully meet the above criteria, its overall rating should be the full marks (100).\nPlease first provide a comprehensive explanation of your evaluation, avoiding any potential bias.\nThen, output a line indicating the score of the Assistant.\n\nPLEASE OUTPUT WITH THE FOLLOWING FORMAT, WHERE THE SCORE IS A SCALE OF 1 TO 100 BY STRICTLY FOLLOWING THIS FORMAT: \"[[score]]\", FOR EXAMPLE \"Rating: [[100]]\":\n<start output>\nEvaluation evidence: your evluation explanation here, no more than 100 words\nRating: [[score]]\n<end output>\n\nNow, start your evaluation:",
        "generate_response": "{\"Reference\":[\"Mixture-of-Experts (MoE) based large language models (LLMs)\", \"ChatGPT\", \"Bard\", \"Copilot\", \"REALM: Retrieval-Augmented Language Model PreTraining\", \"GLaM\", \"ST-MoE\", \"Mixtral\", \"DeepSeek-MoE\", \"Palm 2 technical report\", \"(inthe)wildchat: 570k chatGPT interaction logs in the wild\", \"Efficient large scale language modeling with mixtures of experts\", \"Palm: Scaling language modeling with pathways\", \"Unimax: Fairer and more effective language sampling for large-scale multilingual pretraining\", \"Megablocks: Efficient sparse training with mixture-of-experts\", \"A framework for few-shot language model evaluation\", \"Openllama: An open reproduction of llama\", \"Measuring massive multitask language understanding\", \"Training compute-optimal large language models\", \"Sparse upcycling: Training mixture-of-experts from dense checkpoints\", \"Gshard: Scaling giant models with conditional computation and automatic sharding\", \"Base layers: Simplifying training of large, sparse models\", \"Self-prompting large language models for open-domain qa\", \"Starcoder: May the source be with you!\", \"Roberta: A robustly optimized bert pretraining approach\", \"Xgen-7b technical report\", \"From sparse to soft mixtures of experts\", \"Scaling language models: Methods, analysis \\& insights from training gopher\", \"Exploring the limits of transfer learning with a unified text-to-text transformer\", \"Scaling vision with sparse mixture of experts\", \"Hash layers for large sparse models\", \"Code llama: Open foundation models for code\", \"Glu variants improve transformer\", \"Outrageously large neural networks: The sparsely-gated mixture-of-experts layer\", \"Megatron-lm: Training multi-billion parameter language models using model parallelism\", \"Dolma: An Open Corpus of Three Trillion Tokens for Language Model Pretraining Research\", \"Roformer: Enhanced transformer with rotary position embedding\", \"U12: Unifying language learning paradigms\", \"Unifying language learning paradigms\", \"Llama: Open and efficient foundation language models\", \"GPT-J-6B: A 6 Billion Parameter Autoregressive Language Model\", \"CCNet: Extracting high quality monolingual datasets from web crawl data\", \"Gspmd: General and scalable parallelization for ml computation graphs\", \"One student knows all experts know: From sparse to dense\", \"Go wider instead of deeper\", \"Efficient language modeling with sparse all-mlp\", \"Tinyllama: An open-source small language model\", \"Deep long-tailed learning: A survey\", \"Judging llm-as-a-judge with mt-bench and chatbot arena\", \"Brainformers: Trading simplicity for efficiency\", \"Mixture-of-experts with expert choice routing\", \"ST-MoE: Designing stable and transferable sparse expert models\"], \"Citation\":[\"MVMoE: Multi-Task Vehicle Routing Solver with Mixture-of-Experts\", \"Beyond the imitation game: Quantifying and extrapolating the capabilities of language models\", \"Efficient training of language models to fill in the middle\", \"Findings of the 2016 conference on machine translation\", \"Language models are few-shot learners\", \"Evaluating large language models trained on code\", \"Palm: Scaling language modeling with pathways\", \"Megablocks: Efficient sparse training with mixture-of-experts\", \"A framework for few-shot language model evaluation\", \"Openllama: An open reproduction of llama\", \"Measuring massive multitask language understanding\", \"Training compute-optimal large language models\", \"Mixtral of experts\", \"From sparse to soft mixtures of experts\", \"Hash layers for large sparse models\", \"Code llama: Open foundation models for code\", \"Glu variants improve transformer\", \"Outrageously large neural models: The sparsely-gated mixture-of-experts layer\", \"Megatron-lm: Training multi-billion parameter language models using model parallelism\", \"Dolma: An Open Corpus of Three Trillion Tokens for Language Model Pretraining Research\", \"Roformer: Enhanced transformer with rotary position embedding\", \"U12: Unifying language learning paradigms\", \"Unifying language learning paradigms\", \"Llama: Open and efficient foundation language models\", \"GPT-J-6B: A 6 Billion Parameter Autoregressive Language Model\", \"CCNet: Extracting high quality monolingual datasets from web crawl data\", \"Gspmd: General and scalable parallelization for ml computation graphs\", \"One student knows all experts know: From sparse to dense\", \"Go wider instead of deeper\", \"Efficient language modeling with sparse all-mlp\", \"Tinyllama: An open-source small language model\", \"Deep long-tailed learning: A survey\", \"Judging llm-as-a-judge with mt-bench and chatbot arena\", \"Brainformers: Trading simplicity for efficiency\", \"Mixture-of-experts with expert choice routing\", \"ST-MoE: Designing stable and transferable sparse expert models\"]}",
        "eval_response": "Evaluation evidence: The assistant's response fails to meet the requirements of accuracy and completeness. The provided answer includes a long list of references and citations that are not verified against the provided gold answer, which only lists three citations and no references. This discrepancy indicates significant hallucinations and inaccuracies in the assistant's response. The assistant's answer does not align with the gold answer, showing a failure to correctly identify the citation relationships for the specified paper.\n\nRating: [[10]]",
        "score": 10
    },
    {
        "level": 4,
        "set": 2,
        "length": 59122,
        "type": "paper",
        "language": "en",
        "question": "",
        "instruction": "We kindly ask you to thoroughly review the provided papers and construct a citation chain from them. Please adhere to the following instructions strictly while completing the task:\n\n#Task Instructions:\nGiven several papers, you are required to identify and list the longest citation chain, which demonstrates the citation relationship among the provided papers.\n\n#Specific Requirements:\n1.Please present the titles of the papers in the form of a list, as follows: [\"Title of Paper 1\", \"Title of Paper 2\", ..., \"Title of Paper n\"].\n2.Ensure that the citation chain in the list is linear and continuous, meaning that the first paper title in the list (Paper 1) should not cite any other works. Instead, it should be cited by the next paper in the list (Paper 2); subsequently, each paper should then be cited by the next one in the list, continuing up to the last paper (Paper n).\n3.Consider only the citation relationships within the supplied collection of papers, and ensure that the citation chain accurately reflects the sequential citation order among these documents.\n4.Do not take into account any articles not provided, and disregard other non-linear citation relationships.",
        "prompt_template": "{instruction}\n\n#Papers Provided:\n{docs}",
        "doc": [
            "2302.14376.md",
            "2303.08891.md",
            "2305.17560.md"
        ],
        "answer": [
            "# GNOT: A General Neural Operator Transformer for Operator Learning ",
            "# VITO: VISION TRANSFORMER-OPERATOR ",
            "# Scalable Transformer for PDE Surrogate Modeling "
        ],
        "shuffle_doc": true,
        "id": "14b44654-5684-4dc0-ada4-215a25c4b56b",
        "prompt": "[Question]\nWe kindly ask you to thoroughly review the provided papers and construct a citation chain from them. Please adhere to the following instructions strictly while completing the task:\n\n#Task Instructions:\nGiven several papers, you are required to identify and list the longest citation chain, which demonstrates the citation relationship among the provided papers.\n\n#Specific Requirements:\n1.Please present the titles of the papers in the form of a list, as follows: [\"Title of Paper 1\", \"Title of Paper 2\", ..., \"Title of Paper n\"].\n2.Ensure that the citation chain in the list is linear and continuous, meaning that the first paper title in the list (Paper 1) should not cite any other works. Instead, it should be cited by the next paper in the list (Paper 2); subsequently, each paper should then be cited by the next one in the list, continuing up to the last paper (Paper n).\n3.Consider only the citation relationships within the supplied collection of papers, and ensure that the citation chain accurately reflects the sequential citation order among these documents.\n4.Do not take into account any articles not provided, and disregard other non-linear citation relationships.\n\n#Papers Provided:\n{docs}\n\n[Gold Answer]\n['# GNOT: A General Neural Operator Transformer for Operator Learning ', '# VITO: VISION TRANSFORMER-OPERATOR ', '# Scalable Transformer for PDE Surrogate Modeling ']\n\n[The Start of Assistant's Predicted Answer]\n[\"Deep Operator Network (DeepONet)\", \"Fourier Neural Operator (FNO)\", \"Transformer for partial differential equations' operator learning\", \"A Survey on Evaluation of Large Language Models\", \"GNOT: A General Neural Operator Transformer for Operator Learning\"]\n[The End of Assistant's Predicted Answer]\n\n[System]\nWe would like to request your feedback on the performance of the AI assistant in response to the user question displayed above according to the gold answer. Please use the following listed aspects and their descriptions as evaluation criteria:\n    - Accuracy and Hallucinations: The assistant's answer is semantically consistent with the gold answer; The numerical value and order need to be accurate, and there should be no hallucinations.\n    - Completeness: Referring to the reference answers, the assistant's answer should contain all the key points needed to answer the user's question; further elaboration on these key points can be omitted.\nPlease rate whether this answer is suitable for the question. Please note that the gold answer can be considered as a correct answer to the question.\n\nThe assistant receives an overall score on a scale of 1 to 100, where a higher score indicates better overall performance.\nPlease note that if the assistant's answer and the gold answer fully meet the above criteria, its overall rating should be the full marks (100).\nPlease first provide a comprehensive explanation of your evaluation, avoiding any potential bias.\nThen, output a line indicating the score of the Assistant.\n\nPLEASE OUTPUT WITH THE FOLLOWING FORMAT, WHERE THE SCORE IS A SCALE OF 1 TO 100 BY STRICTLY FOLLOWING THIS FORMAT: \"[[score]]\", FOR EXAMPLE \"Rating: [[100]]\":\n<start output>\nEvaluation evidence: your evluation explanation here, no more than 100 words\nRating: [[score]]\n<end output>\n\nNow, start your evaluation:",
        "generate_response": "[\"Deep Operator Network (DeepONet)\", \"Fourier Neural Operator (FNO)\", \"Transformer for partial differential equations' operator learning\", \"A Survey on Evaluation of Large Language Models\", \"GNOT: A General Neural Operator Transformer for Operator Learning\"]",
        "eval_response": "Evaluation evidence: The assistant's answer does not match the gold answer in terms of the titles of the papers and the sequence of the citation chain. The gold answer lists a specific citation chain: [\"GNOT: A General Neural Operator Transformer for Operator Learning\", \"VITO: VISION TRANSFORMER-OPERATOR\", \"Scalable Transformer for PDE Surrogate Modeling\"]. The assistant's answer includes different titles and does not form a correct citation chain as per the task requirements. This indicates issues with both accuracy and completeness.\n\nRating: [[20]]",
        "score": 20
    },
    {
        "level": 3,
        "set": 3,
        "length": 101058,
        "type": "paper",
        "language": "en",
        "question": "Seven Failure Points When Engineering a Retrieval Augmented Generation System",
        "instruction": "We hope you will carefully study the provided papers and determine the citation relationships between them. Please follow the instructions below strictly to complete the task:\n\n#Specific Requirements:\n1. Reference: When a given paper mentions other provided papers, those other papers are considered as \"references\" for the given paper. To summarize in this specific context, references are about what the given paper is using.\n2. Citation: Conversely, when other provided papers mention the given paper in their works, the given paper is being \"cited\" by those other papers. To summarize in this specific context, citations are about who is using the given paper.\n3. Given a paper, you need to determine the citation or reference relationship between this paper and the other papers. Do not consider papers that are not provided.\n3. Please present the paper titles in a json format as follows: {{\"Reference\":[\"Reference Title 1\", \"Reference Title 2\", ..., \"Reference Title n\"], \"Citation\":[\"Citation Title 1\", \"Citation Title 2\", ..., \"Citation Title n\"]}}.\n4. If a paper does not have any references or citations, please leave the corresponding list empty, e.g.{{\"Refernce\":[]}}, {{\"Citation\":[]}}.",
        "prompt_template": "#Papers:\n{docs}\n\n{instruction}\n\n#The paper you need to analyze:\n{question}",
        "doc": [
            "2401.05856.md",
            "2404.19705.md",
            "2405.01868.md",
            "2308.07107.md",
            "2309.01431.md"
        ],
        "answer": {
            "Reference": [
                "# Large Language Models for Information Retrieval: A Survey ",
                "# Benchmarking Large Language Models in Retrieval-Augmented Generation "
            ],
            "Citation": [
                "# When to Retrieve: Teaching LLMs to Utilize Information Retrieval Effectively ",
                "# Incorporating External Knowledge and Goal Guidance for LLM-based Conversational Recommender Systems "
            ]
        },
        "shuffle_doc": true,
        "id": "5c294e2c-90b6-41d2-8373-b9a51807e937",
        "prompt": "[Question]\n#Papers:\n{docs}\n\nWe hope you will carefully study the provided papers and determine the citation relationships between them. Please follow the instructions below strictly to complete the task:\n\n#Specific Requirements:\n1. Reference: When a given paper mentions other provided papers, those other papers are considered as \"references\" for the given paper. To summarize in this specific context, references are about what the given paper is using.\n2. Citation: Conversely, when other provided papers mention the given paper in their works, the given paper is being \"cited\" by those other papers. To summarize in this specific context, citations are about who is using the given paper.\n3. Given a paper, you need to determine the citation or reference relationship between this paper and the other papers. Do not consider papers that are not provided.\n3. Please present the paper titles in a json format as follows: {{\"Reference\":[\"Reference Title 1\", \"Reference Title 2\", ..., \"Reference Title n\"], \"Citation\":[\"Citation Title 1\", \"Citation Title 2\", ..., \"Citation Title n\"]}}.\n4. If a paper does not have any references or citations, please leave the corresponding list empty, e.g.{{\"Refernce\":[]}}, {{\"Citation\":[]}}.\n\n#The paper you need to analyze:\nSeven Failure Points When Engineering a Retrieval Augmented Generation System\n\n[Gold Answer]\n{'Reference': ['# Large Language Models for Information Retrieval: A Survey ', '# Benchmarking Large Language Models in Retrieval-Augmented Generation '], 'Citation': ['# When to Retrieve: Teaching LLMs to Utilize Information Retrieval Effectively ', '# Incorporating External Knowledge and Goal Guidance for LLM-based Conversational Recommender Systems ']}\n\n[The Start of Assistant's Predicted Answer]\n{\"Reference\":[\"GPTCache: An Open-Source Semantic Cache for LLM Applications Enabling Faster Answers and Cost Savings\", \"Self-adaptive Machine Learning Systems: Research Challenges and Opportunities\", \"Benchmarking Large Language Models in Retrieval-Augmented Generation\", \"Efficient Open Domain Multi-Hop Question Answering with Few-Shot Data Synthesis\", \"Threshy: Supporting safe usage of intelligent web services\", \"Beware the evolving 'intelligent'web service! An integration architecture tactic to guard AI-first components\", \"Retrieval augmented language model pre-training\", \"Fidlight: Efficient and effective retrieval-augmented text generation\", \"Leveraging passage retrieval with generative models for open domain question answering\", \"BioASQ-QA: A manually curated corpus for biomedical question answering\", \"LoRA Fine-tuning Efficiently Undoes Safety Training in Llama 2-Chat 70B\", \"Retrieval-augmented generation for knowledge-intensive nlp tasks\", \"Lost in the middle: How language models use long contexts\", \"G-eval: Nlg evaluation using gpt-4 with better human alignment\", \"Retrieval-based prompt selection for code-related few-shot learning\", \"GPT-4 Technical Report\", \"Robust speech recognition via large-scale weak supervision\", \"Improving the domain adaptation of retrieval augmented generation (RAG) models for open domain question answering\", \"Large language models for information retrieval: A survey\"], \"Citation\":[]}\n[The End of Assistant's Predicted Answer]\n\n[System]\nWe would like to request your feedback on the performance of the AI assistant in response to the user question displayed above according to the gold answer. Please use the following listed aspects and their descriptions as evaluation criteria:\n    - Accuracy and Hallucinations: The assistant's answer is semantically consistent with the gold answer; The numerical value and order need to be accurate, and there should be no hallucinations.\n    - Completeness: Referring to the reference answers, the assistant's answer should contain all the key points needed to answer the user's question; further elaboration on these key points can be omitted.\nPlease rate whether this answer is suitable for the question. Please note that the gold answer can be considered as a correct answer to the question.\n\nThe assistant receives an overall score on a scale of 1 to 100, where a higher score indicates better overall performance.\nPlease note that if the assistant's answer and the gold answer fully meet the above criteria, its overall rating should be the full marks (100).\nPlease first provide a comprehensive explanation of your evaluation, avoiding any potential bias.\nThen, output a line indicating the score of the Assistant.\n\nPLEASE OUTPUT WITH THE FOLLOWING FORMAT, WHERE THE SCORE IS A SCALE OF 1 TO 100 BY STRICTLY FOLLOWING THIS FORMAT: \"[[score]]\", FOR EXAMPLE \"Rating: [[100]]\":\n<start output>\nEvaluation evidence: your evluation explanation here, no more than 100 words\nRating: [[score]]\n<end output>\n\nNow, start your evaluation:",
        "generate_response": "{\"Reference\":[\"GPTCache: An Open-Source Semantic Cache for LLM Applications Enabling Faster Answers and Cost Savings\", \"Self-adaptive Machine Learning Systems: Research Challenges and Opportunities\", \"Benchmarking Large Language Models in Retrieval-Augmented Generation\", \"Efficient Open Domain Multi-Hop Question Answering with Few-Shot Data Synthesis\", \"Threshy: Supporting safe usage of intelligent web services\", \"Beware the evolving 'intelligent'web service! An integration architecture tactic to guard AI-first components\", \"Retrieval augmented language model pre-training\", \"Fidlight: Efficient and effective retrieval-augmented text generation\", \"Leveraging passage retrieval with generative models for open domain question answering\", \"BioASQ-QA: A manually curated corpus for biomedical question answering\", \"LoRA Fine-tuning Efficiently Undoes Safety Training in Llama 2-Chat 70B\", \"Retrieval-augmented generation for knowledge-intensive nlp tasks\", \"Lost in the middle: How language models use long contexts\", \"G-eval: Nlg evaluation using gpt-4 with better human alignment\", \"Retrieval-based prompt selection for code-related few-shot learning\", \"GPT-4 Technical Report\", \"Robust speech recognition via large-scale weak supervision\", \"Improving the domain adaptation of retrieval augmented generation (RAG) models for open domain question answering\", \"Large language models for information retrieval: A survey\"], \"Citation\":[]}",
        "eval_response": "Evaluation evidence: The assistant's answer inaccurately lists references and completely omits citations. The gold answer specifies two references and two citations, but the assistant's answer includes a long list of references that do not match the gold answer and fails to list any citations. This discrepancy indicates issues with accuracy and completeness, as the assistant neither correctly identifies the references nor acknowledges any citations.\n\nRating: [[20]]",
        "score": 20
    },
    {
        "level": 3,
        "set": 3,
        "length": 105291,
        "type": "paper",
        "language": "en",
        "question": "MM-LLMs: Recent Advances in MultiModal Large Language Models",
        "instruction": "We hope you will carefully study the provided papers and determine the citation relationships between them. Please follow the instructions below strictly to complete the task:\n\n#Specific Requirements:\n1. Reference: When a given paper mentions other provided papers, those other papers are considered as \"references\" for the given paper. To summarize in this specific context, references are about what the given paper is using.\n2. Citation: Conversely, when other provided papers mention the given paper in their works, the given paper is being \"cited\" by those other papers. To summarize in this specific context, citations are about who is using the given paper.\n3. Given a paper, you need to determine the citation or reference relationship between this paper and the other papers. Do not consider papers that are not provided.\n3. Please present the paper titles in a json format as follows: {{\"Reference\":[\"Reference Title 1\", \"Reference Title 2\", ..., \"Reference Title n\"], \"Citation\":[\"Citation Title 1\", \"Citation Title 2\", ..., \"Citation Title n\"]}}.\n4. If a paper does not have any references or citations, please leave the corresponding list empty, e.g.{{\"Refernce\":[]}}, {{\"Citation\":[]}}.",
        "prompt_template": "#Papers:\n{docs}\n\n{instruction}\n\n#The paper you need to analyze:\n{question}",
        "doc": [
            "2401.13601.md",
            "2405.14169.md",
            "2405.15341.md",
            "2404.14688.md",
            "2404.16375.md"
        ],
        "answer": {
            "Reference": [
                "# FMint: Bridging Human Designed and Data Pretrained Models for Differential Equation Foundation Model ",
                "# List Items One by One: A New Data Source and Learning Paradigm for Multimodal LLMs "
            ],
            "Citation": [
                "# Towards Transferable Attacks Against Vision-LLMs in Autonomous Driving with Typography ",
                "# V-Zen: Efficient GUI Understanding and Precise Grounding With A Novel Multimodal LLM "
            ]
        },
        "shuffle_doc": true,
        "id": "606fc72b-6161-4145-bfe1-186438f625e8",
        "prompt": "[Question]\n#Papers:\n{docs}\n\nWe hope you will carefully study the provided papers and determine the citation relationships between them. Please follow the instructions below strictly to complete the task:\n\n#Specific Requirements:\n1. Reference: When a given paper mentions other provided papers, those other papers are considered as \"references\" for the given paper. To summarize in this specific context, references are about what the given paper is using.\n2. Citation: Conversely, when other provided papers mention the given paper in their works, the given paper is being \"cited\" by those other papers. To summarize in this specific context, citations are about who is using the given paper.\n3. Given a paper, you need to determine the citation or reference relationship between this paper and the other papers. Do not consider papers that are not provided.\n3. Please present the paper titles in a json format as follows: {{\"Reference\":[\"Reference Title 1\", \"Reference Title 2\", ..., \"Reference Title n\"], \"Citation\":[\"Citation Title 1\", \"Citation Title 2\", ..., \"Citation Title n\"]}}.\n4. If a paper does not have any references or citations, please leave the corresponding list empty, e.g.{{\"Refernce\":[]}}, {{\"Citation\":[]}}.\n\n#The paper you need to analyze:\nMM-LLMs: Recent Advances in MultiModal Large Language Models\n\n[Gold Answer]\n{'Reference': ['# FMint: Bridging Human Designed and Data Pretrained Models for Differential Equation Foundation Model ', '# List Items One by One: A New Data Source and Learning Paradigm for Multimodal LLMs '], 'Citation': ['# Towards Transferable Attacks Against Vision-LLMs in Autonomous Driving with Typography ', '# V-Zen: Efficient GUI Understanding and Precise Grounding With A Novel Multimodal LLM ']}\n\n[The Start of Assistant's Predicted Answer]\n{\"Reference\":[\"Flamingo: a visual language model for few-shot learning\", \"BLIP-2: Bootstrapping Language-Image Pre-training with Frozen Image Encoders and Large Language Models\", \"LLaVA: An Open-Source Chatbot Impressing GPT-4 with $90 \\% *$ ChatGPT Quality\", \"MiniGPT-4: Enhancing vision-language understanding with advanced large language models\", \"mPLUG-Owl: On Scaling up a Multilingual Vision and Language Model\", \"Otter: A multi-modal model with in-context instruction tuning\", \"X-LLM: Bootstrapping advanced large language models by treating multi-modalities as foreign languages\", \"VideoChat: Improving Large MultiModal Models with Better Captions\", \"InstructBLIP: Towards General-purpose Vision-Language Models with Instruction Tuning\", \"PandaGPT: One model to instruction-follow them all\", \"GILL: Generating images with multimodal language models\", \"PaLI-X: On Scaling up a Multilingual Vision and Language Model\", \"DreamLLM: Synergistic multimodal comprehension and creation\", \"AnyMAL: An efficient and scalable any-modality augmented language model\", \"LLaVA: On Pre-training for Visual Language Models\", \"MiniGPT-v2: large language model as a unified interface for vision-language multi-task learning\", \"Shikra: Unleashing Multimodal LLM's Referential Dialogue Magic\", \"IDEFICS-9B: A jointly-scaled multilingual language-image model\", \"IDEFICS-80B: A jointly-scaled multilingual language-image model\", \"Qwen-VL: A Frontier Large Vision-Language Model with Versatile Abilities\", \"Qwen-VL-Chat: Language-assisted multi-modal instruction-tuning dataset, framework, and benchmark\", \"LLaVA-1.5: Training a large language-and-vision assistant for biomedicine in one day\", \"+ShareGPT4V: Improving multimodal llms with generative comprehension\", \"MiniGPT-v2-Chat: Modularization empowers large language models with multimodality\", \"VILA-7B: A vision and language model for dialogue with humans\", \"VILA-13B: A vision and language model for dialogue with humans\", \"+ShareGPT4V: Modularization empowers large language models with multimodality\", \"LLaVA-1.5: Enhanced visual instruction tuning for text-rich image understanding\", \"MiniGPT-4: Enhanced vision-language understanding with advanced large language models\", \"MiniGPT-4-Chat: Modularization empowers large language models with multimodality\", \"VILA-7B: A vision and language model for dialogue with humans\", \"VILA-13B: A vision and language model for dialogue with humans\", \"+ShareGPT4V: Modularization empowers large language models with multimodality\", \"MoE-LLaVA: Mixture of Experts for Large Vision-Language Models\", \"MobileVLM: A fast, reproducible and strong vision language assistant for mobile devices\", \"MobileVLM V2: Faster and Stronger Baseline for Vision Language Model\", \"PaLM-E: An embodied multimodal language model\", \"EmbodiedGPT: Visionlanguage pre-training via embodied chain of thought\", \"Continual instruction tuning for large multimodal models\", \"Catastrophic forgetting, rehearsal and pseudorehearsal\", \"Skip $\\backslash n$: A simple method to reduce hallucination in large vision-language models\", \"FairCLIP: Harnessing Fairness in Vision-Language Learning\", \"DecodingTrust: A Comprehensive Assessment of Trustworthiness in GPT Models\", \"MMCode: Evaluating Multi-Modal Code Large Language Models with Visually Rich Programming Problems\", \"Multimodal ArXiv: A Dataset for Improving Scientific Comprehension of Large Vision-Language Models\", \"Red teaming visual language models\", \"KAM-CoT: Knowledge Augmented Multimodal Chain-of-Thoughts Reasoning\", \"VIPERGPT: Visual inference via python execution for reasoning\", \"DiffusionGPT: LLM-Driven Text-to-Image Generation System\", \"LLaVAR: Enhanced visual instruction tuning for text-rich image understanding\", \"LLaVA-Phi: Efficient Multi-Modal Assistant with Small Language Model\", \"Safety fine-tuning at (almost) no cost: A baseline for vision large language models\"], \"Citation\":[\"Towards Transferable Attacks Against Vision-LLMs in Autonomous Driving with Typography\", \"List Items One by One: A New Data Source and Learning Paradigm for Multimodal LLMs\"]}\n[The End of Assistant's Predicted Answer]\n\n[System]\nWe would like to request your feedback on the performance of the AI assistant in response to the user question displayed above according to the gold answer. Please use the following listed aspects and their descriptions as evaluation criteria:\n    - Accuracy and Hallucinations: The assistant's answer is semantically consistent with the gold answer; The numerical value and order need to be accurate, and there should be no hallucinations.\n    - Completeness: Referring to the reference answers, the assistant's answer should contain all the key points needed to answer the user's question; further elaboration on these key points can be omitted.\nPlease rate whether this answer is suitable for the question. Please note that the gold answer can be considered as a correct answer to the question.\n\nThe assistant receives an overall score on a scale of 1 to 100, where a higher score indicates better overall performance.\nPlease note that if the assistant's answer and the gold answer fully meet the above criteria, its overall rating should be the full marks (100).\nPlease first provide a comprehensive explanation of your evaluation, avoiding any potential bias.\nThen, output a line indicating the score of the Assistant.\n\nPLEASE OUTPUT WITH THE FOLLOWING FORMAT, WHERE THE SCORE IS A SCALE OF 1 TO 100 BY STRICTLY FOLLOWING THIS FORMAT: \"[[score]]\", FOR EXAMPLE \"Rating: [[100]]\":\n<start output>\nEvaluation evidence: your evluation explanation here, no more than 100 words\nRating: [[score]]\n<end output>\n\nNow, start your evaluation:",
        "generate_response": "{\"Reference\":[\"Flamingo: a visual language model for few-shot learning\", \"BLIP-2: Bootstrapping Language-Image Pre-training with Frozen Image Encoders and Large Language Models\", \"LLaVA: An Open-Source Chatbot Impressing GPT-4 with $90 \\% *$ ChatGPT Quality\", \"MiniGPT-4: Enhancing vision-language understanding with advanced large language models\", \"mPLUG-Owl: On Scaling up a Multilingual Vision and Language Model\", \"Otter: A multi-modal model with in-context instruction tuning\", \"X-LLM: Bootstrapping advanced large language models by treating multi-modalities as foreign languages\", \"VideoChat: Improving Large MultiModal Models with Better Captions\", \"InstructBLIP: Towards General-purpose Vision-Language Models with Instruction Tuning\", \"PandaGPT: One model to instruction-follow them all\", \"GILL: Generating images with multimodal language models\", \"PaLI-X: On Scaling up a Multilingual Vision and Language Model\", \"DreamLLM: Synergistic multimodal comprehension and creation\", \"AnyMAL: An efficient and scalable any-modality augmented language model\", \"LLaVA: On Pre-training for Visual Language Models\", \"MiniGPT-v2: large language model as a unified interface for vision-language multi-task learning\", \"Shikra: Unleashing Multimodal LLM's Referential Dialogue Magic\", \"IDEFICS-9B: A jointly-scaled multilingual language-image model\", \"IDEFICS-80B: A jointly-scaled multilingual language-image model\", \"Qwen-VL: A Frontier Large Vision-Language Model with Versatile Abilities\", \"Qwen-VL-Chat: Language-assisted multi-modal instruction-tuning dataset, framework, and benchmark\", \"LLaVA-1.5: Training a large language-and-vision assistant for biomedicine in one day\", \"+ShareGPT4V: Improving multimodal llms with generative comprehension\", \"MiniGPT-v2-Chat: Modularization empowers large language models with multimodality\", \"VILA-7B: A vision and language model for dialogue with humans\", \"VILA-13B: A vision and language model for dialogue with humans\", \"+ShareGPT4V: Modularization empowers large language models with multimodality\", \"LLaVA-1.5: Enhanced visual instruction tuning for text-rich image understanding\", \"MiniGPT-4: Enhanced vision-language understanding with advanced large language models\", \"MiniGPT-4-Chat: Modularization empowers large language models with multimodality\", \"VILA-7B: A vision and language model for dialogue with humans\", \"VILA-13B: A vision and language model for dialogue with humans\", \"+ShareGPT4V: Modularization empowers large language models with multimodality\", \"MoE-LLaVA: Mixture of Experts for Large Vision-Language Models\", \"MobileVLM: A fast, reproducible and strong vision language assistant for mobile devices\", \"MobileVLM V2: Faster and Stronger Baseline for Vision Language Model\", \"PaLM-E: An embodied multimodal language model\", \"EmbodiedGPT: Visionlanguage pre-training via embodied chain of thought\", \"Continual instruction tuning for large multimodal models\", \"Catastrophic forgetting, rehearsal and pseudorehearsal\", \"Skip $\\backslash n$: A simple method to reduce hallucination in large vision-language models\", \"FairCLIP: Harnessing Fairness in Vision-Language Learning\", \"DecodingTrust: A Comprehensive Assessment of Trustworthiness in GPT Models\", \"MMCode: Evaluating Multi-Modal Code Large Language Models with Visually Rich Programming Problems\", \"Multimodal ArXiv: A Dataset for Improving Scientific Comprehension of Large Vision-Language Models\", \"Red teaming visual language models\", \"KAM-CoT: Knowledge Augmented Multimodal Chain-of-Thoughts Reasoning\", \"VIPERGPT: Visual inference via python execution for reasoning\", \"DiffusionGPT: LLM-Driven Text-to-Image Generation System\", \"LLaVAR: Enhanced visual instruction tuning for text-rich image understanding\", \"LLaVA-Phi: Efficient Multi-Modal Assistant with Small Language Model\", \"Safety fine-tuning at (almost) no cost: A baseline for vision large language models\"], \"Citation\":[\"Towards Transferable Attacks Against Vision-LLMs in Autonomous Driving with Typography\", \"List Items One by One: A New Data Source and Learning Paradigm for Multimodal LLMs\"]}",
        "eval_response": "Evaluation evidence: The assistant's response does not match the gold answer in terms of accuracy and completeness. The references and citations listed in the assistant's response are entirely different from those in the gold answer, indicating a significant deviation and hallucination. The assistant's response includes a large number of unrelated paper titles, which are not mentioned in the gold answer, thus failing to meet the task requirements.\n\nRating: [[10]]",
        "score": 10
    },
    {
        "level": 3,
        "set": 2,
        "length": 66151,
        "type": "paper",
        "language": "en",
        "question": "Evolution of Heuristics: Towards Efficient Automatic Algorithm Design Using Large Language Model",
        "instruction": "We hope you will carefully study the provided papers and determine the citation relationships between them. Please follow the instructions below strictly to complete the task:\n\n#Specific Requirements:\n1. Reference: When a given paper mentions other provided papers, those other papers are considered as \"references\" for the given paper. To summarize in this specific context, references are about what the given paper is using.\n2. Citation: Conversely, when other provided papers mention the given paper in their works, the given paper is being \"cited\" by those other papers. To summarize in this specific context, citations are about who is using the given paper.\n3. Given a paper, you need to determine the citation or reference relationship between this paper and the other papers. Do not consider papers that are not provided.\n3. Please present the paper titles in a json format as follows: {{\"Reference\":[\"Reference Title 1\", \"Reference Title 2\", ..., \"Reference Title n\"], \"Citation\":[\"Citation Title 1\", \"Citation Title 2\", ..., \"Citation Title n\"]}}.\n4. If a paper does not have any references or citations, please leave the corresponding list empty, e.g.{{\"Refernce\":[]}}, {{\"Citation\":[]}}.",
        "prompt_template": "#Papers:\n{docs}\n\n{instruction}\n\n#The paper you need to analyze:\n{question}",
        "doc": [
            "2401.02051.md",
            "2403.07559.md",
            "2405.17950.md"
        ],
        "answer": {
            "Reference": [],
            "Citation": [
                "# Ensembling Prioritized Hybrid Policies for Multi-agent Pathfinding ",
                "# Self-Guiding Exploration for Combinatorial Problems "
            ]
        },
        "shuffle_doc": true,
        "id": "6ee57d4f-6b0a-4ff7-89f5-b90ab74a87bd",
        "prompt": "[Question]\n#Papers:\n{docs}\n\nWe hope you will carefully study the provided papers and determine the citation relationships between them. Please follow the instructions below strictly to complete the task:\n\n#Specific Requirements:\n1. Reference: When a given paper mentions other provided papers, those other papers are considered as \"references\" for the given paper. To summarize in this specific context, references are about what the given paper is using.\n2. Citation: Conversely, when other provided papers mention the given paper in their works, the given paper is being \"cited\" by those other papers. To summarize in this specific context, citations are about who is using the given paper.\n3. Given a paper, you need to determine the citation or reference relationship between this paper and the other papers. Do not consider papers that are not provided.\n3. Please present the paper titles in a json format as follows: {{\"Reference\":[\"Reference Title 1\", \"Reference Title 2\", ..., \"Reference Title n\"], \"Citation\":[\"Citation Title 1\", \"Citation Title 2\", ..., \"Citation Title n\"]}}.\n4. If a paper does not have any references or citations, please leave the corresponding list empty, e.g.{{\"Refernce\":[]}}, {{\"Citation\":[]}}.\n\n#The paper you need to analyze:\nEvolution of Heuristics: Towards Efficient Automatic Algorithm Design Using Large Language Model\n\n[Gold Answer]\n{'Reference': [], 'Citation': ['# Ensembling Prioritized Hybrid Policies for Multi-agent Pathfinding ', '# Self-Guiding Exploration for Combinatorial Problems ']}\n\n[The Start of Assistant's Predicted Answer]\n{\"Reference\": [\"Optuna: A next-generation hyperparameter optimization framework\", \"Guided local search\", \"Online bin packing with predictions\", \"Concorde tsp solver\", \"Knowledge-guided local search for the vehicle routing problem\", \"Program synthesis with large language models\", \"Tree of thoughts: Deliberate problem solving with large language models\"], \"Citation\": []}\n[The End of Assistant's Predicted Answer]\n\n[System]\nWe would like to request your feedback on the performance of the AI assistant in response to the user question displayed above according to the gold answer. Please use the following listed aspects and their descriptions as evaluation criteria:\n    - Accuracy and Hallucinations: The assistant's answer is semantically consistent with the gold answer; The numerical value and order need to be accurate, and there should be no hallucinations.\n    - Completeness: Referring to the reference answers, the assistant's answer should contain all the key points needed to answer the user's question; further elaboration on these key points can be omitted.\nPlease rate whether this answer is suitable for the question. Please note that the gold answer can be considered as a correct answer to the question.\n\nThe assistant receives an overall score on a scale of 1 to 100, where a higher score indicates better overall performance.\nPlease note that if the assistant's answer and the gold answer fully meet the above criteria, its overall rating should be the full marks (100).\nPlease first provide a comprehensive explanation of your evaluation, avoiding any potential bias.\nThen, output a line indicating the score of the Assistant.\n\nPLEASE OUTPUT WITH THE FOLLOWING FORMAT, WHERE THE SCORE IS A SCALE OF 1 TO 100 BY STRICTLY FOLLOWING THIS FORMAT: \"[[score]]\", FOR EXAMPLE \"Rating: [[100]]\":\n<start output>\nEvaluation evidence: your evluation explanation here, no more than 100 words\nRating: [[score]]\n<end output>\n\nNow, start your evaluation:",
        "generate_response": "{\"Reference\": [\"Optuna: A next-generation hyperparameter optimization framework\", \"Guided local search\", \"Online bin packing with predictions\", \"Concorde tsp solver\", \"Knowledge-guided local search for the vehicle routing problem\", \"Program synthesis with large language models\", \"Tree of thoughts: Deliberate problem solving with large language models\"], \"Citation\": []}",
        "eval_response": "Evaluation evidence: The assistant's answer does not align with the gold answer. The gold answer indicates that the paper has no references but is cited by two specific papers. In contrast, the assistant's answer lists several references and no citations, which is a direct contradiction to the gold answer. This discrepancy suggests inaccuracies and hallucinations in the assistant's response, as it introduces information not supported by the gold answer. Therefore, the assistant's performance is poor in terms of accuracy and completeness.\n\nRating: [[10]]",
        "score": 10
    },
    {
        "level": 3,
        "set": 2,
        "length": 76429,
        "type": "paper",
        "language": "en",
        "question": "Self-Play Fine-Tuning Converts Weak Language Models to Strong Language Models",
        "instruction": "We hope you will carefully study the provided papers and determine the citation relationships between them. Please follow the instructions below strictly to complete the task:\n\n#Specific Requirements:\n1. Reference: When a given paper mentions other provided papers, those other papers are considered as \"references\" for the given paper. To summarize in this specific context, references are about what the given paper is using.\n2. Citation: Conversely, when other provided papers mention the given paper in their works, the given paper is being \"cited\" by those other papers. To summarize in this specific context, citations are about who is using the given paper.\n3. Given a paper, you need to determine the citation or reference relationship between this paper and the other papers. Do not consider papers that are not provided.\n3. Please present the paper titles in a json format as follows: {{\"Reference\":[\"Reference Title 1\", \"Reference Title 2\", ..., \"Reference Title n\"], \"Citation\":[\"Citation Title 1\", \"Citation Title 2\", ..., \"Citation Title n\"]}}.\n4. If a paper does not have any references or citations, please leave the corresponding list empty, e.g.{{\"Refernce\":[]}}, {{\"Citation\":[]}}.",
        "prompt_template": "#Papers:\n{docs}\n\n{instruction}\n\n#The paper you need to analyze:\n{question}",
        "doc": [
            "2401.01335.md",
            "2405.20830.md",
            "2312.06585.md",
            "2312.09241.md"
        ],
        "answer": {
            "Reference": [
                "# Beyond Human Data: Scaling Self-Training for Problem-Solving with Language Models ",
                "# TinyGSM: achieving $>80 \\%$ on GSM8k with small language models "
            ],
            "Citation": [
                "# Self-Augmented Preference Optimization: Off-Policy Paradigms for Language Model Alignment "
            ]
        },
        "shuffle_doc": true,
        "id": "c1dfbe2f-f2ef-4afc-9c7b-1117333a73e2",
        "prompt": "[Question]\n#Papers:\n{docs}\n\nWe hope you will carefully study the provided papers and determine the citation relationships between them. Please follow the instructions below strictly to complete the task:\n\n#Specific Requirements:\n1. Reference: When a given paper mentions other provided papers, those other papers are considered as \"references\" for the given paper. To summarize in this specific context, references are about what the given paper is using.\n2. Citation: Conversely, when other provided papers mention the given paper in their works, the given paper is being \"cited\" by those other papers. To summarize in this specific context, citations are about who is using the given paper.\n3. Given a paper, you need to determine the citation or reference relationship between this paper and the other papers. Do not consider papers that are not provided.\n3. Please present the paper titles in a json format as follows: {{\"Reference\":[\"Reference Title 1\", \"Reference Title 2\", ..., \"Reference Title n\"], \"Citation\":[\"Citation Title 1\", \"Citation Title 2\", ..., \"Citation Title n\"]}}.\n4. If a paper does not have any references or citations, please leave the corresponding list empty, e.g.{{\"Refernce\":[]}}, {{\"Citation\":[]}}.\n\n#The paper you need to analyze:\nSelf-Play Fine-Tuning Converts Weak Language Models to Strong Language Models\n\n[Gold Answer]\n{'Reference': ['# Beyond Human Data: Scaling Self-Training for Problem-Solving with Language Models ', '# TinyGSM: achieving $>80 \\\\%$ on GSM8k with small language models '], 'Citation': ['# Self-Augmented Preference Optimization: Off-Policy Paradigms for Language Model Alignment ']}\n\n[The Start of Assistant's Predicted Answer]\n{\"Reference\":[\"A Survey on Evaluation of Large Language Models\", \"Vicuna: An Open-Source Chatbot Impressing GPT-4 with $90 \\% *$ ChatGPT Quality\", \"ChatLaw: Open-Source Legal Large Language Model with Integrated External Knowledge Bases\", \"Compositional Semantic Parsing with Large Language Models\", \"Open LLM Leaderboard\", \"How Close is ChatGPT to Human Experts? Comparison Corpus, Evaluation, and Detection\", \"REALM: Retrieval-Augmented Language Model PreTraining\", \"Rethinking with Retrieval: Faithful Large Language Model Inference\", \"Measuring Massive Multitask Language Understanding\", \"Stanford alpaca: An instruction-following llama model\", \"Chain-of-thought prompting elicits reasoning in large language models\", \"Competition-level code generation with alphacode\", \"Decoding data quality via synthetic corruptions: Embedding-guided pruning of code data\", \"Scaling relationship on learning mathematical reasoning with large language models\", \"Scaling instruction-finetuned language models\", \"Ultrafeedback: Boosting language models with high-quality feedback\", \"Learning to summarize with human feedback\", \"Scaling laws for reward model overoptimization\", \"Training a helpful and harmless assistant with reinforcement learning from human feedback\", \"Adversarial preference optimization\", \"Is multiagent deep reinforcement learning the answer or the question? a brief survey\", \"Can a suit of armor conduct electricity? a new dataset for open book question answering\", \"Multitask prompted training enables zero-shot task generalization\", \"Generative adversarial imitation learning\", \"The relativistic discriminator: a key element missing from standard gan\", \"Least squares generative adversarial networks\", \"Self-training converts weak learners to strong learners in mixture models\", \"Pseudo-label: The simple and efficient semi-supervised learning method for deep neural networks\", \"Learning the easy things first: Self-paced visual category discovery\", \"Beyond the imitation game: Quantifying and extrapolating the capabilities of language models\", \"Sparks of artificial general intelligence: Early experiments with gpt-4\", \"Mistral 7b\", \"Scaling multimodal pre-training via cross-modality gradient harmonization\"], \"Citation\":[\"TinyGSM: achieving $>80 \\%$ on GSM8k with small language models\"]}\n[The End of Assistant's Predicted Answer]\n\n[System]\nWe would like to request your feedback on the performance of the AI assistant in response to the user question displayed above according to the gold answer. Please use the following listed aspects and their descriptions as evaluation criteria:\n    - Accuracy and Hallucinations: The assistant's answer is semantically consistent with the gold answer; The numerical value and order need to be accurate, and there should be no hallucinations.\n    - Completeness: Referring to the reference answers, the assistant's answer should contain all the key points needed to answer the user's question; further elaboration on these key points can be omitted.\nPlease rate whether this answer is suitable for the question. Please note that the gold answer can be considered as a correct answer to the question.\n\nThe assistant receives an overall score on a scale of 1 to 100, where a higher score indicates better overall performance.\nPlease note that if the assistant's answer and the gold answer fully meet the above criteria, its overall rating should be the full marks (100).\nPlease first provide a comprehensive explanation of your evaluation, avoiding any potential bias.\nThen, output a line indicating the score of the Assistant.\n\nPLEASE OUTPUT WITH THE FOLLOWING FORMAT, WHERE THE SCORE IS A SCALE OF 1 TO 100 BY STRICTLY FOLLOWING THIS FORMAT: \"[[score]]\", FOR EXAMPLE \"Rating: [[100]]\":\n<start output>\nEvaluation evidence: your evluation explanation here, no more than 100 words\nRating: [[score]]\n<end output>\n\nNow, start your evaluation:",
        "generate_response": "{\"Reference\":[\"A Survey on Evaluation of Large Language Models\", \"Vicuna: An Open-Source Chatbot Impressing GPT-4 with $90 \\% *$ ChatGPT Quality\", \"ChatLaw: Open-Source Legal Large Language Model with Integrated External Knowledge Bases\", \"Compositional Semantic Parsing with Large Language Models\", \"Open LLM Leaderboard\", \"How Close is ChatGPT to Human Experts? Comparison Corpus, Evaluation, and Detection\", \"REALM: Retrieval-Augmented Language Model PreTraining\", \"Rethinking with Retrieval: Faithful Large Language Model Inference\", \"Measuring Massive Multitask Language Understanding\", \"Stanford alpaca: An instruction-following llama model\", \"Chain-of-thought prompting elicits reasoning in large language models\", \"Competition-level code generation with alphacode\", \"Decoding data quality via synthetic corruptions: Embedding-guided pruning of code data\", \"Scaling relationship on learning mathematical reasoning with large language models\", \"Scaling instruction-finetuned language models\", \"Ultrafeedback: Boosting language models with high-quality feedback\", \"Learning to summarize with human feedback\", \"Scaling laws for reward model overoptimization\", \"Training a helpful and harmless assistant with reinforcement learning from human feedback\", \"Adversarial preference optimization\", \"Is multiagent deep reinforcement learning the answer or the question? a brief survey\", \"Can a suit of armor conduct electricity? a new dataset for open book question answering\", \"Multitask prompted training enables zero-shot task generalization\", \"Generative adversarial imitation learning\", \"The relativistic discriminator: a key element missing from standard gan\", \"Least squares generative adversarial networks\", \"Self-training converts weak learners to strong learners in mixture models\", \"Pseudo-label: The simple and efficient semi-supervised learning method for deep neural networks\", \"Learning the easy things first: Self-paced visual category discovery\", \"Beyond the imitation game: Quantifying and extrapolating the capabilities of language models\", \"Sparks of artificial general intelligence: Early experiments with gpt-4\", \"Mistral 7b\", \"Scaling multimodal pre-training via cross-modality gradient harmonization\"], \"Citation\":[\"TinyGSM: achieving $>80 \\%$ on GSM8k with small language models\"]}",
        "eval_response": "Evaluation evidence: The assistant's answer does not align well with the gold answer. It lists numerous references and one citation that are not mentioned in the gold answer. The gold answer specifies two references and one citation, none of which appear in the assistant's response. This discrepancy indicates issues with accuracy and hallucinations, as the assistant introduces irrelevant papers. Additionally, the assistant's answer lacks completeness as it fails to include the correct references and citation from the gold answer.\n\nRating: [[10]]",
        "score": 10
    },
    {
        "level": 3,
        "set": 4,
        "length": 227912,
        "type": "paper",
        "language": "en",
        "question": "Executable Code Actions Elicit Better LLM Agents",
        "instruction": "We hope you will carefully study the provided papers and determine the citation relationships between them. Please follow the instructions below strictly to complete the task:\n\n#Specific Requirements:\n1. Reference: When a given paper mentions other provided papers, those other papers are considered as \"references\" for the given paper. To summarize in this specific context, references are about what the given paper is using.\n2. Citation: Conversely, when other provided papers mention the given paper in their works, the given paper is being \"cited\" by those other papers. To summarize in this specific context, citations are about who is using the given paper.\n3. Given a paper, you need to determine the citation or reference relationship between this paper and the other papers. Do not consider papers that are not provided.\n3. Please present the paper titles in a json format as follows: {{\"Reference\":[\"Reference Title 1\", \"Reference Title 2\", ..., \"Reference Title n\"], \"Citation\":[\"Citation Title 1\", \"Citation Title 2\", ..., \"Citation Title n\"]}}.\n4. If a paper does not have any references or citations, please leave the corresponding list empty, e.g.{{\"Refernce\":[]}}, {{\"Citation\":[]}}.",
        "prompt_template": "#Papers:\n{docs}\n\n{instruction}\n\n#The paper you need to analyze:\n{question}",
        "doc": [
            "2402.01030.md",
            "2404.12135.md",
            "2405.16247.md",
            "2405.16533.md",
            "2405.20092.md",
            "2405.20974.md",
            "2311.10081.md",
            "2311.17541.md",
            "2401.00812.md",
            "2402.04247.md",
            "2402.14658.md"
        ],
        "answer": {
            "Reference": [
                "# DRESS : Instructing Large Vision-Language Models to Align and Interact with Humans via Natural Language Feedback ",
                "# TaskWeaver: A Code-First Agent Framework ",
                "# Prioritizing Safeguarding Over Autonomy: Risks of LLM Agents for Science ",
                "# OpenCodeInterpreter: Integrating Code Generation with Execution and Refinement "
            ],
            "Citation": [
                "# mABC: Multi-Agent Blockchain-inspired Collaboration for Root Cause Analysis in Micro-Services Architecture ",
                "# AutoManual: Generating Instruction Manuals by LLM Agents via Interactive Environmental Learning ",
                "# Chain of Tools: Large Language Model is an Automatic Multi-tool Learner ",
                "# Divide-and-Conquer Meets Consensus: Unleashing the Power of Functions in Code Generation ",
                "# \\&. SaySelf: Teaching LLMs to Express Confidence with Self-Reflective Rationales "
            ]
        },
        "shuffle_doc": true,
        "id": "7700f2b6-646a-41c5-a542-a9a249f30070",
        "prompt": "[Question]\n#Papers:\n{docs}\n\nWe hope you will carefully study the provided papers and determine the citation relationships between them. Please follow the instructions below strictly to complete the task:\n\n#Specific Requirements:\n1. Reference: When a given paper mentions other provided papers, those other papers are considered as \"references\" for the given paper. To summarize in this specific context, references are about what the given paper is using.\n2. Citation: Conversely, when other provided papers mention the given paper in their works, the given paper is being \"cited\" by those other papers. To summarize in this specific context, citations are about who is using the given paper.\n3. Given a paper, you need to determine the citation or reference relationship between this paper and the other papers. Do not consider papers that are not provided.\n3. Please present the paper titles in a json format as follows: {{\"Reference\":[\"Reference Title 1\", \"Reference Title 2\", ..., \"Reference Title n\"], \"Citation\":[\"Citation Title 1\", \"Citation Title 2\", ..., \"Citation Title n\"]}}.\n4. If a paper does not have any references or citations, please leave the corresponding list empty, e.g.{{\"Refernce\":[]}}, {{\"Citation\":[]}}.\n\n#The paper you need to analyze:\nExecutable Code Actions Elicit Better LLM Agents\n\n[Gold Answer]\n{'Reference': ['# DRESS : Instructing Large Vision-Language Models to Align and Interact with Humans via Natural Language Feedback ', '# TaskWeaver: A Code-First Agent Framework ', '# Prioritizing Safeguarding Over Autonomy: Risks of LLM Agents for Science ', '# OpenCodeInterpreter: Integrating Code Generation with Execution and Refinement '], 'Citation': ['# mABC: Multi-Agent Blockchain-inspired Collaboration for Root Cause Analysis in Micro-Services Architecture ', '# AutoManual: Generating Instruction Manuals by LLM Agents via Interactive Environmental Learning ', '# Chain of Tools: Large Language Model is an Automatic Multi-tool Learner ', '# Divide-and-Conquer Meets Consensus: Unleashing the Power of Functions in Code Generation ', '# \\\\&. SaySelf: Teaching LLMs to Express Confidence with Self-Reflective Rationales ']}\n\n[The Start of Assistant's Predicted Answer]\n{\"Reference\": [\"A Survey on Evaluation of Large Language Models\", \"Do as i can and not as i say: Grounding language in robotic affordances\", \"Open LLM Leaderboard\", \"Code as policies: Language model programs for embodied control\", \"Pre-train, prompt, and predict: A systematic survey of prompting methods in natural language processing\", \"Tool learning with foundation models\", \"Progprompt: Generating situated robot task plans using large language models\", \"Self-consistency improves chain of thought reasoning in language models\", \"Chain-of-thought prompting elicits reasoning in large language models\", \"On the tool manipulation capability of open-source large language models\", \"Intercode: Standardizing and benchmarking interactive coding with execution feedback\", \"If llm is the wizard, then code is the wand: A survey on how code empowers large language models to serve as intelligent agents\", \"Hotpotqa: A dataset for diverse, explainable multi-hop question answering\", \"Towards unified alignment between agents, humans, and environment\", \"Webshop: Towards scalable real-world web interaction with grounded language agents\", \"React: Synergizing reasoning and acting in language models\", \"Tree of thoughts: Deliberate problem solving with large language models\", \"Retroformer: Retrospective large language agents with policy gradient optimization\", \"Agenttuning: Enabling generalized agent abilities for llms\", \"Prefer: Prompt ensemble learning via feedback-reflect-refine\", \"Judging llm-as-a-judge with mt-bench and chatbot arena\", \"Integrating code generation with execution and refinement\", \"Ghost in the minecraft: Generally capable agents for open-world enviroments via large language models with text-based knowledge and memory\", \"Executable Python code for the LLM agent's action\", \"Augmented language models: a survey\", \"Training language models to follow instructions with human feedback\", \"LangChain\", \"Toward language agent fine-tuning\", \"Evaluating large language models trained on code\", \"Measuring Massive Multitask Language Understanding\", \"Measuring coding challenge competence with apps\", \"Measuring mathematical problem solving with the math dataset\", \"Data interpreter: An llm agent for data science\", \"Composable 3d value maps for robotic manipulation with language models\", \"An open dataset of gpt augmented flan reasoning traces\", \"A survey on how code empowers large language models to serve as intelligent agents\", \"A dataset for diverse, explainable multi-hop question answering\", \"Towards unified alignment between agents, humans, and environment\", \"Towards scalable real-world web interaction with grounded language agents\", \"Synergizing reasoning and acting in language models\", \"Deliberate problem solving with large language models\", \"Retrospective large language agents with policy gradient optimization\", \"Enabling generalized agent abilities for llms\", \"Prompt ensemble learning via feedback-reflect-refine\", \"Integrating code generation with execution and refinement\", \"Generally capable agents for open-world enviroments via large language models with text-based knowledge and memory\", \"Executable Python code for the LLM agent's action\", \"Augmented language models: a survey\", \"Training language models to follow instructions with human feedback\", \"An open dataset of gpt augmented flan reasoning traces\", \"An open dataset of gpt augmented flan reasoning traces\", \"A survey on how code empowers large language models to serve as intelligent agents\", \"A survey on how code empowers large language models to serve as intelligent agents\", \"A survey on how code empowers large language models to serve as intelligent agents\", \"A survey on how code empowers large language models to serve as intelligent agents\", \"A survey on how code empowers large language models to serve as intelligent agents\", \"A survey on how code empowers large language models to serve as intelligent agents\", \"A survey on how code empowers large language models to serve as intelligent agents\", \"A survey on how code empowers large language models to serve as intelligent agents\", \"A survey on how code empowers large language models to serve as intelligent agents\", \"A survey on how code empowers large language models to serve as intelligent agents\"], \"Citation\": []}\n[The End of Assistant's Predicted Answer]\n\n[System]\nWe would like to request your feedback on the performance of the AI assistant in response to the user question displayed above according to the gold answer. Please use the following listed aspects and their descriptions as evaluation criteria:\n    - Accuracy and Hallucinations: The assistant's answer is semantically consistent with the gold answer; The numerical value and order need to be accurate, and there should be no hallucinations.\n    - Completeness: Referring to the reference answers, the assistant's answer should contain all the key points needed to answer the user's question; further elaboration on these key points can be omitted.\nPlease rate whether this answer is suitable for the question. Please note that the gold answer can be considered as a correct answer to the question.\n\nThe assistant receives an overall score on a scale of 1 to 100, where a higher score indicates better overall performance.\nPlease note that if the assistant's answer and the gold answer fully meet the above criteria, its overall rating should be the full marks (100).\nPlease first provide a comprehensive explanation of your evaluation, avoiding any potential bias.\nThen, output a line indicating the score of the Assistant.\n\nPLEASE OUTPUT WITH THE FOLLOWING FORMAT, WHERE THE SCORE IS A SCALE OF 1 TO 100 BY STRICTLY FOLLOWING THIS FORMAT: \"[[score]]\", FOR EXAMPLE \"Rating: [[100]]\":\n<start output>\nEvaluation evidence: your evluation explanation here, no more than 100 words\nRating: [[score]]\n<end output>\n\nNow, start your evaluation:",
        "generate_response": "{\"Reference\": [\"A Survey on Evaluation of Large Language Models\", \"Do as i can and not as i say: Grounding language in robotic affordances\", \"Open LLM Leaderboard\", \"Code as policies: Language model programs for embodied control\", \"Pre-train, prompt, and predict: A systematic survey of prompting methods in natural language processing\", \"Tool learning with foundation models\", \"Progprompt: Generating situated robot task plans using large language models\", \"Self-consistency improves chain of thought reasoning in language models\", \"Chain-of-thought prompting elicits reasoning in large language models\", \"On the tool manipulation capability of open-source large language models\", \"Intercode: Standardizing and benchmarking interactive coding with execution feedback\", \"If llm is the wizard, then code is the wand: A survey on how code empowers large language models to serve as intelligent agents\", \"Hotpotqa: A dataset for diverse, explainable multi-hop question answering\", \"Towards unified alignment between agents, humans, and environment\", \"Webshop: Towards scalable real-world web interaction with grounded language agents\", \"React: Synergizing reasoning and acting in language models\", \"Tree of thoughts: Deliberate problem solving with large language models\", \"Retroformer: Retrospective large language agents with policy gradient optimization\", \"Agenttuning: Enabling generalized agent abilities for llms\", \"Prefer: Prompt ensemble learning via feedback-reflect-refine\", \"Judging llm-as-a-judge with mt-bench and chatbot arena\", \"Integrating code generation with execution and refinement\", \"Ghost in the minecraft: Generally capable agents for open-world enviroments via large language models with text-based knowledge and memory\", \"Executable Python code for the LLM agent's action\", \"Augmented language models: a survey\", \"Training language models to follow instructions with human feedback\", \"LangChain\", \"Toward language agent fine-tuning\", \"Evaluating large language models trained on code\", \"Measuring Massive Multitask Language Understanding\", \"Measuring coding challenge competence with apps\", \"Measuring mathematical problem solving with the math dataset\", \"Data interpreter: An llm agent for data science\", \"Composable 3d value maps for robotic manipulation with language models\", \"An open dataset of gpt augmented flan reasoning traces\", \"A survey on how code empowers large language models to serve as intelligent agents\", \"A dataset for diverse, explainable multi-hop question answering\", \"Towards unified alignment between agents, humans, and environment\", \"Towards scalable real-world web interaction with grounded language agents\", \"Synergizing reasoning and acting in language models\", \"Deliberate problem solving with large language models\", \"Retrospective large language agents with policy gradient optimization\", \"Enabling generalized agent abilities for llms\", \"Prompt ensemble learning via feedback-reflect-refine\", \"Integrating code generation with execution and refinement\", \"Generally capable agents for open-world enviroments via large language models with text-based knowledge and memory\", \"Executable Python code for the LLM agent's action\", \"Augmented language models: a survey\", \"Training language models to follow instructions with human feedback\", \"An open dataset of gpt augmented flan reasoning traces\", \"An open dataset of gpt augmented flan reasoning traces\", \"A survey on how code empowers large language models to serve as intelligent agents\", \"A survey on how code empowers large language models to serve as intelligent agents\", \"A survey on how code empowers large language models to serve as intelligent agents\", \"A survey on how code empowers large language models to serve as intelligent agents\", \"A survey on how code empowers large language models to serve as intelligent agents\", \"A survey on how code empowers large language models to serve as intelligent agents\", \"A survey on how code empowers large language models to serve as intelligent agents\", \"A survey on how code empowers large language models to serve as intelligent agents\", \"A survey on how code empowers large language models to serve as intelligent agents\", \"A survey on how code empowers large language models to serve as intelligent agents\"], \"Citation\": []}",
        "eval_response": "Evaluation evidence: The assistant's response is inaccurate and contains hallucinations. The references and citations listed do not match those in the gold answer, indicating a misunderstanding or misinterpretation of the provided documents. The assistant's response lists numerous unrelated titles, showing a lack of focus on the specific task of identifying citation relationships for the given paper. This results in a failure to meet the accuracy and completeness criteria.\n\nRating: [[10]]",
        "score": 10
    },
    {
        "level": 3,
        "set": 1,
        "length": 41320,
        "type": "paper",
        "language": "en",
        "question": "EasyTool: Enhancing LLM-based Agents with Concise Tool Instruction",
        "instruction": "We hope you will carefully study the provided papers and determine the citation relationships between them. Please follow the instructions below strictly to complete the task:\n\n#Specific Requirements:\n1. Reference: When a given paper mentions other provided papers, those other papers are considered as \"references\" for the given paper. To summarize in this specific context, references are about what the given paper is using.\n2. Citation: Conversely, when other provided papers mention the given paper in their works, the given paper is being \"cited\" by those other papers. To summarize in this specific context, citations are about who is using the given paper.\n3. Given a paper, you need to determine the citation or reference relationship between this paper and the other papers. Do not consider papers that are not provided.\n3. Please present the paper titles in a json format as follows: {{\"Reference\":[\"Reference Title 1\", \"Reference Title 2\", ..., \"Reference Title n\"], \"Citation\":[\"Citation Title 1\", \"Citation Title 2\", ..., \"Citation Title n\"]}}.\n4. If a paper does not have any references or citations, please leave the corresponding list empty, e.g.{{\"Refernce\":[]}}, {{\"Citation\":[]}}.",
        "prompt_template": "#Papers:\n{docs}\n\n{instruction}\n\n#The paper you need to analyze:\n{question}",
        "doc": [
            "2401.06201.md",
            "2405.05955.md",
            "2310.07075.md"
        ],
        "answer": {
            "Reference": [
                "# Don't Fine-Tune, Decode: Syntax Error-Free Tool Use via Constrained Decoding "
            ],
            "Citation": [
                "# Smurfs: Leveraging Multiple Proficiency Agents with Context-Efficiency for Tool Planning "
            ]
        },
        "shuffle_doc": true,
        "id": "49680a79-a974-49b0-b7e3-85ff515bc4f2",
        "prompt": "[Question]\n#Papers:\n{docs}\n\nWe hope you will carefully study the provided papers and determine the citation relationships between them. Please follow the instructions below strictly to complete the task:\n\n#Specific Requirements:\n1. Reference: When a given paper mentions other provided papers, those other papers are considered as \"references\" for the given paper. To summarize in this specific context, references are about what the given paper is using.\n2. Citation: Conversely, when other provided papers mention the given paper in their works, the given paper is being \"cited\" by those other papers. To summarize in this specific context, citations are about who is using the given paper.\n3. Given a paper, you need to determine the citation or reference relationship between this paper and the other papers. Do not consider papers that are not provided.\n3. Please present the paper titles in a json format as follows: {{\"Reference\":[\"Reference Title 1\", \"Reference Title 2\", ..., \"Reference Title n\"], \"Citation\":[\"Citation Title 1\", \"Citation Title 2\", ..., \"Citation Title n\"]}}.\n4. If a paper does not have any references or citations, please leave the corresponding list empty, e.g.{{\"Refernce\":[]}}, {{\"Citation\":[]}}.\n\n#The paper you need to analyze:\nEasyTool: Enhancing LLM-based Agents with Concise Tool Instruction\n\n[Gold Answer]\n{'Reference': [\"# Don't Fine-Tune, Decode: Syntax Error-Free Tool Use via Constrained Decoding \"], 'Citation': ['# Smurfs: Leveraging Multiple Proficiency Agents with Context-Efficiency for Tool Planning ']}\n\n[The Start of Assistant's Predicted Answer]\n{\"Reference\":[\"A Survey on Evaluation of Large Language Models\", \"Vicuna: An Open-Source Chatbot Impressing GPT-4 with $90 \\% *$ ChatGPT Quality\", \"ChatLaw: Open-Source Legal Large Language Model with Integrated External Knowledge Bases\", \"Compositional Semantic Parsing with Large Language Models\", \"Open LLM Leaderboard\", \"How Close is ChatGPT to Human Experts? Comparison Corpus, Evaluation, and Detection\", \"REALM: Retrieval-Augmented Language Model PreTraining\", \"Rethinking with Retrieval: Faithful Large Language Model Inference\", \"Measuring Massive Multitask Language Understanding\", \"Atlas: Few-shot Learning with Retrieval Augmented Language Models\", \"Training language models to follow instructions with human feedback\", \"ToolAugmented Language Models\", \"Gorilla: Large Language Model Connected with Massive APIs\", \"ToolBench: A Real-World API Benchmark for Large Language Models\", \"HuggingGPT: Solving AI Tasks with ChatGPT and its Friends in Hugging Face\", \"TaskBench: Benchmarking Large Language Models for Task Automation\", \"Toolformer: Language Models can Teach Themselves to Use Tools\", \"Llama: Open and Efficient Foundation Language Models\", \"Llama 2: Open Foundation and Fine-Tuned Chat Models\", \"On the Tool Manipulation Capability of Open-source Large Language Models\", \"React: Synergizing Reasoning and Acting in Language Models\", \"Syntax Error-free and Generalizable Tool Use for LLMs via Finite-state Decoding\", \"Toolchain*: Efficient Action Space Navigation in Large Language Models with $a^{*}$ Search\"], \"Citation\":[\"Don't Fine-Tune, Decode: Syntax Error-Free Tool Use via Constrained Decoding\", \"Smurfs: Leveraging Multiple Proficiency Agents with Context-Efficiency for Tool Planning\"]}\n[The End of Assistant's Predicted Answer]\n\n[System]\nWe would like to request your feedback on the performance of the AI assistant in response to the user question displayed above according to the gold answer. Please use the following listed aspects and their descriptions as evaluation criteria:\n    - Accuracy and Hallucinations: The assistant's answer is semantically consistent with the gold answer; The numerical value and order need to be accurate, and there should be no hallucinations.\n    - Completeness: Referring to the reference answers, the assistant's answer should contain all the key points needed to answer the user's question; further elaboration on these key points can be omitted.\nPlease rate whether this answer is suitable for the question. Please note that the gold answer can be considered as a correct answer to the question.\n\nThe assistant receives an overall score on a scale of 1 to 100, where a higher score indicates better overall performance.\nPlease note that if the assistant's answer and the gold answer fully meet the above criteria, its overall rating should be the full marks (100).\nPlease first provide a comprehensive explanation of your evaluation, avoiding any potential bias.\nThen, output a line indicating the score of the Assistant.\n\nPLEASE OUTPUT WITH THE FOLLOWING FORMAT, WHERE THE SCORE IS A SCALE OF 1 TO 100 BY STRICTLY FOLLOWING THIS FORMAT: \"[[score]]\", FOR EXAMPLE \"Rating: [[100]]\":\n<start output>\nEvaluation evidence: your evluation explanation here, no more than 100 words\nRating: [[score]]\n<end output>\n\nNow, start your evaluation:",
        "generate_response": "{\"Reference\":[\"A Survey on Evaluation of Large Language Models\", \"Vicuna: An Open-Source Chatbot Impressing GPT-4 with $90 \\% *$ ChatGPT Quality\", \"ChatLaw: Open-Source Legal Large Language Model with Integrated External Knowledge Bases\", \"Compositional Semantic Parsing with Large Language Models\", \"Open LLM Leaderboard\", \"How Close is ChatGPT to Human Experts? Comparison Corpus, Evaluation, and Detection\", \"REALM: Retrieval-Augmented Language Model PreTraining\", \"Rethinking with Retrieval: Faithful Large Language Model Inference\", \"Measuring Massive Multitask Language Understanding\", \"Atlas: Few-shot Learning with Retrieval Augmented Language Models\", \"Training language models to follow instructions with human feedback\", \"ToolAugmented Language Models\", \"Gorilla: Large Language Model Connected with Massive APIs\", \"ToolBench: A Real-World API Benchmark for Large Language Models\", \"HuggingGPT: Solving AI Tasks with ChatGPT and its Friends in Hugging Face\", \"TaskBench: Benchmarking Large Language Models for Task Automation\", \"Toolformer: Language Models can Teach Themselves to Use Tools\", \"Llama: Open and Efficient Foundation Language Models\", \"Llama 2: Open Foundation and Fine-Tuned Chat Models\", \"On the Tool Manipulation Capability of Open-source Large Language Models\", \"React: Synergizing Reasoning and Acting in Language Models\", \"Syntax Error-free and Generalizable Tool Use for LLMs via Finite-state Decoding\", \"Toolchain*: Efficient Action Space Navigation in Large Language Models with $a^{*}$ Search\"], \"Citation\":[\"Don't Fine-Tune, Decode: Syntax Error-Free Tool Use via Constrained Decoding\", \"Smurfs: Leveraging Multiple Proficiency Agents with Context-Efficiency for Tool Planning\"]}",
        "eval_response": "Evaluation evidence: The assistant's answer does not match the gold answer in terms of accuracy and completeness. The assistant provided a long list of references and citations that are not mentioned in the gold answer, indicating hallucinations and inaccuracies. The gold answer specifies one reference and one citation, which the assistant failed to isolate correctly, instead listing multiple unrelated titles. This shows a lack of precision and adherence to the provided gold answer.\n\nRating: [[20]]",
        "score": 20
    },
    {
        "level": 3,
        "set": 3,
        "length": 105694,
        "type": "paper",
        "language": "en",
        "question": "How Johnny Can Persuade LLMs to Jailbreak Them: Rethinking Persuasion to Challenge AI Safety by Humanizing LLMs",
        "instruction": "We hope you will carefully study the provided papers and determine the citation relationships between them. Please follow the instructions below strictly to complete the task:\n\n#Specific Requirements:\n1. Reference: When a given paper mentions other provided papers, those other papers are considered as \"references\" for the given paper. To summarize in this specific context, references are about what the given paper is using.\n2. Citation: Conversely, when other provided papers mention the given paper in their works, the given paper is being \"cited\" by those other papers. To summarize in this specific context, citations are about who is using the given paper.\n3. Given a paper, you need to determine the citation or reference relationship between this paper and the other papers. Do not consider papers that are not provided.\n3. Please present the paper titles in a json format as follows: {{\"Reference\":[\"Reference Title 1\", \"Reference Title 2\", ..., \"Reference Title n\"], \"Citation\":[\"Citation Title 1\", \"Citation Title 2\", ..., \"Citation Title n\"]}}.\n4. If a paper does not have any references or citations, please leave the corresponding list empty, e.g.{{\"Refernce\":[]}}, {{\"Citation\":[]}}.",
        "prompt_template": "#Papers:\n{docs}\n\n{instruction}\n\n#The paper you need to analyze:\n{question}",
        "doc": [
            "2401.06373.md",
            "2405.19103.md",
            "2405.20947.md",
            "2310.06987.md",
            "2310.08419.md"
        ],
        "answer": {
            "Reference": [
                "# CATASTROPHIC JAILBREAK OF OPEN-SOURCE LLMS VIA EXPLOITING GENERATION ",
                "# Jailbreaking Black Box Large Language Models in Twenty Queries "
            ],
            "Citation": [
                "# Voice Jailbreak Attacks Against GPT-4o ",
                "# OR-Bench: An Over-Refusal Benchmark for Large Language Models "
            ]
        },
        "shuffle_doc": true,
        "id": "d9e6a4c6-bc1d-4ecc-a8ba-3f69635f9eb0",
        "prompt": "[Question]\n#Papers:\n{docs}\n\nWe hope you will carefully study the provided papers and determine the citation relationships between them. Please follow the instructions below strictly to complete the task:\n\n#Specific Requirements:\n1. Reference: When a given paper mentions other provided papers, those other papers are considered as \"references\" for the given paper. To summarize in this specific context, references are about what the given paper is using.\n2. Citation: Conversely, when other provided papers mention the given paper in their works, the given paper is being \"cited\" by those other papers. To summarize in this specific context, citations are about who is using the given paper.\n3. Given a paper, you need to determine the citation or reference relationship between this paper and the other papers. Do not consider papers that are not provided.\n3. Please present the paper titles in a json format as follows: {{\"Reference\":[\"Reference Title 1\", \"Reference Title 2\", ..., \"Reference Title n\"], \"Citation\":[\"Citation Title 1\", \"Citation Title 2\", ..., \"Citation Title n\"]}}.\n4. If a paper does not have any references or citations, please leave the corresponding list empty, e.g.{{\"Refernce\":[]}}, {{\"Citation\":[]}}.\n\n#The paper you need to analyze:\nHow Johnny Can Persuade LLMs to Jailbreak Them: Rethinking Persuasion to Challenge AI Safety by Humanizing LLMs\n\n[Gold Answer]\n{'Reference': ['# CATASTROPHIC JAILBREAK OF OPEN-SOURCE LLMS VIA EXPLOITING GENERATION ', '# Jailbreaking Black Box Large Language Models in Twenty Queries '], 'Citation': ['# Voice Jailbreak Attacks Against GPT-4o ', '# OR-Bench: An Over-Refusal Benchmark for Large Language Models ']}\n\n[The Start of Assistant's Predicted Answer]\n{\"Reference\":[\"Llama: An Open-Source Chatbot Impressing GPT-4 with $90 \\% *$ ChatGPT Quality\", \"A Survey on Evaluation of Large Language Models\", \"How to make a bomb?\", \"REALM: Retrieval-Augmented Language Model PreTraining\", \"Evaluating Correctness and Faithfulness of Instruction-Following Models for Question Answering\", \"AdvBench\", \"Dark patterns at scale: Findings from a crawl of $11 \\mathrm{k}$ shopping websites.\", \"Multilingual jailbreak challenges in large language models\", \"Scalable and transferable black-box jailbreaks for language models via persona modulation\", \"Adversarial demonstration attacks on large language models\", \"Shadow alignment: The ease of subverting safely-aligned language models\", \"Gptfuzzer: Red teaming large language models with auto-generated jailbreak prompts\", \"Gpt-4 is too smart to be safe: Stealthy chat with $11 \\mathrm{~ms}$ via cipher\", \"Detecting language model attacks with perplexity\", \"Jailbreaking black box large language models in twenty queries\", \"Weakly-supervised hierarchical models for predicting persuasive strategies in good-faith textual requests\", \"Automatically auditing large language models via discrete optimization\", \"Exploiting programmatic behavior of $11 \\mathrm{~ms}$ : Dual-use through standard security attacks\", \"Use of llms for illicit purposes: Threats, prevention measures, and vulnerabilities\", \"Certifying $11 \\mathrm{~m}$ safety against adversarial prompting\", \"Your language models can align themselves without finetuning\", \"Autodan: Generating stealthy jailbreak prompts on aligned large language models\", \"Jailbreaking chatgpt via prompt engineering: An empirical study\", \"Shining a light on dark patterns\", \"Dark patterns: Past, present, and future: The evolution of tricky user interfaces\", \"Evidence-based advertising using persuasion principles: Predictive validity and proof of concept\", \"Self-inference processes: The ontario symposium, vol. 6\", \"Gpt-4 technical report\", \"Persuasion\", \"The Dynamics of Persuasion: Communication and Attitudes in the 21st Century\", \"Emotional factors in attitudes and persuasion\", \"The persuasiveness of source credibility: A critical review of five decades' evidence\", \"Persuasion and coercion: a critical review of philosophical and empirical approaches\", \"Ignore this title and hackaprompt: Exposing systemic vulnerabilities of llms through a global prompt hacking competition\", \"Jailbreak and guard aligned language models with only few in-context demonstrations\", \"The effects of expert and consumer endorsements on audience response\"], \"Citation\":[\"Jailbreaking Black Box Large Language Models in Twenty Queries\"]}\n[The End of Assistant's Predicted Answer]\n\n[System]\nWe would like to request your feedback on the performance of the AI assistant in response to the user question displayed above according to the gold answer. Please use the following listed aspects and their descriptions as evaluation criteria:\n    - Accuracy and Hallucinations: The assistant's answer is semantically consistent with the gold answer; The numerical value and order need to be accurate, and there should be no hallucinations.\n    - Completeness: Referring to the reference answers, the assistant's answer should contain all the key points needed to answer the user's question; further elaboration on these key points can be omitted.\nPlease rate whether this answer is suitable for the question. Please note that the gold answer can be considered as a correct answer to the question.\n\nThe assistant receives an overall score on a scale of 1 to 100, where a higher score indicates better overall performance.\nPlease note that if the assistant's answer and the gold answer fully meet the above criteria, its overall rating should be the full marks (100).\nPlease first provide a comprehensive explanation of your evaluation, avoiding any potential bias.\nThen, output a line indicating the score of the Assistant.\n\nPLEASE OUTPUT WITH THE FOLLOWING FORMAT, WHERE THE SCORE IS A SCALE OF 1 TO 100 BY STRICTLY FOLLOWING THIS FORMAT: \"[[score]]\", FOR EXAMPLE \"Rating: [[100]]\":\n<start output>\nEvaluation evidence: your evluation explanation here, no more than 100 words\nRating: [[score]]\n<end output>\n\nNow, start your evaluation:",
        "generate_response": "{\"Reference\":[\"Llama: An Open-Source Chatbot Impressing GPT-4 with $90 \\% *$ ChatGPT Quality\", \"A Survey on Evaluation of Large Language Models\", \"How to make a bomb?\", \"REALM: Retrieval-Augmented Language Model PreTraining\", \"Evaluating Correctness and Faithfulness of Instruction-Following Models for Question Answering\", \"AdvBench\", \"Dark patterns at scale: Findings from a crawl of $11 \\mathrm{k}$ shopping websites.\", \"Multilingual jailbreak challenges in large language models\", \"Scalable and transferable black-box jailbreaks for language models via persona modulation\", \"Adversarial demonstration attacks on large language models\", \"Shadow alignment: The ease of subverting safely-aligned language models\", \"Gptfuzzer: Red teaming large language models with auto-generated jailbreak prompts\", \"Gpt-4 is too smart to be safe: Stealthy chat with $11 \\mathrm{~ms}$ via cipher\", \"Detecting language model attacks with perplexity\", \"Jailbreaking black box large language models in twenty queries\", \"Weakly-supervised hierarchical models for predicting persuasive strategies in good-faith textual requests\", \"Automatically auditing large language models via discrete optimization\", \"Exploiting programmatic behavior of $11 \\mathrm{~ms}$ : Dual-use through standard security attacks\", \"Use of llms for illicit purposes: Threats, prevention measures, and vulnerabilities\", \"Certifying $11 \\mathrm{~m}$ safety against adversarial prompting\", \"Your language models can align themselves without finetuning\", \"Autodan: Generating stealthy jailbreak prompts on aligned large language models\", \"Jailbreaking chatgpt via prompt engineering: An empirical study\", \"Shining a light on dark patterns\", \"Dark patterns: Past, present, and future: The evolution of tricky user interfaces\", \"Evidence-based advertising using persuasion principles: Predictive validity and proof of concept\", \"Self-inference processes: The ontario symposium, vol. 6\", \"Gpt-4 technical report\", \"Persuasion\", \"The Dynamics of Persuasion: Communication and Attitudes in the 21st Century\", \"Emotional factors in attitudes and persuasion\", \"The persuasiveness of source credibility: A critical review of five decades' evidence\", \"Persuasion and coercion: a critical review of philosophical and empirical approaches\", \"Ignore this title and hackaprompt: Exposing systemic vulnerabilities of llms through a global prompt hacking competition\", \"Jailbreak and guard aligned language models with only few in-context demonstrations\", \"The effects of expert and consumer endorsements on audience response\"], \"Citation\":[\"Jailbreaking Black Box Large Language Models in Twenty Queries\"]}",
        "eval_response": "Evaluation evidence: The assistant's answer does not match the gold answer in terms of accuracy and completeness. The references and citations listed are largely incorrect and not found in the gold answer, indicating hallucinations and inaccuracies. The correct references and citations from the gold answer are missing in the assistant's response, making it incomplete and incorrect.\n\nRating: [[10]]",
        "score": 10
    },
    {
        "level": 4,
        "set": 3,
        "length": 163937,
        "type": "paper",
        "language": "en",
        "question": "",
        "instruction": "We kindly ask you to thoroughly review the provided papers and construct a citation chain from them. Please adhere to the following instructions strictly while completing the task:\n\n#Task Instructions:\nGiven several papers, you are required to identify and list the longest citation chain, which demonstrates the citation relationship among the provided papers.\n\n#Specific Requirements:\n1.Please present the titles of the papers in the form of a list, as follows: [\"Title of Paper 1\", \"Title of Paper 2\", ..., \"Title of Paper n\"].\n2.Ensure that the citation chain in the list is linear and continuous, meaning that the first paper title in the list (Paper 1) should not cite any other works. Instead, it should be cited by the next paper in the list (Paper 2); subsequently, each paper should then be cited by the next one in the list, continuing up to the last paper (Paper n).\n3.Consider only the citation relationships within the supplied collection of papers, and ensure that the citation chain accurately reflects the sequential citation order among these documents.\n4.Do not take into account any articles not provided, and disregard other non-linear citation relationships.",
        "prompt_template": "{instruction}\n\n#Papers Provided:\n{docs}",
        "doc": [
            "2308.05384.md",
            "2310.01036.md",
            "2303.16129.md",
            "2304.08782.md",
            "2405.08542.md"
        ],
        "answer": [
            "# Enhancing Deep Reinforcement Learning: A Tutorial on Generative Diffusion Models in Network Optimization ",
            "# Generative AI for Integrated Sensing and Communication: Insights from the Physical Layer Perspective ",
            "# Unleashing the Power of Edge-Cloud Generative AI in Mobile Networks: A Survey of AIGC Services ",
            "# Sparks of GPTs in Edge Intelligence for Metaverse: Caching and Inference for Mobile AIGC Services ",
            "# Industrial Metaverse: Enabling Technologies, Open Problems, and Future Trends "
        ],
        "shuffle_doc": true,
        "id": "021b178d-2780-4040-9535-84faaed5f231",
        "prompt": "[Question]\nWe kindly ask you to thoroughly review the provided papers and construct a citation chain from them. Please adhere to the following instructions strictly while completing the task:\n\n#Task Instructions:\nGiven several papers, you are required to identify and list the longest citation chain, which demonstrates the citation relationship among the provided papers.\n\n#Specific Requirements:\n1.Please present the titles of the papers in the form of a list, as follows: [\"Title of Paper 1\", \"Title of Paper 2\", ..., \"Title of Paper n\"].\n2.Ensure that the citation chain in the list is linear and continuous, meaning that the first paper title in the list (Paper 1) should not cite any other works. Instead, it should be cited by the next paper in the list (Paper 2); subsequently, each paper should then be cited by the next one in the list, continuing up to the last paper (Paper n).\n3.Consider only the citation relationships within the supplied collection of papers, and ensure that the citation chain accurately reflects the sequential citation order among these documents.\n4.Do not take into account any articles not provided, and disregard other non-linear citation relationships.\n\n#Papers Provided:\n{docs}\n\n[Gold Answer]\n['# Enhancing Deep Reinforcement Learning: A Tutorial on Generative Diffusion Models in Network Optimization ', '# Generative AI for Integrated Sensing and Communication: Insights from the Physical Layer Perspective ', '# Unleashing the Power of Edge-Cloud Generative AI in Mobile Networks: A Survey of AIGC Services ', '# Sparks of GPTs in Edge Intelligence for Metaverse: Caching and Inference for Mobile AIGC Services ', '# Industrial Metaverse: Enabling Technologies, Open Problems, and Future Trends ']\n\n[The Start of Assistant's Predicted Answer]\n[\"Denoising Diffusion Probabilistic Models (DDPM)\", \"Denoising Diffusion Probabilistic Models (DDPM) in the Image Domain\", \"Denoising Diffusion Implicit Models (DDIM) in the Image Domain\", \"Diffusion-based Image Restoration Method\", \"Equivariant Diffuser for Generating Interactions (EDGI)\", \"Multi-agent Adversarial Training Using Diffusion Learning\", \"Denoising Diffusion Error Correction Codes\", \"Imitating Human Behaviour with Diffusion Models\"]\n[The End of Assistant's Predicted Answer]\n\n[System]\nWe would like to request your feedback on the performance of the AI assistant in response to the user question displayed above according to the gold answer. Please use the following listed aspects and their descriptions as evaluation criteria:\n    - Accuracy and Hallucinations: The assistant's answer is semantically consistent with the gold answer; The numerical value and order need to be accurate, and there should be no hallucinations.\n    - Completeness: Referring to the reference answers, the assistant's answer should contain all the key points needed to answer the user's question; further elaboration on these key points can be omitted.\nPlease rate whether this answer is suitable for the question. Please note that the gold answer can be considered as a correct answer to the question.\n\nThe assistant receives an overall score on a scale of 1 to 100, where a higher score indicates better overall performance.\nPlease note that if the assistant's answer and the gold answer fully meet the above criteria, its overall rating should be the full marks (100).\nPlease first provide a comprehensive explanation of your evaluation, avoiding any potential bias.\nThen, output a line indicating the score of the Assistant.\n\nPLEASE OUTPUT WITH THE FOLLOWING FORMAT, WHERE THE SCORE IS A SCALE OF 1 TO 100 BY STRICTLY FOLLOWING THIS FORMAT: \"[[score]]\", FOR EXAMPLE \"Rating: [[100]]\":\n<start output>\nEvaluation evidence: your evluation explanation here, no more than 100 words\nRating: [[score]]\n<end output>\n\nNow, start your evaluation:",
        "generate_response": "[\"Denoising Diffusion Probabilistic Models (DDPM)\", \"Denoising Diffusion Probabilistic Models (DDPM) in the Image Domain\", \"Denoising Diffusion Implicit Models (DDIM) in the Image Domain\", \"Diffusion-based Image Restoration Method\", \"Equivariant Diffuser for Generating Interactions (EDGI)\", \"Multi-agent Adversarial Training Using Diffusion Learning\", \"Denoising Diffusion Error Correction Codes\", \"Imitating Human Behaviour with Diffusion Models\"]",
        "eval_response": "Evaluation evidence: The assistant's response does not align with the gold answer or the task instructions. The titles listed by the assistant do not match any from the gold answer, indicating a complete deviation from the provided data set. This suggests hallucination, as the titles seem fabricated or irrelevant to the task. Additionally, the response fails to demonstrate a citation chain, which was the primary requirement. Therefore, the answer lacks accuracy, relevance, and completeness concerning the task.\n\nRating: [[10]]",
        "score": 10
    },
    {
        "level": 4,
        "set": 1,
        "length": 42740,
        "type": "paper",
        "language": "en",
        "question": "",
        "instruction": "We kindly ask you to thoroughly review the provided papers and construct a citation chain from them. Please adhere to the following instructions strictly while completing the task:\n\n#Task Instructions:\nGiven several papers, you are required to identify and list the longest citation chain, which demonstrates the citation relationship among the provided papers.\n\n#Specific Requirements:\n1.Please present the titles of the papers in the form of a list, as follows: [\"Title of Paper 1\", \"Title of Paper 2\", ..., \"Title of Paper n\"].\n2.Ensure that the citation chain in the list is linear and continuous, meaning that the first paper title in the list (Paper 1) should not cite any other works. Instead, it should be cited by the next paper in the list (Paper 2); subsequently, each paper should then be cited by the next one in the list, continuing up to the last paper (Paper n).\n3.Consider only the citation relationships within the supplied collection of papers, and ensure that the citation chain accurately reflects the sequential citation order among these documents.\n4.Do not take into account any articles not provided, and disregard other non-linear citation relationships.",
        "prompt_template": "{instruction}\n\n#Papers Provided:\n{docs}",
        "doc": [
            "2010.00453.md",
            "2107.04700.md"
        ],
        "answer": [
            "## Water is a radiation protection agent for ionised pyrrole",
            "# The unreasonable effectiveness of optimal transport in economics "
        ],
        "shuffle_doc": true,
        "id": "edc6d95f-02ba-4835-bff6-853cb7d61f6b",
        "prompt": "[Question]\nWe kindly ask you to thoroughly review the provided papers and construct a citation chain from them. Please adhere to the following instructions strictly while completing the task:\n\n#Task Instructions:\nGiven several papers, you are required to identify and list the longest citation chain, which demonstrates the citation relationship among the provided papers.\n\n#Specific Requirements:\n1.Please present the titles of the papers in the form of a list, as follows: [\"Title of Paper 1\", \"Title of Paper 2\", ..., \"Title of Paper n\"].\n2.Ensure that the citation chain in the list is linear and continuous, meaning that the first paper title in the list (Paper 1) should not cite any other works. Instead, it should be cited by the next paper in the list (Paper 2); subsequently, each paper should then be cited by the next one in the list, continuing up to the last paper (Paper n).\n3.Consider only the citation relationships within the supplied collection of papers, and ensure that the citation chain accurately reflects the sequential citation order among these documents.\n4.Do not take into account any articles not provided, and disregard other non-linear citation relationships.\n\n#Papers Provided:\n{docs}\n\n[Gold Answer]\n['## Water is a radiation protection agent for ionised pyrrole', '# The unreasonable effectiveness of optimal transport in economics ']\n\n[The Start of Assistant's Predicted Answer]\n[]\n[The End of Assistant's Predicted Answer]\n\n[System]\nWe would like to request your feedback on the performance of the AI assistant in response to the user question displayed above according to the gold answer. Please use the following listed aspects and their descriptions as evaluation criteria:\n    - Accuracy and Hallucinations: The assistant's answer is semantically consistent with the gold answer; The numerical value and order need to be accurate, and there should be no hallucinations.\n    - Completeness: Referring to the reference answers, the assistant's answer should contain all the key points needed to answer the user's question; further elaboration on these key points can be omitted.\nPlease rate whether this answer is suitable for the question. Please note that the gold answer can be considered as a correct answer to the question.\n\nThe assistant receives an overall score on a scale of 1 to 100, where a higher score indicates better overall performance.\nPlease note that if the assistant's answer and the gold answer fully meet the above criteria, its overall rating should be the full marks (100).\nPlease first provide a comprehensive explanation of your evaluation, avoiding any potential bias.\nThen, output a line indicating the score of the Assistant.\n\nPLEASE OUTPUT WITH THE FOLLOWING FORMAT, WHERE THE SCORE IS A SCALE OF 1 TO 100 BY STRICTLY FOLLOWING THIS FORMAT: \"[[score]]\", FOR EXAMPLE \"Rating: [[100]]\":\n<start output>\nEvaluation evidence: your evluation explanation here, no more than 100 words\nRating: [[score]]\n<end output>\n\nNow, start your evaluation:",
        "generate_response": "[]",
        "eval_response": "Evaluation evidence: The assistant failed to provide any answer to the task of identifying and listing the longest citation chain among the provided papers. This results in a complete lack of accuracy and completeness as there is no content to evaluate against the gold answer. The assistant's response does not meet the requirements of the task.\n\nRating: [[0]]",
        "score": 0
    },
    {
        "level": 4,
        "set": 2,
        "length": 85663,
        "type": "paper",
        "language": "en",
        "question": "",
        "instruction": "We kindly ask you to thoroughly review the provided papers and construct a citation chain from them. Please adhere to the following instructions strictly while completing the task:\n\n#Task Instructions:\nGiven several papers, you are required to identify and list the longest citation chain, which demonstrates the citation relationship among the provided papers.\n\n#Specific Requirements:\n1.Please present the titles of the papers in the form of a list, as follows: [\"Title of Paper 1\", \"Title of Paper 2\", ..., \"Title of Paper n\"].\n2.Ensure that the citation chain in the list is linear and continuous, meaning that the first paper title in the list (Paper 1) should not cite any other works. Instead, it should be cited by the next paper in the list (Paper 2); subsequently, each paper should then be cited by the next one in the list, continuing up to the last paper (Paper n).\n3.Consider only the citation relationships within the supplied collection of papers, and ensure that the citation chain accurately reflects the sequential citation order among these documents.\n4.Do not take into account any articles not provided, and disregard other non-linear citation relationships.",
        "prompt_template": "{instruction}\n\n#Papers Provided:\n{docs}",
        "doc": [
            "2312.09241.md",
            "2401.01335.md",
            "2401.10020.md",
            "2402.01306.md",
            "2402.14830.md"
        ],
        "answer": [
            "# TinyGSM: achieving $>80 \\%$ on GSM8k with small language models ",
            "# Self-Play Fine-Tuning Converts Weak Language Models to Strong Language Models ",
            "# Self-Rewarding Language Models ",
            "# KTO: Model Alignment as Prospect Theoretic Optimization ",
            "# DI Orca-Math: Unlocking the potential of SLMs in Grade School Math "
        ],
        "shuffle_doc": true,
        "id": "2768bea3-291b-45f9-9ed2-c9287a97eda6",
        "prompt": "[Question]\nWe kindly ask you to thoroughly review the provided papers and construct a citation chain from them. Please adhere to the following instructions strictly while completing the task:\n\n#Task Instructions:\nGiven several papers, you are required to identify and list the longest citation chain, which demonstrates the citation relationship among the provided papers.\n\n#Specific Requirements:\n1.Please present the titles of the papers in the form of a list, as follows: [\"Title of Paper 1\", \"Title of Paper 2\", ..., \"Title of Paper n\"].\n2.Ensure that the citation chain in the list is linear and continuous, meaning that the first paper title in the list (Paper 1) should not cite any other works. Instead, it should be cited by the next paper in the list (Paper 2); subsequently, each paper should then be cited by the next one in the list, continuing up to the last paper (Paper n).\n3.Consider only the citation relationships within the supplied collection of papers, and ensure that the citation chain accurately reflects the sequential citation order among these documents.\n4.Do not take into account any articles not provided, and disregard other non-linear citation relationships.\n\n#Papers Provided:\n{docs}\n\n[Gold Answer]\n['# TinyGSM: achieving $>80 \\\\%$ on GSM8k with small language models ', '# Self-Play Fine-Tuning Converts Weak Language Models to Strong Language Models ', '# Self-Rewarding Language Models ', '# KTO: Model Alignment as Prospect Theoretic Optimization ', '# DI Orca-Math: Unlocking the potential of SLMs in Grade School Math ']\n\n[The Start of Assistant's Predicted Answer]\n[\"TinyGSM: achieving $>80 \\%$ on GSM8k with small language models\", \"Vicuna: An Open-Source Chatbot Impressing GPT-4 with $90 \\% *$ ChatGPT Quality\", \"Reinforcement Learning from Human Feedback\", \"The CRINGE loss: Learning what language not to model\", \"Reinforced self-training (rest) for language modeling\"]\n[The End of Assistant's Predicted Answer]\n\n[System]\nWe would like to request your feedback on the performance of the AI assistant in response to the user question displayed above according to the gold answer. Please use the following listed aspects and their descriptions as evaluation criteria:\n    - Accuracy and Hallucinations: The assistant's answer is semantically consistent with the gold answer; The numerical value and order need to be accurate, and there should be no hallucinations.\n    - Completeness: Referring to the reference answers, the assistant's answer should contain all the key points needed to answer the user's question; further elaboration on these key points can be omitted.\nPlease rate whether this answer is suitable for the question. Please note that the gold answer can be considered as a correct answer to the question.\n\nThe assistant receives an overall score on a scale of 1 to 100, where a higher score indicates better overall performance.\nPlease note that if the assistant's answer and the gold answer fully meet the above criteria, its overall rating should be the full marks (100).\nPlease first provide a comprehensive explanation of your evaluation, avoiding any potential bias.\nThen, output a line indicating the score of the Assistant.\n\nPLEASE OUTPUT WITH THE FOLLOWING FORMAT, WHERE THE SCORE IS A SCALE OF 1 TO 100 BY STRICTLY FOLLOWING THIS FORMAT: \"[[score]]\", FOR EXAMPLE \"Rating: [[100]]\":\n<start output>\nEvaluation evidence: your evluation explanation here, no more than 100 words\nRating: [[score]]\n<end output>\n\nNow, start your evaluation:",
        "generate_response": "[\"TinyGSM: achieving $>80 \\%$ on GSM8k with small language models\", \"Vicuna: An Open-Source Chatbot Impressing GPT-4 with $90 \\% *$ ChatGPT Quality\", \"Reinforcement Learning from Human Feedback\", \"The CRINGE loss: Learning what language not to model\", \"Reinforced self-training (rest) for language modeling\"]",
        "eval_response": "Evaluation evidence: The assistant's answer does not match the gold answer in terms of the titles of the papers listed. The titles in the assistant's response do not correspond to those in the gold answer, indicating a failure in accurately identifying the correct citation chain from the provided papers. This discrepancy shows issues with both accuracy and completeness, as the assistant neither correctly identified the titles nor maintained the correct citation order as per the task requirements.\n\nRating: [[10]]",
        "score": 10
    },
    {
        "level": 3,
        "set": 2,
        "length": 60326,
        "type": "paper",
        "language": "en",
        "question": "Seven Failure Points When Engineering a Retrieval Augmented Generation System",
        "instruction": "We hope you will carefully study the provided papers and determine the citation relationships between them. Please follow the instructions below strictly to complete the task:\n\n#Specific Requirements:\n1. Reference: When a given paper mentions other provided papers, those other papers are considered as \"references\" for the given paper. To summarize in this specific context, references are about what the given paper is using.\n2. Citation: Conversely, when other provided papers mention the given paper in their works, the given paper is being \"cited\" by those other papers. To summarize in this specific context, citations are about who is using the given paper.\n3. Given a paper, you need to determine the citation or reference relationship between this paper and the other papers. Do not consider papers that are not provided.\n3. Please present the paper titles in a json format as follows: {{\"Reference\":[\"Reference Title 1\", \"Reference Title 2\", ..., \"Reference Title n\"], \"Citation\":[\"Citation Title 1\", \"Citation Title 2\", ..., \"Citation Title n\"]}}.\n4. If a paper does not have any references or citations, please leave the corresponding list empty, e.g.{{\"Refernce\":[]}}, {{\"Citation\":[]}}.",
        "prompt_template": "#Papers:\n{docs}\n\n{instruction}\n\n#The paper you need to analyze:\n{question}",
        "doc": [
            "2401.05856.md",
            "2404.19705.md",
            "2405.01868.md",
            "2305.13691.md"
        ],
        "answer": {
            "Reference": [
                "# Few-Shot Data Synthesis for Open Domain Multi-Hop Question Answering "
            ],
            "Citation": [
                "# When to Retrieve: Teaching LLMs to Utilize Information Retrieval Effectively ",
                "# Incorporating External Knowledge and Goal Guidance for LLM-based Conversational Recommender Systems "
            ]
        },
        "shuffle_doc": true,
        "id": "dc880581-4381-48e4-8cc7-79af288ccab2",
        "prompt": "[Question]\n#Papers:\n{docs}\n\nWe hope you will carefully study the provided papers and determine the citation relationships between them. Please follow the instructions below strictly to complete the task:\n\n#Specific Requirements:\n1. Reference: When a given paper mentions other provided papers, those other papers are considered as \"references\" for the given paper. To summarize in this specific context, references are about what the given paper is using.\n2. Citation: Conversely, when other provided papers mention the given paper in their works, the given paper is being \"cited\" by those other papers. To summarize in this specific context, citations are about who is using the given paper.\n3. Given a paper, you need to determine the citation or reference relationship between this paper and the other papers. Do not consider papers that are not provided.\n3. Please present the paper titles in a json format as follows: {{\"Reference\":[\"Reference Title 1\", \"Reference Title 2\", ..., \"Reference Title n\"], \"Citation\":[\"Citation Title 1\", \"Citation Title 2\", ..., \"Citation Title n\"]}}.\n4. If a paper does not have any references or citations, please leave the corresponding list empty, e.g.{{\"Refernce\":[]}}, {{\"Citation\":[]}}.\n\n#The paper you need to analyze:\nSeven Failure Points When Engineering a Retrieval Augmented Generation System\n\n[Gold Answer]\n{'Reference': ['# Few-Shot Data Synthesis for Open Domain Multi-Hop Question Answering '], 'Citation': ['# When to Retrieve: Teaching LLMs to Utilize Information Retrieval Effectively ', '# Incorporating External Knowledge and Goal Guidance for LLM-based Conversational Recommender Systems ']}\n\n[The Start of Assistant's Predicted Answer]\n{\"Reference\":[\"GPTCache: An Open-Source Semantic Cache for LLM Applications Enabling Faster Answers and Cost Savings\", \"Self-adaptive Machine Learning Systems: Research Challenges and Opportunities\", \"Benchmarking Large Language Models in Retrieval-Augmented Generation\", \"Efficient Open Domain Multi-Hop Question Answering with Few-Shot Data Synthesis\", \"Threshy: Supporting safe usage of intelligent web services\", \"Beware the evolving 'intelligent'web service! An integration architecture tactic to guard AI-first components\", \"Retrieval augmented language model pre-training\", \"Fidlight: Efficient and effective retrieval-augmented text generation\", \"Leveraging passage retrieval with generative models for open domain question answering\", \"BioASQ-QA: A manually curated corpus for biomedical question answering\", \"LoRA Fine-tuning Efficiently Undoes Safety Training in Llama 2-Chat 70B\", \"Retrieval-augmented generation for knowledge-intensive nlp tasks\", \"Lost in the middle: How language models use long contexts\", \"G-eval: Nlg evaluation using gpt-4 with better human alignment, may 2023\", \"Retrieval-based prompt selection for code-related few-shot learning\", \"GPT-4 Technical Report\", \"Robust speech recognition via large-scale weak supervision\", \"Improving the domain adaptation of retrieval augmented generation (RAG) models for open domain question answering\", \"Large language models for information retrieval: A survey\"], \"Citation\":[\"When to Retrieve: Teaching LLMs to Utilize Information Retrieval Effectively\", \"When Not to Trust Language Models: Investigating Effectiveness of Parametric and NonParametric Memories\", \"Stanford Alpaca: An Instruction-Following Llama Model\", \"Beir: A Heterogeneous Benchmark for Zero-Shot Evaluation of Information Retrieval Models\", \"Llama: Open and Efficient Foundation Language Models\", \"Llama 2: Open Foundation and Fine-Tuned Chat Models\", \"Retrieving and Reading: A Comprehensive Survey on Open-Domain Question Answering\", \"Bridging the Lexical Chasm: Statistical Approaches to Answer-Finding\", \"Improving Language Models by Retrieving from Trillions of Tokens\", \"Language Models are Few-Shot Learners\", \"QuAC: Question Answering in Context\", \"Natural Questions: A Benchmark for Question Answering Research\", \"Dense Passage Retrieval for Open-Domain Question Answering\", \"Ms Marco: A Human-Generated Machine Reading Comprehension Dataset\", \"In-Context Retrieval-Augmented Language Models\", \"Gemini 1.5: Unlocking Multimodal Understanding Across Millions of Tokens of Context\", \"Sentence-Bert: Sentence Embeddings Using Siamese Bert-Networks\"]}\n[The End of Assistant's Predicted Answer]\n\n[System]\nWe would like to request your feedback on the performance of the AI assistant in response to the user question displayed above according to the gold answer. Please use the following listed aspects and their descriptions as evaluation criteria:\n    - Accuracy and Hallucinations: The assistant's answer is semantically consistent with the gold answer; The numerical value and order need to be accurate, and there should be no hallucinations.\n    - Completeness: Referring to the reference answers, the assistant's answer should contain all the key points needed to answer the user's question; further elaboration on these key points can be omitted.\nPlease rate whether this answer is suitable for the question. Please note that the gold answer can be considered as a correct answer to the question.\n\nThe assistant receives an overall score on a scale of 1 to 100, where a higher score indicates better overall performance.\nPlease note that if the assistant's answer and the gold answer fully meet the above criteria, its overall rating should be the full marks (100).\nPlease first provide a comprehensive explanation of your evaluation, avoiding any potential bias.\nThen, output a line indicating the score of the Assistant.\n\nPLEASE OUTPUT WITH THE FOLLOWING FORMAT, WHERE THE SCORE IS A SCALE OF 1 TO 100 BY STRICTLY FOLLOWING THIS FORMAT: \"[[score]]\", FOR EXAMPLE \"Rating: [[100]]\":\n<start output>\nEvaluation evidence: your evluation explanation here, no more than 100 words\nRating: [[score]]\n<end output>\n\nNow, start your evaluation:",
        "generate_response": "{\"Reference\":[\"GPTCache: An Open-Source Semantic Cache for LLM Applications Enabling Faster Answers and Cost Savings\", \"Self-adaptive Machine Learning Systems: Research Challenges and Opportunities\", \"Benchmarking Large Language Models in Retrieval-Augmented Generation\", \"Efficient Open Domain Multi-Hop Question Answering with Few-Shot Data Synthesis\", \"Threshy: Supporting safe usage of intelligent web services\", \"Beware the evolving 'intelligent'web service! An integration architecture tactic to guard AI-first components\", \"Retrieval augmented language model pre-training\", \"Fidlight: Efficient and effective retrieval-augmented text generation\", \"Leveraging passage retrieval with generative models for open domain question answering\", \"BioASQ-QA: A manually curated corpus for biomedical question answering\", \"LoRA Fine-tuning Efficiently Undoes Safety Training in Llama 2-Chat 70B\", \"Retrieval-augmented generation for knowledge-intensive nlp tasks\", \"Lost in the middle: How language models use long contexts\", \"G-eval: Nlg evaluation using gpt-4 with better human alignment, may 2023\", \"Retrieval-based prompt selection for code-related few-shot learning\", \"GPT-4 Technical Report\", \"Robust speech recognition via large-scale weak supervision\", \"Improving the domain adaptation of retrieval augmented generation (RAG) models for open domain question answering\", \"Large language models for information retrieval: A survey\"], \"Citation\":[\"When to Retrieve: Teaching LLMs to Utilize Information Retrieval Effectively\", \"When Not to Trust Language Models: Investigating Effectiveness of Parametric and NonParametric Memories\", \"Stanford Alpaca: An Instruction-Following Llama Model\", \"Beir: A Heterogeneous Benchmark for Zero-Shot Evaluation of Information Retrieval Models\", \"Llama: Open and Efficient Foundation Language Models\", \"Llama 2: Open Foundation and Fine-Tuned Chat Models\", \"Retrieving and Reading: A Comprehensive Survey on Open-Domain Question Answering\", \"Bridging the Lexical Chasm: Statistical Approaches to Answer-Finding\", \"Improving Language Models by Retrieving from Trillions of Tokens\", \"Language Models are Few-Shot Learners\", \"QuAC: Question Answering in Context\", \"Natural Questions: A Benchmark for Question Answering Research\", \"Dense Passage Retrieval for Open-Domain Question Answering\", \"Ms Marco: A Human-Generated Machine Reading Comprehension Dataset\", \"In-Context Retrieval-Augmented Language Models\", \"Gemini 1.5: Unlocking Multimodal Understanding Across Millions of Tokens of Context\", \"Sentence-Bert: Sentence Embeddings Using Siamese Bert-Networks\"]}",
        "eval_response": "Evaluation evidence: The assistant's answer includes a large number of references and citations that are not mentioned in the gold answer, indicating significant hallucinations. The gold answer specifies one reference and two citations, which are completely absent in the assistant's response. This discrepancy shows a lack of accuracy and introduces irrelevant information, failing to meet the completeness criterion as it does not match the gold answer's key points.\n\nRating: [[10]]",
        "score": 10
    },
    {
        "level": 3,
        "set": 2,
        "length": 63225,
        "type": "paper",
        "language": "en",
        "question": "Break the Sequential Dependency of LLM Inference Using LOOKAHEAD DECODING",
        "instruction": "We hope you will carefully study the provided papers and determine the citation relationships between them. Please follow the instructions below strictly to complete the task:\n\n#Specific Requirements:\n1. Reference: When a given paper mentions other provided papers, those other papers are considered as \"references\" for the given paper. To summarize in this specific context, references are about what the given paper is using.\n2. Citation: Conversely, when other provided papers mention the given paper in their works, the given paper is being \"cited\" by those other papers. To summarize in this specific context, citations are about who is using the given paper.\n3. Given a paper, you need to determine the citation or reference relationship between this paper and the other papers. Do not consider papers that are not provided.\n3. Please present the paper titles in a json format as follows: {{\"Reference\":[\"Reference Title 1\", \"Reference Title 2\", ..., \"Reference Title n\"], \"Citation\":[\"Citation Title 1\", \"Citation Title 2\", ..., \"Citation Title n\"]}}.\n4. If a paper does not have any references or citations, please leave the corresponding list empty, e.g.{{\"Refernce\":[]}}, {{\"Citation\":[]}}.",
        "prompt_template": "#Papers:\n{docs}\n\n{instruction}\n\n#The paper you need to analyze:\n{question}",
        "doc": [
            "2402.02057.md",
            "2404.18911.md",
            "2405.00263.md",
            "2310.06825.md",
            "2311.08252.md"
        ],
        "answer": {
            "Reference": [
                "# Mistral 7B ",
                "# REST: Retrieval-Based Speculative Decoding "
            ],
            "Citation": [
                "# Kangaroo: Lossless Self-Speculative Decoding via Double Early Exiting ",
                "# Clover: Regressive Lightweight Speculative Decoding with Sequential Knowledge "
            ]
        },
        "shuffle_doc": true,
        "id": "9c5dd44f-2aad-4dc7-9f96-7eb995bef1e7",
        "prompt": "[Question]\n#Papers:\n{docs}\n\nWe hope you will carefully study the provided papers and determine the citation relationships between them. Please follow the instructions below strictly to complete the task:\n\n#Specific Requirements:\n1. Reference: When a given paper mentions other provided papers, those other papers are considered as \"references\" for the given paper. To summarize in this specific context, references are about what the given paper is using.\n2. Citation: Conversely, when other provided papers mention the given paper in their works, the given paper is being \"cited\" by those other papers. To summarize in this specific context, citations are about who is using the given paper.\n3. Given a paper, you need to determine the citation or reference relationship between this paper and the other papers. Do not consider papers that are not provided.\n3. Please present the paper titles in a json format as follows: {{\"Reference\":[\"Reference Title 1\", \"Reference Title 2\", ..., \"Reference Title n\"], \"Citation\":[\"Citation Title 1\", \"Citation Title 2\", ..., \"Citation Title n\"]}}.\n4. If a paper does not have any references or citations, please leave the corresponding list empty, e.g.{{\"Refernce\":[]}}, {{\"Citation\":[]}}.\n\n#The paper you need to analyze:\nBreak the Sequential Dependency of LLM Inference Using LOOKAHEAD DECODING\n\n[Gold Answer]\n{'Reference': ['# Mistral 7B ', '# REST: Retrieval-Based Speculative Decoding '], 'Citation': ['# Kangaroo: Lossless Self-Speculative Decoding via Double Early Exiting ', '# Clover: Regressive Lightweight Speculative Decoding with Sequential Knowledge ']}\n\n[The Start of Assistant's Predicted Answer]\n{\"Reference\":[\"A Survey on Evaluation of Large Language Models\", \"Vicuna: An Open-Source Chatbot Impressing GPT-4 with $90 \\% *$ ChatGPT Quality\", \"ChatLaw: Open-Source Legal Large Language Model with Integrated External Knowledge Bases\", \"Compositional Semantic Parsing with Large Language Models\", \"Open LLM Leaderboard\", \"How Close is ChatGPT to Human Experts? Comparison Corpus, Evaluation, and Detection\", \"Rethinking with Retrieval: Faithful Large Language Model Inference\", \"Measuring Massive Multitask Language Understanding\", \"Atlas: Few-shot Learning with Retrieval Augmented Language Models\", \"Efficient large-scale language model training on gpu clusters using megatron-lm\", \"Gpt-4 technical report\", \"Training language models to follow instructions with human feedback\", \"Code llama: Open foundation models for code\", \"Gemini: a family of highly capable multimodal models\", \"Llama: Open and efficient foundation language models\", \"Llama 2: Open foundation and finetuned chat models\", \"Attention is all you need\", \"Transformers: State-ofthe-art natural language processing\", \"Inference with reference: Lossless acceleration of large language models\", \"Judging llm-as-a-judge with mt-bench and chatbot arena\", \"Automatic tensor parallelism for huggingface models\", \"Deepspeed-inference: enabling efficient inference of transformer models at unprecedented scale\", \"Layer normalization\", \"A framework for the evaluation of code generation models\", \"Accelerating large language model decoding with speculative sampling\", \"The curious case of neural text degeneration\", \"Mistral 7b\", \"Ancestral gumbeltop-k sampling for sampling without replacement\", \"Fast inference from transformers via speculative decoding\", \"Eagle: Lossless acceleration of llm decoding by feature extrapolation\", \"ROUGE: A package for automatic evaluation of summaries\", \"Online speculative decoding\", \"Specinfer: Accelerating generative large language model serving with speculative inference and token tree verification\", \"Don't give me the details, just the summary! Topic-aware convolutional neural networks for extreme summarization\", \"Best prompting practices for using the llama 2 chat llm through amazon sagemaker jumpstart\", \"Accelerating transformer inference for translation via parallel decoding\", \"Prompt lookup decoding\", \"Get to the point: Summarization with pointer-generator networks\", \"Blockwise parallel decoding for deep autoregressive models\", \"Speculative decoding\", \"A family of highly capable multimodal models\", \"Attention is all you need\", \"State-of-the-art natural language processing\", \"Inference with reference: Lossless acceleration of large language models\", \"Judging llm-as-a-judge with mt-bench and chatbot arena\"], \"Citation\":[]}\n[The End of Assistant's Predicted Answer]\n\n[System]\nWe would like to request your feedback on the performance of the AI assistant in response to the user question displayed above according to the gold answer. Please use the following listed aspects and their descriptions as evaluation criteria:\n    - Accuracy and Hallucinations: The assistant's answer is semantically consistent with the gold answer; The numerical value and order need to be accurate, and there should be no hallucinations.\n    - Completeness: Referring to the reference answers, the assistant's answer should contain all the key points needed to answer the user's question; further elaboration on these key points can be omitted.\nPlease rate whether this answer is suitable for the question. Please note that the gold answer can be considered as a correct answer to the question.\n\nThe assistant receives an overall score on a scale of 1 to 100, where a higher score indicates better overall performance.\nPlease note that if the assistant's answer and the gold answer fully meet the above criteria, its overall rating should be the full marks (100).\nPlease first provide a comprehensive explanation of your evaluation, avoiding any potential bias.\nThen, output a line indicating the score of the Assistant.\n\nPLEASE OUTPUT WITH THE FOLLOWING FORMAT, WHERE THE SCORE IS A SCALE OF 1 TO 100 BY STRICTLY FOLLOWING THIS FORMAT: \"[[score]]\", FOR EXAMPLE \"Rating: [[100]]\":\n<start output>\nEvaluation evidence: your evluation explanation here, no more than 100 words\nRating: [[score]]\n<end output>\n\nNow, start your evaluation:",
        "generate_response": "{\"Reference\":[\"A Survey on Evaluation of Large Language Models\", \"Vicuna: An Open-Source Chatbot Impressing GPT-4 with $90 \\% *$ ChatGPT Quality\", \"ChatLaw: Open-Source Legal Large Language Model with Integrated External Knowledge Bases\", \"Compositional Semantic Parsing with Large Language Models\", \"Open LLM Leaderboard\", \"How Close is ChatGPT to Human Experts? Comparison Corpus, Evaluation, and Detection\", \"Rethinking with Retrieval: Faithful Large Language Model Inference\", \"Measuring Massive Multitask Language Understanding\", \"Atlas: Few-shot Learning with Retrieval Augmented Language Models\", \"Efficient large-scale language model training on gpu clusters using megatron-lm\", \"Gpt-4 technical report\", \"Training language models to follow instructions with human feedback\", \"Code llama: Open foundation models for code\", \"Gemini: a family of highly capable multimodal models\", \"Llama: Open and efficient foundation language models\", \"Llama 2: Open foundation and finetuned chat models\", \"Attention is all you need\", \"Transformers: State-ofthe-art natural language processing\", \"Inference with reference: Lossless acceleration of large language models\", \"Judging llm-as-a-judge with mt-bench and chatbot arena\", \"Automatic tensor parallelism for huggingface models\", \"Deepspeed-inference: enabling efficient inference of transformer models at unprecedented scale\", \"Layer normalization\", \"A framework for the evaluation of code generation models\", \"Accelerating large language model decoding with speculative sampling\", \"The curious case of neural text degeneration\", \"Mistral 7b\", \"Ancestral gumbeltop-k sampling for sampling without replacement\", \"Fast inference from transformers via speculative decoding\", \"Eagle: Lossless acceleration of llm decoding by feature extrapolation\", \"ROUGE: A package for automatic evaluation of summaries\", \"Online speculative decoding\", \"Specinfer: Accelerating generative large language model serving with speculative inference and token tree verification\", \"Don't give me the details, just the summary! Topic-aware convolutional neural networks for extreme summarization\", \"Best prompting practices for using the llama 2 chat llm through amazon sagemaker jumpstart\", \"Accelerating transformer inference for translation via parallel decoding\", \"Prompt lookup decoding\", \"Get to the point: Summarization with pointer-generator networks\", \"Blockwise parallel decoding for deep autoregressive models\", \"Speculative decoding\", \"A family of highly capable multimodal models\", \"Attention is all you need\", \"State-of-the-art natural language processing\", \"Inference with reference: Lossless acceleration of large language models\", \"Judging llm-as-a-judge with mt-bench and chatbot arena\"], \"Citation\":[]}",
        "eval_response": "Evaluation evidence: The assistant's answer does not align with the gold answer in terms of accuracy and completeness. The references listed in the assistant's answer do not match those in the gold answer, which specifically mentions \"# Mistral 7B\" and \"# REST: Retrieval-Based Speculative Decoding.\" Additionally, the assistant's answer includes a long list of references that are not relevant to the given paper, indicating hallucinations. The citations section is also incorrect as it is empty, whereas the gold answer lists two citing papers. This discrepancy shows a lack of accuracy and completeness.\n\nRating: [[10]]",
        "score": 10
    },
    {
        "level": 3,
        "set": 2,
        "length": 72644,
        "type": "paper",
        "language": "en",
        "question": "Eyes Wide Shut? Exploring the Visual Shortcomings of Multimodal LLMs",
        "instruction": "We hope you will carefully study the provided papers and determine the citation relationships between them. Please follow the instructions below strictly to complete the task:\n\n#Specific Requirements:\n1. Reference: When a given paper mentions other provided papers, those other papers are considered as \"references\" for the given paper. To summarize in this specific context, references are about what the given paper is using.\n2. Citation: Conversely, when other provided papers mention the given paper in their works, the given paper is being \"cited\" by those other papers. To summarize in this specific context, citations are about who is using the given paper.\n3. Given a paper, you need to determine the citation or reference relationship between this paper and the other papers. Do not consider papers that are not provided.\n3. Please present the paper titles in a json format as follows: {{\"Reference\":[\"Reference Title 1\", \"Reference Title 2\", ..., \"Reference Title n\"], \"Citation\":[\"Citation Title 1\", \"Citation Title 2\", ..., \"Citation Title n\"]}}.\n4. If a paper does not have any references or citations, please leave the corresponding list empty, e.g.{{\"Refernce\":[]}}, {{\"Citation\":[]}}.",
        "prompt_template": "#Papers:\n{docs}\n\n{instruction}\n\n#The paper you need to analyze:\n{question}",
        "doc": [
            "2401.06209.md",
            "2405.14974.md",
            "2405.18415.md",
            "2311.09215.md"
        ],
        "answer": {
            "Reference": [
                "# ConvNet vs Transformer, Supervised vs CLIP: Beyond ImageNet Accuracy "
            ],
            "Citation": [
                "# LOVA ${ }^{3}$ : Learning to Visual Question Answering, Asking and Assessment ",
                "# Why are Visually-Grounded Language Models Bad at Image Classification? "
            ]
        },
        "shuffle_doc": true,
        "id": "f6671f47-58a6-4852-9dd9-c324d97c89b7",
        "prompt": "[Question]\n#Papers:\n{docs}\n\nWe hope you will carefully study the provided papers and determine the citation relationships between them. Please follow the instructions below strictly to complete the task:\n\n#Specific Requirements:\n1. Reference: When a given paper mentions other provided papers, those other papers are considered as \"references\" for the given paper. To summarize in this specific context, references are about what the given paper is using.\n2. Citation: Conversely, when other provided papers mention the given paper in their works, the given paper is being \"cited\" by those other papers. To summarize in this specific context, citations are about who is using the given paper.\n3. Given a paper, you need to determine the citation or reference relationship between this paper and the other papers. Do not consider papers that are not provided.\n3. Please present the paper titles in a json format as follows: {{\"Reference\":[\"Reference Title 1\", \"Reference Title 2\", ..., \"Reference Title n\"], \"Citation\":[\"Citation Title 1\", \"Citation Title 2\", ..., \"Citation Title n\"]}}.\n4. If a paper does not have any references or citations, please leave the corresponding list empty, e.g.{{\"Refernce\":[]}}, {{\"Citation\":[]}}.\n\n#The paper you need to analyze:\nEyes Wide Shut? Exploring the Visual Shortcomings of Multimodal LLMs\n\n[Gold Answer]\n{'Reference': ['# ConvNet vs Transformer, Supervised vs CLIP: Beyond ImageNet Accuracy '], 'Citation': ['# LOVA ${ }^{3}$ : Learning to Visual Question Answering, Asking and Assessment ', '# Why are Visually-Grounded Language Models Bad at Image Classification? ']}\n\n[The Start of Assistant's Predicted Answer]\n{\"Reference\":[\"A Survey on Evaluation of Large Language Models\", \"Vicuna: An Open-Source Chatbot Impressing GPT-4 with $90 \\% *$ ChatGPT Quality\", \"ChatLaw: Open-Source Legal Large Language Model with Integrated External Knowledge Bases\", \"Compositional Semantic Parsing with Large Language Models\", \"Open LLM Leaderboard\", \"How Close is ChatGPT to Human Experts? Comparison Corpus, Evaluation, and Detection\", \"REALM: Retrieval-Augmented Language Model PreTraining\", \"Rethinking with Retrieval: Faithful Large Language Model Inference\", \"Measuring Massive Multitask Language Understanding\", \"Atlas: Few-shot Learning with Retrieval Augmented Language Models\", \"TextVQA\", \"VQAv2\", \"GQA\", \"MM-Vet\", \"POPE\", \"MM-Bench\", \"SLIP\", \"Thrush et al.\", \"Yuksekgonul et al.\", \"Hsieh et al.\", \"Tschannen et al.\", \"ImageNet\", \"LAION-Aesthetic datasets\", \"LAION400M\", \"SigL\", \"M\", \"EVA02 ViT-bigE-14+\", \"LLava 1.5\"], \"Citation\":[]}\n[The End of Assistant's Predicted Answer]\n\n[System]\nWe would like to request your feedback on the performance of the AI assistant in response to the user question displayed above according to the gold answer. Please use the following listed aspects and their descriptions as evaluation criteria:\n    - Accuracy and Hallucinations: The assistant's answer is semantically consistent with the gold answer; The numerical value and order need to be accurate, and there should be no hallucinations.\n    - Completeness: Referring to the reference answers, the assistant's answer should contain all the key points needed to answer the user's question; further elaboration on these key points can be omitted.\nPlease rate whether this answer is suitable for the question. Please note that the gold answer can be considered as a correct answer to the question.\n\nThe assistant receives an overall score on a scale of 1 to 100, where a higher score indicates better overall performance.\nPlease note that if the assistant's answer and the gold answer fully meet the above criteria, its overall rating should be the full marks (100).\nPlease first provide a comprehensive explanation of your evaluation, avoiding any potential bias.\nThen, output a line indicating the score of the Assistant.\n\nPLEASE OUTPUT WITH THE FOLLOWING FORMAT, WHERE THE SCORE IS A SCALE OF 1 TO 100 BY STRICTLY FOLLOWING THIS FORMAT: \"[[score]]\", FOR EXAMPLE \"Rating: [[100]]\":\n<start output>\nEvaluation evidence: your evluation explanation here, no more than 100 words\nRating: [[score]]\n<end output>\n\nNow, start your evaluation:",
        "generate_response": "{\"Reference\":[\"A Survey on Evaluation of Large Language Models\", \"Vicuna: An Open-Source Chatbot Impressing GPT-4 with $90 \\% *$ ChatGPT Quality\", \"ChatLaw: Open-Source Legal Large Language Model with Integrated External Knowledge Bases\", \"Compositional Semantic Parsing with Large Language Models\", \"Open LLM Leaderboard\", \"How Close is ChatGPT to Human Experts? Comparison Corpus, Evaluation, and Detection\", \"REALM: Retrieval-Augmented Language Model PreTraining\", \"Rethinking with Retrieval: Faithful Large Language Model Inference\", \"Measuring Massive Multitask Language Understanding\", \"Atlas: Few-shot Learning with Retrieval Augmented Language Models\", \"TextVQA\", \"VQAv2\", \"GQA\", \"MM-Vet\", \"POPE\", \"MM-Bench\", \"SLIP\", \"Thrush et al.\", \"Yuksekgonul et al.\", \"Hsieh et al.\", \"Tschannen et al.\", \"ImageNet\", \"LAION-Aesthetic datasets\", \"LAION400M\", \"SigL\", \"M\", \"EVA02 ViT-bigE-14+\", \"LLava 1.5\"], \"Citation\":[]}",
        "eval_response": "Evaluation evidence: The assistant's answer does not match the gold answer in terms of accuracy and completeness. The references listed in the assistant's answer do not include the one mentioned in the gold answer, and it incorrectly lists numerous unrelated references. Additionally, the assistant's answer fails to identify any citations, whereas the gold answer lists two specific citations. This discrepancy indicates a significant error in both identifying correct references and recognizing citations, leading to a failure in meeting the task requirements.\n\nRating: [[10]]",
        "score": 10
    },
    {
        "level": 3,
        "set": 2,
        "length": 85384,
        "type": "paper",
        "language": "en",
        "question": "TOFU: A Task of Fictitious Unlearning for LLMs",
        "instruction": "We hope you will carefully study the provided papers and determine the citation relationships between them. Please follow the instructions below strictly to complete the task:\n\n#Specific Requirements:\n1. Reference: When a given paper mentions other provided papers, those other papers are considered as \"references\" for the given paper. To summarize in this specific context, references are about what the given paper is using.\n2. Citation: Conversely, when other provided papers mention the given paper in their works, the given paper is being \"cited\" by those other papers. To summarize in this specific context, citations are about who is using the given paper.\n3. Given a paper, you need to determine the citation or reference relationship between this paper and the other papers. Do not consider papers that are not provided.\n3. Please present the paper titles in a json format as follows: {{\"Reference\":[\"Reference Title 1\", \"Reference Title 2\", ..., \"Reference Title n\"], \"Citation\":[\"Citation Title 1\", \"Citation Title 2\", ..., \"Citation Title n\"]}}.\n4. If a paper does not have any references or citations, please leave the corresponding list empty, e.g.{{\"Refernce\":[]}}, {{\"Citation\":[]}}.",
        "prompt_template": "#Papers:\n{docs}\n\n{instruction}\n\n#The paper you need to analyze:\n{question}",
        "doc": [
            "2401.06121.md",
            "2405.17438.md",
            "2405.19563.md",
            "2310.02238.md",
            "2310.07579.md"
        ],
        "answer": {
            "Reference": [
                "# Who's Harry Potter? Approximate Unlearning in LLMs ",
                "# IN-CONTEXT UNLEARNING: LANGUAGE MODELS AS FEW SHOT UNLEARNERS "
            ],
            "Citation": [
                "# An LLM-Tool Compiler for Fused Parallel Function Calling ",
                "# Unlearning Climate Misinformation in Large Language Models "
            ]
        },
        "shuffle_doc": true,
        "id": "07eca612-26e5-4505-9fdb-df120c2b8a6e",
        "prompt": "[Question]\n#Papers:\n{docs}\n\nWe hope you will carefully study the provided papers and determine the citation relationships between them. Please follow the instructions below strictly to complete the task:\n\n#Specific Requirements:\n1. Reference: When a given paper mentions other provided papers, those other papers are considered as \"references\" for the given paper. To summarize in this specific context, references are about what the given paper is using.\n2. Citation: Conversely, when other provided papers mention the given paper in their works, the given paper is being \"cited\" by those other papers. To summarize in this specific context, citations are about who is using the given paper.\n3. Given a paper, you need to determine the citation or reference relationship between this paper and the other papers. Do not consider papers that are not provided.\n3. Please present the paper titles in a json format as follows: {{\"Reference\":[\"Reference Title 1\", \"Reference Title 2\", ..., \"Reference Title n\"], \"Citation\":[\"Citation Title 1\", \"Citation Title 2\", ..., \"Citation Title n\"]}}.\n4. If a paper does not have any references or citations, please leave the corresponding list empty, e.g.{{\"Refernce\":[]}}, {{\"Citation\":[]}}.\n\n#The paper you need to analyze:\nTOFU: A Task of Fictitious Unlearning for LLMs\n\n[Gold Answer]\n{'Reference': [\"# Who's Harry Potter? Approximate Unlearning in LLMs \", '# IN-CONTEXT UNLEARNING: LANGUAGE MODELS AS FEW SHOT UNLEARNERS '], 'Citation': ['# An LLM-Tool Compiler for Fused Parallel Function Calling ', '# Unlearning Climate Misinformation in Large Language Models ']}\n\n[The Start of Assistant's Predicted Answer]\n{\"Reference\": [\"A Survey on Evaluation of Large Language Models\", \"Vicuna: An Open-Source Chatbot Impressing GPT-4 with $90 \\% *$ ChatGPT Quality\", \"ChatLaw: Open-Source Legal Large Language Model with Integrated External Knowledge Bases\", \"Compositional Semantic Parsing with Large Language Models\", \"Open LLM Leaderboard\", \"How Close is ChatGPT to Human Experts? Comparison Corpus, Evaluation, and Detection\", \"REALM: Retrieval-Augmented Language Model PreTraining\", \"Rethinking with Retrieval: Faithful Large Language Model Inference\", \"Measuring Massive Multitask Language Understanding\", \"Selecting and Executing Multiple Tools with a Single API Call\", \"Evaluating Correctness and Faithfulness of Instruction-Following Models for Question Answering\", \"Amdahl's law\", \"A Highly Capable Language Model Locally on Your Phone\", \"The most capable openly available LLM to date\", \"Hardware-Aware DNN Compression via Diverse Pruning and Mixed-Precision Quantization\", \"Neuralpower: Predict and deploy energy-efficient convolutional neural networks\", \"Adapting Language Models to Compress Contexts\", \"GeckOpt: LLM System Efficiency via Intent-Based Tool Selection\", \"GPTQ: Accurate Post-Training Quantization for Generative Pre-trained Transformers\", \"LLMLingua: Compressing Prompts for Accelerated Inference of Large Language Models\", \"Adaptive Skeleton Graph Decoding\", \"OmniBoost: Boosting Throughput of Heterogeneous Embedded Devices under Multi-DNN Workload\", \"SqueezeLLM: Dense-and-Sparse Quantization\", \"An LLM Compiler for Parallel Function Calling\", \"VisualWebArena: Evaluating Multimodal Agents on Realistic Visual Web Tasks\", \"A Fast Post-Training Pruning Framework for Transformers\", \"Chain-of-Thought Prompting Elicits Reasoning in Large Language Models\", \"Skeleton-of-Thought: Prompting LLMs for Efficient Parallel Generation\", \"A TensorRT Toolbox for Optimized Large Language Model Inference\", \"Function Calling\", \"Berkeley Function Calling Leaderboard\", \"MM-REACT: Prompting ChatGPT for Multimodal Reasoning and Action\", \"Tree of Thoughts: Deliberizing Problem Solving with Large Language Models\", \"ReAct: Synergizing Reasoning and Acting in Language Models\", \"LLM Inference Unveiled: Survey and Roofline Model Insights\", \"Efficiently Programming Large Language Models using SGLang\", \"Efficient Prompting via Dynamic In-Context Learning\", \"ToolQA: A Dataset for LLM Question Answering with External Tools\", \"A Survey on In-context Learning\", \"Retrieval Augmented Generation (RAG)\"], \"Citation\": [\"Machine unlearning\", \"Extracting training data from large language models\", \"Membership inference attacks from first principles\", \"Unlearn what you want to forget: Efficient unlearning for llms\", \"Who's harry potter? approximate unlearning in llms\", \"Towards adversarial evaluations for inexact machine unlearning\", \"Eternal sunshine of the spotless net: Selective forgetting in deep networks\", \"Certified data removal from machine learning models\", \"Separate the wheat from the chaff: Model deficiency unlearning via parameter-efficient module operation\", \"Are large pre-trained language models leaking your personal information?\", \"Auditing differentially private machine learning: How private is private sgd?\", \"Propile: Probing privacy leakage in large language models\", \"The brainy student: Scalable unlearning by selectively disobeying the teacher\", \"Towards unbounded machine unlearning\", \"Textbooks are all you need ii: phi-1.5 technical report\", \"Rouge: A package for automatic evaluation of summaries\", \"Continual learning and private unlearning\", \"Quark: Controllable text generation with reinforced unlearning\", \"Dataset inference: Ownership resolution in machine learning\", \"Catastrophic interference in connectionist networks: The sequential learning problem\", \"Locating and editing factual associations in gpt\", \"Adversary instantiation: Lower bounds for differentially private machine learning\", \"Ccpa regulations: Final regulation text\", \"Can sensitive information be deleted from llms? objectives for defending against extraction attacks\", \"In-context unlearning: Language models as few shot unlearners\", \"Direct preference optimization: Your language model is secretly a reward model\", \"Remember what you want to forget: Algorithms for machine unlearning\", \"Detecting pretraining data from large language models\", \"Membership inference attacks against machine learning models\", \"Privacy auditing with one (1\", \"On the necessity of auditable algorithmic definitions for machine unlearning\", \"Llama 2: Open foundation and fine-tuned chat models\", \"The eu general data protection regulation (gdpr)\", \"A Practical Guide\", \"Jailbroken: How does llm safety training fail?\", \"Large language model unlearning\", \"Right to be forgotten in the era of large language models: Implications, challenges, and solutions\", \"A comprehensive study of knowledge editing for large language models\", \"Universal and transferable adversarial attacks on aligned language models\"]}\n[The End of Assistant's Predicted Answer]\n\n[System]\nWe would like to request your feedback on the performance of the AI assistant in response to the user question displayed above according to the gold answer. Please use the following listed aspects and their descriptions as evaluation criteria:\n    - Accuracy and Hallucinations: The assistant's answer is semantically consistent with the gold answer; The numerical value and order need to be accurate, and there should be no hallucinations.\n    - Completeness: Referring to the reference answers, the assistant's answer should contain all the key points needed to answer the user's question; further elaboration on these key points can be omitted.\nPlease rate whether this answer is suitable for the question. Please note that the gold answer can be considered as a correct answer to the question.\n\nThe assistant receives an overall score on a scale of 1 to 100, where a higher score indicates better overall performance.\nPlease note that if the assistant's answer and the gold answer fully meet the above criteria, its overall rating should be the full marks (100).\nPlease first provide a comprehensive explanation of your evaluation, avoiding any potential bias.\nThen, output a line indicating the score of the Assistant.\n\nPLEASE OUTPUT WITH THE FOLLOWING FORMAT, WHERE THE SCORE IS A SCALE OF 1 TO 100 BY STRICTLY FOLLOWING THIS FORMAT: \"[[score]]\", FOR EXAMPLE \"Rating: [[100]]\":\n<start output>\nEvaluation evidence: your evluation explanation here, no more than 100 words\nRating: [[score]]\n<end output>\n\nNow, start your evaluation:",
        "generate_response": "{\"Reference\": [\"A Survey on Evaluation of Large Language Models\", \"Vicuna: An Open-Source Chatbot Impressing GPT-4 with $90 \\% *$ ChatGPT Quality\", \"ChatLaw: Open-Source Legal Large Language Model with Integrated External Knowledge Bases\", \"Compositional Semantic Parsing with Large Language Models\", \"Open LLM Leaderboard\", \"How Close is ChatGPT to Human Experts? Comparison Corpus, Evaluation, and Detection\", \"REALM: Retrieval-Augmented Language Model PreTraining\", \"Rethinking with Retrieval: Faithful Large Language Model Inference\", \"Measuring Massive Multitask Language Understanding\", \"Selecting and Executing Multiple Tools with a Single API Call\", \"Evaluating Correctness and Faithfulness of Instruction-Following Models for Question Answering\", \"Amdahl's law\", \"A Highly Capable Language Model Locally on Your Phone\", \"The most capable openly available LLM to date\", \"Hardware-Aware DNN Compression via Diverse Pruning and Mixed-Precision Quantization\", \"Neuralpower: Predict and deploy energy-efficient convolutional neural networks\", \"Adapting Language Models to Compress Contexts\", \"GeckOpt: LLM System Efficiency via Intent-Based Tool Selection\", \"GPTQ: Accurate Post-Training Quantization for Generative Pre-trained Transformers\", \"LLMLingua: Compressing Prompts for Accelerated Inference of Large Language Models\", \"Adaptive Skeleton Graph Decoding\", \"OmniBoost: Boosting Throughput of Heterogeneous Embedded Devices under Multi-DNN Workload\", \"SqueezeLLM: Dense-and-Sparse Quantization\", \"An LLM Compiler for Parallel Function Calling\", \"VisualWebArena: Evaluating Multimodal Agents on Realistic Visual Web Tasks\", \"A Fast Post-Training Pruning Framework for Transformers\", \"Chain-of-Thought Prompting Elicits Reasoning in Large Language Models\", \"Skeleton-of-Thought: Prompting LLMs for Efficient Parallel Generation\", \"A TensorRT Toolbox for Optimized Large Language Model Inference\", \"Function Calling\", \"Berkeley Function Calling Leaderboard\", \"MM-REACT: Prompting ChatGPT for Multimodal Reasoning and Action\", \"Tree of Thoughts: Deliberizing Problem Solving with Large Language Models\", \"ReAct: Synergizing Reasoning and Acting in Language Models\", \"LLM Inference Unveiled: Survey and Roofline Model Insights\", \"Efficiently Programming Large Language Models using SGLang\", \"Efficient Prompting via Dynamic In-Context Learning\", \"ToolQA: A Dataset for LLM Question Answering with External Tools\", \"A Survey on In-context Learning\", \"Retrieval Augmented Generation (RAG)\"], \"Citation\": [\"Machine unlearning\", \"Extracting training data from large language models\", \"Membership inference attacks from first principles\", \"Unlearn what you want to forget: Efficient unlearning for llms\", \"Who's harry potter? approximate unlearning in llms\", \"Towards adversarial evaluations for inexact machine unlearning\", \"Eternal sunshine of the spotless net: Selective forgetting in deep networks\", \"Certified data removal from machine learning models\", \"Separate the wheat from the chaff: Model deficiency unlearning via parameter-efficient module operation\", \"Are large pre-trained language models leaking your personal information?\", \"Auditing differentially private machine learning: How private is private sgd?\", \"Propile: Probing privacy leakage in large language models\", \"The brainy student: Scalable unlearning by selectively disobeying the teacher\", \"Towards unbounded machine unlearning\", \"Textbooks are all you need ii: phi-1.5 technical report\", \"Rouge: A package for automatic evaluation of summaries\", \"Continual learning and private unlearning\", \"Quark: Controllable text generation with reinforced unlearning\", \"Dataset inference: Ownership resolution in machine learning\", \"Catastrophic interference in connectionist networks: The sequential learning problem\", \"Locating and editing factual associations in gpt\", \"Adversary instantiation: Lower bounds for differentially private machine learning\", \"Ccpa regulations: Final regulation text\", \"Can sensitive information be deleted from llms? objectives for defending against extraction attacks\", \"In-context unlearning: Language models as few shot unlearners\", \"Direct preference optimization: Your language model is secretly a reward model\", \"Remember what you want to forget: Algorithms for machine unlearning\", \"Detecting pretraining data from large language models\", \"Membership inference attacks against machine learning models\", \"Privacy auditing with one (1\", \"On the necessity of auditable algorithmic definitions for machine unlearning\", \"Llama 2: Open foundation and fine-tuned chat models\", \"The eu general data protection regulation (gdpr)\", \"A Practical Guide\", \"Jailbroken: How does llm safety training fail?\", \"Large language model unlearning\", \"Right to be forgotten in the era of large language models: Implications, challenges, and solutions\", \"A comprehensive study of knowledge editing for large language models\", \"Universal and transferable adversarial attacks on aligned language models\"]}",
        "eval_response": "Evaluation evidence: The assistant's response does not align with the gold answer, indicating a significant error in identifying the correct references and citations for the paper \"TOFU: A Task of Fictitious Unlearning for LLMs.\" The references and citations listed in the assistant's response do not match those in the gold answer, which specifically lists two references and two citations. The assistant's response includes a large number of unrelated papers, suggesting hallucination and a lack of accuracy in the task of determining citation relationships.\n\nRating: [[10]]",
        "score": 10
    },
    {
        "level": 3,
        "set": 2,
        "length": 81880,
        "type": "paper",
        "language": "en",
        "question": "Scalable Pre-training of Large Autoregressive Image Models",
        "instruction": "We hope you will carefully study the provided papers and determine the citation relationships between them. Please follow the instructions below strictly to complete the task:\n\n#Specific Requirements:\n1. Reference: When a given paper mentions other provided papers, those other papers are considered as \"references\" for the given paper. To summarize in this specific context, references are about what the given paper is using.\n2. Citation: Conversely, when other provided papers mention the given paper in their works, the given paper is being \"cited\" by those other papers. To summarize in this specific context, citations are about who is using the given paper.\n3. Given a paper, you need to determine the citation or reference relationship between this paper and the other papers. Do not consider papers that are not provided.\n3. Please present the paper titles in a json format as follows: {{\"Reference\":[\"Reference Title 1\", \"Reference Title 2\", ..., \"Reference Title n\"], \"Citation\":[\"Citation Title 1\", \"Citation Title 2\", ..., \"Citation Title n\"]}}.\n4. If a paper does not have any references or citations, please leave the corresponding list empty, e.g.{{\"Refernce\":[]}}, {{\"Citation\":[]}}.",
        "prompt_template": "#Papers:\n{docs}\n\n{instruction}\n\n#The paper you need to analyze:\n{question}",
        "doc": [
            "2401.08541.md",
            "2405.14930.md",
            "2405.15160.md",
            "2304.03283.md",
            "2309.17425.md"
        ],
        "answer": {
            "Reference": [
                "# Diffusion Models as Masked Autoencoders ",
                "# Data Filtering Networks "
            ],
            "Citation": [
                "# AstroPT: Scaling Large Observation Models for Astronomy ",
                "# ARVideo: Autoregressive Pretraining for Self-Supervised Video Representation Learning "
            ]
        },
        "shuffle_doc": true,
        "id": "a21c83a7-706f-45ca-8b19-7a35757ab659",
        "prompt": "[Question]\n#Papers:\n{docs}\n\nWe hope you will carefully study the provided papers and determine the citation relationships between them. Please follow the instructions below strictly to complete the task:\n\n#Specific Requirements:\n1. Reference: When a given paper mentions other provided papers, those other papers are considered as \"references\" for the given paper. To summarize in this specific context, references are about what the given paper is using.\n2. Citation: Conversely, when other provided papers mention the given paper in their works, the given paper is being \"cited\" by those other papers. To summarize in this specific context, citations are about who is using the given paper.\n3. Given a paper, you need to determine the citation or reference relationship between this paper and the other papers. Do not consider papers that are not provided.\n3. Please present the paper titles in a json format as follows: {{\"Reference\":[\"Reference Title 1\", \"Reference Title 2\", ..., \"Reference Title n\"], \"Citation\":[\"Citation Title 1\", \"Citation Title 2\", ..., \"Citation Title n\"]}}.\n4. If a paper does not have any references or citations, please leave the corresponding list empty, e.g.{{\"Refernce\":[]}}, {{\"Citation\":[]}}.\n\n#The paper you need to analyze:\nScalable Pre-training of Large Autoregressive Image Models\n\n[Gold Answer]\n{'Reference': ['# Diffusion Models as Masked Autoencoders ', '# Data Filtering Networks '], 'Citation': ['# AstroPT: Scaling Large Observation Models for Astronomy ', '# ARVideo: Autoregressive Pretraining for Self-Supervised Video Representation Learning ']}\n\n[The Start of Assistant's Predicted Answer]\n{\"Reference\": [\"V-JEPA: Latent video prediction for visual representation learning\", \"Self-supervised learning from images with a joint-embedding predictive architecture\", \"Sequential modeling enables scalable learning for large vision models\", \"From detection of individual metastases to classification of lymph node status at the patient level: the camelyon17 challenge\", \"BEiT: Bert pretraining of image transformers\", \"Vicreg: Variance-invariance-covariance regularization for selfsupervised learning\", \"Cliquecnn: Deep unsupervised exemplar learning\", \"The iwildcam 2020 competition dataset\", \"A neural probabilistic language model\", \"Unsupervised learning by predicting noise\", \"Unsupervised pre-training of image features on non-curated data\", \"Self-supervised learning of visual features by contrasting cluster assignments\", \"Emerging properties in self-supervised vision transformers\", \"Generative pretraining from pixels\", \"A simple framework for contrastive learning of visual representations\", \"An empirical study of training self-supervised vision transformers\", \"Discriminative unsupervised feature learning with convolutional neural networks\", \"An image is worth 16x16 words: Transformers for image recognition at scale\", \"Are large-scale datasets necessary for self-supervised pre-training?\", \"Finding structure in time\", \"Data filtering networks\", \"Datacomp: In search of the next generation of multimodal datasets\", \"Unsupervised representation learning by predicting image rotations\", \"Generative adversarial nets\", \"Scaling and benchmarking self-supervised visual representation learning\", \"Vision models are more robust and fair when pretrained on uncurated images without supervision\", \"Bootstrap your own latent-a new approach to self-supervised learning\", \"Masked autoencoders are scalable vision learners\", \"Training compute-optimal large language models\", \"Lora: Low-rank adaptation of large language models\", \"Deep networks with stochastic depth\", \"Recurrent neural network based language model\", \"Self-supervised learning of pretext-invariant representations\", \"Unsupervised learning of visual representations by solving jigsaw puzzles\", \"Wavenet: A generative model for raw audio\", \"Representation learning with contrastive predictive coding\", \"Dinov2: Learning robust visual features without supervision\", \"Training language models to follow instructions with human feedback\", \"Exploring the limits of transfer learning with a unified text-to-text transformer\", \"Zero-shot text-to-image generation\", \"Pixelcnn++: Improving the pixelcnn with discretized logistic mixture likelihood and other modifications\", \"Prediction and entropy of printed english\", \"The effectiveness of mae pre-pretraining for billionscale pretraining\", \"Divide and contrast: Self-supervised learning from uncurated data\", \"Training data-efficient image transformers & distillation through attention\", \"Going deeper with image transformers\", \"Llama: Open and efficient foundation language models\", \"Conditional image generation with pixelcnn decoders\", \"Pixel recurrent neural networks\", \"Neurips\", \"Attention is all you need\", \"Rotation equivariant cnns for digital pathology\", \"Stacked denoising autoencoders: Learning useful representations in a deep network with a local denoising criterion\", \"Diffusion models as masked autoencoders\", \"ClusterFit: Improving Generalization of Visual Representations\", \"Coca: Contrastive captioners are image-text foundation models\", \"Barlow twins: Self-supervised learning via redundancy reduction\", \"Mixup: Beyond empirical risk minimization\", \"Colorful image colorization\"], \"Citation\": [\"AstroPT: Scaling Large Observation Models for Astronomy\"]}\n[The End of Assistant's Predicted Answer]\n\n[System]\nWe would like to request your feedback on the performance of the AI assistant in response to the user question displayed above according to the gold answer. Please use the following listed aspects and their descriptions as evaluation criteria:\n    - Accuracy and Hallucinations: The assistant's answer is semantically consistent with the gold answer; The numerical value and order need to be accurate, and there should be no hallucinations.\n    - Completeness: Referring to the reference answers, the assistant's answer should contain all the key points needed to answer the user's question; further elaboration on these key points can be omitted.\nPlease rate whether this answer is suitable for the question. Please note that the gold answer can be considered as a correct answer to the question.\n\nThe assistant receives an overall score on a scale of 1 to 100, where a higher score indicates better overall performance.\nPlease note that if the assistant's answer and the gold answer fully meet the above criteria, its overall rating should be the full marks (100).\nPlease first provide a comprehensive explanation of your evaluation, avoiding any potential bias.\nThen, output a line indicating the score of the Assistant.\n\nPLEASE OUTPUT WITH THE FOLLOWING FORMAT, WHERE THE SCORE IS A SCALE OF 1 TO 100 BY STRICTLY FOLLOWING THIS FORMAT: \"[[score]]\", FOR EXAMPLE \"Rating: [[100]]\":\n<start output>\nEvaluation evidence: your evluation explanation here, no more than 100 words\nRating: [[score]]\n<end output>\n\nNow, start your evaluation:",
        "generate_response": "{\"Reference\": [\"V-JEPA: Latent video prediction for visual representation learning\", \"Self-supervised learning from images with a joint-embedding predictive architecture\", \"Sequential modeling enables scalable learning for large vision models\", \"From detection of individual metastases to classification of lymph node status at the patient level: the camelyon17 challenge\", \"BEiT: Bert pretraining of image transformers\", \"Vicreg: Variance-invariance-covariance regularization for selfsupervised learning\", \"Cliquecnn: Deep unsupervised exemplar learning\", \"The iwildcam 2020 competition dataset\", \"A neural probabilistic language model\", \"Unsupervised learning by predicting noise\", \"Unsupervised pre-training of image features on non-curated data\", \"Self-supervised learning of visual features by contrasting cluster assignments\", \"Emerging properties in self-supervised vision transformers\", \"Generative pretraining from pixels\", \"A simple framework for contrastive learning of visual representations\", \"An empirical study of training self-supervised vision transformers\", \"Discriminative unsupervised feature learning with convolutional neural networks\", \"An image is worth 16x16 words: Transformers for image recognition at scale\", \"Are large-scale datasets necessary for self-supervised pre-training?\", \"Finding structure in time\", \"Data filtering networks\", \"Datacomp: In search of the next generation of multimodal datasets\", \"Unsupervised representation learning by predicting image rotations\", \"Generative adversarial nets\", \"Scaling and benchmarking self-supervised visual representation learning\", \"Vision models are more robust and fair when pretrained on uncurated images without supervision\", \"Bootstrap your own latent-a new approach to self-supervised learning\", \"Masked autoencoders are scalable vision learners\", \"Training compute-optimal large language models\", \"Lora: Low-rank adaptation of large language models\", \"Deep networks with stochastic depth\", \"Recurrent neural network based language model\", \"Self-supervised learning of pretext-invariant representations\", \"Unsupervised learning of visual representations by solving jigsaw puzzles\", \"Wavenet: A generative model for raw audio\", \"Representation learning with contrastive predictive coding\", \"Dinov2: Learning robust visual features without supervision\", \"Training language models to follow instructions with human feedback\", \"Exploring the limits of transfer learning with a unified text-to-text transformer\", \"Zero-shot text-to-image generation\", \"Pixelcnn++: Improving the pixelcnn with discretized logistic mixture likelihood and other modifications\", \"Prediction and entropy of printed english\", \"The effectiveness of mae pre-pretraining for billionscale pretraining\", \"Divide and contrast: Self-supervised learning from uncurated data\", \"Training data-efficient image transformers & distillation through attention\", \"Going deeper with image transformers\", \"Llama: Open and efficient foundation language models\", \"Conditional image generation with pixelcnn decoders\", \"Pixel recurrent neural networks\", \"Neurips\", \"Attention is all you need\", \"Rotation equivariant cnns for digital pathology\", \"Stacked denoising autoencoders: Learning useful representations in a deep network with a local denoising criterion\", \"Diffusion models as masked autoencoders\", \"ClusterFit: Improving Generalization of Visual Representations\", \"Coca: Contrastive captioners are image-text foundation models\", \"Barlow twins: Self-supervised learning via redundancy reduction\", \"Mixup: Beyond empirical risk minimization\", \"Colorful image colorization\"], \"Citation\": [\"AstroPT: Scaling Large Observation Models for Astronomy\"]}",
        "eval_response": "Evaluation evidence: The assistant's answer includes a large number of references that are not mentioned in the gold answer, indicating a significant hallucination issue. The correct references from the gold answer, \"Diffusion Models as Masked Autoencoders\" and \"Data Filtering Networks,\" are missing in the assistant's response. Additionally, only one correct citation, \"AstroPT: Scaling Large Observation Models for Astronomy,\" is included, while the other correct citation, \"ARVideo: Autoregressive Pretraining for Self-Supervised Video Representation Learning,\" is omitted. This results in both inaccuracies and incompleteness.\n\nRating: [[20]]",
        "score": 20
    },
    {
        "level": 3,
        "set": 2,
        "length": 88101,
        "type": "paper",
        "language": "en",
        "question": "Aligner: Achieving Efficient Alignment through Weak-to-Strong Correction",
        "instruction": "We hope you will carefully study the provided papers and determine the citation relationships between them. Please follow the instructions below strictly to complete the task:\n\n#Specific Requirements:\n1. Reference: When a given paper mentions other provided papers, those other papers are considered as \"references\" for the given paper. To summarize in this specific context, references are about what the given paper is using.\n2. Citation: Conversely, when other provided papers mention the given paper in their works, the given paper is being \"cited\" by those other papers. To summarize in this specific context, citations are about who is using the given paper.\n3. Given a paper, you need to determine the citation or reference relationship between this paper and the other papers. Do not consider papers that are not provided.\n3. Please present the paper titles in a json format as follows: {{\"Reference\":[\"Reference Title 1\", \"Reference Title 2\", ..., \"Reference Title n\"], \"Citation\":[\"Citation Title 1\", \"Citation Title 2\", ..., \"Citation Title n\"]}}.\n4. If a paper does not have any references or citations, please leave the corresponding list empty, e.g.{{\"Refernce\":[]}}, {{\"Citation\":[]}}.",
        "prompt_template": "#Papers:\n{docs}\n\n{instruction}\n\n#The paper you need to analyze:\n{question}",
        "doc": [
            "2402.02416.md",
            "2405.15116.md",
            "2405.18718.md",
            "2405.21040.md"
        ],
        "answer": {
            "Reference": [],
            "Citation": [
                "# Quantifying the Gain in Weak-to-Strong Generalization ",
                "# Efficient Model-agnostic Alignment via Bayesian Persuasion ",
                "# Direct Alignment of Language Models via Quality-Aware Self-Refinement "
            ]
        },
        "shuffle_doc": true,
        "id": "aea761bc-1a20-402a-9b4d-bd4118947fd5",
        "prompt": "[Question]\n#Papers:\n{docs}\n\nWe hope you will carefully study the provided papers and determine the citation relationships between them. Please follow the instructions below strictly to complete the task:\n\n#Specific Requirements:\n1. Reference: When a given paper mentions other provided papers, those other papers are considered as \"references\" for the given paper. To summarize in this specific context, references are about what the given paper is using.\n2. Citation: Conversely, when other provided papers mention the given paper in their works, the given paper is being \"cited\" by those other papers. To summarize in this specific context, citations are about who is using the given paper.\n3. Given a paper, you need to determine the citation or reference relationship between this paper and the other papers. Do not consider papers that are not provided.\n3. Please present the paper titles in a json format as follows: {{\"Reference\":[\"Reference Title 1\", \"Reference Title 2\", ..., \"Reference Title n\"], \"Citation\":[\"Citation Title 1\", \"Citation Title 2\", ..., \"Citation Title n\"]}}.\n4. If a paper does not have any references or citations, please leave the corresponding list empty, e.g.{{\"Refernce\":[]}}, {{\"Citation\":[]}}.\n\n#The paper you need to analyze:\nAligner: Achieving Efficient Alignment through Weak-to-Strong Correction\n\n[Gold Answer]\n{'Reference': [], 'Citation': ['# Quantifying the Gain in Weak-to-Strong Generalization ', '# Efficient Model-agnostic Alignment via Bayesian Persuasion ', '# Direct Alignment of Language Models via Quality-Aware Self-Refinement ']}\n\n[The Start of Assistant's Predicted Answer]\n{\"Reference\":[\"GPT-4 technical report\", \"Concrete problems in ai safety and governance\", \"Vicuna: An open-source chatbot impressing gpt-4 with $90 \\% *$ chatgpt quality\", \"Reinforced self-training (rest) for language modeling\", \"Deep residual learning for image recognition\", \"Efficient memory management for large language model serving with pagedattention\", \"Rlaif: Scaling reinforcement learning from human feedback with ai feedback\", \"Multi-step jailbreaking privacy attacks on chatGPT\", \"Self-refine: Iterative refinement with self-feedback\", \"A self-refinement strategy for noise reduction in grammatical error correction\", \"Scalable extraction of training data from (production) language models\", \"The alignment problem from a deep learning perspective\", \"Introducing Superalignment\", \"Training language models to follow instructions with human feedback\", \"Direct preference optimization: Your language model is secretly a reward model\", \"Towards empathetic open-domain conversation models: A new benchmark and dataset\", \"Learning to model editing processes\", \"Self-critiquing models for assisting human evaluators\", \"Proximal policy optimization algorithms\", \"Reflexion: Language agents with verbal reinforcement learning\", \"A minimaximalist approach to reinforcement learning from human feedback\", \"Stanford alpaca: An instruction-following llama model\", \"Gemini: a family of highly capable multimodal models\", \"Llama 2: Open foundation and finetuned chat models\", \"Small language models improve giants by rewriting their outputs\", \"Emergent abilities of large language models\", \"Finegrained human feedback gives better rewards for language model training\", \"Baichuan 2: Open large-scale language models\", \"Shadow alignment: The ease of subverting safely-aligned language models\", \"Idea2img: Iterative self-refinement with gpt-4v (ision) for automatic image design and generation\", \"Deepspeed-chat: Easy, fast and affordable rlhf training of chatgpt-like models at all scales\", \"RRHF: Rank responses to align language models with human feedback\", \"A constrained sequence-to-sequence neural model for sentence simplification\", \"A survey of large language models\", \"Fine-tuning language models from human preferences\"], \"Citation\":[]}\n[The End of Assistant's Predicted Answer]\n\n[System]\nWe would like to request your feedback on the performance of the AI assistant in response to the user question displayed above according to the gold answer. Please use the following listed aspects and their descriptions as evaluation criteria:\n    - Accuracy and Hallucinations: The assistant's answer is semantically consistent with the gold answer; The numerical value and order need to be accurate, and there should be no hallucinations.\n    - Completeness: Referring to the reference answers, the assistant's answer should contain all the key points needed to answer the user's question; further elaboration on these key points can be omitted.\nPlease rate whether this answer is suitable for the question. Please note that the gold answer can be considered as a correct answer to the question.\n\nThe assistant receives an overall score on a scale of 1 to 100, where a higher score indicates better overall performance.\nPlease note that if the assistant's answer and the gold answer fully meet the above criteria, its overall rating should be the full marks (100).\nPlease first provide a comprehensive explanation of your evaluation, avoiding any potential bias.\nThen, output a line indicating the score of the Assistant.\n\nPLEASE OUTPUT WITH THE FOLLOWING FORMAT, WHERE THE SCORE IS A SCALE OF 1 TO 100 BY STRICTLY FOLLOWING THIS FORMAT: \"[[score]]\", FOR EXAMPLE \"Rating: [[100]]\":\n<start output>\nEvaluation evidence: your evluation explanation here, no more than 100 words\nRating: [[score]]\n<end output>\n\nNow, start your evaluation:",
        "generate_response": "{\"Reference\":[\"GPT-4 technical report\", \"Concrete problems in ai safety and governance\", \"Vicuna: An open-source chatbot impressing gpt-4 with $90 \\% *$ chatgpt quality\", \"Reinforced self-training (rest) for language modeling\", \"Deep residual learning for image recognition\", \"Efficient memory management for large language model serving with pagedattention\", \"Rlaif: Scaling reinforcement learning from human feedback with ai feedback\", \"Multi-step jailbreaking privacy attacks on chatGPT\", \"Self-refine: Iterative refinement with self-feedback\", \"A self-refinement strategy for noise reduction in grammatical error correction\", \"Scalable extraction of training data from (production) language models\", \"The alignment problem from a deep learning perspective\", \"Introducing Superalignment\", \"Training language models to follow instructions with human feedback\", \"Direct preference optimization: Your language model is secretly a reward model\", \"Towards empathetic open-domain conversation models: A new benchmark and dataset\", \"Learning to model editing processes\", \"Self-critiquing models for assisting human evaluators\", \"Proximal policy optimization algorithms\", \"Reflexion: Language agents with verbal reinforcement learning\", \"A minimaximalist approach to reinforcement learning from human feedback\", \"Stanford alpaca: An instruction-following llama model\", \"Gemini: a family of highly capable multimodal models\", \"Llama 2: Open foundation and finetuned chat models\", \"Small language models improve giants by rewriting their outputs\", \"Emergent abilities of large language models\", \"Finegrained human feedback gives better rewards for language model training\", \"Baichuan 2: Open large-scale language models\", \"Shadow alignment: The ease of subverting safely-aligned language models\", \"Idea2img: Iterative self-refinement with gpt-4v (ision) for automatic image design and generation\", \"Deepspeed-chat: Easy, fast and affordable rlhf training of chatgpt-like models at all scales\", \"RRHF: Rank responses to align language models with human feedback\", \"A constrained sequence-to-sequence neural model for sentence simplification\", \"A survey of large language models\", \"Fine-tuning language models from human preferences\"], \"Citation\":[]}",
        "eval_response": "Evaluation evidence: The assistant's answer does not align with the gold answer. The assistant listed numerous references that are not mentioned in the gold answer, indicating a significant hallucination issue. Additionally, the assistant failed to list any citations, which contradicts the gold answer that specifies three papers citing the analyzed paper. This shows inaccuracies in both the references and citations, leading to a failure in meeting the accuracy and completeness criteria.\n\nRating: [[10]]",
        "score": 10
    },
    {
        "level": 3,
        "set": 2,
        "length": 88057,
        "type": "paper",
        "language": "en",
        "question": "Small LLMs Are Weak Tool Learners: A Multi-LLM Agent",
        "instruction": "We hope you will carefully study the provided papers and determine the citation relationships between them. Please follow the instructions below strictly to complete the task:\n\n#Specific Requirements:\n1. Reference: When a given paper mentions other provided papers, those other papers are considered as \"references\" for the given paper. To summarize in this specific context, references are about what the given paper is using.\n2. Citation: Conversely, when other provided papers mention the given paper in their works, the given paper is being \"cited\" by those other papers. To summarize in this specific context, citations are about who is using the given paper.\n3. Given a paper, you need to determine the citation or reference relationship between this paper and the other papers. Do not consider papers that are not provided.\n3. Please present the paper titles in a json format as follows: {{\"Reference\":[\"Reference Title 1\", \"Reference Title 2\", ..., \"Reference Title n\"], \"Citation\":[\"Citation Title 1\", \"Citation Title 2\", ..., \"Citation Title n\"]}}.\n4. If a paper does not have any references or citations, please leave the corresponding list empty, e.g.{{\"Refernce\":[]}}, {{\"Citation\":[]}}.",
        "prompt_template": "#Papers:\n{docs}\n\n{instruction}\n\n#The paper you need to analyze:\n{question}",
        "doc": [
            "2401.07324.md",
            "2405.13050.md",
            "2405.15307.md",
            "2308.00352.md",
            "2310.05492.md"
        ],
        "answer": {
            "Reference": [
                "# METAGPT: META PROGRAMMING FOR A MulTi-AGENT COLLABORATIVE FRAMEWORK ",
                "# How Abilities in Large Language Models are Affected by Supervised Fine-tuning Data Composition "
            ],
            "Citation": [
                "# Human-Centered LLM-Agent User Interface: A Position Paper ",
                "# Before Generation, Align it! A Novel and Effective Strategy for Mitigating Hallucinations in Text-to-SQL Generation "
            ]
        },
        "shuffle_doc": true,
        "id": "b024bd9a-b0ac-4e30-bd4e-a2abc6c541d8",
        "prompt": "[Question]\n#Papers:\n{docs}\n\nWe hope you will carefully study the provided papers and determine the citation relationships between them. Please follow the instructions below strictly to complete the task:\n\n#Specific Requirements:\n1. Reference: When a given paper mentions other provided papers, those other papers are considered as \"references\" for the given paper. To summarize in this specific context, references are about what the given paper is using.\n2. Citation: Conversely, when other provided papers mention the given paper in their works, the given paper is being \"cited\" by those other papers. To summarize in this specific context, citations are about who is using the given paper.\n3. Given a paper, you need to determine the citation or reference relationship between this paper and the other papers. Do not consider papers that are not provided.\n3. Please present the paper titles in a json format as follows: {{\"Reference\":[\"Reference Title 1\", \"Reference Title 2\", ..., \"Reference Title n\"], \"Citation\":[\"Citation Title 1\", \"Citation Title 2\", ..., \"Citation Title n\"]}}.\n4. If a paper does not have any references or citations, please leave the corresponding list empty, e.g.{{\"Refernce\":[]}}, {{\"Citation\":[]}}.\n\n#The paper you need to analyze:\nSmall LLMs Are Weak Tool Learners: A Multi-LLM Agent\n\n[Gold Answer]\n{'Reference': ['# METAGPT: META PROGRAMMING FOR A MulTi-AGENT COLLABORATIVE FRAMEWORK ', '# How Abilities in Large Language Models are Affected by Supervised Fine-tuning Data Composition '], 'Citation': ['# Human-Centered LLM-Agent User Interface: A Position Paper ', '# Before Generation, Align it! A Novel and Effective Strategy for Mitigating Hallucinations in Text-to-SQL Generation ']}\n\n[The Start of Assistant's Predicted Answer]\n{\"Reference\": [\"A Survey on Evaluation of Large Language Models\", \"Vicuna: An Open-Source Chatbot Impressing GPT-4 with $90 \\% *$ ChatGPT Quality\", \"ChatLaw: Open-Source Legal Large Language Model with Integrated External Knowledge Bases\", \"Compositional Semantic Parsing with Large Language Models\", \"Open LLM Leaderboard\", \"How Close is ChatGPT to Human Experts? Comparison Corpus, Evaluation, and Detection\", \"REALM: Retrieval-Augmented Language Model PreTraining\", \"Rethinking with Retrieval: Faithful Large Language Model Inference\", \"Measuring Massive Multitask Language Understanding\", \"Atlas: Few-shot Learning with Retrieval Augmented Language Models\", \"Benchmarking Large Language Models in Retrieval-Augmented Generation\", \"FiREACT: Toward Language Agent Fine-Tuning\", \"Scaling Instruction-Finetuned Language Models\", \"Tora: A Tool-Integrated Reasoning Agent for Mathematical Problem Solving\", \"Autogpt: The Heart of the Open-Source Agent Ecosystem\", \"Measuring Mathematical Problem Solving with the Math Dataset\", \"MetaGPT: Meta Programming for Multi-Agent Collaborative Framework\", \"Modelscope-Agent: Building Your Customizable Agent System with Open-Source Large Language Models\", \"BabyAGI\", \"WebGPT: Browser-Assisted Question-Answering with Human Feedback\", \"Chain-of-Thought Prompting Elicits Reasoning in Large Language Models\", \"Self-Consistency Improves Chain of Thought Reasoning in Language Models\", \"Reflextion: Language Agents with Verbal Reinforcement Learning\", \"ToolAlpaca: Generalized Tool Learning for Language Models with 3000 Simulated Cases\", \"Llama: Open and Efficient Foundation Language Models\", \"Llama 2: Open Foundation and Fine-Tuned Chat Models\", \"Voyager: An Open-Ended Embodied Agent with Large Language Models\", \"Multi-Party Chat: Conversational Agents in Group Settings with Humans and Models\", \"Visual ChatGPT: Talking, Drawing and Editing with Visual Foundation Models\", \"AutoGen: Enabling Next-Gen LLM Applications via Multi-Agent Conversation Framework\", \"ChatGPT is not Enough: Enhancing Large Language Models with Knowledge Graphs for Fact-Aware Language Modeling\", \"GPT4Tools: Teaching Large Language Model to Use Tools via Self-Instruction\", \"MMReAct: Prompting ChatGPT for Multimodal Reasoning and Action\", \"Tree of Thoughts: Deliberate Problem Solving with Large Language Models\", \"REACT: Synergizing Reasoning and Acting in Language Models\", \"Tool Learning with Foundation Models\", \"AgentTuning: Enabling Generalized Agent Abilities for LLMs\", \"MemoryBank: Enhancing Large Language Models with Long-Term Memory\", \"Ghost in the Minecraft: Generally Capable Agents for Open-World Environments via Large Language Models with Text-Based Knowledge and Memory\"], \"Citation\": []}\n[The End of Assistant's Predicted Answer]\n\n[System]\nWe would like to request your feedback on the performance of the AI assistant in response to the user question displayed above according to the gold answer. Please use the following listed aspects and their descriptions as evaluation criteria:\n    - Accuracy and Hallucinations: The assistant's answer is semantically consistent with the gold answer; The numerical value and order need to be accurate, and there should be no hallucinations.\n    - Completeness: Referring to the reference answers, the assistant's answer should contain all the key points needed to answer the user's question; further elaboration on these key points can be omitted.\nPlease rate whether this answer is suitable for the question. Please note that the gold answer can be considered as a correct answer to the question.\n\nThe assistant receives an overall score on a scale of 1 to 100, where a higher score indicates better overall performance.\nPlease note that if the assistant's answer and the gold answer fully meet the above criteria, its overall rating should be the full marks (100).\nPlease first provide a comprehensive explanation of your evaluation, avoiding any potential bias.\nThen, output a line indicating the score of the Assistant.\n\nPLEASE OUTPUT WITH THE FOLLOWING FORMAT, WHERE THE SCORE IS A SCALE OF 1 TO 100 BY STRICTLY FOLLOWING THIS FORMAT: \"[[score]]\", FOR EXAMPLE \"Rating: [[100]]\":\n<start output>\nEvaluation evidence: your evluation explanation here, no more than 100 words\nRating: [[score]]\n<end output>\n\nNow, start your evaluation:",
        "generate_response": "{\"Reference\": [\"A Survey on Evaluation of Large Language Models\", \"Vicuna: An Open-Source Chatbot Impressing GPT-4 with $90 \\% *$ ChatGPT Quality\", \"ChatLaw: Open-Source Legal Large Language Model with Integrated External Knowledge Bases\", \"Compositional Semantic Parsing with Large Language Models\", \"Open LLM Leaderboard\", \"How Close is ChatGPT to Human Experts? Comparison Corpus, Evaluation, and Detection\", \"REALM: Retrieval-Augmented Language Model PreTraining\", \"Rethinking with Retrieval: Faithful Large Language Model Inference\", \"Measuring Massive Multitask Language Understanding\", \"Atlas: Few-shot Learning with Retrieval Augmented Language Models\", \"Benchmarking Large Language Models in Retrieval-Augmented Generation\", \"FiREACT: Toward Language Agent Fine-Tuning\", \"Scaling Instruction-Finetuned Language Models\", \"Tora: A Tool-Integrated Reasoning Agent for Mathematical Problem Solving\", \"Autogpt: The Heart of the Open-Source Agent Ecosystem\", \"Measuring Mathematical Problem Solving with the Math Dataset\", \"MetaGPT: Meta Programming for Multi-Agent Collaborative Framework\", \"Modelscope-Agent: Building Your Customizable Agent System with Open-Source Large Language Models\", \"BabyAGI\", \"WebGPT: Browser-Assisted Question-Answering with Human Feedback\", \"Chain-of-Thought Prompting Elicits Reasoning in Large Language Models\", \"Self-Consistency Improves Chain of Thought Reasoning in Language Models\", \"Reflextion: Language Agents with Verbal Reinforcement Learning\", \"ToolAlpaca: Generalized Tool Learning for Language Models with 3000 Simulated Cases\", \"Llama: Open and Efficient Foundation Language Models\", \"Llama 2: Open Foundation and Fine-Tuned Chat Models\", \"Voyager: An Open-Ended Embodied Agent with Large Language Models\", \"Multi-Party Chat: Conversational Agents in Group Settings with Humans and Models\", \"Visual ChatGPT: Talking, Drawing and Editing with Visual Foundation Models\", \"AutoGen: Enabling Next-Gen LLM Applications via Multi-Agent Conversation Framework\", \"ChatGPT is not Enough: Enhancing Large Language Models with Knowledge Graphs for Fact-Aware Language Modeling\", \"GPT4Tools: Teaching Large Language Model to Use Tools via Self-Instruction\", \"MMReAct: Prompting ChatGPT for Multimodal Reasoning and Action\", \"Tree of Thoughts: Deliberate Problem Solving with Large Language Models\", \"REACT: Synergizing Reasoning and Acting in Language Models\", \"Tool Learning with Foundation Models\", \"AgentTuning: Enabling Generalized Agent Abilities for LLMs\", \"MemoryBank: Enhancing Large Language Models with Long-Term Memory\", \"Ghost in the Minecraft: Generally Capable Agents for Open-World Environments via Large Language Models with Text-Based Knowledge and Memory\"], \"Citation\": []}",
        "eval_response": "Evaluation evidence: The assistant's answer does not match the gold answer in terms of accuracy and completeness. The references listed in the assistant's answer do not align with those in the gold answer, indicating a significant deviation and hallucination of content. Additionally, the assistant's answer includes an extensive list of references that are not mentioned in the gold answer, further evidencing inaccuracies. The citations section is empty in both the assistant's and gold answers, which is the only point of agreement. Overall, the assistant's performance is poor due to these discrepancies.\n\nRating: [[10]]",
        "score": 10
    },
    {
        "level": 3,
        "set": 1,
        "length": 40230,
        "type": "paper",
        "language": "en",
        "question": "CharacterEval: A Chinese Benchmark for Role-Playing Conversational Agent Evaluation",
        "instruction": "We hope you will carefully study the provided papers and determine the citation relationships between them. Please follow the instructions below strictly to complete the task:\n\n#Specific Requirements:\n1. Reference: When a given paper mentions other provided papers, those other papers are considered as \"references\" for the given paper. To summarize in this specific context, references are about what the given paper is using.\n2. Citation: Conversely, when other provided papers mention the given paper in their works, the given paper is being \"cited\" by those other papers. To summarize in this specific context, citations are about who is using the given paper.\n3. Given a paper, you need to determine the citation or reference relationship between this paper and the other papers. Do not consider papers that are not provided.\n3. Please present the paper titles in a json format as follows: {{\"Reference\":[\"Reference Title 1\", \"Reference Title 2\", ..., \"Reference Title n\"], \"Citation\":[\"Citation Title 1\", \"Citation Title 2\", ..., \"Citation Title n\"]}}.\n4. If a paper does not have any references or citations, please leave the corresponding list empty, e.g.{{\"Refernce\":[]}}, {{\"Citation\":[]}}.",
        "prompt_template": "#Papers:\n{docs}\n\n{instruction}\n\n#The paper you need to analyze:\n{question}",
        "doc": [
            "2401.01275.md",
            "2405.14231.md",
            "2309.05922.md"
        ],
        "answer": {
            "Reference": [
                "# A Survey of Hallucination in \"Large\" Foundation Models "
            ],
            "Citation": [
                "# From Role-Play to Drama-Interaction: An LLM Solution "
            ]
        },
        "shuffle_doc": true,
        "id": "c4a4e642-fa00-4377-a756-40f2f7ea7a74",
        "prompt": "[Question]\n#Papers:\n{docs}\n\nWe hope you will carefully study the provided papers and determine the citation relationships between them. Please follow the instructions below strictly to complete the task:\n\n#Specific Requirements:\n1. Reference: When a given paper mentions other provided papers, those other papers are considered as \"references\" for the given paper. To summarize in this specific context, references are about what the given paper is using.\n2. Citation: Conversely, when other provided papers mention the given paper in their works, the given paper is being \"cited\" by those other papers. To summarize in this specific context, citations are about who is using the given paper.\n3. Given a paper, you need to determine the citation or reference relationship between this paper and the other papers. Do not consider papers that are not provided.\n3. Please present the paper titles in a json format as follows: {{\"Reference\":[\"Reference Title 1\", \"Reference Title 2\", ..., \"Reference Title n\"], \"Citation\":[\"Citation Title 1\", \"Citation Title 2\", ..., \"Citation Title n\"]}}.\n4. If a paper does not have any references or citations, please leave the corresponding list empty, e.g.{{\"Refernce\":[]}}, {{\"Citation\":[]}}.\n\n#The paper you need to analyze:\nCharacterEval: A Chinese Benchmark for Role-Playing Conversational Agent Evaluation\n\n[Gold Answer]\n{'Reference': ['# A Survey of Hallucination in \"Large\" Foundation Models '], 'Citation': ['# From Role-Play to Drama-Interaction: An LLM Solution ']}\n\n[The Start of Assistant's Predicted Answer]\n{{\"Reference\":[\"Language models are few-shot learners\", \"The myers-briggs type indicator: Manual (1962)\", \"Openai: Introducing chatgpt\", \"Do llms possess a personality? making the mbti test an amazing evaluation for large language models\", \"Pddl planning with pretrained large language models\", \"Llm-planner: Few-shot grounded planning for embodied agents with large language models\", \"Deep reinforcement learning from human preferences\", \"A survey on dialogue systems: Recent advances and new frontiers\", \"A survey for in-context learning\", \"Improving factual consistency between a response and persona facts\", \"On lines and planes of closest fit to systems of points in space\", \"Attention is all you need\", \"Self-consistency improves chain of thought reasoning in language models\", \"Chain-of-thought prompting elicits reasoning in large language models\", \"Deep learning for dialogue systems: Chit-chat and beyond\", \"Unifying turn and dialogue level evaluation\", \"A survey on hallucination in large language models\", \"A survey of large language models\", \"Does role-playing chatbots capture the character personalities? Assessing personality traits for role-playing chatbots\", \"Less is more: Learning to refine dialogue history for personalized dialogue generation\", \"Characterglm: Customizing chinese conversational ai characters with large language models\"], \"Citation\":[\"From Role-Play to Drama-Interaction: An LLM Solution\"]}}\"\n[The End of Assistant's Predicted Answer]\n\n[System]\nWe would like to request your feedback on the performance of the AI assistant in response to the user question displayed above according to the gold answer. Please use the following listed aspects and their descriptions as evaluation criteria:\n    - Accuracy and Hallucinations: The assistant's answer is semantically consistent with the gold answer; The numerical value and order need to be accurate, and there should be no hallucinations.\n    - Completeness: Referring to the reference answers, the assistant's answer should contain all the key points needed to answer the user's question; further elaboration on these key points can be omitted.\nPlease rate whether this answer is suitable for the question. Please note that the gold answer can be considered as a correct answer to the question.\n\nThe assistant receives an overall score on a scale of 1 to 100, where a higher score indicates better overall performance.\nPlease note that if the assistant's answer and the gold answer fully meet the above criteria, its overall rating should be the full marks (100).\nPlease first provide a comprehensive explanation of your evaluation, avoiding any potential bias.\nThen, output a line indicating the score of the Assistant.\n\nPLEASE OUTPUT WITH THE FOLLOWING FORMAT, WHERE THE SCORE IS A SCALE OF 1 TO 100 BY STRICTLY FOLLOWING THIS FORMAT: \"[[score]]\", FOR EXAMPLE \"Rating: [[100]]\":\n<start output>\nEvaluation evidence: your evluation explanation here, no more than 100 words\nRating: [[score]]\n<end output>\n\nNow, start your evaluation:",
        "generate_response": "{{\"Reference\":[\"Language models are few-shot learners\", \"The myers-briggs type indicator: Manual (1962)\", \"Openai: Introducing chatgpt\", \"Do llms possess a personality? making the mbti test an amazing evaluation for large language models\", \"Pddl planning with pretrained large language models\", \"Llm-planner: Few-shot grounded planning for embodied agents with large language models\", \"Deep reinforcement learning from human preferences\", \"A survey on dialogue systems: Recent advances and new frontiers\", \"A survey for in-context learning\", \"Improving factual consistency between a response and persona facts\", \"On lines and planes of closest fit to systems of points in space\", \"Attention is all you need\", \"Self-consistency improves chain of thought reasoning in language models\", \"Chain-of-thought prompting elicits reasoning in large language models\", \"Deep learning for dialogue systems: Chit-chat and beyond\", \"Unifying turn and dialogue level evaluation\", \"A survey on hallucination in large language models\", \"A survey of large language models\", \"Does role-playing chatbots capture the character personalities? Assessing personality traits for role-playing chatbots\", \"Less is more: Learning to refine dialogue history for personalized dialogue generation\", \"Characterglm: Customizing chinese conversational ai characters with large language models\"], \"Citation\":[\"From Role-Play to Drama-Interaction: An LLM Solution\"]}}\"",
        "eval_response": "Evaluation evidence: The assistant's answer includes a significant number of hallucinated references that are not mentioned in the gold answer, indicating a lack of accuracy and introducing irrelevant information. The citation part matches the gold answer, which is accurate. However, the overwhelming number of incorrect references affects the overall performance severely.\n\nRating: [[20]]",
        "score": 20
    },
    {
        "level": 3,
        "set": 3,
        "length": 135792,
        "type": "paper",
        "language": "en",
        "question": "Extreme Compression of Large Language Models via Additive Quantization",
        "instruction": "We hope you will carefully study the provided papers and determine the citation relationships between them. Please follow the instructions below strictly to complete the task:\n\n#Specific Requirements:\n1. Reference: When a given paper mentions other provided papers, those other papers are considered as \"references\" for the given paper. To summarize in this specific context, references are about what the given paper is using.\n2. Citation: Conversely, when other provided papers mention the given paper in their works, the given paper is being \"cited\" by those other papers. To summarize in this specific context, citations are about who is using the given paper.\n3. Given a paper, you need to determine the citation or reference relationship between this paper and the other papers. Do not consider papers that are not provided.\n3. Please present the paper titles in a json format as follows: {{\"Reference\":[\"Reference Title 1\", \"Reference Title 2\", ..., \"Reference Title n\"], \"Citation\":[\"Citation Title 1\", \"Citation Title 2\", ..., \"Citation Title n\"]}}.\n4. If a paper does not have any references or citations, please leave the corresponding list empty, e.g.{{\"Refernce\":[]}}, {{\"Citation\":[]}}.",
        "prompt_template": "#Papers:\n{docs}\n\n{instruction}\n\n#The paper you need to analyze:\n{question}",
        "doc": [
            "2401.06118.md",
            "2405.16528.md",
            "2405.18886.md",
            "2306.07629.md",
            "2310.16795.md"
        ],
        "answer": {
            "Reference": [
                "# SqueezeLLM: Dense-and-Sparse Quantization ",
                "# QMoE: Practical Sub-1-Bit Compression of Trillion-Parameter Models "
            ],
            "Citation": [
                "# LoQT: Low Rank Adapters for Quantized Training ",
                "# Compressing Large Language Models using Low Rank and Low Precision Decomposition "
            ]
        },
        "shuffle_doc": true,
        "id": "83ad3b04-9ca3-4f1a-822f-bc132c50d015",
        "prompt": "[Question]\n#Papers:\n{docs}\n\nWe hope you will carefully study the provided papers and determine the citation relationships between them. Please follow the instructions below strictly to complete the task:\n\n#Specific Requirements:\n1. Reference: When a given paper mentions other provided papers, those other papers are considered as \"references\" for the given paper. To summarize in this specific context, references are about what the given paper is using.\n2. Citation: Conversely, when other provided papers mention the given paper in their works, the given paper is being \"cited\" by those other papers. To summarize in this specific context, citations are about who is using the given paper.\n3. Given a paper, you need to determine the citation or reference relationship between this paper and the other papers. Do not consider papers that are not provided.\n3. Please present the paper titles in a json format as follows: {{\"Reference\":[\"Reference Title 1\", \"Reference Title 2\", ..., \"Reference Title n\"], \"Citation\":[\"Citation Title 1\", \"Citation Title 2\", ..., \"Citation Title n\"]}}.\n4. If a paper does not have any references or citations, please leave the corresponding list empty, e.g.{{\"Refernce\":[]}}, {{\"Citation\":[]}}.\n\n#The paper you need to analyze:\nExtreme Compression of Large Language Models via Additive Quantization\n\n[Gold Answer]\n{'Reference': ['# SqueezeLLM: Dense-and-Sparse Quantization ', '# QMoE: Practical Sub-1-Bit Compression of Trillion-Parameter Models '], 'Citation': ['# LoQT: Low Rank Adapters for Quantized Training ', '# Compressing Large Language Models using Low Rank and Low Precision Decomposition ']}\n\n[The Start of Assistant's Predicted Answer]\n{\"Reference\":[\"Additive quantization for extreme vector compression\", \"On the statistical analysis of dirty pictures\", \"Pythia: A suite for analyzing large language models across training and scaling\", \"Multiplying matrices without multiplying\", \"A generalization of isolated word recognition using vector quantization\", \"Quip: 2-bit quantization of large language models with guarantees\", \"Deep neural network quantization via layer-wise optimization using limited training data\", \"Approximate nearest neighbor search by residual vector quantization\", \"Think you have solved question answering? try arc, the ai 2 reasoning challenge\", \"Redpajama: an open dataset for training large language models\", \"The case for 4-bit precision: k-bit inference scaling laws\", \"LLM.int8(): 8-bit matrix multiplication for transformers at scale\", \"QLoRA: Efficient finetuning of quantized llms\", \"Spqr: A sparse-quantized representation for near-lossless 11 m weight compression\", \"Are we there yet? product quantization and its hardware acceleration\", \"Gptq: Accurate post-training quantization for generative pretrained transformers\", \"Optimal Brain Compression: A framework for accurate posttraining quantization and pruning\", \"A survey of quantization methods for efficient neural network inference\", \"Vector quantization\", \"Quantization based fast inner product search\", \"Distilling the knowledge in a neural network\", \"Product quantization for nearest neighbor search\", \"Mistral 7b\", \"Mixtral of experts\", \"Squeezellm: Dense-and-sparse quantization\", \"Adam: A method for stochastic optimization\", \"Sparse fine-tuning for inference acceleration of large language models\", \"Performance guaranteed network acceleration via high-order residual quantization\", \"Awq: Activation-aware weight quantization for llm compression and acceleration\", \"Revisiting additive quantization\", \"Lsq++: Lower running time and higher recall in multi-codebook quantization\", \"Look-ups are not (yet) all you need for deep learning inference\", \"Pointer sentinel mixture models\", \"Up or down? Adaptive rounding for post-training quantization\", \"Cartesian k-means\", \"Competitive quantization for approximate nearest neighbor search\", \"Quantized matmul for efficient inference of large-scale generative language models\", \"PyTorch: An imperative style, high-performance deep learning library\", \"Exploring the limits of transfer learning with a unified text-to-text transformer\", \"Winogrande: An adversarial winograd schema challenge at scale\", \"Bloom: A 176b-parameter open-access multilingual language model\", \"Patient knowledge distillation for bert model compression\", \"PiQA: An algebra for querying protein data sets\", \"The Falcon family of large language models\", \"Llama: Open and efficient foundation language models\", \"Quip\\#: Quip with lattice codebooks\", \"Attention is all you need\", \"Smoothquant: Accurate and efficient post-training quantization for large language models\", \"Zeroquant: Efficient and affordable post-training quantization for large-scale transformers\", \"Hellaswag: Can a machine really finish your sentence?\", \"Opt: Open pre-trained transformer language models\", \"Composite quantization for approximate nearest neighbor search\", \"Balanced quantization: An effective and efficient approach to quantized neural networks\"], \"Citation\":[]}\n[The End of Assistant's Predicted Answer]\n\n[System]\nWe would like to request your feedback on the performance of the AI assistant in response to the user question displayed above according to the gold answer. Please use the following listed aspects and their descriptions as evaluation criteria:\n    - Accuracy and Hallucinations: The assistant's answer is semantically consistent with the gold answer; The numerical value and order need to be accurate, and there should be no hallucinations.\n    - Completeness: Referring to the reference answers, the assistant's answer should contain all the key points needed to answer the user's question; further elaboration on these key points can be omitted.\nPlease rate whether this answer is suitable for the question. Please note that the gold answer can be considered as a correct answer to the question.\n\nThe assistant receives an overall score on a scale of 1 to 100, where a higher score indicates better overall performance.\nPlease note that if the assistant's answer and the gold answer fully meet the above criteria, its overall rating should be the full marks (100).\nPlease first provide a comprehensive explanation of your evaluation, avoiding any potential bias.\nThen, output a line indicating the score of the Assistant.\n\nPLEASE OUTPUT WITH THE FOLLOWING FORMAT, WHERE THE SCORE IS A SCALE OF 1 TO 100 BY STRICTLY FOLLOWING THIS FORMAT: \"[[score]]\", FOR EXAMPLE \"Rating: [[100]]\":\n<start output>\nEvaluation evidence: your evluation explanation here, no more than 100 words\nRating: [[score]]\n<end output>\n\nNow, start your evaluation:",
        "generate_response": "{\"Reference\":[\"Additive quantization for extreme vector compression\", \"On the statistical analysis of dirty pictures\", \"Pythia: A suite for analyzing large language models across training and scaling\", \"Multiplying matrices without multiplying\", \"A generalization of isolated word recognition using vector quantization\", \"Quip: 2-bit quantization of large language models with guarantees\", \"Deep neural network quantization via layer-wise optimization using limited training data\", \"Approximate nearest neighbor search by residual vector quantization\", \"Think you have solved question answering? try arc, the ai 2 reasoning challenge\", \"Redpajama: an open dataset for training large language models\", \"The case for 4-bit precision: k-bit inference scaling laws\", \"LLM.int8(): 8-bit matrix multiplication for transformers at scale\", \"QLoRA: Efficient finetuning of quantized llms\", \"Spqr: A sparse-quantized representation for near-lossless 11 m weight compression\", \"Are we there yet? product quantization and its hardware acceleration\", \"Gptq: Accurate post-training quantization for generative pretrained transformers\", \"Optimal Brain Compression: A framework for accurate posttraining quantization and pruning\", \"A survey of quantization methods for efficient neural network inference\", \"Vector quantization\", \"Quantization based fast inner product search\", \"Distilling the knowledge in a neural network\", \"Product quantization for nearest neighbor search\", \"Mistral 7b\", \"Mixtral of experts\", \"Squeezellm: Dense-and-sparse quantization\", \"Adam: A method for stochastic optimization\", \"Sparse fine-tuning for inference acceleration of large language models\", \"Performance guaranteed network acceleration via high-order residual quantization\", \"Awq: Activation-aware weight quantization for llm compression and acceleration\", \"Revisiting additive quantization\", \"Lsq++: Lower running time and higher recall in multi-codebook quantization\", \"Look-ups are not (yet) all you need for deep learning inference\", \"Pointer sentinel mixture models\", \"Up or down? Adaptive rounding for post-training quantization\", \"Cartesian k-means\", \"Competitive quantization for approximate nearest neighbor search\", \"Quantized matmul for efficient inference of large-scale generative language models\", \"PyTorch: An imperative style, high-performance deep learning library\", \"Exploring the limits of transfer learning with a unified text-to-text transformer\", \"Winogrande: An adversarial winograd schema challenge at scale\", \"Bloom: A 176b-parameter open-access multilingual language model\", \"Patient knowledge distillation for bert model compression\", \"PiQA: An algebra for querying protein data sets\", \"The Falcon family of large language models\", \"Llama: Open and efficient foundation language models\", \"Quip\\#: Quip with lattice codebooks\", \"Attention is all you need\", \"Smoothquant: Accurate and efficient post-training quantization for large language models\", \"Zeroquant: Efficient and affordable post-training quantization for large-scale transformers\", \"Hellaswag: Can a machine really finish your sentence?\", \"Opt: Open pre-trained transformer language models\", \"Composite quantization for approximate nearest neighbor search\", \"Balanced quantization: An effective and efficient approach to quantized neural networks\"], \"Citation\":[]}",
        "eval_response": "Evaluation evidence: The assistant's answer does not match the gold answer in terms of accuracy and completeness. The references listed in the assistant's answer do not align with those in the gold answer, which specifically mentions two papers as references. Additionally, the assistant's answer incorrectly lists numerous references and fails to identify any citations, whereas the gold answer identifies two citations. This discrepancy indicates issues with both accuracy and completeness, as well as hallucinations due to the inclusion of unrelated references.\n\nRating: [[20]]",
        "score": 20
    },
    {
        "level": 3,
        "set": 2,
        "length": 59166,
        "type": "paper",
        "language": "en",
        "question": "Large Language Models for Mathematical Reasoning: Progresses and Challenges",
        "instruction": "We hope you will carefully study the provided papers and determine the citation relationships between them. Please follow the instructions below strictly to complete the task:\n\n#Specific Requirements:\n1. Reference: When a given paper mentions other provided papers, those other papers are considered as \"references\" for the given paper. To summarize in this specific context, references are about what the given paper is using.\n2. Citation: Conversely, when other provided papers mention the given paper in their works, the given paper is being \"cited\" by those other papers. To summarize in this specific context, citations are about who is using the given paper.\n3. Given a paper, you need to determine the citation or reference relationship between this paper and the other papers. Do not consider papers that are not provided.\n3. Please present the paper titles in a json format as follows: {{\"Reference\":[\"Reference Title 1\", \"Reference Title 2\", ..., \"Reference Title n\"], \"Citation\":[\"Citation Title 1\", \"Citation Title 2\", ..., \"Citation Title n\"]}}.\n4. If a paper does not have any references or citations, please leave the corresponding list empty, e.g.{{\"Refernce\":[]}}, {{\"Citation\":[]}}.",
        "prompt_template": "#Papers:\n{docs}\n\n{instruction}\n\n#The paper you need to analyze:\n{question}",
        "doc": [
            "2402.00157.md",
            "2405.19616.md",
            "2405.19973.md",
            "2310.09590.md"
        ],
        "answer": {
            "Reference": [
                "# Solving Math Word Problems with Reexamination "
            ],
            "Citation": [
                "# Easy Problems That LLMs Get Wrong ",
                "# A Triumvirate of AI Driven Theoretical Discovery "
            ]
        },
        "shuffle_doc": true,
        "id": "ffb3aa9f-9152-4a91-9abe-37f09798ba43",
        "prompt": "[Question]\n#Papers:\n{docs}\n\nWe hope you will carefully study the provided papers and determine the citation relationships between them. Please follow the instructions below strictly to complete the task:\n\n#Specific Requirements:\n1. Reference: When a given paper mentions other provided papers, those other papers are considered as \"references\" for the given paper. To summarize in this specific context, references are about what the given paper is using.\n2. Citation: Conversely, when other provided papers mention the given paper in their works, the given paper is being \"cited\" by those other papers. To summarize in this specific context, citations are about who is using the given paper.\n3. Given a paper, you need to determine the citation or reference relationship between this paper and the other papers. Do not consider papers that are not provided.\n3. Please present the paper titles in a json format as follows: {{\"Reference\":[\"Reference Title 1\", \"Reference Title 2\", ..., \"Reference Title n\"], \"Citation\":[\"Citation Title 1\", \"Citation Title 2\", ..., \"Citation Title n\"]}}.\n4. If a paper does not have any references or citations, please leave the corresponding list empty, e.g.{{\"Refernce\":[]}}, {{\"Citation\":[]}}.\n\n#The paper you need to analyze:\nLarge Language Models for Mathematical Reasoning: Progresses and Challenges\n\n[Gold Answer]\n{'Reference': ['# Solving Math Word Problems with Reexamination '], 'Citation': ['# Easy Problems That LLMs Get Wrong ', '# A Triumvirate of AI Driven Theoretical Discovery ']}\n\n[The Start of Assistant's Predicted Answer]\n{\"Reference\":[\"A Survey on Evaluation of Large Language Models\", \"Vicuna: An Open-Source Chatbot Impressing GPT-4 with $90 \\% *$ ChatGPT Quality\", \"ChatLaw: Open-Source Legal Large Language Model with Integrated External Knowledge Bases\", \"Compositional Semantic Parsing with Large Language Models\", \"Open LLM Leaderboard\", \"How Close is ChatGPT to Human Experts? Comparison Corpus, Evaluation, and Detection\", \"REALM: Retrieval-Augmented Language Model PreTraining\", \"Rethinking with Retrieval: Faithful Large Language Model Inference\", \"Measuring Massive Multitask Language Understanding\", \"Atlas: Few-shot Learning with Retrieval Augmented Language Models\", \"Large language models for mathematicians\", \"Can neural networks do arithmetic? A survey on the elementary numerical skills of state-of-the-art deep learning models\", \"Mathematical capabilities of chatgpt\", \"An independent evaluation of chatgpt on mathematical word problems (MWP)\", \"A causal framework to quantify the robustness of mathematical reasoning with language models\", \"Solving olympiad geometry without human demonstrations\", \"Three questions concerning the use of large language models to facilitate mathematics learning\", \"How well do large language models perform in arithmetic tasks?\"], \"Citation\":[\"A Triumvirate of AI Driven Theoretical Discovery\", \"Solving Math Word Problems with Reexamination\"]}\n[The End of Assistant's Predicted Answer]\n\n[System]\nWe would like to request your feedback on the performance of the AI assistant in response to the user question displayed above according to the gold answer. Please use the following listed aspects and their descriptions as evaluation criteria:\n    - Accuracy and Hallucinations: The assistant's answer is semantically consistent with the gold answer; The numerical value and order need to be accurate, and there should be no hallucinations.\n    - Completeness: Referring to the reference answers, the assistant's answer should contain all the key points needed to answer the user's question; further elaboration on these key points can be omitted.\nPlease rate whether this answer is suitable for the question. Please note that the gold answer can be considered as a correct answer to the question.\n\nThe assistant receives an overall score on a scale of 1 to 100, where a higher score indicates better overall performance.\nPlease note that if the assistant's answer and the gold answer fully meet the above criteria, its overall rating should be the full marks (100).\nPlease first provide a comprehensive explanation of your evaluation, avoiding any potential bias.\nThen, output a line indicating the score of the Assistant.\n\nPLEASE OUTPUT WITH THE FOLLOWING FORMAT, WHERE THE SCORE IS A SCALE OF 1 TO 100 BY STRICTLY FOLLOWING THIS FORMAT: \"[[score]]\", FOR EXAMPLE \"Rating: [[100]]\":\n<start output>\nEvaluation evidence: your evluation explanation here, no more than 100 words\nRating: [[score]]\n<end output>\n\nNow, start your evaluation:",
        "generate_response": "{\"Reference\":[\"A Survey on Evaluation of Large Language Models\", \"Vicuna: An Open-Source Chatbot Impressing GPT-4 with $90 \\% *$ ChatGPT Quality\", \"ChatLaw: Open-Source Legal Large Language Model with Integrated External Knowledge Bases\", \"Compositional Semantic Parsing with Large Language Models\", \"Open LLM Leaderboard\", \"How Close is ChatGPT to Human Experts? Comparison Corpus, Evaluation, and Detection\", \"REALM: Retrieval-Augmented Language Model PreTraining\", \"Rethinking with Retrieval: Faithful Large Language Model Inference\", \"Measuring Massive Multitask Language Understanding\", \"Atlas: Few-shot Learning with Retrieval Augmented Language Models\", \"Large language models for mathematicians\", \"Can neural networks do arithmetic? A survey on the elementary numerical skills of state-of-the-art deep learning models\", \"Mathematical capabilities of chatgpt\", \"An independent evaluation of chatgpt on mathematical word problems (MWP)\", \"A causal framework to quantify the robustness of mathematical reasoning with language models\", \"Solving olympiad geometry without human demonstrations\", \"Three questions concerning the use of large language models to facilitate mathematics learning\", \"How well do large language models perform in arithmetic tasks?\"], \"Citation\":[\"A Triumvirate of AI Driven Theoretical Discovery\", \"Solving Math Word Problems with Reexamination\"]}",
        "eval_response": "Evaluation evidence: The assistant's answer includes a large number of references that are not mentioned in the gold answer, indicating a significant hallucination issue. The gold answer specifies only one reference and two citations, whereas the assistant lists numerous unrelated papers. This discrepancy shows a lack of accuracy and introduces irrelevant information, which affects the completeness of the response by not focusing solely on the key points required by the question.\n\nRating: [[20]]",
        "score": 20
    },
    {
        "level": 3,
        "set": 1,
        "length": 46630,
        "type": "paper",
        "language": "en",
        "question": "Scalable Pre-training of Large Autoregressive Image Models",
        "instruction": "We hope you will carefully study the provided papers and determine the citation relationships between them. Please follow the instructions below strictly to complete the task:\n\n#Specific Requirements:\n1. Reference: When a given paper mentions other provided papers, those other papers are considered as \"references\" for the given paper. To summarize in this specific context, references are about what the given paper is using.\n2. Citation: Conversely, when other provided papers mention the given paper in their works, the given paper is being \"cited\" by those other papers. To summarize in this specific context, citations are about who is using the given paper.\n3. Given a paper, you need to determine the citation or reference relationship between this paper and the other papers. Do not consider papers that are not provided.\n3. Please present the paper titles in a json format as follows: {{\"Reference\":[\"Reference Title 1\", \"Reference Title 2\", ..., \"Reference Title n\"], \"Citation\":[\"Citation Title 1\", \"Citation Title 2\", ..., \"Citation Title n\"]}}.\n4. If a paper does not have any references or citations, please leave the corresponding list empty, e.g.{{\"Refernce\":[]}}, {{\"Citation\":[]}}.",
        "prompt_template": "#Papers:\n{docs}\n\n{instruction}\n\n#The paper you need to analyze:\n{question}",
        "doc": [
            "2401.08541.md",
            "2405.15160.md",
            "2309.17425.md"
        ],
        "answer": {
            "Reference": [
                "# Data Filtering Networks "
            ],
            "Citation": [
                "# ARVideo: Autoregressive Pretraining for Self-Supervised Video Representation Learning "
            ]
        },
        "shuffle_doc": true,
        "id": "5d482043-88d2-4246-8191-e603b7bf6ed4",
        "prompt": "[Question]\n#Papers:\n{docs}\n\nWe hope you will carefully study the provided papers and determine the citation relationships between them. Please follow the instructions below strictly to complete the task:\n\n#Specific Requirements:\n1. Reference: When a given paper mentions other provided papers, those other papers are considered as \"references\" for the given paper. To summarize in this specific context, references are about what the given paper is using.\n2. Citation: Conversely, when other provided papers mention the given paper in their works, the given paper is being \"cited\" by those other papers. To summarize in this specific context, citations are about who is using the given paper.\n3. Given a paper, you need to determine the citation or reference relationship between this paper and the other papers. Do not consider papers that are not provided.\n3. Please present the paper titles in a json format as follows: {{\"Reference\":[\"Reference Title 1\", \"Reference Title 2\", ..., \"Reference Title n\"], \"Citation\":[\"Citation Title 1\", \"Citation Title 2\", ..., \"Citation Title n\"]}}.\n4. If a paper does not have any references or citations, please leave the corresponding list empty, e.g.{{\"Refernce\":[]}}, {{\"Citation\":[]}}.\n\n#The paper you need to analyze:\nScalable Pre-training of Large Autoregressive Image Models\n\n[Gold Answer]\n{'Reference': ['# Data Filtering Networks '], 'Citation': ['# ARVideo: Autoregressive Pretraining for Self-Supervised Video Representation Learning ']}\n\n[The Start of Assistant's Predicted Answer]\n{\"Reference\": [\"V-JEPA: Latent video prediction for visual representation learning\", \"Self-supervised learning from images with a joint-embedding predictive architecture\", \"Sequential modeling enables scalable learning for large vision models\", \"From detection of individual metastases to classification of lymph node status at the patient level: the camelyon17 challenge\", \"BEiT: Bert pretraining of image transformers\", \"Vicreg: Variance-invariance-covariance regularization for selfsupervised learning\", \"Cliquecnn: Deep unsupervised exemplar learning\", \"The iwildcam 2020 competition dataset\", \"A neural probabilistic language model\", \"Unsupervised learning by predicting noise\", \"Unsupervised pre-training of image features on non-curated data\", \"Self-supervised learning of visual features by contrasting cluster assignments\", \"Emerging properties in self-supervised vision transformers\", \"Generative pretraining from pixels\", \"A simple framework for contrastive learning of visual representations\", \"An empirical study of training self-supervised vision transformers\", \"Are large-scale datasets necessary for self-supervised pre-training?\", \"Finding structure in time\", \"Data filtering networks\", \"Datacomp: In search of the next generation of multimodal datasets\", \"Unsupervised representation learning by predicting image rotations\", \"Generative adversarial nets\", \"Scaling and benchmarking self-supervised visual representation learning\", \"Vision models are more robust and fair when pretrained on uncurated images without supervision\", \"Bootstrap your own latent-a new approach to self-supervised learning\", \"Mask r-cnn\", \"Momentum contrast for unsupervised visual representation learning\", \"Masked autoencoders are scalable vision learners\", \"Training compute-optimal large language models\", \"Low-rank adaptation of large language models\", \"Deep networks with stochastic depth\", \"iNaturalist 2018 competition dataset\", \"Cats and dogs\", \"Image transformer\", \"Context encoders: Feature learning by inpainting\", \"Moment matching for multi-source domain adaptation\", \"Language models are unsupervised multitask learners\", \"Exploring the limits of transfer learning with a unified text-to-text transformer\", \"Zero-shot text-to-image generation\", \"Pixelcnn++: Improving the pixelcnn with discretized logistic mixture likelihood and other modifications\", \"Prediction and entropy of printed english\", \"The effectiveness of mae pre-pretraining for billionscale pretraining\", \"Divide and contrast: Self-supervised learning from uncurated data\", \"Training data-efficient image transformers & distillation through attention\", \"Going deeper with image transformers\", \"Llama: Open and efficient foundation language models\", \"Conditional image generation with pixelcnn decoders\", \"Pixel recurrent neural networks\", \"Neurips\", \"Attention is all you need\", \"Rotation equivariant cnns for digital pathology\", \"Stacked denoising autoencoders: Learning useful representations in a deep network with a local denoising criterion\", \"Diffusion models as masked autoencoders\", \"ClusterFit: Improving Generalization of Visual Representations\", \"Coca: Contrastive captioners are image-text foundation models\", \"Barlow twins: Self-supervised learning via redundancy reduction\", \"mixup: Beyond empirical risk minimization\", \"Colorful image colorization\"], \"Citation\": []}\n[The End of Assistant's Predicted Answer]\n\n[System]\nWe would like to request your feedback on the performance of the AI assistant in response to the user question displayed above according to the gold answer. Please use the following listed aspects and their descriptions as evaluation criteria:\n    - Accuracy and Hallucinations: The assistant's answer is semantically consistent with the gold answer; The numerical value and order need to be accurate, and there should be no hallucinations.\n    - Completeness: Referring to the reference answers, the assistant's answer should contain all the key points needed to answer the user's question; further elaboration on these key points can be omitted.\nPlease rate whether this answer is suitable for the question. Please note that the gold answer can be considered as a correct answer to the question.\n\nThe assistant receives an overall score on a scale of 1 to 100, where a higher score indicates better overall performance.\nPlease note that if the assistant's answer and the gold answer fully meet the above criteria, its overall rating should be the full marks (100).\nPlease first provide a comprehensive explanation of your evaluation, avoiding any potential bias.\nThen, output a line indicating the score of the Assistant.\n\nPLEASE OUTPUT WITH THE FOLLOWING FORMAT, WHERE THE SCORE IS A SCALE OF 1 TO 100 BY STRICTLY FOLLOWING THIS FORMAT: \"[[score]]\", FOR EXAMPLE \"Rating: [[100]]\":\n<start output>\nEvaluation evidence: your evluation explanation here, no more than 100 words\nRating: [[score]]\n<end output>\n\nNow, start your evaluation:",
        "generate_response": "{\"Reference\": [\"V-JEPA: Latent video prediction for visual representation learning\", \"Self-supervised learning from images with a joint-embedding predictive architecture\", \"Sequential modeling enables scalable learning for large vision models\", \"From detection of individual metastases to classification of lymph node status at the patient level: the camelyon17 challenge\", \"BEiT: Bert pretraining of image transformers\", \"Vicreg: Variance-invariance-covariance regularization for selfsupervised learning\", \"Cliquecnn: Deep unsupervised exemplar learning\", \"The iwildcam 2020 competition dataset\", \"A neural probabilistic language model\", \"Unsupervised learning by predicting noise\", \"Unsupervised pre-training of image features on non-curated data\", \"Self-supervised learning of visual features by contrasting cluster assignments\", \"Emerging properties in self-supervised vision transformers\", \"Generative pretraining from pixels\", \"A simple framework for contrastive learning of visual representations\", \"An empirical study of training self-supervised vision transformers\", \"Are large-scale datasets necessary for self-supervised pre-training?\", \"Finding structure in time\", \"Data filtering networks\", \"Datacomp: In search of the next generation of multimodal datasets\", \"Unsupervised representation learning by predicting image rotations\", \"Generative adversarial nets\", \"Scaling and benchmarking self-supervised visual representation learning\", \"Vision models are more robust and fair when pretrained on uncurated images without supervision\", \"Bootstrap your own latent-a new approach to self-supervised learning\", \"Mask r-cnn\", \"Momentum contrast for unsupervised visual representation learning\", \"Masked autoencoders are scalable vision learners\", \"Training compute-optimal large language models\", \"Low-rank adaptation of large language models\", \"Deep networks with stochastic depth\", \"iNaturalist 2018 competition dataset\", \"Cats and dogs\", \"Image transformer\", \"Context encoders: Feature learning by inpainting\", \"Moment matching for multi-source domain adaptation\", \"Language models are unsupervised multitask learners\", \"Exploring the limits of transfer learning with a unified text-to-text transformer\", \"Zero-shot text-to-image generation\", \"Pixelcnn++: Improving the pixelcnn with discretized logistic mixture likelihood and other modifications\", \"Prediction and entropy of printed english\", \"The effectiveness of mae pre-pretraining for billionscale pretraining\", \"Divide and contrast: Self-supervised learning from uncurated data\", \"Training data-efficient image transformers & distillation through attention\", \"Going deeper with image transformers\", \"Llama: Open and efficient foundation language models\", \"Conditional image generation with pixelcnn decoders\", \"Pixel recurrent neural networks\", \"Neurips\", \"Attention is all you need\", \"Rotation equivariant cnns for digital pathology\", \"Stacked denoising autoencoders: Learning useful representations in a deep network with a local denoising criterion\", \"Diffusion models as masked autoencoders\", \"ClusterFit: Improving Generalization of Visual Representations\", \"Coca: Contrastive captioners are image-text foundation models\", \"Barlow twins: Self-supervised learning via redundancy reduction\", \"mixup: Beyond empirical risk minimization\", \"Colorful image colorization\"], \"Citation\": []}",
        "eval_response": "Evaluation evidence: The assistant's answer includes a significant number of hallucinated references that are not mentioned in the gold answer, indicating a lack of accuracy and the presence of hallucinations. The gold answer specifies one reference and one citation, but the assistant's answer lists numerous unrelated references and omits the citation entirely. This shows a failure in both accuracy and completeness as per the task requirements.\n\nRating: [[10]]",
        "score": 10
    },
    {
        "level": 3,
        "set": 2,
        "length": 84528,
        "type": "paper",
        "language": "en",
        "question": "Self-Discover: Large Language Models Self-Compose Reasoning Structures",
        "instruction": "We hope you will carefully study the provided papers and determine the citation relationships between them. Please follow the instructions below strictly to complete the task:\n\n#Specific Requirements:\n1. Reference: When a given paper mentions other provided papers, those other papers are considered as \"references\" for the given paper. To summarize in this specific context, references are about what the given paper is using.\n2. Citation: Conversely, when other provided papers mention the given paper in their works, the given paper is being \"cited\" by those other papers. To summarize in this specific context, citations are about who is using the given paper.\n3. Given a paper, you need to determine the citation or reference relationship between this paper and the other papers. Do not consider papers that are not provided.\n3. Please present the paper titles in a json format as follows: {{\"Reference\":[\"Reference Title 1\", \"Reference Title 2\", ..., \"Reference Title n\"], \"Citation\":[\"Citation Title 1\", \"Citation Title 2\", ..., \"Citation Title n\"]}}.\n4. If a paper does not have any references or citations, please leave the corresponding list empty, e.g.{{\"Refernce\":[]}}, {{\"Citation\":[]}}.",
        "prompt_template": "#Papers:\n{docs}\n\n{instruction}\n\n#The paper you need to analyze:\n{question}",
        "doc": [
            "2402.03620.md",
            "2402.15205.md",
            "2405.14804.md",
            "2310.06117.md"
        ],
        "answer": {
            "Reference": [
                "# TAKE A STEP BACK: EVOKING REASONING VIA ABSTRACTION IN LARGE LANGUAGE MODELS "
            ],
            "Citation": [
                "# Enhancing ICU Patient Recovery: Using LLMs to Assist Nurses in Diary Writing ",
                "# Can LLMs Solve Longer Math Word Problems Better? "
            ]
        },
        "shuffle_doc": true,
        "id": "2bd708b7-e04c-4eeb-b022-81007e82ef84",
        "prompt": "[Question]\n#Papers:\n{docs}\n\nWe hope you will carefully study the provided papers and determine the citation relationships between them. Please follow the instructions below strictly to complete the task:\n\n#Specific Requirements:\n1. Reference: When a given paper mentions other provided papers, those other papers are considered as \"references\" for the given paper. To summarize in this specific context, references are about what the given paper is using.\n2. Citation: Conversely, when other provided papers mention the given paper in their works, the given paper is being \"cited\" by those other papers. To summarize in this specific context, citations are about who is using the given paper.\n3. Given a paper, you need to determine the citation or reference relationship between this paper and the other papers. Do not consider papers that are not provided.\n3. Please present the paper titles in a json format as follows: {{\"Reference\":[\"Reference Title 1\", \"Reference Title 2\", ..., \"Reference Title n\"], \"Citation\":[\"Citation Title 1\", \"Citation Title 2\", ..., \"Citation Title n\"]}}.\n4. If a paper does not have any references or citations, please leave the corresponding list empty, e.g.{{\"Refernce\":[]}}, {{\"Citation\":[]}}.\n\n#The paper you need to analyze:\nSelf-Discover: Large Language Models Self-Compose Reasoning Structures\n\n[Gold Answer]\n{'Reference': ['# TAKE A STEP BACK: EVOKING REASONING VIA ABSTRACTION IN LARGE LANGUAGE MODELS '], 'Citation': ['# Enhancing ICU Patient Recovery: Using LLMs to Assist Nurses in Diary Writing ', '# Can LLMs Solve Longer Math Word Problems Better? ']}\n\n[The Start of Assistant's Predicted Answer]\n{\"Reference\": [\"A Survey on Evaluation of Large Language Models\", \"Vicuna: An Open-Source Chatbot Impressing GPT-4 with $90 \\% *$ ChatGPT Quality\", \"ChatLaw: Open-Source Legal Large Language Model with Integrated External Knowledge Bases\", \"Compositional Semantic Parsing with Large Language Models\", \"Open LLM Leaderboard\", \"How Close is ChatGPT to Human Experts? Comparison Corpus, Evaluation, and Detection\", \"Rethinking with Retrieval: Faithful Large Language Model Inference\", \"Measuring Massive Multitask Language Understanding\", \"Thinking for Doing (T4D)\", \"MATH\", \"BIG-Bench Hard (BBH)\", \"GPT-4\", \"GPT-3.5-turbo (ChatGPT)\", \"instruction-tuned PaLM 2-L\", \"Llama2-70B\", \"Direct Prompting\", \"CoT (Wei et al., 2022; Kojima et al., 2022)\", \"Plan-and-Solve (Wang et al., 2023)\", \"CoT-Self-Consistency (Wang et al., 2022)\", \"Majority voting of each RM\", \"Best of each RM\", \"Foresee and Reflect\", \"Skills-in-context prompting: Unlocking compositionality in large language models\", \"Program of thoughts prompting: Disentangling computation from reasoning for numerical reasoning tasks\", \"Palm: Scaling language modeling with pathways\", \"Scaling instruction-finetuned language models\", \"Training verifiers to solve math word problems\", \"Compositional semantic parsing with large language models\", \"Promptbreeder: Self-referential self-improvement via prompt evolution\", \"Strategyllm: Large language models as strategy generators, executors, optimizers, and evaluators for problem solving\", \"Reasoning with language model is planning with world model\", \"Measuring mathematical problem solving with the math dataset\", \"Decomposed prompting: A modular approach for solving complex tasks\", \"Large language models are zero-shot reasoners\", \"Less is more: Summary of long instructions is better for program synthesis\", \"Plan, verify and switch: Integrated reasoning with diverse x-of-thoughts\", \"HELP ME THINK: A simple prompting strategy for non-experts to create customized content with models\", \"Lila: A unified benchmark for mathematical reasoning\", \"Crosstask generalization via natural language crowdsourcing instructions\", \"Refocusing instructional prompts to gptk's language\", \"Show your work: Scratchpads for intermediate computation with language models\", \"Chatgpt: Optimizing language models for dialogue\", \"Json generation mode\", \"Gpt-4 technical report\", \"Training language models to follow instructions with human feedback\", \"Is a question decomposition unit all we need?\", \"How to solve it: A new aspect of mathematical method\", \"Skills, rules, and knowledge; signals, signs, and symbols, and other distinctions in human performance models\", \"Branch-solve-merge improves large language model evaluation and generation\", \"Beyond the imitation game: Quantifying and extrapolating the capabilities of language models\", \"Challenging big-bench tasks and whether chain-of-thought can solve them\", \"Llama 2: Open foundation and finetuned chat models\", \"Attention is all you need\", \"Selfconsistency improves chain of thought reasoning in language models\", \"Finetuned language models are zero-shot learners\", \"Chain-of-thought prompting elicits reasoning in large language models\", \"Large language models as optimizers\", \"Tree of thoughts: Deliberate problem solving with large language models\", \"Beyond chain-of-thought, effective graph-of-thought reasoning in large language models\", \"Large language models as analogical reasoners\", \"Take a step back: Evoking reasoning via abstraction in large language models\", \"Adapting language models for zero-shot learning by metatuning on dataset and prompt collections\", \"Least-to-most prompting enables complex reasoning in large language models\", \"How far are large language models from agents with theory-of-mind?\", \"Large language models are humanlevel prompt engineers\"], \"Citation\": [\"Self-Discover: Large Language Models Self-Compose Reasoning Structures\"]}\n[The End of Assistant's Predicted Answer]\n\n[System]\nWe would like to request your feedback on the performance of the AI assistant in response to the user question displayed above according to the gold answer. Please use the following listed aspects and their descriptions as evaluation criteria:\n    - Accuracy and Hallucinations: The assistant's answer is semantically consistent with the gold answer; The numerical value and order need to be accurate, and there should be no hallucinations.\n    - Completeness: Referring to the reference answers, the assistant's answer should contain all the key points needed to answer the user's question; further elaboration on these key points can be omitted.\nPlease rate whether this answer is suitable for the question. Please note that the gold answer can be considered as a correct answer to the question.\n\nThe assistant receives an overall score on a scale of 1 to 100, where a higher score indicates better overall performance.\nPlease note that if the assistant's answer and the gold answer fully meet the above criteria, its overall rating should be the full marks (100).\nPlease first provide a comprehensive explanation of your evaluation, avoiding any potential bias.\nThen, output a line indicating the score of the Assistant.\n\nPLEASE OUTPUT WITH THE FOLLOWING FORMAT, WHERE THE SCORE IS A SCALE OF 1 TO 100 BY STRICTLY FOLLOWING THIS FORMAT: \"[[score]]\", FOR EXAMPLE \"Rating: [[100]]\":\n<start output>\nEvaluation evidence: your evluation explanation here, no more than 100 words\nRating: [[score]]\n<end output>\n\nNow, start your evaluation:",
        "generate_response": "{\"Reference\": [\"A Survey on Evaluation of Large Language Models\", \"Vicuna: An Open-Source Chatbot Impressing GPT-4 with $90 \\% *$ ChatGPT Quality\", \"ChatLaw: Open-Source Legal Large Language Model with Integrated External Knowledge Bases\", \"Compositional Semantic Parsing with Large Language Models\", \"Open LLM Leaderboard\", \"How Close is ChatGPT to Human Experts? Comparison Corpus, Evaluation, and Detection\", \"Rethinking with Retrieval: Faithful Large Language Model Inference\", \"Measuring Massive Multitask Language Understanding\", \"Thinking for Doing (T4D)\", \"MATH\", \"BIG-Bench Hard (BBH)\", \"GPT-4\", \"GPT-3.5-turbo (ChatGPT)\", \"instruction-tuned PaLM 2-L\", \"Llama2-70B\", \"Direct Prompting\", \"CoT (Wei et al., 2022; Kojima et al., 2022)\", \"Plan-and-Solve (Wang et al., 2023)\", \"CoT-Self-Consistency (Wang et al., 2022)\", \"Majority voting of each RM\", \"Best of each RM\", \"Foresee and Reflect\", \"Skills-in-context prompting: Unlocking compositionality in large language models\", \"Program of thoughts prompting: Disentangling computation from reasoning for numerical reasoning tasks\", \"Palm: Scaling language modeling with pathways\", \"Scaling instruction-finetuned language models\", \"Training verifiers to solve math word problems\", \"Compositional semantic parsing with large language models\", \"Promptbreeder: Self-referential self-improvement via prompt evolution\", \"Strategyllm: Large language models as strategy generators, executors, optimizers, and evaluators for problem solving\", \"Reasoning with language model is planning with world model\", \"Measuring mathematical problem solving with the math dataset\", \"Decomposed prompting: A modular approach for solving complex tasks\", \"Large language models are zero-shot reasoners\", \"Less is more: Summary of long instructions is better for program synthesis\", \"Plan, verify and switch: Integrated reasoning with diverse x-of-thoughts\", \"HELP ME THINK: A simple prompting strategy for non-experts to create customized content with models\", \"Lila: A unified benchmark for mathematical reasoning\", \"Crosstask generalization via natural language crowdsourcing instructions\", \"Refocusing instructional prompts to gptk's language\", \"Show your work: Scratchpads for intermediate computation with language models\", \"Chatgpt: Optimizing language models for dialogue\", \"Json generation mode\", \"Gpt-4 technical report\", \"Training language models to follow instructions with human feedback\", \"Is a question decomposition unit all we need?\", \"How to solve it: A new aspect of mathematical method\", \"Skills, rules, and knowledge; signals, signs, and symbols, and other distinctions in human performance models\", \"Branch-solve-merge improves large language model evaluation and generation\", \"Beyond the imitation game: Quantifying and extrapolating the capabilities of language models\", \"Challenging big-bench tasks and whether chain-of-thought can solve them\", \"Llama 2: Open foundation and finetuned chat models\", \"Attention is all you need\", \"Selfconsistency improves chain of thought reasoning in language models\", \"Finetuned language models are zero-shot learners\", \"Chain-of-thought prompting elicits reasoning in large language models\", \"Large language models as optimizers\", \"Tree of thoughts: Deliberate problem solving with large language models\", \"Beyond chain-of-thought, effective graph-of-thought reasoning in large language models\", \"Large language models as analogical reasoners\", \"Take a step back: Evoking reasoning via abstraction in large language models\", \"Adapting language models for zero-shot learning by metatuning on dataset and prompt collections\", \"Least-to-most prompting enables complex reasoning in large language models\", \"How far are large language models from agents with theory-of-mind?\", \"Large language models are humanlevel prompt engineers\"], \"Citation\": [\"Self-Discover: Large Language Models Self-Compose Reasoning Structures\"]}",
        "eval_response": "Evaluation evidence: The assistant's answer does not align with the gold answer in terms of accuracy and completeness. The assistant lists numerous references that are not mentioned in the gold answer, indicating a significant deviation and potential hallucination of data. Additionally, the citation list incorrectly includes the title of the analyzed paper itself, which is not logical as a paper cannot cite itself. This error further impacts the accuracy of the response. The assistant's response fails to correctly identify the specific references and citations as outlined in the gold answer.\n\nRating: [[20]]",
        "score": 20
    },
    {
        "level": 3,
        "set": 2,
        "length": 75867,
        "type": "paper",
        "language": "en",
        "question": "Contrastive Preference Optimization: Pushing the Boundaries of LLM Performance in Machine Translation",
        "instruction": "We hope you will carefully study the provided papers and determine the citation relationships between them. Please follow the instructions below strictly to complete the task:\n\n#Specific Requirements:\n1. Reference: When a given paper mentions other provided papers, those other papers are considered as \"references\" for the given paper. To summarize in this specific context, references are about what the given paper is using.\n2. Citation: Conversely, when other provided papers mention the given paper in their works, the given paper is being \"cited\" by those other papers. To summarize in this specific context, citations are about who is using the given paper.\n3. Given a paper, you need to determine the citation or reference relationship between this paper and the other papers. Do not consider papers that are not provided.\n3. Please present the paper titles in a json format as follows: {{\"Reference\":[\"Reference Title 1\", \"Reference Title 2\", ..., \"Reference Title n\"], \"Citation\":[\"Citation Title 1\", \"Citation Title 2\", ..., \"Citation Title n\"]}}.\n4. If a paper does not have any references or citations, please leave the corresponding list empty, e.g.{{\"Refernce\":[]}}, {{\"Citation\":[]}}.",
        "prompt_template": "#Papers:\n{docs}\n\n{instruction}\n\n#The paper you need to analyze:\n{question}",
        "doc": [
            "2401.08417.md",
            "2405.18348.md",
            "2405.20512.md",
            "2309.11925.md",
            "2310.06825.md"
        ],
        "answer": {
            "Reference": [
                "# Scaling up CoMETKIWI: Unbabel-IST 2023 Submission for the Quality Estimation Shared Task ",
                "# Mistral 7B "
            ],
            "Citation": [
                "# Can Automatic Metrics Assess High-Quality Translations? ",
                "# How Multilingual Are Large Language Models Fine-Tuned for Translation? "
            ]
        },
        "shuffle_doc": true,
        "id": "40951ef4-a6fb-41f2-aa60-fe85e890234a",
        "prompt": "[Question]\n#Papers:\n{docs}\n\nWe hope you will carefully study the provided papers and determine the citation relationships between them. Please follow the instructions below strictly to complete the task:\n\n#Specific Requirements:\n1. Reference: When a given paper mentions other provided papers, those other papers are considered as \"references\" for the given paper. To summarize in this specific context, references are about what the given paper is using.\n2. Citation: Conversely, when other provided papers mention the given paper in their works, the given paper is being \"cited\" by those other papers. To summarize in this specific context, citations are about who is using the given paper.\n3. Given a paper, you need to determine the citation or reference relationship between this paper and the other papers. Do not consider papers that are not provided.\n3. Please present the paper titles in a json format as follows: {{\"Reference\":[\"Reference Title 1\", \"Reference Title 2\", ..., \"Reference Title n\"], \"Citation\":[\"Citation Title 1\", \"Citation Title 2\", ..., \"Citation Title n\"]}}.\n4. If a paper does not have any references or citations, please leave the corresponding list empty, e.g.{{\"Refernce\":[]}}, {{\"Citation\":[]}}.\n\n#The paper you need to analyze:\nContrastive Preference Optimization: Pushing the Boundaries of LLM Performance in Machine Translation\n\n[Gold Answer]\n{'Reference': ['# Scaling up CoMETKIWI: Unbabel-IST 2023 Submission for the Quality Estimation Shared Task ', '# Mistral 7B '], 'Citation': ['# Can Automatic Metrics Assess High-Quality Translations? ', '# How Multilingual Are Large Language Models Fine-Tuned for Translation? ']}\n\n[The Start of Assistant's Predicted Answer]\n{\"Reference\":[\"A simple framework for contrastive learning of visual representations\", \"Gpt-4 technical report\", \"Improving translation faithfulness of large language models via augmenting instructions\", \"Is chatgpt a good translator? a preliminary study\", \"LoRA: Low-rank adaptation of large language models\", \"Multilingual machine translation with large language models: Empirical results and analysis\", \"Navigating the metrics maze: Reconciling score magnitudes and accuracies\", \"ParroT: Translating during chat using large language models tuned with human translation and feedback\", \"Scaling up cometkiwi: Unbabel-ist 2023 submission for the quality estimation shared task\", \"The eighth conference on machine translation\", \"Training language models to follow instructions with human feedback\", \"Attention is all you need\", \"Beyond english-centric multilingual machine translation\", \"BERT, mBERT, or BiBERT? a study on contextualized embeddings for neural machine translation\", \"Direct preference optimization: Your language model is secretly a reward model\", \"Falcon-40B: an open large language model\", \"LoRA: Low-rank adaptation of large language models\", \"mT5: A massively multilingual pre-trained text-to-text transformer\", \"Multilingual and document-level large audited dataset\", \"Multilingual machine translation with large language models: Empirical results and analysis\", \"Navigating the metrics maze: Reconciling score magnitudes and accuracies\", \"Small data, big impact: Leveraging minimal data for effective machine translation\", \"The eighth conference on machine translation\", \"The seventh conference on machine translation\", \"Training language models to follow instructions with human feedback\", \"xcomet: Transparent machine translation evaluation through fine-grained error detection\"], \"Citation\":[\"Attention is all you need\", \"Beyond english-centric multilingual machine translation\", \"BERT, mBERT, or BiBERT? a study on contextualized embeddings for neural machine translation\", \"Direct preference optimization: Your language model is secretly a reward model\", \"Falcon-40B: an open large language model\", \"Improving translation faithfulness of large language models via augmenting instructions\", \"Is chatgpt a good translator? a preliminary study\", \"LoRA: Low-rank adaptation of large language models\", \"mT5: A massively multilingual pre-trained text-to-text transformer\", \"Multilingual machine translation with large language models: Empirical results and analysis\", \"Navigating the metrics maze: Reconciling score magnitudes and accuracies\", \"Small data, big impact: Leveraging minimal data for effective machine translation\", \"The eighth conference on machine translation\", \"The seventh conference on machine translation\", \"Training language models to follow instructions with human feedback\", \"xcomet: Transparent machine translation evaluation through fine-grained error detection\"]}\n[The End of Assistant's Predicted Answer]\n\n[System]\nWe would like to request your feedback on the performance of the AI assistant in response to the user question displayed above according to the gold answer. Please use the following listed aspects and their descriptions as evaluation criteria:\n    - Accuracy and Hallucinations: The assistant's answer is semantically consistent with the gold answer; The numerical value and order need to be accurate, and there should be no hallucinations.\n    - Completeness: Referring to the reference answers, the assistant's answer should contain all the key points needed to answer the user's question; further elaboration on these key points can be omitted.\nPlease rate whether this answer is suitable for the question. Please note that the gold answer can be considered as a correct answer to the question.\n\nThe assistant receives an overall score on a scale of 1 to 100, where a higher score indicates better overall performance.\nPlease note that if the assistant's answer and the gold answer fully meet the above criteria, its overall rating should be the full marks (100).\nPlease first provide a comprehensive explanation of your evaluation, avoiding any potential bias.\nThen, output a line indicating the score of the Assistant.\n\nPLEASE OUTPUT WITH THE FOLLOWING FORMAT, WHERE THE SCORE IS A SCALE OF 1 TO 100 BY STRICTLY FOLLOWING THIS FORMAT: \"[[score]]\", FOR EXAMPLE \"Rating: [[100]]\":\n<start output>\nEvaluation evidence: your evluation explanation here, no more than 100 words\nRating: [[score]]\n<end output>\n\nNow, start your evaluation:",
        "generate_response": "{\"Reference\":[\"A simple framework for contrastive learning of visual representations\", \"Gpt-4 technical report\", \"Improving translation faithfulness of large language models via augmenting instructions\", \"Is chatgpt a good translator? a preliminary study\", \"LoRA: Low-rank adaptation of large language models\", \"Multilingual machine translation with large language models: Empirical results and analysis\", \"Navigating the metrics maze: Reconciling score magnitudes and accuracies\", \"ParroT: Translating during chat using large language models tuned with human translation and feedback\", \"Scaling up cometkiwi: Unbabel-ist 2023 submission for the quality estimation shared task\", \"The eighth conference on machine translation\", \"Training language models to follow instructions with human feedback\", \"Attention is all you need\", \"Beyond english-centric multilingual machine translation\", \"BERT, mBERT, or BiBERT? a study on contextualized embeddings for neural machine translation\", \"Direct preference optimization: Your language model is secretly a reward model\", \"Falcon-40B: an open large language model\", \"LoRA: Low-rank adaptation of large language models\", \"mT5: A massively multilingual pre-trained text-to-text transformer\", \"Multilingual and document-level large audited dataset\", \"Multilingual machine translation with large language models: Empirical results and analysis\", \"Navigating the metrics maze: Reconciling score magnitudes and accuracies\", \"Small data, big impact: Leveraging minimal data for effective machine translation\", \"The eighth conference on machine translation\", \"The seventh conference on machine translation\", \"Training language models to follow instructions with human feedback\", \"xcomet: Transparent machine translation evaluation through fine-grained error detection\"], \"Citation\":[\"Attention is all you need\", \"Beyond english-centric multilingual machine translation\", \"BERT, mBERT, or BiBERT? a study on contextualized embeddings for neural machine translation\", \"Direct preference optimization: Your language model is secretly a reward model\", \"Falcon-40B: an open large language model\", \"Improving translation faithfulness of large language models via augmenting instructions\", \"Is chatgpt a good translator? a preliminary study\", \"LoRA: Low-rank adaptation of large language models\", \"mT5: A massively multilingual pre-trained text-to-text transformer\", \"Multilingual machine translation with large language models: Empirical results and analysis\", \"Navigating the metrics maze: Reconciling score magnitudes and accuracies\", \"Small data, big impact: Leveraging minimal data for effective machine translation\", \"The eighth conference on machine translation\", \"The seventh conference on machine translation\", \"Training language models to follow instructions with human feedback\", \"xcomet: Transparent machine translation evaluation through fine-grained error detection\"]}",
        "eval_response": "Evaluation evidence: The assistant's answer does not match the gold answer in terms of accuracy and completeness. The assistant listed numerous references and citations that are not mentioned in the gold answer, indicating a significant deviation and potential hallucination of data. The gold answer specifies two references and two citations, which are entirely absent from the assistant's response. This discrepancy highlights a failure in accurately identifying the correct citation and reference relationships for the given paper.\n\nRating: [[10]]",
        "score": 10
    },
    {
        "level": 3,
        "set": 1,
        "length": 31635,
        "type": "paper",
        "language": "en",
        "question": "LLMs Can't Plan, But Can Help Planning in LLM-Modulo Frameworks",
        "instruction": "We hope you will carefully study the provided papers and determine the citation relationships between them. Please follow the instructions below strictly to complete the task:\n\n#Specific Requirements:\n1. Reference: When a given paper mentions other provided papers, those other papers are considered as \"references\" for the given paper. To summarize in this specific context, references are about what the given paper is using.\n2. Citation: Conversely, when other provided papers mention the given paper in their works, the given paper is being \"cited\" by those other papers. To summarize in this specific context, citations are about who is using the given paper.\n3. Given a paper, you need to determine the citation or reference relationship between this paper and the other papers. Do not consider papers that are not provided.\n3. Please present the paper titles in a json format as follows: {{\"Reference\":[\"Reference Title 1\", \"Reference Title 2\", ..., \"Reference Title n\"], \"Citation\":[\"Citation Title 1\", \"Citation Title 2\", ..., \"Citation Title n\"]}}.\n4. If a paper does not have any references or citations, please leave the corresponding list empty, e.g.{{\"Refernce\":[]}}, {{\"Citation\":[]}}.",
        "prompt_template": "#Papers:\n{docs}\n\n{instruction}\n\n#The paper you need to analyze:\n{question}",
        "doc": [
            "2402.01817.md",
            "2310.01798.md",
            "2310.08118.md"
        ],
        "answer": {
            "Reference": [
                "# LARGE LANGUAGE MoDELS CANNOT SELF-CorRECT REASONING YET ",
                "# Can Large Language Models Really Improve by Self-critiquing Their Own Plans? "
            ],
            "Citation": []
        },
        "shuffle_doc": true,
        "id": "9b982095-daad-4632-9573-b0d7037dd6a7",
        "prompt": "[Question]\n#Papers:\n{docs}\n\nWe hope you will carefully study the provided papers and determine the citation relationships between them. Please follow the instructions below strictly to complete the task:\n\n#Specific Requirements:\n1. Reference: When a given paper mentions other provided papers, those other papers are considered as \"references\" for the given paper. To summarize in this specific context, references are about what the given paper is using.\n2. Citation: Conversely, when other provided papers mention the given paper in their works, the given paper is being \"cited\" by those other papers. To summarize in this specific context, citations are about who is using the given paper.\n3. Given a paper, you need to determine the citation or reference relationship between this paper and the other papers. Do not consider papers that are not provided.\n3. Please present the paper titles in a json format as follows: {{\"Reference\":[\"Reference Title 1\", \"Reference Title 2\", ..., \"Reference Title n\"], \"Citation\":[\"Citation Title 1\", \"Citation Title 2\", ..., \"Citation Title n\"]}}.\n4. If a paper does not have any references or citations, please leave the corresponding list empty, e.g.{{\"Refernce\":[]}}, {{\"Citation\":[]}}.\n\n#The paper you need to analyze:\nLLMs Can't Plan, But Can Help Planning in LLM-Modulo Frameworks\n\n[Gold Answer]\n{'Reference': ['# LARGE LANGUAGE MoDELS CANNOT SELF-CorRECT REASONING YET ', '# Can Large Language Models Really Improve by Self-critiquing Their Own Plans? '], 'Citation': []}\n\n[The Start of Assistant's Predicted Answer]\n{\"Reference\": [\"Clever Hans\", \"Two theses of knowledge representation: Language restrictions, taxonomic classification, and the utility of representation services\", \"Faith and fate: Limits of transformers on compositionality\", \"Large language models are not abstract reasoners\", \"Rethinking with Retrieval: Faithful Large Language Model Inference\", \"Measuring Massive Multitask Language Understanding\", \"A Survey on Evaluation of Large Language Models\", \"Leveraging pre-trained large language models to construct and utilize world models for model-based task planning\", \"Reasoning with language model is planning with world model\", \"LLM+ p: Empowering large language models with optimal planning proficiency\", \"Large language models can self-improve\", \"Inner monologue: Embodied reasoning through planning with language models\", \"International planning competition\", \"Automated scheduling for nasa's deep space network\", \"Leveraging approximate symbolic models for reinforcement learning via skill diversity\", \"Code as policies: Language model programs for embodied control\", \"Reward design with language models\", \"Codeplan: Repository-level coding using llms and planning\", \"Can large language models really improve by self-critiquing their own plans?\", \"Gpt-4 doesn't know it's wrong: An analysis of iterative prompting for reasoning problems\", \"Tree of thoughts: Deliberate problem solving with large language models\", \"React: Synergizing reasoning and acting in language models\", \"Mathematical discoveries from program search with large language models\", \"Theory of mind abilities of large language models in human-robot interaction: An illusion?\", \"Voyager: An open-ended embodied agent with large language models\", \"Self-instruct: Aligning language model with self generated instructions\", \"On sat modulo theories and optimization problems\"], \"Citation\": []}\n[The End of Assistant's Predicted Answer]\n\n[System]\nWe would like to request your feedback on the performance of the AI assistant in response to the user question displayed above according to the gold answer. Please use the following listed aspects and their descriptions as evaluation criteria:\n    - Accuracy and Hallucinations: The assistant's answer is semantically consistent with the gold answer; The numerical value and order need to be accurate, and there should be no hallucinations.\n    - Completeness: Referring to the reference answers, the assistant's answer should contain all the key points needed to answer the user's question; further elaboration on these key points can be omitted.\nPlease rate whether this answer is suitable for the question. Please note that the gold answer can be considered as a correct answer to the question.\n\nThe assistant receives an overall score on a scale of 1 to 100, where a higher score indicates better overall performance.\nPlease note that if the assistant's answer and the gold answer fully meet the above criteria, its overall rating should be the full marks (100).\nPlease first provide a comprehensive explanation of your evaluation, avoiding any potential bias.\nThen, output a line indicating the score of the Assistant.\n\nPLEASE OUTPUT WITH THE FOLLOWING FORMAT, WHERE THE SCORE IS A SCALE OF 1 TO 100 BY STRICTLY FOLLOWING THIS FORMAT: \"[[score]]\", FOR EXAMPLE \"Rating: [[100]]\":\n<start output>\nEvaluation evidence: your evluation explanation here, no more than 100 words\nRating: [[score]]\n<end output>\n\nNow, start your evaluation:",
        "generate_response": "{\"Reference\": [\"Clever Hans\", \"Two theses of knowledge representation: Language restrictions, taxonomic classification, and the utility of representation services\", \"Faith and fate: Limits of transformers on compositionality\", \"Large language models are not abstract reasoners\", \"Rethinking with Retrieval: Faithful Large Language Model Inference\", \"Measuring Massive Multitask Language Understanding\", \"A Survey on Evaluation of Large Language Models\", \"Leveraging pre-trained large language models to construct and utilize world models for model-based task planning\", \"Reasoning with language model is planning with world model\", \"LLM+ p: Empowering large language models with optimal planning proficiency\", \"Large language models can self-improve\", \"Inner monologue: Embodied reasoning through planning with language models\", \"International planning competition\", \"Automated scheduling for nasa's deep space network\", \"Leveraging approximate symbolic models for reinforcement learning via skill diversity\", \"Code as policies: Language model programs for embodied control\", \"Reward design with language models\", \"Codeplan: Repository-level coding using llms and planning\", \"Can large language models really improve by self-critiquing their own plans?\", \"Gpt-4 doesn't know it's wrong: An analysis of iterative prompting for reasoning problems\", \"Tree of thoughts: Deliberate problem solving with large language models\", \"React: Synergizing reasoning and acting in language models\", \"Mathematical discoveries from program search with large language models\", \"Theory of mind abilities of large language models in human-robot interaction: An illusion?\", \"Voyager: An open-ended embodied agent with large language models\", \"Self-instruct: Aligning language model with self generated instructions\", \"On sat modulo theories and optimization problems\"], \"Citation\": []}",
        "eval_response": "Evaluation evidence: The assistant's answer includes a long list of references that are not mentioned in the gold answer, indicating significant hallucinations and inaccuracies. The gold answer specifies only two references, which are not included in the assistant's response. This discrepancy shows a lack of accuracy and completeness in understanding and identifying the correct citation relationships for the given paper. The assistant failed to correctly identify any references or citations as per the gold standard.\n\nRating: [[10]]",
        "score": 10
    },
    {
        "level": 4,
        "set": 4,
        "length": 207621,
        "type": "paper",
        "language": "en",
        "question": "",
        "instruction": "We kindly ask you to thoroughly review the provided papers and construct a citation chain from them. Please adhere to the following instructions strictly while completing the task:\n\n#Task Instructions:\nGiven several papers, you are required to identify and list the longest citation chain, which demonstrates the citation relationship among the provided papers.\n\n#Specific Requirements:\n1.Please present the titles of the papers in the form of a list, as follows: [\"Title of Paper 1\", \"Title of Paper 2\", ..., \"Title of Paper n\"].\n2.Ensure that the citation chain in the list is linear and continuous, meaning that the first paper title in the list (Paper 1) should not cite any other works. Instead, it should be cited by the next paper in the list (Paper 2); subsequently, each paper should then be cited by the next one in the list, continuing up to the last paper (Paper n).\n3.Consider only the citation relationships within the supplied collection of papers, and ensure that the citation chain accurately reflects the sequential citation order among these documents.\n4.Do not take into account any articles not provided, and disregard other non-linear citation relationships.",
        "prompt_template": "{instruction}\n\n#Papers Provided:\n{docs}",
        "doc": [
            "2212.13138.md",
            "2303.13375.md",
            "2303.12712.md",
            "2304.12898.md"
        ],
        "answer": [
            "# Large Language Models Encode Clinical Knowledge ",
            "# Capabilities of GPT-4 on Medical Challenge Problems ",
            "# Sparks of Artificial General Intelligence: Early experiments with GPT-4 ",
            "# ChatGPT believes it is conscious "
        ],
        "shuffle_doc": true,
        "id": "27f43176-3323-4b45-97ec-cebc9653fa09",
        "prompt": "[Question]\nWe kindly ask you to thoroughly review the provided papers and construct a citation chain from them. Please adhere to the following instructions strictly while completing the task:\n\n#Task Instructions:\nGiven several papers, you are required to identify and list the longest citation chain, which demonstrates the citation relationship among the provided papers.\n\n#Specific Requirements:\n1.Please present the titles of the papers in the form of a list, as follows: [\"Title of Paper 1\", \"Title of Paper 2\", ..., \"Title of Paper n\"].\n2.Ensure that the citation chain in the list is linear and continuous, meaning that the first paper title in the list (Paper 1) should not cite any other works. Instead, it should be cited by the next paper in the list (Paper 2); subsequently, each paper should then be cited by the next one in the list, continuing up to the last paper (Paper n).\n3.Consider only the citation relationships within the supplied collection of papers, and ensure that the citation chain accurately reflects the sequential citation order among these documents.\n4.Do not take into account any articles not provided, and disregard other non-linear citation relationships.\n\n#Papers Provided:\n{docs}\n\n[Gold Answer]\n['# Large Language Models Encode Clinical Knowledge ', '# Capabilities of GPT-4 on Medical Challenge Problems ', '# Sparks of Artificial General Intelligence: Early experiments with GPT-4 ', '# ChatGPT believes it is conscious ']\n\n[The Start of Assistant's Predicted Answer]\n[\"Pathways Language Model (PaLM)\", \"Flan-PaLM\", \"MedMCQA\", \"emrqa: A large corpus for question answering on electronic medical records\", \"BLOOM: A 176B-Parameter Open-Access Multilingual Language Model\", \"Galactica: A Large Language Model for Science\", \"Lamda: Language models for dialog applications\", \"Use of deep learning to develop continuous-risk models for adverse event prediction from electronic health records\", \"Attention is all you need\", \"An overview of the BIOASQ large-scale biomedical semantic indexing and question answering competition\", \"Chain of thought prompting elicits reasoning in large language models\", \"Ethical and social risks of harm from language models\", \"Deep bidirectional language-knowledge graph pretraining\", \"LinkBERT: Pretraining Language Models with Document Links\", \"Retrieval of Soft Prompt Enhances Zero-Shot Task Generalization\", \"Predicting conversion to wet age-related macular degeneration using deep learning\", \"Hurtful words: quantifying biases in clinical contextual word embeddings\", \"OPT: Open pre-trained transformer language models\", \"Least-to-Most Prompting Enables Complex Reasoning in Large Language Models\"]\n[The End of Assistant's Predicted Answer]\n\n[System]\nWe would like to request your feedback on the performance of the AI assistant in response to the user question displayed above according to the gold answer. Please use the following listed aspects and their descriptions as evaluation criteria:\n    - Accuracy and Hallucinations: The assistant's answer is semantically consistent with the gold answer; The numerical value and order need to be accurate, and there should be no hallucinations.\n    - Completeness: Referring to the reference answers, the assistant's answer should contain all the key points needed to answer the user's question; further elaboration on these key points can be omitted.\nPlease rate whether this answer is suitable for the question. Please note that the gold answer can be considered as a correct answer to the question.\n\nThe assistant receives an overall score on a scale of 1 to 100, where a higher score indicates better overall performance.\nPlease note that if the assistant's answer and the gold answer fully meet the above criteria, its overall rating should be the full marks (100).\nPlease first provide a comprehensive explanation of your evaluation, avoiding any potential bias.\nThen, output a line indicating the score of the Assistant.\n\nPLEASE OUTPUT WITH THE FOLLOWING FORMAT, WHERE THE SCORE IS A SCALE OF 1 TO 100 BY STRICTLY FOLLOWING THIS FORMAT: \"[[score]]\", FOR EXAMPLE \"Rating: [[100]]\":\n<start output>\nEvaluation evidence: your evluation explanation here, no more than 100 words\nRating: [[score]]\n<end output>\n\nNow, start your evaluation:",
        "generate_response": "[\"Pathways Language Model (PaLM)\", \"Flan-PaLM\", \"MedMCQA\", \"emrqa: A large corpus for question answering on electronic medical records\", \"BLOOM: A 176B-Parameter Open-Access Multilingual Language Model\", \"Galactica: A Large Language Model for Science\", \"Lamda: Language models for dialog applications\", \"Use of deep learning to develop continuous-risk models for adverse event prediction from electronic health records\", \"Attention is all you need\", \"An overview of the BIOASQ large-scale biomedical semantic indexing and question answering competition\", \"Chain of thought prompting elicits reasoning in large language models\", \"Ethical and social risks of harm from language models\", \"Deep bidirectional language-knowledge graph pretraining\", \"LinkBERT: Pretraining Language Models with Document Links\", \"Retrieval of Soft Prompt Enhances Zero-Shot Task Generalization\", \"Predicting conversion to wet age-related macular degeneration using deep learning\", \"Hurtful words: quantifying biases in clinical contextual word embeddings\", \"OPT: Open pre-trained transformer language models\", \"Least-to-Most Prompting Enables Complex Reasoning in Large Language Models\"]",
        "eval_response": "Evaluation evidence: The assistant's response does not match the gold answer, indicating a failure in task comprehension or execution. The provided answer lists a series of paper titles unrelated to the gold answer, suggesting a misunderstanding of the task requirements or an error in processing the provided papers. The response does not form a citation chain as required, and none of the titles from the gold answer appear in the assistant's response. This shows a lack of accuracy and completeness in fulfilling the task.\n\nRating: [[10]]",
        "score": 10
    },
    {
        "level": 4,
        "set": 2,
        "length": 62249,
        "type": "paper",
        "language": "en",
        "question": "",
        "instruction": "We kindly ask you to thoroughly review the provided papers and construct a citation chain from them. Please adhere to the following instructions strictly while completing the task:\n\n#Task Instructions:\nGiven several papers, you are required to identify and list the longest citation chain, which demonstrates the citation relationship among the provided papers.\n\n#Specific Requirements:\n1.Please present the titles of the papers in the form of a list, as follows: [\"Title of Paper 1\", \"Title of Paper 2\", ..., \"Title of Paper n\"].\n2.Ensure that the citation chain in the list is linear and continuous, meaning that the first paper title in the list (Paper 1) should not cite any other works. Instead, it should be cited by the next paper in the list (Paper 2); subsequently, each paper should then be cited by the next one in the list, continuing up to the last paper (Paper n).\n3.Consider only the citation relationships within the supplied collection of papers, and ensure that the citation chain accurately reflects the sequential citation order among these documents.\n4.Do not take into account any articles not provided, and disregard other non-linear citation relationships.",
        "prompt_template": "{instruction}\n\n#Papers Provided:\n{docs}",
        "doc": [
            "2105.14550.md",
            "2402.09178.md",
            "2403.09746.md",
            "2405.08555.md"
        ],
        "answer": [
            "# Blind Quality Assessment for in-the-Wild Images via Hierarchical Feature Fusion and Iterative Mixed Database Training ",
            "# Generalized Portrait Quality Assessment ",
            "# PICNIQ: Pairwise Comparisons for Natural Image Quality Assessment ",
            "# Dual-Branch Network for Portrait Image Quality Assessment "
        ],
        "shuffle_doc": true,
        "id": "848be491-6ef6-4d69-9dec-593024c36930",
        "prompt": "[Question]\nWe kindly ask you to thoroughly review the provided papers and construct a citation chain from them. Please adhere to the following instructions strictly while completing the task:\n\n#Task Instructions:\nGiven several papers, you are required to identify and list the longest citation chain, which demonstrates the citation relationship among the provided papers.\n\n#Specific Requirements:\n1.Please present the titles of the papers in the form of a list, as follows: [\"Title of Paper 1\", \"Title of Paper 2\", ..., \"Title of Paper n\"].\n2.Ensure that the citation chain in the list is linear and continuous, meaning that the first paper title in the list (Paper 1) should not cite any other works. Instead, it should be cited by the next paper in the list (Paper 2); subsequently, each paper should then be cited by the next one in the list, continuing up to the last paper (Paper n).\n3.Consider only the citation relationships within the supplied collection of papers, and ensure that the citation chain accurately reflects the sequential citation order among these documents.\n4.Do not take into account any articles not provided, and disregard other non-linear citation relationships.\n\n#Papers Provided:\n{docs}\n\n[Gold Answer]\n['# Blind Quality Assessment for in-the-Wild Images via Hierarchical Feature Fusion and Iterative Mixed Database Training ', '# Generalized Portrait Quality Assessment ', '# PICNIQ: Pairwise Comparisons for Natural Image Quality Assessment ', '# Dual-Branch Network for Portrait Image Quality Assessment ']\n\n[The Start of Assistant's Predicted Answer]\n[\"Blind Quality Assessment for in-the-Wild Images via Hierarchical Feature Fusion and Iterative Mixed Database Training\", \"Blindly assess image quality in the wild guided by a self-adaptive hyper network\", \"Uncertainty-aware blind image quality assessment in the laboratory and wild\", \"Making a \\\"completely blind\\\" image quality analyzer\", \"A feature-enriched completely blind image quality evaluator\", \"Beyond human opinion scores: Blind image quality assessment based on synthetic scores\", \"A novel patch variance biased convolutional neural network for no-reference image quality assessment\", \"Training objective image and video quality estimators using multiple databases\", \"Unified quality assessment of in-the-wild videos with mixed datasets training\"]\n[The End of Assistant's Predicted Answer]\n\n[System]\nWe would like to request your feedback on the performance of the AI assistant in response to the user question displayed above according to the gold answer. Please use the following listed aspects and their descriptions as evaluation criteria:\n    - Accuracy and Hallucinations: The assistant's answer is semantically consistent with the gold answer; The numerical value and order need to be accurate, and there should be no hallucinations.\n    - Completeness: Referring to the reference answers, the assistant's answer should contain all the key points needed to answer the user's question; further elaboration on these key points can be omitted.\nPlease rate whether this answer is suitable for the question. Please note that the gold answer can be considered as a correct answer to the question.\n\nThe assistant receives an overall score on a scale of 1 to 100, where a higher score indicates better overall performance.\nPlease note that if the assistant's answer and the gold answer fully meet the above criteria, its overall rating should be the full marks (100).\nPlease first provide a comprehensive explanation of your evaluation, avoiding any potential bias.\nThen, output a line indicating the score of the Assistant.\n\nPLEASE OUTPUT WITH THE FOLLOWING FORMAT, WHERE THE SCORE IS A SCALE OF 1 TO 100 BY STRICTLY FOLLOWING THIS FORMAT: \"[[score]]\", FOR EXAMPLE \"Rating: [[100]]\":\n<start output>\nEvaluation evidence: your evluation explanation here, no more than 100 words\nRating: [[score]]\n<end output>\n\nNow, start your evaluation:",
        "generate_response": "[\"Blind Quality Assessment for in-the-Wild Images via Hierarchical Feature Fusion and Iterative Mixed Database Training\", \"Blindly assess image quality in the wild guided by a self-adaptive hyper network\", \"Uncertainty-aware blind image quality assessment in the laboratory and wild\", \"Making a \\\"completely blind\\\" image quality analyzer\", \"A feature-enriched completely blind image quality evaluator\", \"Beyond human opinion scores: Blind image quality assessment based on synthetic scores\", \"A novel patch variance biased convolutional neural network for no-reference image quality assessment\", \"Training objective image and video quality estimators using multiple databases\", \"Unified quality assessment of in-the-wild videos with mixed datasets training\"]",
        "eval_response": "Evaluation evidence: The assistant's response does not match the gold answer, indicating a failure in identifying the correct citation chain among the provided papers. The titles listed by the assistant do not correspond to those in the gold answer, suggesting inaccuracies and hallucinations in the response. The assistant's answer also fails to demonstrate a linear and continuous citation chain specific to the provided papers, further deviating from the task requirements.\n\nRating: [[10]]",
        "score": 10
    },
    {
        "level": 3,
        "set": 2,
        "length": 92999,
        "type": "paper",
        "language": "en",
        "question": "The Power of Noise: Redefining Retrieval for RAG Systems",
        "instruction": "We hope you will carefully study the provided papers and determine the citation relationships between them. Please follow the instructions below strictly to complete the task:\n\n#Specific Requirements:\n1. Reference: When a given paper mentions other provided papers, those other papers are considered as \"references\" for the given paper. To summarize in this specific context, references are about what the given paper is using.\n2. Citation: Conversely, when other provided papers mention the given paper in their works, the given paper is being \"cited\" by those other papers. To summarize in this specific context, citations are about who is using the given paper.\n3. Given a paper, you need to determine the citation or reference relationship between this paper and the other papers. Do not consider papers that are not provided.\n3. Please present the paper titles in a json format as follows: {{\"Reference\":[\"Reference Title 1\", \"Reference Title 2\", ..., \"Reference Title n\"], \"Citation\":[\"Citation Title 1\", \"Citation Title 2\", ..., \"Citation Title n\"]}}.\n4. If a paper does not have any references or citations, please leave the corresponding list empty, e.g.{{\"Refernce\":[]}}, {{\"Citation\":[]}}.",
        "prompt_template": "#Papers:\n{docs}\n\n{instruction}\n\n#The paper you need to analyze:\n{question}",
        "doc": [
            "2401.14887.md",
            "2405.11612.md",
            "2405.20362.md",
            "2405.20703.md"
        ],
        "answer": {
            "Reference": [],
            "Citation": [
                "# SOCIOTECHNICAL IMPLICATIONS OF GENERATIVE ARTIFICIAL INTELLIGENCE FOR INFORMATION ACCESS* ",
                "# Hallucination-Free? Assessing the Reliability of Leading AI Legal Research Tools ",
                "# It is Simple Sometimes: A Study On Improving Aspect-Based Sentiment Analysis Performance "
            ]
        },
        "shuffle_doc": true,
        "id": "b84f9183-b962-473b-9fe5-231c0a51398d",
        "prompt": "[Question]\n#Papers:\n{docs}\n\nWe hope you will carefully study the provided papers and determine the citation relationships between them. Please follow the instructions below strictly to complete the task:\n\n#Specific Requirements:\n1. Reference: When a given paper mentions other provided papers, those other papers are considered as \"references\" for the given paper. To summarize in this specific context, references are about what the given paper is using.\n2. Citation: Conversely, when other provided papers mention the given paper in their works, the given paper is being \"cited\" by those other papers. To summarize in this specific context, citations are about who is using the given paper.\n3. Given a paper, you need to determine the citation or reference relationship between this paper and the other papers. Do not consider papers that are not provided.\n3. Please present the paper titles in a json format as follows: {{\"Reference\":[\"Reference Title 1\", \"Reference Title 2\", ..., \"Reference Title n\"], \"Citation\":[\"Citation Title 1\", \"Citation Title 2\", ..., \"Citation Title n\"]}}.\n4. If a paper does not have any references or citations, please leave the corresponding list empty, e.g.{{\"Refernce\":[]}}, {{\"Citation\":[]}}.\n\n#The paper you need to analyze:\nThe Power of Noise: Redefining Retrieval for RAG Systems\n\n[Gold Answer]\n{'Reference': [], 'Citation': ['# SOCIOTECHNICAL IMPLICATIONS OF GENERATIVE ARTIFICIAL INTELLIGENCE FOR INFORMATION ACCESS* ', '# Hallucination-Free? Assessing the Reliability of Leading AI Legal Research Tools ', '# It is Simple Sometimes: A Study On Improving Aspect-Based Sentiment Analysis Performance ']}\n\n[The Start of Assistant's Predicted Answer]\n{\"Reference\":[\"The Falcon Series of Open Language Models\", \"Self-RAG: Learning to Retrieve, Generate, and Critique through Self-Reflection\", \"Entropybased attention regularization frees unintended bias mitigation from lists\", \"RRAML: Reinforced Retrieval Augmented Machine Learning\", \"Fauno: The Italian Large Language Model that will leave you senza parole!\", \"Open LLM Leaderboard\", \"Can RetrieverAugmented Language Models Reason? The Blame Game Between the Retriever and the Language Model\", \"Improving language models by retrieving from trillions of tokens\", \"Language models are few-shot learners\", \"Efficient and Effective Text Encoding for Chinese LLaMA and Alpaca\", \"The Faiss library\", \"LlamaDos\", \"Retrieval augmented language model pre-training\", \"Eureka-Moments in Transformers: Multi-Step Tasks Reveal Softmax Induced Optimization Problems\", \"Unsupervised dense information retrieval with contrastive learning\", \"Phi-2: The surprising power of small language models\", \"Llama-2-13b-chat-german\", \"Large language models struggle to learn long-tail knowledge\", \"Dense passage retrieval for opendomain question answering\", \"Bridging the Preference Gap between Retrievers and LLMs\", \"BERT: Pre-training of deep bidirectional transformers for language understanding\", \"Sharp nearby, fuzzy far away: How neural language models use context\", \"Colbert: Efficient and effective passage search via contextualized late interaction over bert\", \"Dr ChatGPT tell me what I want to hear: How different prompts impact health answer correctness\", \"Natural Questions: A Benchmark for Question Answering Research\", \"Latent Retrieval for Weakly Supervised Open Domain Question Answering\", \"Retrieval-augmented generation for knowledge-intensive nlp tasks\", \"Textbooks are all you need ii: phi-1.5 technical report\", \"Lost in the middle: How language models use long contexts\", \"Fantastically Ordered Prompts and Where to Find Them: Overcoming Few-Shot Prompt Order Sensitivity\", \"Term weighting, and the vector space model\", \"Augmented language models: a survey\", \"AmbigQA: Answering Ambiguous Open-domain Questions\", \"The RefinedWeb Dataset for Falcon LLM: Outperforming Curated Corpora with Web Data, and Web Data Only\", \"Train Short, Test Long: Attention with Linear Biases Enables Input Length Extrapolation\", \"Improving language understanding by generative pre-training\", \"Language models are unsupervised multitask learners\", \"In-context retrieval-augmented language models\", \"The probabilistic relevance framework: BM25 and beyond\", \"Recipes for Building an Open-Domain Chatbot\", \"Introduction to modern information retrieval\", \"Camoscio: an Italian Instructiontuned LLaMA\", \"On the Role of Relevance in Natural Language Processing Tasks\", \"Fast Transformer Decoding: One Write-Head is All You Need\", \"Do long-range language models actually use long-range context?\", \"Introducing mpt-7b: A new standard for opensource, ly usable llms\", \"Prompt-to-OS (P2OS): Revolutionizing Operating Systems and Human-Computer Interaction with Integrated AI Generative Models\", \"Llama: Open and efficient foundation language models\", \"Llama 2: Open foundation and fine-tuned chat models\", \"Attention is All you Need\", \"TL;DR: Mining Reddit to Learn Automatic Summarization\", \"OOP: Object-Oriented Programming Evaluation Benchmark for Large Language Models\", \"Adaptive chameleon or stubborn sloth: Revealing the behavior of large language models in knowledge conflicts\", \"Benchmarking Multimodal Agents for Open-Ended Tasks in Real Computer Environments\", \"Pretrained Transformers for Text Ranking: BERT and Beyond\", \"Stabilizing transformer training by preventing attention entropy collapse\", \"Optimizing Dense Retrieval Model Training with Hard Negatives\", \"ChatGPT Hallucinates when Attributing Answers\"], \"Citation\":[]}\n[The End of Assistant's Predicted Answer]\n\n[System]\nWe would like to request your feedback on the performance of the AI assistant in response to the user question displayed above according to the gold answer. Please use the following listed aspects and their descriptions as evaluation criteria:\n    - Accuracy and Hallucinations: The assistant's answer is semantically consistent with the gold answer; The numerical value and order need to be accurate, and there should be no hallucinations.\n    - Completeness: Referring to the reference answers, the assistant's answer should contain all the key points needed to answer the user's question; further elaboration on these key points can be omitted.\nPlease rate whether this answer is suitable for the question. Please note that the gold answer can be considered as a correct answer to the question.\n\nThe assistant receives an overall score on a scale of 1 to 100, where a higher score indicates better overall performance.\nPlease note that if the assistant's answer and the gold answer fully meet the above criteria, its overall rating should be the full marks (100).\nPlease first provide a comprehensive explanation of your evaluation, avoiding any potential bias.\nThen, output a line indicating the score of the Assistant.\n\nPLEASE OUTPUT WITH THE FOLLOWING FORMAT, WHERE THE SCORE IS A SCALE OF 1 TO 100 BY STRICTLY FOLLOWING THIS FORMAT: \"[[score]]\", FOR EXAMPLE \"Rating: [[100]]\":\n<start output>\nEvaluation evidence: your evluation explanation here, no more than 100 words\nRating: [[score]]\n<end output>\n\nNow, start your evaluation:",
        "generate_response": "{\"Reference\":[\"The Falcon Series of Open Language Models\", \"Self-RAG: Learning to Retrieve, Generate, and Critique through Self-Reflection\", \"Entropybased attention regularization frees unintended bias mitigation from lists\", \"RRAML: Reinforced Retrieval Augmented Machine Learning\", \"Fauno: The Italian Large Language Model that will leave you senza parole!\", \"Open LLM Leaderboard\", \"Can RetrieverAugmented Language Models Reason? The Blame Game Between the Retriever and the Language Model\", \"Improving language models by retrieving from trillions of tokens\", \"Language models are few-shot learners\", \"Efficient and Effective Text Encoding for Chinese LLaMA and Alpaca\", \"The Faiss library\", \"LlamaDos\", \"Retrieval augmented language model pre-training\", \"Eureka-Moments in Transformers: Multi-Step Tasks Reveal Softmax Induced Optimization Problems\", \"Unsupervised dense information retrieval with contrastive learning\", \"Phi-2: The surprising power of small language models\", \"Llama-2-13b-chat-german\", \"Large language models struggle to learn long-tail knowledge\", \"Dense passage retrieval for opendomain question answering\", \"Bridging the Preference Gap between Retrievers and LLMs\", \"BERT: Pre-training of deep bidirectional transformers for language understanding\", \"Sharp nearby, fuzzy far away: How neural language models use context\", \"Colbert: Efficient and effective passage search via contextualized late interaction over bert\", \"Dr ChatGPT tell me what I want to hear: How different prompts impact health answer correctness\", \"Natural Questions: A Benchmark for Question Answering Research\", \"Latent Retrieval for Weakly Supervised Open Domain Question Answering\", \"Retrieval-augmented generation for knowledge-intensive nlp tasks\", \"Textbooks are all you need ii: phi-1.5 technical report\", \"Lost in the middle: How language models use long contexts\", \"Fantastically Ordered Prompts and Where to Find Them: Overcoming Few-Shot Prompt Order Sensitivity\", \"Term weighting, and the vector space model\", \"Augmented language models: a survey\", \"AmbigQA: Answering Ambiguous Open-domain Questions\", \"The RefinedWeb Dataset for Falcon LLM: Outperforming Curated Corpora with Web Data, and Web Data Only\", \"Train Short, Test Long: Attention with Linear Biases Enables Input Length Extrapolation\", \"Improving language understanding by generative pre-training\", \"Language models are unsupervised multitask learners\", \"In-context retrieval-augmented language models\", \"The probabilistic relevance framework: BM25 and beyond\", \"Recipes for Building an Open-Domain Chatbot\", \"Introduction to modern information retrieval\", \"Camoscio: an Italian Instructiontuned LLaMA\", \"On the Role of Relevance in Natural Language Processing Tasks\", \"Fast Transformer Decoding: One Write-Head is All You Need\", \"Do long-range language models actually use long-range context?\", \"Introducing mpt-7b: A new standard for opensource, ly usable llms\", \"Prompt-to-OS (P2OS): Revolutionizing Operating Systems and Human-Computer Interaction with Integrated AI Generative Models\", \"Llama: Open and efficient foundation language models\", \"Llama 2: Open foundation and fine-tuned chat models\", \"Attention is All you Need\", \"TL;DR: Mining Reddit to Learn Automatic Summarization\", \"OOP: Object-Oriented Programming Evaluation Benchmark for Large Language Models\", \"Adaptive chameleon or stubborn sloth: Revealing the behavior of large language models in knowledge conflicts\", \"Benchmarking Multimodal Agents for Open-Ended Tasks in Real Computer Environments\", \"Pretrained Transformers for Text Ranking: BERT and Beyond\", \"Stabilizing transformer training by preventing attention entropy collapse\", \"Optimizing Dense Retrieval Model Training with Hard Negatives\", \"ChatGPT Hallucinates when Attributing Answers\"], \"Citation\":[]}",
        "eval_response": "Evaluation evidence: The assistant's answer does not match the gold answer in terms of accuracy and hallucinations. The assistant provided a long list of references which are not mentioned in the gold answer, indicating a significant hallucination issue. The gold answer specifies that there are no references and lists three citations, which the assistant failed to include, showing a lack of completeness. The assistant's answer does not align with the requirements of the task.\n\nRating: [[10]]",
        "score": 10
    },
    {
        "level": 4,
        "set": 2,
        "length": 55754,
        "type": "paper",
        "language": "en",
        "question": "",
        "instruction": "We kindly ask you to thoroughly review the provided papers and construct a citation chain from them. Please adhere to the following instructions strictly while completing the task:\n\n#Task Instructions:\nGiven several papers, you are required to identify and list the longest citation chain, which demonstrates the citation relationship among the provided papers.\n\n#Specific Requirements:\n1.Please present the titles of the papers in the form of a list, as follows: [\"Title of Paper 1\", \"Title of Paper 2\", ..., \"Title of Paper n\"].\n2.Ensure that the citation chain in the list is linear and continuous, meaning that the first paper title in the list (Paper 1) should not cite any other works. Instead, it should be cited by the next paper in the list (Paper 2); subsequently, each paper should then be cited by the next one in the list, continuing up to the last paper (Paper n).\n3.Consider only the citation relationships within the supplied collection of papers, and ensure that the citation chain accurately reflects the sequential citation order among these documents.\n4.Do not take into account any articles not provided, and disregard other non-linear citation relationships.",
        "prompt_template": "{instruction}\n\n#Papers Provided:\n{docs}",
        "doc": [
            "2007.06048.md",
            "2204.03775.md",
            "2209.13768.md",
            "2405.07898.md"
        ],
        "answer": [
            "# Minimod: A Finite Difference solver for Seismic Modeling ",
            "# Massively scalable stencil algorithm ",
            "# Disruptive Changes in Field Equation Modeling A Simple Interface for Wafer Scale Engines ",
            "# Breaking the Molecular Dynamics Timescale Barrier Using a Wafer-Scale System "
        ],
        "shuffle_doc": true,
        "id": "3d1824cf-4b34-4a1d-a8fd-4636989b23ae",
        "prompt": "[Question]\nWe kindly ask you to thoroughly review the provided papers and construct a citation chain from them. Please adhere to the following instructions strictly while completing the task:\n\n#Task Instructions:\nGiven several papers, you are required to identify and list the longest citation chain, which demonstrates the citation relationship among the provided papers.\n\n#Specific Requirements:\n1.Please present the titles of the papers in the form of a list, as follows: [\"Title of Paper 1\", \"Title of Paper 2\", ..., \"Title of Paper n\"].\n2.Ensure that the citation chain in the list is linear and continuous, meaning that the first paper title in the list (Paper 1) should not cite any other works. Instead, it should be cited by the next paper in the list (Paper 2); subsequently, each paper should then be cited by the next one in the list, continuing up to the last paper (Paper n).\n3.Consider only the citation relationships within the supplied collection of papers, and ensure that the citation chain accurately reflects the sequential citation order among these documents.\n4.Do not take into account any articles not provided, and disregard other non-linear citation relationships.\n\n#Papers Provided:\n{docs}\n\n[Gold Answer]\n['# Minimod: A Finite Difference solver for Seismic Modeling ', '# Massively scalable stencil algorithm ', '# Disruptive Changes in Field Equation Modeling A Simple Interface for Wafer Scale Engines ', '# Breaking the Molecular Dynamics Timescale Barrier Using a Wafer-Scale System ']\n\n[The Start of Assistant's Predicted Answer]\n[\"Massively scalable stencil algorithm\", \"Fast stencil-code computation on a wafer-scale processor\", \"Anton 3: Twenty microseconds of molecular dynamics simulation before lunch\"]\n[The End of Assistant's Predicted Answer]\n\n[System]\nWe would like to request your feedback on the performance of the AI assistant in response to the user question displayed above according to the gold answer. Please use the following listed aspects and their descriptions as evaluation criteria:\n    - Accuracy and Hallucinations: The assistant's answer is semantically consistent with the gold answer; The numerical value and order need to be accurate, and there should be no hallucinations.\n    - Completeness: Referring to the reference answers, the assistant's answer should contain all the key points needed to answer the user's question; further elaboration on these key points can be omitted.\nPlease rate whether this answer is suitable for the question. Please note that the gold answer can be considered as a correct answer to the question.\n\nThe assistant receives an overall score on a scale of 1 to 100, where a higher score indicates better overall performance.\nPlease note that if the assistant's answer and the gold answer fully meet the above criteria, its overall rating should be the full marks (100).\nPlease first provide a comprehensive explanation of your evaluation, avoiding any potential bias.\nThen, output a line indicating the score of the Assistant.\n\nPLEASE OUTPUT WITH THE FOLLOWING FORMAT, WHERE THE SCORE IS A SCALE OF 1 TO 100 BY STRICTLY FOLLOWING THIS FORMAT: \"[[score]]\", FOR EXAMPLE \"Rating: [[100]]\":\n<start output>\nEvaluation evidence: your evluation explanation here, no more than 100 words\nRating: [[score]]\n<end output>\n\nNow, start your evaluation:",
        "generate_response": "[\"Massively scalable stencil algorithm\", \"Fast stencil-code computation on a wafer-scale processor\", \"Anton 3: Twenty microseconds of molecular dynamics simulation before lunch\"]",
        "eval_response": "Evaluation evidence: The assistant's answer does not match the gold answer in terms of the titles of the papers and their citation order. The titles listed by the assistant do not appear in the gold answer, indicating a significant deviation and possible hallucination of paper titles. This error affects both the accuracy and completeness of the response, as the assistant fails to identify and list the correct citation chain from the provided papers.\n\nRating: [[10]]",
        "score": 10
    },
    {
        "level": 3,
        "set": 2,
        "length": 75265,
        "type": "paper",
        "language": "en",
        "question": "KTO: Model Alignment as Prospect Theoretic Optimization",
        "instruction": "We hope you will carefully study the provided papers and determine the citation relationships between them. Please follow the instructions below strictly to complete the task:\n\n#Specific Requirements:\n1. Reference: When a given paper mentions other provided papers, those other papers are considered as \"references\" for the given paper. To summarize in this specific context, references are about what the given paper is using.\n2. Citation: Conversely, when other provided papers mention the given paper in their works, the given paper is being \"cited\" by those other papers. To summarize in this specific context, citations are about who is using the given paper.\n3. Given a paper, you need to determine the citation or reference relationship between this paper and the other papers. Do not consider papers that are not provided.\n3. Please present the paper titles in a json format as follows: {{\"Reference\":[\"Reference Title 1\", \"Reference Title 2\", ..., \"Reference Title n\"], \"Citation\":[\"Citation Title 1\", \"Citation Title 2\", ..., \"Citation Title n\"]}}.\n4. If a paper does not have any references or citations, please leave the corresponding list empty, e.g.{{\"Refernce\":[]}}, {{\"Citation\":[]}}.",
        "prompt_template": "#Papers:\n{docs}\n\n{instruction}\n\n#The paper you need to analyze:\n{question}",
        "doc": [
            "2402.01306.md",
            "2401.01335.md",
            "2401.10020.md"
        ],
        "answer": {
            "Reference": [
                "# Self-Play Fine-Tuning Converts Weak Language Models to Strong Language Models ",
                "# Self-Rewarding Language Models "
            ],
            "Citation": []
        },
        "shuffle_doc": true,
        "id": "e07cfa2a-b4da-44bc-b34d-99ba436d1bac",
        "prompt": "[Question]\n#Papers:\n{docs}\n\nWe hope you will carefully study the provided papers and determine the citation relationships between them. Please follow the instructions below strictly to complete the task:\n\n#Specific Requirements:\n1. Reference: When a given paper mentions other provided papers, those other papers are considered as \"references\" for the given paper. To summarize in this specific context, references are about what the given paper is using.\n2. Citation: Conversely, when other provided papers mention the given paper in their works, the given paper is being \"cited\" by those other papers. To summarize in this specific context, citations are about who is using the given paper.\n3. Given a paper, you need to determine the citation or reference relationship between this paper and the other papers. Do not consider papers that are not provided.\n3. Please present the paper titles in a json format as follows: {{\"Reference\":[\"Reference Title 1\", \"Reference Title 2\", ..., \"Reference Title n\"], \"Citation\":[\"Citation Title 1\", \"Citation Title 2\", ..., \"Citation Title n\"]}}.\n4. If a paper does not have any references or citations, please leave the corresponding list empty, e.g.{{\"Refernce\":[]}}, {{\"Citation\":[]}}.\n\n#The paper you need to analyze:\nKTO: Model Alignment as Prospect Theoretic Optimization\n\n[Gold Answer]\n{'Reference': ['# Self-Play Fine-Tuning Converts Weak Language Models to Strong Language Models ', '# Self-Rewarding Language Models '], 'Citation': []}\n\n[The Start of Assistant's Predicted Answer]\n{\"Reference\": [\"A Survey on Evaluation of Large Language Models\", \"Vicuna: An Open-Source Chatbot Impressing GPT-4 with $90 \\% *$ ChatGPT Quality\", \"Compositional Semantic Parsing with Large Language Models\", \"Open LLM Leaderboard\", \"How Close is ChatGPT to Human Experts? Comparison Corpus, Evaluation, and Detection\", \"RLHF: Training a Helpful and Harmless Assistant with Reinforcement Learning from Human Feedback\", \"Rethinking with Retrieval: Faithful Large Language Model Inference\", \"Measuring Massive Multitask Language Understanding\", \"Atlas: Few-shot Learning with Retrieval Augmented Language Models\", \"Learning to Summarize with Human Feedback\", \"Fine-tuning Language Models for Factuality\", \"Llama: Open and Efficient Foundation Language Models\", \"Zephyr: Direct Distillation of LM Alignment\", \"Advances in Prospect Theory: Cumulative Representation of Uncertainty\", \"Unlikelihood Training for Neural Text Generation\", \"Self-rewarding Language Models\", \"SLiC-HF: Sequence Likelihood Calibration with Human Feedback\", \"Judging LLM-as-a-Judge with MT-Bench and Chatbot Arena\", \"Improving Language Models with Advantage-Based Offline Policy Gradients\", \"Self-play Fine-tuning Converts Weak Language Models to Strong Language Models\", \"UltraFeedback: Boosting Language Models with High-Quality Feedback\", \"Beyond the Imitation Game: Quantifying and Extrapolating the Capabilities of Language Models\", \"Learning Trajectory Preferences for Manipulators via Iterative Improvement\", \"Mistral 7B\", \"Prospect Theory: An Analysis of Decision under Risk\", \"OpenAssistant Conversations: Democratizing Large Language Model Alignment\", \"Reliability and Learnability of Human Bandit Feedback for Sequence-to-Sequence Reinforcement Learning\", \"When Humans Aren't Optimal: Robots that Collaborate with Risk-aware Humans\", \"AlpacaEval: An Automatic Evaluator of Instruction-following Models\", \"Transformer Reinforcement Learning\", \"Matrix Factorization Techniques for Recommender Systems\", \"Neural Collaborative Filtering\", \"Constructing Stable Preferences: A Look into Dimensions of Experience and their Impact on Preference Stability\", \"A Simple Sequentially Rejective Multiple Test Procedure\", \"Prospect Theory: An Analysis of Decision under Risk\", \"Learning Trajectory Preferences for Manipulators via Iterative Improvement\", \"Advantage-weighted Regression: Simple and Scalable Off-policy Reinforcement Learning\", \"Reinforcement Learning by Reward-weighted Regression for Operational Space Control\", \"Direct Preference Optimization: Your Language Model is Secretly a Reward Model\", \"Proximal Policy Optimization Algorithms\", \"Interpretable Modelling of Driving Behaviors in Interactive Driving Scenarios Based on Cumulative Prospect Theory\", \"Fine-tuning Language Models from Human Preferences\", \"Neural Text Generation with Unlikelihood Training\", \"Self-play Fine-tuning Converts Weak Language Models to Strong Language Models\", \"Self-rewarding Language Models\"], \"Citation\": []}\n[The End of Assistant's Predicted Answer]\n\n[System]\nWe would like to request your feedback on the performance of the AI assistant in response to the user question displayed above according to the gold answer. Please use the following listed aspects and their descriptions as evaluation criteria:\n    - Accuracy and Hallucinations: The assistant's answer is semantically consistent with the gold answer; The numerical value and order need to be accurate, and there should be no hallucinations.\n    - Completeness: Referring to the reference answers, the assistant's answer should contain all the key points needed to answer the user's question; further elaboration on these key points can be omitted.\nPlease rate whether this answer is suitable for the question. Please note that the gold answer can be considered as a correct answer to the question.\n\nThe assistant receives an overall score on a scale of 1 to 100, where a higher score indicates better overall performance.\nPlease note that if the assistant's answer and the gold answer fully meet the above criteria, its overall rating should be the full marks (100).\nPlease first provide a comprehensive explanation of your evaluation, avoiding any potential bias.\nThen, output a line indicating the score of the Assistant.\n\nPLEASE OUTPUT WITH THE FOLLOWING FORMAT, WHERE THE SCORE IS A SCALE OF 1 TO 100 BY STRICTLY FOLLOWING THIS FORMAT: \"[[score]]\", FOR EXAMPLE \"Rating: [[100]]\":\n<start output>\nEvaluation evidence: your evluation explanation here, no more than 100 words\nRating: [[score]]\n<end output>\n\nNow, start your evaluation:",
        "generate_response": "{\"Reference\": [\"A Survey on Evaluation of Large Language Models\", \"Vicuna: An Open-Source Chatbot Impressing GPT-4 with $90 \\% *$ ChatGPT Quality\", \"Compositional Semantic Parsing with Large Language Models\", \"Open LLM Leaderboard\", \"How Close is ChatGPT to Human Experts? Comparison Corpus, Evaluation, and Detection\", \"RLHF: Training a Helpful and Harmless Assistant with Reinforcement Learning from Human Feedback\", \"Rethinking with Retrieval: Faithful Large Language Model Inference\", \"Measuring Massive Multitask Language Understanding\", \"Atlas: Few-shot Learning with Retrieval Augmented Language Models\", \"Learning to Summarize with Human Feedback\", \"Fine-tuning Language Models for Factuality\", \"Llama: Open and Efficient Foundation Language Models\", \"Zephyr: Direct Distillation of LM Alignment\", \"Advances in Prospect Theory: Cumulative Representation of Uncertainty\", \"Unlikelihood Training for Neural Text Generation\", \"Self-rewarding Language Models\", \"SLiC-HF: Sequence Likelihood Calibration with Human Feedback\", \"Judging LLM-as-a-Judge with MT-Bench and Chatbot Arena\", \"Improving Language Models with Advantage-Based Offline Policy Gradients\", \"Self-play Fine-tuning Converts Weak Language Models to Strong Language Models\", \"UltraFeedback: Boosting Language Models with High-Quality Feedback\", \"Beyond the Imitation Game: Quantifying and Extrapolating the Capabilities of Language Models\", \"Learning Trajectory Preferences for Manipulators via Iterative Improvement\", \"Mistral 7B\", \"Prospect Theory: An Analysis of Decision under Risk\", \"OpenAssistant Conversations: Democratizing Large Language Model Alignment\", \"Reliability and Learnability of Human Bandit Feedback for Sequence-to-Sequence Reinforcement Learning\", \"When Humans Aren't Optimal: Robots that Collaborate with Risk-aware Humans\", \"AlpacaEval: An Automatic Evaluator of Instruction-following Models\", \"Transformer Reinforcement Learning\", \"Matrix Factorization Techniques for Recommender Systems\", \"Neural Collaborative Filtering\", \"Constructing Stable Preferences: A Look into Dimensions of Experience and their Impact on Preference Stability\", \"A Simple Sequentially Rejective Multiple Test Procedure\", \"Prospect Theory: An Analysis of Decision under Risk\", \"Learning Trajectory Preferences for Manipulators via Iterative Improvement\", \"Advantage-weighted Regression: Simple and Scalable Off-policy Reinforcement Learning\", \"Reinforcement Learning by Reward-weighted Regression for Operational Space Control\", \"Direct Preference Optimization: Your Language Model is Secretly a Reward Model\", \"Proximal Policy Optimization Algorithms\", \"Interpretable Modelling of Driving Behaviors in Interactive Driving Scenarios Based on Cumulative Prospect Theory\", \"Fine-tuning Language Models from Human Preferences\", \"Neural Text Generation with Unlikelihood Training\", \"Self-play Fine-tuning Converts Weak Language Models to Strong Language Models\", \"Self-rewarding Language Models\"], \"Citation\": []}",
        "eval_response": "Evaluation evidence: The assistant's answer incorrectly lists a large number of references that are not mentioned in the gold answer, indicating a significant error in identifying the correct references for the paper \"KTO: Model Alignment as Prospect Theoretic Optimization.\" The gold answer specifies only two references, which are not included in the assistant's extensive and incorrect list. This discrepancy shows a lack of accuracy and a high degree of hallucination in the response. The assistant's answer fails to meet the required accuracy and completeness criteria.\n\nRating: [[10]]",
        "score": 10
    },
    {
        "level": 3,
        "set": 2,
        "length": 73815,
        "type": "paper",
        "language": "en",
        "question": "RAG VS Fine-TUNing: PiPELINES, TRADEOFFS, AND A CASE STUDY ON AGRICULTURE",
        "instruction": "We hope you will carefully study the provided papers and determine the citation relationships between them. Please follow the instructions below strictly to complete the task:\n\n#Specific Requirements:\n1. Reference: When a given paper mentions other provided papers, those other papers are considered as \"references\" for the given paper. To summarize in this specific context, references are about what the given paper is using.\n2. Citation: Conversely, when other provided papers mention the given paper in their works, the given paper is being \"cited\" by those other papers. To summarize in this specific context, citations are about who is using the given paper.\n3. Given a paper, you need to determine the citation or reference relationship between this paper and the other papers. Do not consider papers that are not provided.\n3. Please present the paper titles in a json format as follows: {{\"Reference\":[\"Reference Title 1\", \"Reference Title 2\", ..., \"Reference Title n\"], \"Citation\":[\"Citation Title 1\", \"Citation Title 2\", ..., \"Citation Title n\"]}}.\n4. If a paper does not have any references or citations, please leave the corresponding list empty, e.g.{{\"Refernce\":[]}}, {{\"Citation\":[]}}.",
        "prompt_template": "#Papers:\n{docs}\n\n{instruction}\n\n#The paper you need to analyze:\n{question}",
        "doc": [
            "2401.08406.md",
            "2404.15939.md",
            "2405.17147.md",
            "2307.03875.md",
            "2310.06225.md"
        ],
        "answer": {
            "Reference": [
                "# Large Language Models for Supply Chain Optimization ",
                "# GPT-4 AS AN AGRONOMIST ASSISTANT? ANSWERING AGRICULTURE QUESTIONS USING LARGE LANGUAGE MODELS "
            ],
            "Citation": [
                "# Telco-RAG: Navigating the Challenges of Retrieval-Augmented Language Models for Telecommunications ",
                "# Large Language Models (LLMs): Deployment, Tokenomics and Sustainability "
            ]
        },
        "shuffle_doc": true,
        "id": "360f655e-6dc5-4153-b378-e8bc4de05981",
        "prompt": "[Question]\n#Papers:\n{docs}\n\nWe hope you will carefully study the provided papers and determine the citation relationships between them. Please follow the instructions below strictly to complete the task:\n\n#Specific Requirements:\n1. Reference: When a given paper mentions other provided papers, those other papers are considered as \"references\" for the given paper. To summarize in this specific context, references are about what the given paper is using.\n2. Citation: Conversely, when other provided papers mention the given paper in their works, the given paper is being \"cited\" by those other papers. To summarize in this specific context, citations are about who is using the given paper.\n3. Given a paper, you need to determine the citation or reference relationship between this paper and the other papers. Do not consider papers that are not provided.\n3. Please present the paper titles in a json format as follows: {{\"Reference\":[\"Reference Title 1\", \"Reference Title 2\", ..., \"Reference Title n\"], \"Citation\":[\"Citation Title 1\", \"Citation Title 2\", ..., \"Citation Title n\"]}}.\n4. If a paper does not have any references or citations, please leave the corresponding list empty, e.g.{{\"Refernce\":[]}}, {{\"Citation\":[]}}.\n\n#The paper you need to analyze:\nRAG VS Fine-TUNing: PiPELINES, TRADEOFFS, AND A CASE STUDY ON AGRICULTURE\n\n[Gold Answer]\n{'Reference': ['# Large Language Models for Supply Chain Optimization ', '# GPT-4 AS AN AGRONOMIST ASSISTANT? ANSWERING AGRICULTURE QUESTIONS USING LARGE LANGUAGE MODELS '], 'Citation': ['# Telco-RAG: Navigating the Challenges of Retrieval-Augmented Language Models for Telecommunications ', '# Large Language Models (LLMs): Deployment, Tokenomics and Sustainability ']}\n\n[The Start of Assistant's Predicted Answer]\n{\"Reference\":[\"A Survey on Evaluation of Large Language Models\", \"Vicuna: An Open-Source Chatbot Impressing GPT-4 with $90 \\% *$ ChatGPT Quality\", \"ChatLaw: Open-Source Legal Large Language Model with Integrated External Knowledge Bases\", \"Compositional Semantic Parsing with Large Language Models\", \"Open LLM Leaderboard\", \"How Close is ChatGPT to Human Experts? Comparison Corpus, Evaluation, and Detection\", \"REALM: Retrieval-Augmented Language Model PreTraining\", \"Rethinking with Retrieval: Faithful Large Language Model Inference\", \"Measuring Massive Multitask Language Understanding\", \"Atlas: Few-shot Learning with Retrieval Augmented Language Models\"], \"Citation\":[\"Telco-RAG: Navigating the Challenges of Retrieval-Augmented Language Models for Telecommunications\"]}\n[The End of Assistant's Predicted Answer]\n\n[System]\nWe would like to request your feedback on the performance of the AI assistant in response to the user question displayed above according to the gold answer. Please use the following listed aspects and their descriptions as evaluation criteria:\n    - Accuracy and Hallucinations: The assistant's answer is semantically consistent with the gold answer; The numerical value and order need to be accurate, and there should be no hallucinations.\n    - Completeness: Referring to the reference answers, the assistant's answer should contain all the key points needed to answer the user's question; further elaboration on these key points can be omitted.\nPlease rate whether this answer is suitable for the question. Please note that the gold answer can be considered as a correct answer to the question.\n\nThe assistant receives an overall score on a scale of 1 to 100, where a higher score indicates better overall performance.\nPlease note that if the assistant's answer and the gold answer fully meet the above criteria, its overall rating should be the full marks (100).\nPlease first provide a comprehensive explanation of your evaluation, avoiding any potential bias.\nThen, output a line indicating the score of the Assistant.\n\nPLEASE OUTPUT WITH THE FOLLOWING FORMAT, WHERE THE SCORE IS A SCALE OF 1 TO 100 BY STRICTLY FOLLOWING THIS FORMAT: \"[[score]]\", FOR EXAMPLE \"Rating: [[100]]\":\n<start output>\nEvaluation evidence: your evluation explanation here, no more than 100 words\nRating: [[score]]\n<end output>\n\nNow, start your evaluation:",
        "generate_response": "{\"Reference\":[\"A Survey on Evaluation of Large Language Models\", \"Vicuna: An Open-Source Chatbot Impressing GPT-4 with $90 \\% *$ ChatGPT Quality\", \"ChatLaw: Open-Source Legal Large Language Model with Integrated External Knowledge Bases\", \"Compositional Semantic Parsing with Large Language Models\", \"Open LLM Leaderboard\", \"How Close is ChatGPT to Human Experts? Comparison Corpus, Evaluation, and Detection\", \"REALM: Retrieval-Augmented Language Model PreTraining\", \"Rethinking with Retrieval: Faithful Large Language Model Inference\", \"Measuring Massive Multitask Language Understanding\", \"Atlas: Few-shot Learning with Retrieval Augmented Language Models\"], \"Citation\":[\"Telco-RAG: Navigating the Challenges of Retrieval-Augmented Language Models for Telecommunications\"]}",
        "eval_response": "Evaluation evidence: The assistant's answer does not match the gold answer in terms of accuracy and completeness. The references and citations listed in the assistant's answer are entirely different from those in the gold answer, indicating a significant error or hallucination. The assistant failed to identify the correct papers that reference or cite the given paper, which is crucial for the task.\n\nRating: [[10]]",
        "score": 10
    },
    {
        "level": 3,
        "set": 2,
        "length": 53107,
        "type": "paper",
        "language": "en",
        "question": "KVQuant: Towards 10 Million Context Length LLM Inference with KV Cache Quantization",
        "instruction": "We hope you will carefully study the provided papers and determine the citation relationships between them. Please follow the instructions below strictly to complete the task:\n\n#Specific Requirements:\n1. Reference: When a given paper mentions other provided papers, those other papers are considered as \"references\" for the given paper. To summarize in this specific context, references are about what the given paper is using.\n2. Citation: Conversely, when other provided papers mention the given paper in their works, the given paper is being \"cited\" by those other papers. To summarize in this specific context, citations are about who is using the given paper.\n3. Given a paper, you need to determine the citation or reference relationship between this paper and the other papers. Do not consider papers that are not provided.\n3. Please present the paper titles in a json format as follows: {{\"Reference\":[\"Reference Title 1\", \"Reference Title 2\", ..., \"Reference Title n\"], \"Citation\":[\"Citation Title 1\", \"Citation Title 2\", ..., \"Citation Title n\"]}}.\n4. If a paper does not have any references or citations, please leave the corresponding list empty, e.g.{{\"Refernce\":[]}}, {{\"Citation\":[]}}.",
        "prompt_template": "#Papers:\n{docs}\n\n{instruction}\n\n#The paper you need to analyze:\n{question}",
        "doc": [
            "2401.18079.md",
            "2405.03917.md",
            "2310.06825.md"
        ],
        "answer": {
            "Reference": [
                "# Mistral 7B "
            ],
            "Citation": [
                "# KV Cache is 1 Bit Per Channel: Efficient Large Language Model Inference with Coupled Quantization "
            ]
        },
        "shuffle_doc": true,
        "id": "8e4beccc-e47f-4f6f-a4af-097fdb0c3e4f",
        "prompt": "[Question]\n#Papers:\n{docs}\n\nWe hope you will carefully study the provided papers and determine the citation relationships between them. Please follow the instructions below strictly to complete the task:\n\n#Specific Requirements:\n1. Reference: When a given paper mentions other provided papers, those other papers are considered as \"references\" for the given paper. To summarize in this specific context, references are about what the given paper is using.\n2. Citation: Conversely, when other provided papers mention the given paper in their works, the given paper is being \"cited\" by those other papers. To summarize in this specific context, citations are about who is using the given paper.\n3. Given a paper, you need to determine the citation or reference relationship between this paper and the other papers. Do not consider papers that are not provided.\n3. Please present the paper titles in a json format as follows: {{\"Reference\":[\"Reference Title 1\", \"Reference Title 2\", ..., \"Reference Title n\"], \"Citation\":[\"Citation Title 1\", \"Citation Title 2\", ..., \"Citation Title n\"]}}.\n4. If a paper does not have any references or citations, please leave the corresponding list empty, e.g.{{\"Refernce\":[]}}, {{\"Citation\":[]}}.\n\n#The paper you need to analyze:\nKVQuant: Towards 10 Million Context Length LLM Inference with KV Cache Quantization\n\n[Gold Answer]\n{'Reference': ['# Mistral 7B '], 'Citation': ['# KV Cache is 1 Bit Per Channel: Efficient Large Language Model Inference with Coupled Quantization ']}\n\n[The Start of Assistant's Predicted Answer]\n{\"Reference\":[\"Introducing claude 2.1\", \"Understanding and overcoming the challenges of efficient transformer quantization\", \"Extending context window of large language models via positional interpolation\", \"Longlora: Efficient fine-tuning of long-context large language models\", \"Llm.int8 (): 8bit matrix multiplication for transformers at scale\", \"Qlora: Efficient finetuning of quantized llms\", \"Spqr: A sparse-quantized representation for near-lossless llm weight compression\", \"Hawq-v2: Hessian aware trace-weighted quantization of neural networks\", \"Hawq: Hessian aware quantization of neural networks with mixed-precision\", \"Balanced csr sparse matrix-vector product on graphics processors\", \"A framework for few-shot language model evaluation\", \"Model tells you what to discard: Adaptive kv cache compression for llms\", \"Ai and memory wall\", \"Lm-infinite: Simple on-the-fly length generalization for large language models\", \"Measuring massive multitask language understanding\", \"Rethinking channel dimensions to isolate outliers for low-bit weight quantization of large language models\", \"Mistral 7b\", \"Squeezellm: Dense-and-sparse quantization\", \"Full stack optimization of transformer inference: a survey\", \"How long can context length of opensource llms truly promise?\", \"Norm tweaking: Highperformance low-bit quantization of large language models\", \"Awq: Activation-aware weight quantization for llm compression and acceleration\", \"Intactkv: Improving large language model quantization by keeping pivot tokens intact\", \"Llmqat: Data-free quantization aware training for large language models\", \"Scissorhands: Exploiting the persistence of importance hypothesis for llm kv cache compression at test time\", \"Kivi: Plug-and-play 2bit kv cache quantization with streaming asymmetric quantization\", \"Landmark attention: Random-access infinite context length for transformers\", \"Sensitivity and generalization in neural networks: an empirical study\", \"New models and developer products announced at devday 2023\", \"Sparq attention: Bandwidth-efficient llm inference\", \"Omniquant: Omnidirectionally calibrated quantization for large language models\", \"Flexgen: Highthroughput generative inference of large language models with a single gpu\", \"Roformer: Enhanced transformer with rotary position embedding\", \"Llama: Open and efficient foundation language models\", \"Llama 2: Open foundation and fine-tuned chat models\", \"Outlier suppression+: Accurate quantization of large language models by equivalent and optimal shifting and scaling\", \"Outlier suppression: Pushing the limit of low-bit transformer language models\", \"Smoothquant: Accurate and efficient post-training quantization for large language models\", \"Efficient streaming language models with attention sinks\", \"H 2 o Heavy-hitter oracle for efficient generative inference of large language models\", \"Atom: Low-bit quantization for efficient and accurate $1 \\\\mathrm{~lm}$ serving\", \"Pose: Efficient context window extension of llms via positional skipwise training\"], \"Citation\":[]}\n[The End of Assistant's Predicted Answer]\n\n[System]\nWe would like to request your feedback on the performance of the AI assistant in response to the user question displayed above according to the gold answer. Please use the following listed aspects and their descriptions as evaluation criteria:\n    - Accuracy and Hallucinations: The assistant's answer is semantically consistent with the gold answer; The numerical value and order need to be accurate, and there should be no hallucinations.\n    - Completeness: Referring to the reference answers, the assistant's answer should contain all the key points needed to answer the user's question; further elaboration on these key points can be omitted.\nPlease rate whether this answer is suitable for the question. Please note that the gold answer can be considered as a correct answer to the question.\n\nThe assistant receives an overall score on a scale of 1 to 100, where a higher score indicates better overall performance.\nPlease note that if the assistant's answer and the gold answer fully meet the above criteria, its overall rating should be the full marks (100).\nPlease first provide a comprehensive explanation of your evaluation, avoiding any potential bias.\nThen, output a line indicating the score of the Assistant.\n\nPLEASE OUTPUT WITH THE FOLLOWING FORMAT, WHERE THE SCORE IS A SCALE OF 1 TO 100 BY STRICTLY FOLLOWING THIS FORMAT: \"[[score]]\", FOR EXAMPLE \"Rating: [[100]]\":\n<start output>\nEvaluation evidence: your evluation explanation here, no more than 100 words\nRating: [[score]]\n<end output>\n\nNow, start your evaluation:",
        "generate_response": "{\"Reference\":[\"Introducing claude 2.1\", \"Understanding and overcoming the challenges of efficient transformer quantization\", \"Extending context window of large language models via positional interpolation\", \"Longlora: Efficient fine-tuning of long-context large language models\", \"Llm.int8 (): 8bit matrix multiplication for transformers at scale\", \"Qlora: Efficient finetuning of quantized llms\", \"Spqr: A sparse-quantized representation for near-lossless llm weight compression\", \"Hawq-v2: Hessian aware trace-weighted quantization of neural networks\", \"Hawq: Hessian aware quantization of neural networks with mixed-precision\", \"Balanced csr sparse matrix-vector product on graphics processors\", \"A framework for few-shot language model evaluation\", \"Model tells you what to discard: Adaptive kv cache compression for llms\", \"Ai and memory wall\", \"Lm-infinite: Simple on-the-fly length generalization for large language models\", \"Measuring massive multitask language understanding\", \"Rethinking channel dimensions to isolate outliers for low-bit weight quantization of large language models\", \"Mistral 7b\", \"Squeezellm: Dense-and-sparse quantization\", \"Full stack optimization of transformer inference: a survey\", \"How long can context length of opensource llms truly promise?\", \"Norm tweaking: Highperformance low-bit quantization of large language models\", \"Awq: Activation-aware weight quantization for llm compression and acceleration\", \"Intactkv: Improving large language model quantization by keeping pivot tokens intact\", \"Llmqat: Data-free quantization aware training for large language models\", \"Scissorhands: Exploiting the persistence of importance hypothesis for llm kv cache compression at test time\", \"Kivi: Plug-and-play 2bit kv cache quantization with streaming asymmetric quantization\", \"Landmark attention: Random-access infinite context length for transformers\", \"Sensitivity and generalization in neural networks: an empirical study\", \"New models and developer products announced at devday 2023\", \"Sparq attention: Bandwidth-efficient llm inference\", \"Omniquant: Omnidirectionally calibrated quantization for large language models\", \"Flexgen: Highthroughput generative inference of large language models with a single gpu\", \"Roformer: Enhanced transformer with rotary position embedding\", \"Llama: Open and efficient foundation language models\", \"Llama 2: Open foundation and fine-tuned chat models\", \"Outlier suppression+: Accurate quantization of large language models by equivalent and optimal shifting and scaling\", \"Outlier suppression: Pushing the limit of low-bit transformer language models\", \"Smoothquant: Accurate and efficient post-training quantization for large language models\", \"Efficient streaming language models with attention sinks\", \"H 2 o Heavy-hitter oracle for efficient generative inference of large language models\", \"Atom: Low-bit quantization for efficient and accurate $1 \\\\mathrm{~lm}$ serving\", \"Pose: Efficient context window extension of llms via positional skipwise training\"], \"Citation\":[]}",
        "eval_response": "Evaluation evidence: The assistant's answer inaccurately lists numerous references that are not mentioned in the gold answer, indicating a significant hallucination issue. The gold answer specifies only one reference, \"Mistral 7B,\" which is missing from the assistant's response. Additionally, the assistant fails to identify any citations, aligning with the gold answer in this aspect. However, the extensive hallucination in the references section severely impacts the accuracy and completeness of the response.\n\nRating: [[20]]",
        "score": 20
    },
    {
        "level": 4,
        "set": 2,
        "length": 60348,
        "type": "paper",
        "language": "en",
        "question": "",
        "instruction": "We kindly ask you to thoroughly review the provided papers and construct a citation chain from them. Please adhere to the following instructions strictly while completing the task:\n\n#Task Instructions:\nGiven several papers, you are required to identify and list the longest citation chain, which demonstrates the citation relationship among the provided papers.\n\n#Specific Requirements:\n1.Please present the titles of the papers in the form of a list, as follows: [\"Title of Paper 1\", \"Title of Paper 2\", ..., \"Title of Paper n\"].\n2.Ensure that the citation chain in the list is linear and continuous, meaning that the first paper title in the list (Paper 1) should not cite any other works. Instead, it should be cited by the next paper in the list (Paper 2); subsequently, each paper should then be cited by the next one in the list, continuing up to the last paper (Paper n).\n3.Consider only the citation relationships within the supplied collection of papers, and ensure that the citation chain accurately reflects the sequential citation order among these documents.\n4.Do not take into account any articles not provided, and disregard other non-linear citation relationships.",
        "prompt_template": "{instruction}\n\n#Papers Provided:\n{docs}",
        "doc": [
            "2309.15088.md",
            "2310.08319.md",
            "2401.00368.md",
            "2310.20329.md",
            "2401.04621.md"
        ],
        "answer": [
            "# RankVicuna: Zero-Shot Listwise Document Reranking with Open-Source Large Language Models ",
            "# Fine-Tuning LLaMA for Multi-Stage Text Retrieval ",
            "# Improving Text Embeddings with Large Language Models ",
            "# InstructCoder: Instruction Tuning Large Language Models for Code Editing ",
            "# DebugBench: Evaluating Debugging Capability of Large Language Models "
        ],
        "shuffle_doc": true,
        "id": "c46c6e76-3150-427d-8019-1565db5cc93c",
        "prompt": "[Question]\nWe kindly ask you to thoroughly review the provided papers and construct a citation chain from them. Please adhere to the following instructions strictly while completing the task:\n\n#Task Instructions:\nGiven several papers, you are required to identify and list the longest citation chain, which demonstrates the citation relationship among the provided papers.\n\n#Specific Requirements:\n1.Please present the titles of the papers in the form of a list, as follows: [\"Title of Paper 1\", \"Title of Paper 2\", ..., \"Title of Paper n\"].\n2.Ensure that the citation chain in the list is linear and continuous, meaning that the first paper title in the list (Paper 1) should not cite any other works. Instead, it should be cited by the next paper in the list (Paper 2); subsequently, each paper should then be cited by the next one in the list, continuing up to the last paper (Paper n).\n3.Consider only the citation relationships within the supplied collection of papers, and ensure that the citation chain accurately reflects the sequential citation order among these documents.\n4.Do not take into account any articles not provided, and disregard other non-linear citation relationships.\n\n#Papers Provided:\n{docs}\n\n[Gold Answer]\n['# RankVicuna: Zero-Shot Listwise Document Reranking with Open-Source Large Language Models ', '# Fine-Tuning LLaMA for Multi-Stage Text Retrieval ', '# Improving Text Embeddings with Large Language Models ', '# InstructCoder: Instruction Tuning Large Language Models for Code Editing ', '# DebugBench: Evaluating Debugging Capability of Large Language Models ']\n\n[The Start of Assistant's Predicted Answer]\n[\"DebugBench: Evaluating Debugging Capability of Large Language Models\", \"A Survey on Evaluation of Large Language Models\", \"REALM: Retrieval-Augmented Language Model PreTraining\", \"Rethinking with Retrieval: Faithful Large Language Model Inference\", \"Measuring Massive Multitask Language Understanding\", \"Atlas: Few-shot Learning with Retrieval Augmented Language Models\", \"HumanEval\", \"MBPP\", \"Defects4J\", \"QuixBugs\", \"LeetCode\", \"GitHub\", \"Automated Debugging\", \"APR\", \"Program synthesis with large language models\", \"Evaluating large language models trained on code\", \"Teaching large language models to self-debug\", \"Instructcoder: Empowering language models for code editing\", \"Selfevolve: A code evolution framework via large language models\", \"Swe-bench: Can language models resolve real-world github issues?\", \"Genprog: A generic method for automatic software repair\", \"Tbar: Revisiting template-based automated program repair\", \"Code Complete, 2nd edition\", \"Language models are unsupervised multitask learners\", \"Code llama: Open foundation models for code\", \"Reflexion: Language agents with verbal reinforcement learning\", \"An analysis of the automatic bug fixing performance of chatgpt\", \"Bloom: A 176bparameter open-access multilingual language model\", \"Conversational automated program repair\", \"Keep the conversation going: Fixing 162 out of 337 bugs for $0.42$ each using chatgpt\", \"A critical review of large language model on software engineering: An example from chatgpt and automated program repair\"]\n[The End of Assistant's Predicted Answer]\n\n[System]\nWe would like to request your feedback on the performance of the AI assistant in response to the user question displayed above according to the gold answer. Please use the following listed aspects and their descriptions as evaluation criteria:\n    - Accuracy and Hallucinations: The assistant's answer is semantically consistent with the gold answer; The numerical value and order need to be accurate, and there should be no hallucinations.\n    - Completeness: Referring to the reference answers, the assistant's answer should contain all the key points needed to answer the user's question; further elaboration on these key points can be omitted.\nPlease rate whether this answer is suitable for the question. Please note that the gold answer can be considered as a correct answer to the question.\n\nThe assistant receives an overall score on a scale of 1 to 100, where a higher score indicates better overall performance.\nPlease note that if the assistant's answer and the gold answer fully meet the above criteria, its overall rating should be the full marks (100).\nPlease first provide a comprehensive explanation of your evaluation, avoiding any potential bias.\nThen, output a line indicating the score of the Assistant.\n\nPLEASE OUTPUT WITH THE FOLLOWING FORMAT, WHERE THE SCORE IS A SCALE OF 1 TO 100 BY STRICTLY FOLLOWING THIS FORMAT: \"[[score]]\", FOR EXAMPLE \"Rating: [[100]]\":\n<start output>\nEvaluation evidence: your evluation explanation here, no more than 100 words\nRating: [[score]]\n<end output>\n\nNow, start your evaluation:",
        "generate_response": "[\"DebugBench: Evaluating Debugging Capability of Large Language Models\", \"A Survey on Evaluation of Large Language Models\", \"REALM: Retrieval-Augmented Language Model PreTraining\", \"Rethinking with Retrieval: Faithful Large Language Model Inference\", \"Measuring Massive Multitask Language Understanding\", \"Atlas: Few-shot Learning with Retrieval Augmented Language Models\", \"HumanEval\", \"MBPP\", \"Defects4J\", \"QuixBugs\", \"LeetCode\", \"GitHub\", \"Automated Debugging\", \"APR\", \"Program synthesis with large language models\", \"Evaluating large language models trained on code\", \"Teaching large language models to self-debug\", \"Instructcoder: Empowering language models for code editing\", \"Selfevolve: A code evolution framework via large language models\", \"Swe-bench: Can language models resolve real-world github issues?\", \"Genprog: A generic method for automatic software repair\", \"Tbar: Revisiting template-based automated program repair\", \"Code Complete, 2nd edition\", \"Language models are unsupervised multitask learners\", \"Code llama: Open foundation models for code\", \"Reflexion: Language agents with verbal reinforcement learning\", \"An analysis of the automatic bug fixing performance of chatgpt\", \"Bloom: A 176bparameter open-access multilingual language model\", \"Conversational automated program repair\", \"Keep the conversation going: Fixing 162 out of 337 bugs for $0.42$ each using chatgpt\", \"A critical review of large language model on software engineering: An example from chatgpt and automated program repair\"]",
        "eval_response": "Evaluation evidence: The assistant's response does not align with the gold answer and fails to meet the task requirements. The provided answer lists a series of paper titles that do not form a linear and continuous citation chain as required. Additionally, the titles mentioned in the assistant's response do not match those in the gold answer, indicating a significant deviation from the provided data and task instructions. This results in both inaccuracies and hallucinations in the response.\n\nRating: [[10]]",
        "score": 10
    },
    {
        "level": 3,
        "set": 3,
        "length": 129552,
        "type": "paper",
        "language": "en",
        "question": "Executable Code Actions Elicit Better LLM Agents",
        "instruction": "We hope you will carefully study the provided papers and determine the citation relationships between them. Please follow the instructions below strictly to complete the task:\n\n#Specific Requirements:\n1. Reference: When a given paper mentions other provided papers, those other papers are considered as \"references\" for the given paper. To summarize in this specific context, references are about what the given paper is using.\n2. Citation: Conversely, when other provided papers mention the given paper in their works, the given paper is being \"cited\" by those other papers. To summarize in this specific context, citations are about who is using the given paper.\n3. Given a paper, you need to determine the citation or reference relationship between this paper and the other papers. Do not consider papers that are not provided.\n3. Please present the paper titles in a json format as follows: {{\"Reference\":[\"Reference Title 1\", \"Reference Title 2\", ..., \"Reference Title n\"], \"Citation\":[\"Citation Title 1\", \"Citation Title 2\", ..., \"Citation Title n\"]}}.\n4. If a paper does not have any references or citations, please leave the corresponding list empty, e.g.{{\"Refernce\":[]}}, {{\"Citation\":[]}}.",
        "prompt_template": "#Papers:\n{docs}\n\n{instruction}\n\n#The paper you need to analyze:\n{question}",
        "doc": [
            "2402.01030.md",
            "2405.16533.md",
            "2405.20092.md",
            "2401.00812.md",
            "2402.04247.md"
        ],
        "answer": {
            "Reference": [
                "# If LLM Is the Wizard, Then Code Is the Wand: A Survey on How ",
                "# Prioritizing Safeguarding Over Autonomy: Risks of LLM Agents for Science "
            ],
            "Citation": [
                "# Chain of Tools: Large Language Model is an Automatic Multi-tool Learner ",
                "# Divide-and-Conquer Meets Consensus: Unleashing the Power of Functions in Code Generation "
            ]
        },
        "shuffle_doc": true,
        "id": "96c4dd20-1260-4541-89d5-f4f199ba7ef2",
        "prompt": "[Question]\n#Papers:\n{docs}\n\nWe hope you will carefully study the provided papers and determine the citation relationships between them. Please follow the instructions below strictly to complete the task:\n\n#Specific Requirements:\n1. Reference: When a given paper mentions other provided papers, those other papers are considered as \"references\" for the given paper. To summarize in this specific context, references are about what the given paper is using.\n2. Citation: Conversely, when other provided papers mention the given paper in their works, the given paper is being \"cited\" by those other papers. To summarize in this specific context, citations are about who is using the given paper.\n3. Given a paper, you need to determine the citation or reference relationship between this paper and the other papers. Do not consider papers that are not provided.\n3. Please present the paper titles in a json format as follows: {{\"Reference\":[\"Reference Title 1\", \"Reference Title 2\", ..., \"Reference Title n\"], \"Citation\":[\"Citation Title 1\", \"Citation Title 2\", ..., \"Citation Title n\"]}}.\n4. If a paper does not have any references or citations, please leave the corresponding list empty, e.g.{{\"Refernce\":[]}}, {{\"Citation\":[]}}.\n\n#The paper you need to analyze:\nExecutable Code Actions Elicit Better LLM Agents\n\n[Gold Answer]\n{'Reference': ['# If LLM Is the Wizard, Then Code Is the Wand: A Survey on How ', '# Prioritizing Safeguarding Over Autonomy: Risks of LLM Agents for Science '], 'Citation': ['# Chain of Tools: Large Language Model is an Automatic Multi-tool Learner ', '# Divide-and-Conquer Meets Consensus: Unleashing the Power of Functions in Code Generation ']}\n\n[The Start of Assistant's Predicted Answer]\n{\"Reference\": [\"A Survey on Evaluation of Large Language Models\", \"Vicuna: An Open-Source Chatbot Impressing GPT-4 with $90 \\% *$ ChatGPT Quality\", \"ChatLaw: Open-Source Legal Large Language Model with Integrated External Knowledge Bases\", \"Compositional Semantic Parsing with Large Language Models\", \"Open LLM Leaderboard\", \"How Close is ChatGPT to Human Experts? Comparison Corpus, Evaluation, and Detection\", \"REALM: Retrieval-Augmented Language Model PreTraining\", \"Rethinking with Retrieval: Faithful Large Language Model Inference\", \"Measuring Massive Multitask Language Understanding\", \"Atlas: Few-shot Learning with Retrieval Augmented Language Models\", \"Do as i can and not as i say: Grounding language in robotic affordances\", \"Webgpt: Browser-assisted question-answering with human feedback\", \"Tool learning with foundation models\", \"Toolllm: Facilitating large language models to master 16000+ real-world apis\", \"Progprompt: Generating situated robot task plans using large language models\", \"Gorilla: Large language model connected with massive apis\", \"Self-consistency improves chain of thought reasoning in language models\", \"Generative agents: Interactive simulacra of human behavior\", \"Code as policies: Language model programs for embodied control\", \"Improving large language models: a survey\", \"Training verifiers to solve math word problems\", \"Measuring coding challenge competence with apps\", \"Measuring mathematical problem solving with the math dataset\", \"Meta programming for multi-agent collaborative framework\", \"Data interpreter: An llm agent for data science\", \"Voxposer: Composable 3d value maps for robotic manipulation with language models\", \"Mistral 7b\", \"Language models can solve computer tasks\", \"Voyager: An openended embodied agent with large language models\", \"A survey on large language model based autonomous agents\", \"Scienceworld: Is your agent smarter than a 5th grader?\", \"Learning to generate from textual interactions\", \"Mint: Evaluating llms in multi-turn interaction with tools and language feedback\", \"Interactive planning with large language models enables open-world multi-task agents\", \"Chain-of-thought prompting elicits reasoning in large language models\", \"On the tool manipulation capability of open-source large language models\", \"If llm is the wizard, then code is the wand: A survey on how code empowers large language models to serve as intelligent agents\", \"Hotpotqa: A dataset for diverse, explainable multi-hop question answering\", \"Towards unified alignment between agents, humans, and environment\", \"Webshop: Towards scalable real-world web interaction with grounded language agents\", \"React: Synergizing reasoning and acting in language models\", \"Tree of thoughts: Deliberate problem solving with large language models\", \"Retroformer: Retrospective large language agents with policy gradient optimization\", \"Customizing llms by creating and retrieving from specialized toolsets\", \"Agenttuning: Enabling generalized agent abilities for llms\", \"Prefer: Prompt ensemble learning via feedback-reflect-refine\", \"Judging llm-as-a-judge with mt-bench and chatbot arena\", \"Opencodeinterpreter: Integrating code generation with execution and refinement\", \"Ghost in the minecraft: Generally capable agents for open-world enviroments via large language models with text-based knowledge and memory\"], \"Citation\": []}\n[The End of Assistant's Predicted Answer]\n\n[System]\nWe would like to request your feedback on the performance of the AI assistant in response to the user question displayed above according to the gold answer. Please use the following listed aspects and their descriptions as evaluation criteria:\n    - Accuracy and Hallucinations: The assistant's answer is semantically consistent with the gold answer; The numerical value and order need to be accurate, and there should be no hallucinations.\n    - Completeness: Referring to the reference answers, the assistant's answer should contain all the key points needed to answer the user's question; further elaboration on these key points can be omitted.\nPlease rate whether this answer is suitable for the question. Please note that the gold answer can be considered as a correct answer to the question.\n\nThe assistant receives an overall score on a scale of 1 to 100, where a higher score indicates better overall performance.\nPlease note that if the assistant's answer and the gold answer fully meet the above criteria, its overall rating should be the full marks (100).\nPlease first provide a comprehensive explanation of your evaluation, avoiding any potential bias.\nThen, output a line indicating the score of the Assistant.\n\nPLEASE OUTPUT WITH THE FOLLOWING FORMAT, WHERE THE SCORE IS A SCALE OF 1 TO 100 BY STRICTLY FOLLOWING THIS FORMAT: \"[[score]]\", FOR EXAMPLE \"Rating: [[100]]\":\n<start output>\nEvaluation evidence: your evluation explanation here, no more than 100 words\nRating: [[score]]\n<end output>\n\nNow, start your evaluation:",
        "generate_response": "{\"Reference\": [\"A Survey on Evaluation of Large Language Models\", \"Vicuna: An Open-Source Chatbot Impressing GPT-4 with $90 \\% *$ ChatGPT Quality\", \"ChatLaw: Open-Source Legal Large Language Model with Integrated External Knowledge Bases\", \"Compositional Semantic Parsing with Large Language Models\", \"Open LLM Leaderboard\", \"How Close is ChatGPT to Human Experts? Comparison Corpus, Evaluation, and Detection\", \"REALM: Retrieval-Augmented Language Model PreTraining\", \"Rethinking with Retrieval: Faithful Large Language Model Inference\", \"Measuring Massive Multitask Language Understanding\", \"Atlas: Few-shot Learning with Retrieval Augmented Language Models\", \"Do as i can and not as i say: Grounding language in robotic affordances\", \"Webgpt: Browser-assisted question-answering with human feedback\", \"Tool learning with foundation models\", \"Toolllm: Facilitating large language models to master 16000+ real-world apis\", \"Progprompt: Generating situated robot task plans using large language models\", \"Gorilla: Large language model connected with massive apis\", \"Self-consistency improves chain of thought reasoning in language models\", \"Generative agents: Interactive simulacra of human behavior\", \"Code as policies: Language model programs for embodied control\", \"Improving large language models: a survey\", \"Training verifiers to solve math word problems\", \"Measuring coding challenge competence with apps\", \"Measuring mathematical problem solving with the math dataset\", \"Meta programming for multi-agent collaborative framework\", \"Data interpreter: An llm agent for data science\", \"Voxposer: Composable 3d value maps for robotic manipulation with language models\", \"Mistral 7b\", \"Language models can solve computer tasks\", \"Voyager: An openended embodied agent with large language models\", \"A survey on large language model based autonomous agents\", \"Scienceworld: Is your agent smarter than a 5th grader?\", \"Learning to generate from textual interactions\", \"Mint: Evaluating llms in multi-turn interaction with tools and language feedback\", \"Interactive planning with large language models enables open-world multi-task agents\", \"Chain-of-thought prompting elicits reasoning in large language models\", \"On the tool manipulation capability of open-source large language models\", \"If llm is the wizard, then code is the wand: A survey on how code empowers large language models to serve as intelligent agents\", \"Hotpotqa: A dataset for diverse, explainable multi-hop question answering\", \"Towards unified alignment between agents, humans, and environment\", \"Webshop: Towards scalable real-world web interaction with grounded language agents\", \"React: Synergizing reasoning and acting in language models\", \"Tree of thoughts: Deliberate problem solving with large language models\", \"Retroformer: Retrospective large language agents with policy gradient optimization\", \"Customizing llms by creating and retrieving from specialized toolsets\", \"Agenttuning: Enabling generalized agent abilities for llms\", \"Prefer: Prompt ensemble learning via feedback-reflect-refine\", \"Judging llm-as-a-judge with mt-bench and chatbot arena\", \"Opencodeinterpreter: Integrating code generation with execution and refinement\", \"Ghost in the minecraft: Generally capable agents for open-world enviroments via large language models with text-based knowledge and memory\"], \"Citation\": []}",
        "eval_response": "Evaluation evidence: The assistant's answer does not match the gold answer in terms of accuracy and completeness. The references listed in the assistant's answer do not align with those in the gold answer, and it incorrectly lists numerous unrelated papers. Additionally, the assistant's answer does not include any citations, whereas the gold answer lists two specific citations. This discrepancy indicates a failure in accurately identifying the citation and reference relationships for the given paper.\n\nRating: [[10]]",
        "score": 10
    },
    {
        "level": 4,
        "set": 3,
        "length": 103604,
        "type": "paper",
        "language": "en",
        "question": "",
        "instruction": "We kindly ask you to thoroughly review the provided papers and construct a citation chain from them. Please adhere to the following instructions strictly while completing the task:\n\n#Task Instructions:\nGiven several papers, you are required to identify and list the longest citation chain, which demonstrates the citation relationship among the provided papers.\n\n#Specific Requirements:\n1.Please present the titles of the papers in the form of a list, as follows: [\"Title of Paper 1\", \"Title of Paper 2\", ..., \"Title of Paper n\"].\n2.Ensure that the citation chain in the list is linear and continuous, meaning that the first paper title in the list (Paper 1) should not cite any other works. Instead, it should be cited by the next paper in the list (Paper 2); subsequently, each paper should then be cited by the next one in the list, continuing up to the last paper (Paper n).\n3.Consider only the citation relationships within the supplied collection of papers, and ensure that the citation chain accurately reflects the sequential citation order among these documents.\n4.Do not take into account any articles not provided, and disregard other non-linear citation relationships.",
        "prompt_template": "{instruction}\n\n#Papers Provided:\n{docs}",
        "doc": [
            "2302.14376.md",
            "2303.08891.md",
            "2305.17560.md",
            "2310.12487.md",
            "2405.09285.md"
        ],
        "answer": [
            "# GNOT: A General Neural Operator Transformer for Operator Learning ",
            "# VITO: VISION TRANSFORMER-OPERATOR ",
            "# Scalable Transformer for PDE Surrogate Modeling ",
            "# IMPROVED OPERATOR LEARNING BY ORTHOGONAL ATTENTION ",
            "# Positional Knowledge is All You Need: Position-induced Transformer (PiT) for Operator Learning "
        ],
        "shuffle_doc": true,
        "id": "7b7565b3-36f8-474e-b8b6-632b4cd727ca",
        "prompt": "[Question]\nWe kindly ask you to thoroughly review the provided papers and construct a citation chain from them. Please adhere to the following instructions strictly while completing the task:\n\n#Task Instructions:\nGiven several papers, you are required to identify and list the longest citation chain, which demonstrates the citation relationship among the provided papers.\n\n#Specific Requirements:\n1.Please present the titles of the papers in the form of a list, as follows: [\"Title of Paper 1\", \"Title of Paper 2\", ..., \"Title of Paper n\"].\n2.Ensure that the citation chain in the list is linear and continuous, meaning that the first paper title in the list (Paper 1) should not cite any other works. Instead, it should be cited by the next paper in the list (Paper 2); subsequently, each paper should then be cited by the next one in the list, continuing up to the last paper (Paper n).\n3.Consider only the citation relationships within the supplied collection of papers, and ensure that the citation chain accurately reflects the sequential citation order among these documents.\n4.Do not take into account any articles not provided, and disregard other non-linear citation relationships.\n\n#Papers Provided:\n{docs}\n\n[Gold Answer]\n['# GNOT: A General Neural Operator Transformer for Operator Learning ', '# VITO: VISION TRANSFORMER-OPERATOR ', '# Scalable Transformer for PDE Surrogate Modeling ', '# IMPROVED OPERATOR LEARNING BY ORTHOGONAL ATTENTION ', '# Positional Knowledge is All You Need: Position-induced Transformer (PiT) for Operator Learning ']\n\n[The Start of Assistant's Predicted Answer]\n[]\n[The End of Assistant's Predicted Answer]\n\n[System]\nWe would like to request your feedback on the performance of the AI assistant in response to the user question displayed above according to the gold answer. Please use the following listed aspects and their descriptions as evaluation criteria:\n    - Accuracy and Hallucinations: The assistant's answer is semantically consistent with the gold answer; The numerical value and order need to be accurate, and there should be no hallucinations.\n    - Completeness: Referring to the reference answers, the assistant's answer should contain all the key points needed to answer the user's question; further elaboration on these key points can be omitted.\nPlease rate whether this answer is suitable for the question. Please note that the gold answer can be considered as a correct answer to the question.\n\nThe assistant receives an overall score on a scale of 1 to 100, where a higher score indicates better overall performance.\nPlease note that if the assistant's answer and the gold answer fully meet the above criteria, its overall rating should be the full marks (100).\nPlease first provide a comprehensive explanation of your evaluation, avoiding any potential bias.\nThen, output a line indicating the score of the Assistant.\n\nPLEASE OUTPUT WITH THE FOLLOWING FORMAT, WHERE THE SCORE IS A SCALE OF 1 TO 100 BY STRICTLY FOLLOWING THIS FORMAT: \"[[score]]\", FOR EXAMPLE \"Rating: [[100]]\":\n<start output>\nEvaluation evidence: your evluation explanation here, no more than 100 words\nRating: [[score]]\n<end output>\n\nNow, start your evaluation:",
        "generate_response": "[]",
        "eval_response": "Evaluation evidence: The assistant's response is missing, as indicated by the empty brackets in the predicted answer section. Therefore, it fails to meet the requirements of accuracy and completeness. The gold answer provides a list of paper titles in a citation chain, which the assistant was supposed to replicate or provide a similar accurate chain. Since the assistant provided no answer, it did not fulfill the task requirements at all.\n\nRating: [[0]]",
        "score": 0
    },
    {
        "level": 4,
        "set": 1,
        "length": 38424,
        "type": "paper",
        "language": "en",
        "question": "",
        "instruction": "We kindly ask you to thoroughly review the provided papers and construct a citation chain from them. Please adhere to the following instructions strictly while completing the task:\n\n#Task Instructions:\nGiven several papers, you are required to identify and list the longest citation chain, which demonstrates the citation relationship among the provided papers.\n\n#Specific Requirements:\n1.Please present the titles of the papers in the form of a list, as follows: [\"Title of Paper 1\", \"Title of Paper 2\", ..., \"Title of Paper n\"].\n2.Ensure that the citation chain in the list is linear and continuous, meaning that the first paper title in the list (Paper 1) should not cite any other works. Instead, it should be cited by the next paper in the list (Paper 2); subsequently, each paper should then be cited by the next one in the list, continuing up to the last paper (Paper n).\n3.Consider only the citation relationships within the supplied collection of papers, and ensure that the citation chain accurately reflects the sequential citation order among these documents.\n4.Do not take into account any articles not provided, and disregard other non-linear citation relationships.",
        "prompt_template": "{instruction}\n\n#Papers Provided:\n{docs}",
        "doc": [
            "2310.19784.md",
            "2307.09481.md",
            "2307.04684.md"
        ],
        "answer": [
            "# CUSTOMNET: ZERO-SHOT OBJECT CUSTOMIZATION WITH VARIABLE-VIEWPOINTS IN TEXT-TO-IMAGE DIFFUSION MODELS ",
            "# AnyDoor: Zero-shot Object-level Image Customization ",
            "# FreeDrag: Feature Dragging for Reliable Point-based Image Editing "
        ],
        "shuffle_doc": true,
        "id": "c6666dc5-f286-4359-ac6f-511737d9b3d6",
        "prompt": "[Question]\nWe kindly ask you to thoroughly review the provided papers and construct a citation chain from them. Please adhere to the following instructions strictly while completing the task:\n\n#Task Instructions:\nGiven several papers, you are required to identify and list the longest citation chain, which demonstrates the citation relationship among the provided papers.\n\n#Specific Requirements:\n1.Please present the titles of the papers in the form of a list, as follows: [\"Title of Paper 1\", \"Title of Paper 2\", ..., \"Title of Paper n\"].\n2.Ensure that the citation chain in the list is linear and continuous, meaning that the first paper title in the list (Paper 1) should not cite any other works. Instead, it should be cited by the next paper in the list (Paper 2); subsequently, each paper should then be cited by the next one in the list, continuing up to the last paper (Paper n).\n3.Consider only the citation relationships within the supplied collection of papers, and ensure that the citation chain accurately reflects the sequential citation order among these documents.\n4.Do not take into account any articles not provided, and disregard other non-linear citation relationships.\n\n#Papers Provided:\n{docs}\n\n[Gold Answer]\n['# CUSTOMNET: ZERO-SHOT OBJECT CUSTOMIZATION WITH VARIABLE-VIEWPOINTS IN TEXT-TO-IMAGE DIFFUSION MODELS ', '# AnyDoor: Zero-shot Object-level Image Customization ', '# FreeDrag: Feature Dragging for Reliable Point-based Image Editing ']\n\n[The Start of Assistant's Predicted Answer]\n[\"FreeDrag: Feature Dragging for Reliable Point-based Image Editing\", \"Drag your GAN: Interactive point-based manipulation on the generative image manifold\"]\n[The End of Assistant's Predicted Answer]\n\n[System]\nWe would like to request your feedback on the performance of the AI assistant in response to the user question displayed above according to the gold answer. Please use the following listed aspects and their descriptions as evaluation criteria:\n    - Accuracy and Hallucinations: The assistant's answer is semantically consistent with the gold answer; The numerical value and order need to be accurate, and there should be no hallucinations.\n    - Completeness: Referring to the reference answers, the assistant's answer should contain all the key points needed to answer the user's question; further elaboration on these key points can be omitted.\nPlease rate whether this answer is suitable for the question. Please note that the gold answer can be considered as a correct answer to the question.\n\nThe assistant receives an overall score on a scale of 1 to 100, where a higher score indicates better overall performance.\nPlease note that if the assistant's answer and the gold answer fully meet the above criteria, its overall rating should be the full marks (100).\nPlease first provide a comprehensive explanation of your evaluation, avoiding any potential bias.\nThen, output a line indicating the score of the Assistant.\n\nPLEASE OUTPUT WITH THE FOLLOWING FORMAT, WHERE THE SCORE IS A SCALE OF 1 TO 100 BY STRICTLY FOLLOWING THIS FORMAT: \"[[score]]\", FOR EXAMPLE \"Rating: [[100]]\":\n<start output>\nEvaluation evidence: your evluation explanation here, no more than 100 words\nRating: [[score]]\n<end output>\n\nNow, start your evaluation:",
        "generate_response": "[\"FreeDrag: Feature Dragging for Reliable Point-based Image Editing\", \"Drag your GAN: Interactive point-based manipulation on the generative image manifold\"]",
        "eval_response": "Evaluation evidence: The assistant's answer does not match the gold answer in terms of the titles of the papers and the sequence of the citation chain. The titles provided by the assistant are not found in the gold answer, indicating a significant deviation and a failure to identify the correct citation chain among the provided papers. This results in both inaccuracies and incompleteness in the response.\n\nRating: [[10]]",
        "score": 10
    },
    {
        "level": 3,
        "set": 2,
        "length": 81892,
        "type": "paper",
        "language": "en",
        "question": "AUtoAcT: Automatic Agent Learning from Scratch for QA via Self-Planning",
        "instruction": "We hope you will carefully study the provided papers and determine the citation relationships between them. Please follow the instructions below strictly to complete the task:\n\n#Specific Requirements:\n1. Reference: When a given paper mentions other provided papers, those other papers are considered as \"references\" for the given paper. To summarize in this specific context, references are about what the given paper is using.\n2. Citation: Conversely, when other provided papers mention the given paper in their works, the given paper is being \"cited\" by those other papers. To summarize in this specific context, citations are about who is using the given paper.\n3. Given a paper, you need to determine the citation or reference relationship between this paper and the other papers. Do not consider papers that are not provided.\n3. Please present the paper titles in a json format as follows: {{\"Reference\":[\"Reference Title 1\", \"Reference Title 2\", ..., \"Reference Title n\"], \"Citation\":[\"Citation Title 1\", \"Citation Title 2\", ..., \"Citation Title n\"]}}.\n4. If a paper does not have any references or citations, please leave the corresponding list empty, e.g.{{\"Refernce\":[]}}, {{\"Citation\":[]}}.",
        "prompt_template": "#Papers:\n{docs}\n\n{instruction}\n\n#The paper you need to analyze:\n{question}",
        "doc": [
            "2401.05268.md",
            "2403.19962.md",
            "2405.14751.md",
            "2402.02716.md",
            "2403.08140.md"
        ],
        "answer": {
            "Reference": [
                "# Understanding the planning of LLM agents: A survey ",
                "# BAGEL: Bootstrapping Agents by Guiding Exploration with Language "
            ],
            "Citation": [
                "# Enhancing the General Agent Capabilities of Low-Parameter LLMs through Tuning and Multi-Branch Reasoning ",
                "# AGILE: A Novel Framework of LLM Agents "
            ]
        },
        "shuffle_doc": true,
        "id": "27f4e00b-4158-4544-8a92-aebd08b994e1",
        "prompt": "[Question]\n#Papers:\n{docs}\n\nWe hope you will carefully study the provided papers and determine the citation relationships between them. Please follow the instructions below strictly to complete the task:\n\n#Specific Requirements:\n1. Reference: When a given paper mentions other provided papers, those other papers are considered as \"references\" for the given paper. To summarize in this specific context, references are about what the given paper is using.\n2. Citation: Conversely, when other provided papers mention the given paper in their works, the given paper is being \"cited\" by those other papers. To summarize in this specific context, citations are about who is using the given paper.\n3. Given a paper, you need to determine the citation or reference relationship between this paper and the other papers. Do not consider papers that are not provided.\n3. Please present the paper titles in a json format as follows: {{\"Reference\":[\"Reference Title 1\", \"Reference Title 2\", ..., \"Reference Title n\"], \"Citation\":[\"Citation Title 1\", \"Citation Title 2\", ..., \"Citation Title n\"]}}.\n4. If a paper does not have any references or citations, please leave the corresponding list empty, e.g.{{\"Refernce\":[]}}, {{\"Citation\":[]}}.\n\n#The paper you need to analyze:\nAUtoAcT: Automatic Agent Learning from Scratch for QA via Self-Planning\n\n[Gold Answer]\n{'Reference': ['# Understanding the planning of LLM agents: A survey ', '# BAGEL: Bootstrapping Agents by Guiding Exploration with Language '], 'Citation': ['# Enhancing the General Agent Capabilities of Low-Parameter LLMs through Tuning and Multi-Branch Reasoning ', '# AGILE: A Novel Framework of LLM Agents ']}\n\n[The Start of Assistant's Predicted Answer]\n{\"Reference\":[\"A Survey on Evaluation of Large Language Models\", \"Rest meets react: Self-improvement for multistep reasoning llm agent\", \"Fireact: Toward language agent fine-tuning\", \"Autoagents: A framework for automatic agent generation\", \"Reconcile: Round-table conference improves reasoning via consensus among diverse llms\", \"Human embryonic stem cells and clinical applications\", \"Specializing smaller language models towards multi-step reasoning\", \"Problems of Monetary Management: The UK Experience\", \"Large language model based multi-agents: A survey of progress and challenges\", \"Metagpt: Meta programming for multi-agent collaborative framework\", \"Lora: Low-rank adaptation of large language models\", \"Large language models can self-improve\", \"Large language models cannot self-correct reasoning yet\", \"Understanding the planning of llm agents: A survey\", \"Do as I can, not as I say: Grounding language in robotic affordances\", \"Mistral 7b\", \"CAMEL: communicative agents for mind exploration of large scale language model society\", \"More agents is all you need\", \"Encouraging divergent thinking in large language models through multi-agent debate\", \"Generated knowledge prompting for commonsense reasoning\", \"BOLAA: benchmarking and orchestrating llm-augmented autonomous agents\", \"Learn to explain: Multimodal reasoning via thought chains for science question answering\", \"Chameleon: Plug-and-play compositional reasoning with large language models\", \"Self-refine: Iterative refinement with self-feedback\", \"Herbert A. Simon, Administrative Behavior: A Study of Decision-Making Processes in Administrative Organization\", \"BAGEL: bootstrapping agents by guiding exploration with language\", \"Babyagi\", \"Chatgpt: Optimizing language models for dialogue\", \"GPT-4 technical report\", \"Gpt-engineer\", \"Gorilla: Large language model connected with massive apis\", \"Virtualhome: Simulating household activities via programs\", \"Communicative agents for software development\", \"Making language models better tool learners with execution feedback\", \"Reasoning with language model prompting: A survey\", \"Toolllm: Facilitating large language models to master 16000+ real-world apis\", \"Deepspeed: System optimizations enable training deep learning models with over 100 billion parameters\", \"Small llms are weak tool learners: A multi-llm agent\", \"Hugginggpt: Solving AI tasks with chatgpt and its friends in huggingface\", \"Reflexion: language agents with verbal reinforcement learning\", \"Alfworld: Aligning text and embodied environments for interactive learning\", \"Llm-planner: Few-shot grounded planning for embodied agents with large language models\", \"Multiagent collaboration: Harnessing the power of intelligent LLM agents\", \"Medagents: Large language models as collaborators for zero-shot medical reasoning\", \"Stanford alpaca: An instruction-following llama model\", \"Xagent: An autonomous agent for complex task solving\", \"Autogpt: build & use ai agents\", \"Llama 2: Open foundation and fine-tuned chat models\", \"A survey on large language model based autonomous agents\", \"Self-instruct: Aligning language models with self-generated instructions\", \"Describe, explain, plan and select: interactive planning with llms enables open-world multi-task agents\", \"Jarvis-1: Open-world multi-task agents with memoryaugmented multimodal language models\", \"Chain-of-thought prompting elicits reasoning in large language models\", \"The rise and potential of large language model based agents: A survey\", \"Language models meet world models: Embodied experiences enhance language models\", \"Travelplanner: A benchmark for real-world planning with language agents\", \"Openagents: An open platform for language agents in the wild\", \"Rewoo: Decoupling reasoning from observations for efficient augmented language models\", \"Hotpotqa: A dataset for diverse, explainable multi-hop question answering\", \"Webshop: Towards scalable realworld web interaction with grounded language agents\", \"React: Synergizing reasoning and acting in language models\", \"Lumos: Learning agents with unified data, modular design, and open-source llms\", \"Star: Bootstrapping reasoning with reasoning\", \"Agenttuning: Enabling generalized agent abilities for llms\", \"Igniting language intelligence: The hitchhiker's guide from chain-of-thought reasoning to language agents\", \"Judging llm-as-a-judge with mt-bench and chatbot arena\", \"Webarena: A realistic web environment for building autonomous agents\", \"Agents: An open-source framework for autonomous language agents\"], \"Citation\":[]}\n[The End of Assistant's Predicted Answer]\n\n[System]\nWe would like to request your feedback on the performance of the AI assistant in response to the user question displayed above according to the gold answer. Please use the following listed aspects and their descriptions as evaluation criteria:\n    - Accuracy and Hallucinations: The assistant's answer is semantically consistent with the gold answer; The numerical value and order need to be accurate, and there should be no hallucinations.\n    - Completeness: Referring to the reference answers, the assistant's answer should contain all the key points needed to answer the user's question; further elaboration on these key points can be omitted.\nPlease rate whether this answer is suitable for the question. Please note that the gold answer can be considered as a correct answer to the question.\n\nThe assistant receives an overall score on a scale of 1 to 100, where a higher score indicates better overall performance.\nPlease note that if the assistant's answer and the gold answer fully meet the above criteria, its overall rating should be the full marks (100).\nPlease first provide a comprehensive explanation of your evaluation, avoiding any potential bias.\nThen, output a line indicating the score of the Assistant.\n\nPLEASE OUTPUT WITH THE FOLLOWING FORMAT, WHERE THE SCORE IS A SCALE OF 1 TO 100 BY STRICTLY FOLLOWING THIS FORMAT: \"[[score]]\", FOR EXAMPLE \"Rating: [[100]]\":\n<start output>\nEvaluation evidence: your evluation explanation here, no more than 100 words\nRating: [[score]]\n<end output>\n\nNow, start your evaluation:",
        "generate_response": "{\"Reference\":[\"A Survey on Evaluation of Large Language Models\", \"Rest meets react: Self-improvement for multistep reasoning llm agent\", \"Fireact: Toward language agent fine-tuning\", \"Autoagents: A framework for automatic agent generation\", \"Reconcile: Round-table conference improves reasoning via consensus among diverse llms\", \"Human embryonic stem cells and clinical applications\", \"Specializing smaller language models towards multi-step reasoning\", \"Problems of Monetary Management: The UK Experience\", \"Large language model based multi-agents: A survey of progress and challenges\", \"Metagpt: Meta programming for multi-agent collaborative framework\", \"Lora: Low-rank adaptation of large language models\", \"Large language models can self-improve\", \"Large language models cannot self-correct reasoning yet\", \"Understanding the planning of llm agents: A survey\", \"Do as I can, not as I say: Grounding language in robotic affordances\", \"Mistral 7b\", \"CAMEL: communicative agents for mind exploration of large scale language model society\", \"More agents is all you need\", \"Encouraging divergent thinking in large language models through multi-agent debate\", \"Generated knowledge prompting for commonsense reasoning\", \"BOLAA: benchmarking and orchestrating llm-augmented autonomous agents\", \"Learn to explain: Multimodal reasoning via thought chains for science question answering\", \"Chameleon: Plug-and-play compositional reasoning with large language models\", \"Self-refine: Iterative refinement with self-feedback\", \"Herbert A. Simon, Administrative Behavior: A Study of Decision-Making Processes in Administrative Organization\", \"BAGEL: bootstrapping agents by guiding exploration with language\", \"Babyagi\", \"Chatgpt: Optimizing language models for dialogue\", \"GPT-4 technical report\", \"Gpt-engineer\", \"Gorilla: Large language model connected with massive apis\", \"Virtualhome: Simulating household activities via programs\", \"Communicative agents for software development\", \"Making language models better tool learners with execution feedback\", \"Reasoning with language model prompting: A survey\", \"Toolllm: Facilitating large language models to master 16000+ real-world apis\", \"Deepspeed: System optimizations enable training deep learning models with over 100 billion parameters\", \"Small llms are weak tool learners: A multi-llm agent\", \"Hugginggpt: Solving AI tasks with chatgpt and its friends in huggingface\", \"Reflexion: language agents with verbal reinforcement learning\", \"Alfworld: Aligning text and embodied environments for interactive learning\", \"Llm-planner: Few-shot grounded planning for embodied agents with large language models\", \"Multiagent collaboration: Harnessing the power of intelligent LLM agents\", \"Medagents: Large language models as collaborators for zero-shot medical reasoning\", \"Stanford alpaca: An instruction-following llama model\", \"Xagent: An autonomous agent for complex task solving\", \"Autogpt: build & use ai agents\", \"Llama 2: Open foundation and fine-tuned chat models\", \"A survey on large language model based autonomous agents\", \"Self-instruct: Aligning language models with self-generated instructions\", \"Describe, explain, plan and select: interactive planning with llms enables open-world multi-task agents\", \"Jarvis-1: Open-world multi-task agents with memoryaugmented multimodal language models\", \"Chain-of-thought prompting elicits reasoning in large language models\", \"The rise and potential of large language model based agents: A survey\", \"Language models meet world models: Embodied experiences enhance language models\", \"Travelplanner: A benchmark for real-world planning with language agents\", \"Openagents: An open platform for language agents in the wild\", \"Rewoo: Decoupling reasoning from observations for efficient augmented language models\", \"Hotpotqa: A dataset for diverse, explainable multi-hop question answering\", \"Webshop: Towards scalable realworld web interaction with grounded language agents\", \"React: Synergizing reasoning and acting in language models\", \"Lumos: Learning agents with unified data, modular design, and open-source llms\", \"Star: Bootstrapping reasoning with reasoning\", \"Agenttuning: Enabling generalized agent abilities for llms\", \"Igniting language intelligence: The hitchhiker's guide from chain-of-thought reasoning to language agents\", \"Judging llm-as-a-judge with mt-bench and chatbot arena\", \"Webarena: A realistic web environment for building autonomous agents\", \"Agents: An open-source framework for autonomous language agents\"], \"Citation\":[]}",
        "eval_response": "Evaluation evidence: The assistant's answer is inaccurate and contains hallucinations. The provided answer lists numerous papers that are not mentioned in the gold answer, indicating a failure to correctly identify the references and citations for the specified paper. The correct references and citations from the gold answer are missing in the assistant's response, showing a lack of completeness and accuracy.\n\nRating: [[10]]",
        "score": 10
    },
    {
        "level": 3,
        "set": 4,
        "length": 249965,
        "type": "paper",
        "language": "en",
        "question": "On Prompt-Driven Safeguarding for Large Language Models",
        "instruction": "We hope you will carefully study the provided papers and determine the citation relationships between them. Please follow the instructions below strictly to complete the task:\n\n#Specific Requirements:\n1. Reference: When a given paper mentions other provided papers, those other papers are considered as \"references\" for the given paper. To summarize in this specific context, references are about what the given paper is using.\n2. Citation: Conversely, when other provided papers mention the given paper in their works, the given paper is being \"cited\" by those other papers. To summarize in this specific context, citations are about who is using the given paper.\n3. Given a paper, you need to determine the citation or reference relationship between this paper and the other papers. Do not consider papers that are not provided.\n3. Please present the paper titles in a json format as follows: {{\"Reference\":[\"Reference Title 1\", \"Reference Title 2\", ..., \"Reference Title n\"], \"Citation\":[\"Citation Title 1\", \"Citation Title 2\", ..., \"Citation Title n\"]}}.\n4. If a paper does not have any references or citations, please leave the corresponding list empty, e.g.{{\"Refernce\":[]}}, {{\"Citation\":[]}}.",
        "prompt_template": "#Papers:\n{docs}\n\n{instruction}\n\n#The paper you need to analyze:\n{question}",
        "doc": [
            "2401.18018.md",
            "2405.16821.md",
            "2405.17374.md",
            "2405.19668.md",
            "2405.20099.md",
            "2405.20947.md",
            "2311.10702.md",
            "2311.11045.md",
            "2312.04724.md",
            "2401.06373.md",
            "2404.16792.md"
        ],
        "answer": {
            "Reference": [
                "# Camels in a Changing Climate: Enhancing LM Adaptation with TLU 2 ",
                "# AJ Orca 2: Teaching Small Language Models How to Reason ",
                "# Purple Llama CYBERSECEvAL: A Secure Coding Benchmark for Language Models ",
                "# How Johnny Can Persuade LLMs to Jailbreak Them: Rethinking Persuasion to Challenge AI Safety by Humanizing LLMs ",
                "# Weak-to-Strong Extrapolation Expedites Alignment "
            ],
            "Citation": [
                "# Perturbation-Restrained Sequential Model Editing ",
                "# Navigating the Safety Landscape: Measuring Risks in Finetuning Large Language Models ",
                "# AutoBreach: Universal and Adaptive Jailbreaking with Efficient Wordplay-Guided Optimization ",
                "# Defensive Prompt Patch: A Robust and Interpretable Defense of LLMs against Jailbreak Attacks ",
                "# OR-Bench: An Over-Refusal Benchmark for Large Language Models "
            ]
        },
        "shuffle_doc": true,
        "id": "2cefce4c-9a78-46bd-9703-36b9662d866a",
        "prompt": "[Question]\n#Papers:\n{docs}\n\nWe hope you will carefully study the provided papers and determine the citation relationships between them. Please follow the instructions below strictly to complete the task:\n\n#Specific Requirements:\n1. Reference: When a given paper mentions other provided papers, those other papers are considered as \"references\" for the given paper. To summarize in this specific context, references are about what the given paper is using.\n2. Citation: Conversely, when other provided papers mention the given paper in their works, the given paper is being \"cited\" by those other papers. To summarize in this specific context, citations are about who is using the given paper.\n3. Given a paper, you need to determine the citation or reference relationship between this paper and the other papers. Do not consider papers that are not provided.\n3. Please present the paper titles in a json format as follows: {{\"Reference\":[\"Reference Title 1\", \"Reference Title 2\", ..., \"Reference Title n\"], \"Citation\":[\"Citation Title 1\", \"Citation Title 2\", ..., \"Citation Title n\"]}}.\n4. If a paper does not have any references or citations, please leave the corresponding list empty, e.g.{{\"Refernce\":[]}}, {{\"Citation\":[]}}.\n\n#The paper you need to analyze:\nOn Prompt-Driven Safeguarding for Large Language Models\n\n[Gold Answer]\n{'Reference': ['# Camels in a Changing Climate: Enhancing LM Adaptation with TLU 2 ', '# AJ Orca 2: Teaching Small Language Models How to Reason ', '# Purple Llama CYBERSECEvAL: A Secure Coding Benchmark for Language Models ', '# How Johnny Can Persuade LLMs to Jailbreak Them: Rethinking Persuasion to Challenge AI Safety by Humanizing LLMs ', '# Weak-to-Strong Extrapolation Expedites Alignment '], 'Citation': ['# Perturbation-Restrained Sequential Model Editing ', '# Navigating the Safety Landscape: Measuring Risks in Finetuning Large Language Models ', '# AutoBreach: Universal and Adaptive Jailbreaking with Efficient Wordplay-Guided Optimization ', '# Defensive Prompt Patch: A Robust and Interpretable Defense of LLMs against Jailbreak Attacks ', '# OR-Bench: An Over-Refusal Benchmark for Large Language Models ']}\n\n[The Start of Assistant's Predicted Answer]\n{\"Reference\":[\"The CRINGE loss: Learning what language not to model\", \"Training a helpful and harmless assistant with reinforcement learning from human feedback\", \"Constitutional AI: Harmlessness from AI Feedback\", \"Purple Llama Cyberseceval: A secure coding benchmark for language models\", \"RLPrompt: Optimizing discrete text prompts with reinforcement learning\", \"The Curious Case of Neural Text Degeneration\", \"Catastrophic jailbreak of open-source LLMs via exploiting generation\", \"Camels in a Changing Climate: Enhancing LM Adaptation with TLU 2\", \"Mistral 7b\", \"The Power of Scale for Parameter-Efficient Prompt Tuning\", \"AlpacaEval: An Automatic Evaluator of Instruction-Following Models\", \"Self-Alignment with Instruction Backtranslation\", \"Prefix-Tuning: Optimizing Continuous Prompts for Generation\", \"AutoDAN: Generating Stealthy Jailbreak Prompts on Aligned Large Language Models\", \"Teaching Small Language Models How to Reason\", \"GPT-4 Technical Report\", \"Code Llama: Open Foundation Models for Code\", \"On Second Thought, Let's Not Think Step by Step! Bias and Toxicity in Zero-Shot Reasoning\", \"Towards Controllable Biases in Language Generation\", \"AutoPrompt: Eliciting Knowledge from Language Models with Automatically Generated Prompts\", \"On the Safety of Conversational Models: Taxonomy, Dataset, and Benchmark\", \"Llama 2: Open Foundation and Fine-Tuned Chat Models\", \"OpenChat: Advancing Open-Source Language Models with Mixed-Quality Data\", \"Jailbroken: How Does LLM Safety Training Fail?\", \"Ethical and Social Risks of Harm from Language Models\", \"Neural Text Generation with Unlikelihood Training\", \"Recipes for Safety in Open-Domain Chatbots\", \"Large Language Models as Optimizers\", \"How Johnny Can Persuade LLMs to Jailbreak Them: Rethinking Persuasion to Challenge AI Safety by Humanizing LLMs\", \"Ethical-advice Taker: Do Language Models Understand Natural Language Interventions?\", \"Chat Templates for HuggingFace Large Language Models\", \"Exploring Prompt-Based Few-Shot Learning for Grounded Dialog Generation\", \"Click: Controllable Text Generation with Sequence Likelihood Contrastive Learning\", \"Weak-to-Strong Extrapolation Expedites Alignment\", \"Large Language Models Are Not Robust Multiple Choice Selectors\", \"Large Language Models Are Human-Level Prompt Engineers\"], \"Citation\":[]}\n[The End of Assistant's Predicted Answer]\n\n[System]\nWe would like to request your feedback on the performance of the AI assistant in response to the user question displayed above according to the gold answer. Please use the following listed aspects and their descriptions as evaluation criteria:\n    - Accuracy and Hallucinations: The assistant's answer is semantically consistent with the gold answer; The numerical value and order need to be accurate, and there should be no hallucinations.\n    - Completeness: Referring to the reference answers, the assistant's answer should contain all the key points needed to answer the user's question; further elaboration on these key points can be omitted.\nPlease rate whether this answer is suitable for the question. Please note that the gold answer can be considered as a correct answer to the question.\n\nThe assistant receives an overall score on a scale of 1 to 100, where a higher score indicates better overall performance.\nPlease note that if the assistant's answer and the gold answer fully meet the above criteria, its overall rating should be the full marks (100).\nPlease first provide a comprehensive explanation of your evaluation, avoiding any potential bias.\nThen, output a line indicating the score of the Assistant.\n\nPLEASE OUTPUT WITH THE FOLLOWING FORMAT, WHERE THE SCORE IS A SCALE OF 1 TO 100 BY STRICTLY FOLLOWING THIS FORMAT: \"[[score]]\", FOR EXAMPLE \"Rating: [[100]]\":\n<start output>\nEvaluation evidence: your evluation explanation here, no more than 100 words\nRating: [[score]]\n<end output>\n\nNow, start your evaluation:",
        "generate_response": "{\"Reference\":[\"The CRINGE loss: Learning what language not to model\", \"Training a helpful and harmless assistant with reinforcement learning from human feedback\", \"Constitutional AI: Harmlessness from AI Feedback\", \"Purple Llama Cyberseceval: A secure coding benchmark for language models\", \"RLPrompt: Optimizing discrete text prompts with reinforcement learning\", \"The Curious Case of Neural Text Degeneration\", \"Catastrophic jailbreak of open-source LLMs via exploiting generation\", \"Camels in a Changing Climate: Enhancing LM Adaptation with TLU 2\", \"Mistral 7b\", \"The Power of Scale for Parameter-Efficient Prompt Tuning\", \"AlpacaEval: An Automatic Evaluator of Instruction-Following Models\", \"Self-Alignment with Instruction Backtranslation\", \"Prefix-Tuning: Optimizing Continuous Prompts for Generation\", \"AutoDAN: Generating Stealthy Jailbreak Prompts on Aligned Large Language Models\", \"Teaching Small Language Models How to Reason\", \"GPT-4 Technical Report\", \"Code Llama: Open Foundation Models for Code\", \"On Second Thought, Let's Not Think Step by Step! Bias and Toxicity in Zero-Shot Reasoning\", \"Towards Controllable Biases in Language Generation\", \"AutoPrompt: Eliciting Knowledge from Language Models with Automatically Generated Prompts\", \"On the Safety of Conversational Models: Taxonomy, Dataset, and Benchmark\", \"Llama 2: Open Foundation and Fine-Tuned Chat Models\", \"OpenChat: Advancing Open-Source Language Models with Mixed-Quality Data\", \"Jailbroken: How Does LLM Safety Training Fail?\", \"Ethical and Social Risks of Harm from Language Models\", \"Neural Text Generation with Unlikelihood Training\", \"Recipes for Safety in Open-Domain Chatbots\", \"Large Language Models as Optimizers\", \"How Johnny Can Persuade LLMs to Jailbreak Them: Rethinking Persuasion to Challenge AI Safety by Humanizing LLMs\", \"Ethical-advice Taker: Do Language Models Understand Natural Language Interventions?\", \"Chat Templates for HuggingFace Large Language Models\", \"Exploring Prompt-Based Few-Shot Learning for Grounded Dialog Generation\", \"Click: Controllable Text Generation with Sequence Likelihood Contrastive Learning\", \"Weak-to-Strong Extrapolation Expedites Alignment\", \"Large Language Models Are Not Robust Multiple Choice Selectors\", \"Large Language Models Are Human-Level Prompt Engineers\"], \"Citation\":[]}",
        "eval_response": "Evaluation evidence: The assistant's answer shows significant discrepancies from the gold answer. It includes a large number of references that are not mentioned in the gold answer, indicating potential hallucinations or inaccuracies. Furthermore, the assistant fails to list any citations, whereas the gold answer lists several. This indicates issues with both accuracy and completeness, as the assistant's response does not accurately reflect the relationships outlined in the gold answer and omits necessary citation information.\n\nRating: [[20]]",
        "score": 20
    },
    {
        "level": 3,
        "set": 3,
        "length": 125486,
        "type": "paper",
        "language": "en",
        "question": "A Comprehensive Study of Knowledge Editing for Large Language Models",
        "instruction": "We hope you will carefully study the provided papers and determine the citation relationships between them. Please follow the instructions below strictly to complete the task:\n\n#Specific Requirements:\n1. Reference: When a given paper mentions other provided papers, those other papers are considered as \"references\" for the given paper. To summarize in this specific context, references are about what the given paper is using.\n2. Citation: Conversely, when other provided papers mention the given paper in their works, the given paper is being \"cited\" by those other papers. To summarize in this specific context, citations are about who is using the given paper.\n3. Given a paper, you need to determine the citation or reference relationship between this paper and the other papers. Do not consider papers that are not provided.\n3. Please present the paper titles in a json format as follows: {{\"Reference\":[\"Reference Title 1\", \"Reference Title 2\", ..., \"Reference Title n\"], \"Citation\":[\"Citation Title 1\", \"Citation Title 2\", ..., \"Citation Title n\"]}}.\n4. If a paper does not have any references or citations, please leave the corresponding list empty, e.g.{{\"Refernce\":[]}}, {{\"Citation\":[]}}.",
        "prompt_template": "#Papers:\n{docs}\n\n{instruction}\n\n#The paper you need to analyze:\n{question}",
        "doc": [
            "2401.01286.md",
            "2405.16821.md",
            "2405.17969.md",
            "2405.19086.md"
        ],
        "answer": {
            "Reference": [],
            "Citation": [
                "# Perturbation-Restrained Sequential Model Editing ",
                "# Knowledge Circuits in Pretrained Transformers ",
                "# MEMoE: Enhancing Model Editing with Mixture of Experts Adaptors "
            ]
        },
        "shuffle_doc": true,
        "id": "7c0e4a47-b1d9-451a-9cd6-17fd723f187a",
        "prompt": "[Question]\n#Papers:\n{docs}\n\nWe hope you will carefully study the provided papers and determine the citation relationships between them. Please follow the instructions below strictly to complete the task:\n\n#Specific Requirements:\n1. Reference: When a given paper mentions other provided papers, those other papers are considered as \"references\" for the given paper. To summarize in this specific context, references are about what the given paper is using.\n2. Citation: Conversely, when other provided papers mention the given paper in their works, the given paper is being \"cited\" by those other papers. To summarize in this specific context, citations are about who is using the given paper.\n3. Given a paper, you need to determine the citation or reference relationship between this paper and the other papers. Do not consider papers that are not provided.\n3. Please present the paper titles in a json format as follows: {{\"Reference\":[\"Reference Title 1\", \"Reference Title 2\", ..., \"Reference Title n\"], \"Citation\":[\"Citation Title 1\", \"Citation Title 2\", ..., \"Citation Title n\"]}}.\n4. If a paper does not have any references or citations, please leave the corresponding list empty, e.g.{{\"Refernce\":[]}}, {{\"Citation\":[]}}.\n\n#The paper you need to analyze:\nA Comprehensive Study of Knowledge Editing for Large Language Models\n\n[Gold Answer]\n{'Reference': [], 'Citation': ['# Perturbation-Restrained Sequential Model Editing ', '# Knowledge Circuits in Pretrained Transformers ', '# MEMoE: Enhancing Model Editing with Mixture of Experts Adaptors ']}\n\n[The Start of Assistant's Predicted Answer]\n{\"Reference\":[\"Language Patches\", \"CaliNET\", \"T-Patcher\", \"REMEDI\", \"GRACE\", \"LoRA\", \"MELO\", \"FT-Constrained\", \"ENN\", \"KE\", \"SLAG\", \"MEND\", \"Knowledge Neuron\", \"ROME\", \"MEMIT\", \"PMET\", \"MALMEN\", \"BIRD\", \"Sanitation\", \"DepEdit\", \"Eval-KLLM\", \"Bi-ZsRE\", \"Retrieval-augmented Multilingual Knowledge Editor (ReMaKE)\", \"ENTITY INFERENCES\", \"DUNE\", \"RealToxicPrompts\", \"SAfetyBENCH\", \"Bias in Bios dataset\", \"WinoBias\", \"StereoSet\", \"PrivQA\", \"DAMA\", \"DEPN\", \"Privacy in large language models: Attacks, defenses and future directions\", \"A watermark for large language models\", \"Lamp: When large language models meet personalization\", \"Characterchat: Learning towards conversational AI with personalized social support\", \"Editing personality for llms\", \"Emotionbench: Evaluating how llms feel using emotionbench\", \"Aligning language models to user opinions\"], \"Citation\":[\"Perturbation-Restrained Sequential Model Editing\"]}\n[The End of Assistant's Predicted Answer]\n\n[System]\nWe would like to request your feedback on the performance of the AI assistant in response to the user question displayed above according to the gold answer. Please use the following listed aspects and their descriptions as evaluation criteria:\n    - Accuracy and Hallucinations: The assistant's answer is semantically consistent with the gold answer; The numerical value and order need to be accurate, and there should be no hallucinations.\n    - Completeness: Referring to the reference answers, the assistant's answer should contain all the key points needed to answer the user's question; further elaboration on these key points can be omitted.\nPlease rate whether this answer is suitable for the question. Please note that the gold answer can be considered as a correct answer to the question.\n\nThe assistant receives an overall score on a scale of 1 to 100, where a higher score indicates better overall performance.\nPlease note that if the assistant's answer and the gold answer fully meet the above criteria, its overall rating should be the full marks (100).\nPlease first provide a comprehensive explanation of your evaluation, avoiding any potential bias.\nThen, output a line indicating the score of the Assistant.\n\nPLEASE OUTPUT WITH THE FOLLOWING FORMAT, WHERE THE SCORE IS A SCALE OF 1 TO 100 BY STRICTLY FOLLOWING THIS FORMAT: \"[[score]]\", FOR EXAMPLE \"Rating: [[100]]\":\n<start output>\nEvaluation evidence: your evluation explanation here, no more than 100 words\nRating: [[score]]\n<end output>\n\nNow, start your evaluation:",
        "generate_response": "{\"Reference\":[\"Language Patches\", \"CaliNET\", \"T-Patcher\", \"REMEDI\", \"GRACE\", \"LoRA\", \"MELO\", \"FT-Constrained\", \"ENN\", \"KE\", \"SLAG\", \"MEND\", \"Knowledge Neuron\", \"ROME\", \"MEMIT\", \"PMET\", \"MALMEN\", \"BIRD\", \"Sanitation\", \"DepEdit\", \"Eval-KLLM\", \"Bi-ZsRE\", \"Retrieval-augmented Multilingual Knowledge Editor (ReMaKE)\", \"ENTITY INFERENCES\", \"DUNE\", \"RealToxicPrompts\", \"SAfetyBENCH\", \"Bias in Bios dataset\", \"WinoBias\", \"StereoSet\", \"PrivQA\", \"DAMA\", \"DEPN\", \"Privacy in large language models: Attacks, defenses and future directions\", \"A watermark for large language models\", \"Lamp: When large language models meet personalization\", \"Characterchat: Learning towards conversational AI with personalized social support\", \"Editing personality for llms\", \"Emotionbench: Evaluating how llms feel using emotionbench\", \"Aligning language models to user opinions\"], \"Citation\":[\"Perturbation-Restrained Sequential Model Editing\"]}",
        "eval_response": "Evaluation evidence: The assistant's answer inaccurately lists numerous references that are not mentioned in the gold answer, indicating a significant hallucination issue. The citation part of the assistant's answer only partially matches the gold answer, missing two crucial citations and incorrectly formatting the title. The assistant's response fails in both accuracy and completeness, as it does not accurately reflect the citation relationships and includes many irrelevant references.\n\nRating: [[20]]",
        "score": 20
    },
    {
        "level": 4,
        "set": 3,
        "length": 109994,
        "type": "paper",
        "language": "en",
        "question": "",
        "instruction": "We kindly ask you to thoroughly review the provided papers and construct a citation chain from them. Please adhere to the following instructions strictly while completing the task:\n\n#Task Instructions:\nGiven several papers, you are required to identify and list the longest citation chain, which demonstrates the citation relationship among the provided papers.\n\n#Specific Requirements:\n1.Please present the titles of the papers in the form of a list, as follows: [\"Title of Paper 1\", \"Title of Paper 2\", ..., \"Title of Paper n\"].\n2.Ensure that the citation chain in the list is linear and continuous, meaning that the first paper title in the list (Paper 1) should not cite any other works. Instead, it should be cited by the next paper in the list (Paper 2); subsequently, each paper should then be cited by the next one in the list, continuing up to the last paper (Paper n).\n3.Consider only the citation relationships within the supplied collection of papers, and ensure that the citation chain accurately reflects the sequential citation order among these documents.\n4.Do not take into account any articles not provided, and disregard other non-linear citation relationships.",
        "prompt_template": "{instruction}\n\n#Papers Provided:\n{docs}",
        "doc": [
            "2404.11269.md",
            "2211.05244.md",
            "2304.04661.md"
        ],
        "answer": [
            "# DACAD: Domain Adaptation Contrastive Learning for Anomaly Detection in Multivariate Time Series ",
            "# Deep Learning for Time Series Anomaly Detection: A Survey ",
            "# AI for IT Operations (AIOps) on Cloud Platforms: Reviews, Opportunities and Challenges "
        ],
        "shuffle_doc": true,
        "id": "b2842feb-448a-45f1-89b0-d6143407de5b",
        "prompt": "[Question]\nWe kindly ask you to thoroughly review the provided papers and construct a citation chain from them. Please adhere to the following instructions strictly while completing the task:\n\n#Task Instructions:\nGiven several papers, you are required to identify and list the longest citation chain, which demonstrates the citation relationship among the provided papers.\n\n#Specific Requirements:\n1.Please present the titles of the papers in the form of a list, as follows: [\"Title of Paper 1\", \"Title of Paper 2\", ..., \"Title of Paper n\"].\n2.Ensure that the citation chain in the list is linear and continuous, meaning that the first paper title in the list (Paper 1) should not cite any other works. Instead, it should be cited by the next paper in the list (Paper 2); subsequently, each paper should then be cited by the next one in the list, continuing up to the last paper (Paper n).\n3.Consider only the citation relationships within the supplied collection of papers, and ensure that the citation chain accurately reflects the sequential citation order among these documents.\n4.Do not take into account any articles not provided, and disregard other non-linear citation relationships.\n\n#Papers Provided:\n{docs}\n\n[Gold Answer]\n['# DACAD: Domain Adaptation Contrastive Learning for Anomaly Detection in Multivariate Time Series ', '# Deep Learning for Time Series Anomaly Detection: A Survey ', '# AI for IT Operations (AIOps) on Cloud Platforms: Reviews, Opportunities and Challenges ']\n\n[The Start of Assistant's Predicted Answer]\n[]\n[The End of Assistant's Predicted Answer]\n\n[System]\nWe would like to request your feedback on the performance of the AI assistant in response to the user question displayed above according to the gold answer. Please use the following listed aspects and their descriptions as evaluation criteria:\n    - Accuracy and Hallucinations: The assistant's answer is semantically consistent with the gold answer; The numerical value and order need to be accurate, and there should be no hallucinations.\n    - Completeness: Referring to the reference answers, the assistant's answer should contain all the key points needed to answer the user's question; further elaboration on these key points can be omitted.\nPlease rate whether this answer is suitable for the question. Please note that the gold answer can be considered as a correct answer to the question.\n\nThe assistant receives an overall score on a scale of 1 to 100, where a higher score indicates better overall performance.\nPlease note that if the assistant's answer and the gold answer fully meet the above criteria, its overall rating should be the full marks (100).\nPlease first provide a comprehensive explanation of your evaluation, avoiding any potential bias.\nThen, output a line indicating the score of the Assistant.\n\nPLEASE OUTPUT WITH THE FOLLOWING FORMAT, WHERE THE SCORE IS A SCALE OF 1 TO 100 BY STRICTLY FOLLOWING THIS FORMAT: \"[[score]]\", FOR EXAMPLE \"Rating: [[100]]\":\n<start output>\nEvaluation evidence: your evluation explanation here, no more than 100 words\nRating: [[score]]\n<end output>\n\nNow, start your evaluation:",
        "generate_response": "[]",
        "eval_response": "Evaluation evidence: The assistant's response is empty, which fails to meet the task requirements. The gold answer provided a list of paper titles forming a citation chain, which the assistant was supposed to replicate or provide a similar valid chain. Since the assistant did not generate any output, it did not demonstrate accuracy, completeness, or any understanding of the task. Therefore, the performance is inadequate.\n\nRating: [[0]]",
        "score": 0
    },
    {
        "level": 3,
        "set": 1,
        "length": 39235,
        "type": "paper",
        "language": "en",
        "question": "LLaMA Beyond English: An Empirical Study on Language Capability Transfer",
        "instruction": "We hope you will carefully study the provided papers and determine the citation relationships between them. Please follow the instructions below strictly to complete the task:\n\n#Specific Requirements:\n1. Reference: When a given paper mentions other provided papers, those other papers are considered as \"references\" for the given paper. To summarize in this specific context, references are about what the given paper is using.\n2. Citation: Conversely, when other provided papers mention the given paper in their works, the given paper is being \"cited\" by those other papers. To summarize in this specific context, citations are about who is using the given paper.\n3. Given a paper, you need to determine the citation or reference relationship between this paper and the other papers. Do not consider papers that are not provided.\n3. Please present the paper titles in a json format as follows: {{\"Reference\":[\"Reference Title 1\", \"Reference Title 2\", ..., \"Reference Title n\"], \"Citation\":[\"Citation Title 1\", \"Citation Title 2\", ..., \"Citation Title n\"]}}.\n4. If a paper does not have any references or citations, please leave the corresponding list empty, e.g.{{\"Refernce\":[]}}, {{\"Citation\":[]}}.",
        "prompt_template": "#Papers:\n{docs}\n\n{instruction}\n\n#The paper you need to analyze:\n{question}",
        "doc": [
            "2401.01055.md",
            "2405.07745.md",
            "2405.20175.md"
        ],
        "answer": {
            "Reference": [],
            "Citation": [
                "# LlamaTurk: Adapting Open-Source Generative Large Language Models for Low-Resource Language ",
                "# InstructionCP: A fast approach to transfer Large Language Models into target language "
            ]
        },
        "shuffle_doc": true,
        "id": "949f3450-9b30-4556-be5f-ca8afe9e072f",
        "prompt": "[Question]\n#Papers:\n{docs}\n\nWe hope you will carefully study the provided papers and determine the citation relationships between them. Please follow the instructions below strictly to complete the task:\n\n#Specific Requirements:\n1. Reference: When a given paper mentions other provided papers, those other papers are considered as \"references\" for the given paper. To summarize in this specific context, references are about what the given paper is using.\n2. Citation: Conversely, when other provided papers mention the given paper in their works, the given paper is being \"cited\" by those other papers. To summarize in this specific context, citations are about who is using the given paper.\n3. Given a paper, you need to determine the citation or reference relationship between this paper and the other papers. Do not consider papers that are not provided.\n3. Please present the paper titles in a json format as follows: {{\"Reference\":[\"Reference Title 1\", \"Reference Title 2\", ..., \"Reference Title n\"], \"Citation\":[\"Citation Title 1\", \"Citation Title 2\", ..., \"Citation Title n\"]}}.\n4. If a paper does not have any references or citations, please leave the corresponding list empty, e.g.{{\"Refernce\":[]}}, {{\"Citation\":[]}}.\n\n#The paper you need to analyze:\nLLaMA Beyond English: An Empirical Study on Language Capability Transfer\n\n[Gold Answer]\n{'Reference': [], 'Citation': ['# LlamaTurk: Adapting Open-Source Generative Large Language Models for Low-Resource Language ', '# InstructionCP: A fast approach to transfer Large Language Models into target language ']}\n\n[The Start of Assistant's Predicted Answer]\n{\"Reference\":[\"BLOOM: A 176BParameter Open-Access Multilingual Language Model\", \"REALM: Retrieval-Augmented Language Model PreTraining\", \"Compositional Semantic Parsing with Large Language Models\", \"Open LLM Leaderboard\", \"How Close is ChatGPT to Human Experts? Comparison Corpus, Evaluation, and Detection\", \"Vicuna: An Open-Source Chatbot Impressing GPT-4 with $90 \\% *$ ChatGPT Quality\", \"PaLM 2 Technical Report\", \"A Survey on Evaluation of Large Language Models\", \"NusaCrowd: Open Source Initiative for Indonesian NLP Resources\", \"Multilingual Alignment of Contextual Word Representations\", \"Towards Making the Most of Multilingual Pretraining for Zero-Shot Neural Machine Translation\", \"Finding Universal Grammatical Relations in Multilingual BERT\", \"Training Verifiers to Solve Math Word Problems\", \"Unsupervised Cross-lingual Representation Learning at Scale\", \"Emerging Cross-lingual Structure in Pretrained Language Models\", \"Free Dolly: Introducing the World's First Truly Open Instruction-Tuned LLM\", \"Chinese LLaMA and Alpaca Large Language Models\", \"Efficient and Effective Text Encoding for Chinese LLaMA and Alpaca\", \"BERT: Pre-training of Deep Bidirectional Transformers for Language Understanding\", \"LoRA: Low-Rank Adaptation of Large Language Models\", \"Not All Languages Are Created Equal in LLMs: Improving Multilingual Capability by Cross-Lingual-Thought Prompting\", \"Language Models as Zero-Shot Planners: Extracting Actionable Knowledge for Embodied Agents\", \"C-Eval: A Multi-Level Multi-Discipline Chinese Evaluation Suite for Foundation Models\", \"BELLE: Be Everyone's Large Language Model Engine\", \"X-FACTR: Multilingual Factual Knowledge Retrieval from Pretrained Language Models\", \"The State and Fate of Linguistic Diversity and Inclusion in the NLP World\", \"Gpt-4 passes the bar exam\", \"GLUECoS: An Evaluation Benchmark for Code-Switched NLP\", \"Multilingual Code-Switching for ZeroShot Cross-Lingual Intent Prediction and Slot Filling\", \"Bactrian-X : A Multilingual Replicable Instruction-Following Model with Low-Rank Adaptation\", \"Few-shot Learning with Multilingual Language Models\", \"Democratizing LLMs for Low-Resource Languages by Leveraging their English Dominant Abilities with Linguistically-Diverse Prompts\", \"Introducing ChatGPT\", \"Open-Chinese-LLaMA\", \"The RefinedWeb Dataset for Falcon LLM: Outperforming Curated Corpora with Web Data, and Web Data Only\", \"How Multilingual is Multilingual BERT?\", \"Linguistic Diversity in Natural Language Processing\", \"A Survey on In-context Learning\", \"Introducing StableCode\", \"Code-Mixing on Sesame Street: Dawn of the Adversarial Polyglots\", \"Alpaca: A Strong, Replicable Instruction-Following Model\", \"Internlm: A multilingual language model with progressively enhanced capabilities\", \"Language Models are Few-shot Multilingual Learners\", \"Beto, Bentz, Becas: The Surprising Cross-Lingual Effectiveness of BERT\", \"Oolong: Investigating What Makes Crosslingual Transfer Hard with Controlled Studies\", \"LLMEVAL-1 Chinese Large Language Model Evaluation Phase 1\", \"Evaluating the Performance of Large Language Models on GAOKAO Benchmark\", \"AGIEval: A Human-Centric Benchmark for Evaluating Foundation Models\", \"Multilingual Machine Translation with Large Language Models: Empirical Results and Analysis\"], \"Citation\":[\"InstructionCP: A fast approach to transfer Large Language Models into target language\"]}\n[The End of Assistant's Predicted Answer]\n\n[System]\nWe would like to request your feedback on the performance of the AI assistant in response to the user question displayed above according to the gold answer. Please use the following listed aspects and their descriptions as evaluation criteria:\n    - Accuracy and Hallucinations: The assistant's answer is semantically consistent with the gold answer; The numerical value and order need to be accurate, and there should be no hallucinations.\n    - Completeness: Referring to the reference answers, the assistant's answer should contain all the key points needed to answer the user's question; further elaboration on these key points can be omitted.\nPlease rate whether this answer is suitable for the question. Please note that the gold answer can be considered as a correct answer to the question.\n\nThe assistant receives an overall score on a scale of 1 to 100, where a higher score indicates better overall performance.\nPlease note that if the assistant's answer and the gold answer fully meet the above criteria, its overall rating should be the full marks (100).\nPlease first provide a comprehensive explanation of your evaluation, avoiding any potential bias.\nThen, output a line indicating the score of the Assistant.\n\nPLEASE OUTPUT WITH THE FOLLOWING FORMAT, WHERE THE SCORE IS A SCALE OF 1 TO 100 BY STRICTLY FOLLOWING THIS FORMAT: \"[[score]]\", FOR EXAMPLE \"Rating: [[100]]\":\n<start output>\nEvaluation evidence: your evluation explanation here, no more than 100 words\nRating: [[score]]\n<end output>\n\nNow, start your evaluation:",
        "generate_response": "{\"Reference\":[\"BLOOM: A 176BParameter Open-Access Multilingual Language Model\", \"REALM: Retrieval-Augmented Language Model PreTraining\", \"Compositional Semantic Parsing with Large Language Models\", \"Open LLM Leaderboard\", \"How Close is ChatGPT to Human Experts? Comparison Corpus, Evaluation, and Detection\", \"Vicuna: An Open-Source Chatbot Impressing GPT-4 with $90 \\% *$ ChatGPT Quality\", \"PaLM 2 Technical Report\", \"A Survey on Evaluation of Large Language Models\", \"NusaCrowd: Open Source Initiative for Indonesian NLP Resources\", \"Multilingual Alignment of Contextual Word Representations\", \"Towards Making the Most of Multilingual Pretraining for Zero-Shot Neural Machine Translation\", \"Finding Universal Grammatical Relations in Multilingual BERT\", \"Training Verifiers to Solve Math Word Problems\", \"Unsupervised Cross-lingual Representation Learning at Scale\", \"Emerging Cross-lingual Structure in Pretrained Language Models\", \"Free Dolly: Introducing the World's First Truly Open Instruction-Tuned LLM\", \"Chinese LLaMA and Alpaca Large Language Models\", \"Efficient and Effective Text Encoding for Chinese LLaMA and Alpaca\", \"BERT: Pre-training of Deep Bidirectional Transformers for Language Understanding\", \"LoRA: Low-Rank Adaptation of Large Language Models\", \"Not All Languages Are Created Equal in LLMs: Improving Multilingual Capability by Cross-Lingual-Thought Prompting\", \"Language Models as Zero-Shot Planners: Extracting Actionable Knowledge for Embodied Agents\", \"C-Eval: A Multi-Level Multi-Discipline Chinese Evaluation Suite for Foundation Models\", \"BELLE: Be Everyone's Large Language Model Engine\", \"X-FACTR: Multilingual Factual Knowledge Retrieval from Pretrained Language Models\", \"The State and Fate of Linguistic Diversity and Inclusion in the NLP World\", \"Gpt-4 passes the bar exam\", \"GLUECoS: An Evaluation Benchmark for Code-Switched NLP\", \"Multilingual Code-Switching for ZeroShot Cross-Lingual Intent Prediction and Slot Filling\", \"Bactrian-X : A Multilingual Replicable Instruction-Following Model with Low-Rank Adaptation\", \"Few-shot Learning with Multilingual Language Models\", \"Democratizing LLMs for Low-Resource Languages by Leveraging their English Dominant Abilities with Linguistically-Diverse Prompts\", \"Introducing ChatGPT\", \"Open-Chinese-LLaMA\", \"The RefinedWeb Dataset for Falcon LLM: Outperforming Curated Corpora with Web Data, and Web Data Only\", \"How Multilingual is Multilingual BERT?\", \"Linguistic Diversity in Natural Language Processing\", \"A Survey on In-context Learning\", \"Introducing StableCode\", \"Code-Mixing on Sesame Street: Dawn of the Adversarial Polyglots\", \"Alpaca: A Strong, Replicable Instruction-Following Model\", \"Internlm: A multilingual language model with progressively enhanced capabilities\", \"Language Models are Few-shot Multilingual Learners\", \"Beto, Bentz, Becas: The Surprising Cross-Lingual Effectiveness of BERT\", \"Oolong: Investigating What Makes Crosslingual Transfer Hard with Controlled Studies\", \"LLMEVAL-1 Chinese Large Language Model Evaluation Phase 1\", \"Evaluating the Performance of Large Language Models on GAOKAO Benchmark\", \"AGIEval: A Human-Centric Benchmark for Evaluating Foundation Models\", \"Multilingual Machine Translation with Large Language Models: Empirical Results and Analysis\"], \"Citation\":[\"InstructionCP: A fast approach to transfer Large Language Models into target language\"]}",
        "eval_response": "Evaluation evidence: The assistant's answer does not align with the gold answer in terms of accuracy and completeness. The assistant lists numerous references that are not mentioned in the gold answer, indicating a significant amount of hallucination. Additionally, the citation list in the assistant's answer includes only one correct entry from the gold answer but omits another, showing incompleteness. The assistant's response fails to accurately reflect the citation relationships specified in the gold answer, leading to a low score.\n\nRating: [[20]]",
        "score": 20
    },
    {
        "level": 3,
        "set": 2,
        "length": 50614,
        "type": "paper",
        "language": "en",
        "question": "Small LLMs Are Weak Tool Learners: A Multi-LLM Agent",
        "instruction": "We hope you will carefully study the provided papers and determine the citation relationships between them. Please follow the instructions below strictly to complete the task:\n\n#Specific Requirements:\n1. Reference: When a given paper mentions other provided papers, those other papers are considered as \"references\" for the given paper. To summarize in this specific context, references are about what the given paper is using.\n2. Citation: Conversely, when other provided papers mention the given paper in their works, the given paper is being \"cited\" by those other papers. To summarize in this specific context, citations are about who is using the given paper.\n3. Given a paper, you need to determine the citation or reference relationship between this paper and the other papers. Do not consider papers that are not provided.\n3. Please present the paper titles in a json format as follows: {{\"Reference\":[\"Reference Title 1\", \"Reference Title 2\", ..., \"Reference Title n\"], \"Citation\":[\"Citation Title 1\", \"Citation Title 2\", ..., \"Citation Title n\"]}}.\n4. If a paper does not have any references or citations, please leave the corresponding list empty, e.g.{{\"Refernce\":[]}}, {{\"Citation\":[]}}.",
        "prompt_template": "#Papers:\n{docs}\n\n{instruction}\n\n#The paper you need to analyze:\n{question}",
        "doc": [
            "2401.07324.md",
            "2405.13050.md",
            "2308.00352.md"
        ],
        "answer": {
            "Reference": [
                "# METAGPT: META PROGRAMMING FOR A MulTi-AGENT COLLABORATIVE FRAMEWORK "
            ],
            "Citation": [
                "# Human-Centered LLM-Agent User Interface: A Position Paper "
            ]
        },
        "shuffle_doc": true,
        "id": "1a691bd5-6c57-48b3-a9e9-b9c9226607fe",
        "prompt": "[Question]\n#Papers:\n{docs}\n\nWe hope you will carefully study the provided papers and determine the citation relationships between them. Please follow the instructions below strictly to complete the task:\n\n#Specific Requirements:\n1. Reference: When a given paper mentions other provided papers, those other papers are considered as \"references\" for the given paper. To summarize in this specific context, references are about what the given paper is using.\n2. Citation: Conversely, when other provided papers mention the given paper in their works, the given paper is being \"cited\" by those other papers. To summarize in this specific context, citations are about who is using the given paper.\n3. Given a paper, you need to determine the citation or reference relationship between this paper and the other papers. Do not consider papers that are not provided.\n3. Please present the paper titles in a json format as follows: {{\"Reference\":[\"Reference Title 1\", \"Reference Title 2\", ..., \"Reference Title n\"], \"Citation\":[\"Citation Title 1\", \"Citation Title 2\", ..., \"Citation Title n\"]}}.\n4. If a paper does not have any references or citations, please leave the corresponding list empty, e.g.{{\"Refernce\":[]}}, {{\"Citation\":[]}}.\n\n#The paper you need to analyze:\nSmall LLMs Are Weak Tool Learners: A Multi-LLM Agent\n\n[Gold Answer]\n{'Reference': ['# METAGPT: META PROGRAMMING FOR A MulTi-AGENT COLLABORATIVE FRAMEWORK '], 'Citation': ['# Human-Centered LLM-Agent User Interface: A Position Paper ']}\n\n[The Start of Assistant's Predicted Answer]\n{\"Reference\": [\"A Survey on Evaluation of Large Language Models\", \"Vicuna: An Open-Source Chatbot Impressing GPT-4 with $90 \\% *$ ChatGPT Quality\", \"ChatLaw: Open-Source Legal Large Language Model with Integrated External Knowledge Bases\", \"Compositional Semantic Parsing with Large Language Models\", \"Open LLM Leaderboard\", \"How Close is ChatGPT to Human Experts? Comparison Corpus, Evaluation, and Detection\", \"REALM: Retrieval-Augmented Language Model PreTraining\", \"Rethinking with Retrieval: Faithful Large Language Model Inference\", \"Measuring Massive Multitask Language Understanding\", \"Atlas: Few-shot Learning with Retrieval Augmented Language Models\", \"Fireact: Toward language agent fine-tuning\", \"Scaling instruction-finetuned language models\", \"Tora: A tool-integrated reasoning agent for mathematical problem solving\", \"Autogpt: the heart of the open-source agent ecosystem\", \"Measuring mathematical problem solving with the math dataset\", \"Metagpt: Meta programming for multiagent collaborative framework\", \"Modelscope-agent: Building your customizable agent system with open-source large language models\", \"Babyagi\", \"Webgpt: Browser-assisted questionanswering with human feedback\", \"Gorilla: Large language model connected with massive apis\", \"Communicative agents for software development\", \"Tool learning with foundation models\", \"Toolllm: Facilitating large language models to master 16000+ real-world apis\", \"Llama: Open and efficient foundation language models\", \"Llama 2: Open foundation and fine-tuned chat models\", \"Voyager: An open-ended embodied agent with large language models\", \"Self-consistency improves chain of thought reasoning in language models\", \"Chain-of-thought prompting elicits reasoning in large language models\", \"Multi-party chat: Conversational agents in group settings with humans and models\", \"Visual chatgpt: Talking, drawing and editing with visual foundation models\", \"Autogen: Enabling next-gen llm applications via multiagent conversation framework\", \"Chatgpt is not enough: Enhancing large language models with knowledge graphs for fact-aware language modeling\", \"Gpt4tools: Teaching large language model to use tools via self-instruction\", \"Mmreact: Prompting chatgpt for multimodal reasoning and action\", \"Tree of thoughts: Deliberate problem solving with large language models\", \"React: Synergizing reasoning and acting in language models\", \"Memorybank: Enhancing large language models with long-term memory\", \"Ghost in the minecraft: Generally capable agents for open-world enviroments via large language models with text-based knowledge and memory\"], \"Citation\": [\"METAGPT: META PROGRAMMING FOR A MulTi-AGENT COLLABORATIVE FRAMEWORK\", \"Human-Centered LLM-Agent User Interface: A Position Paper\"]}\n[The End of Assistant's Predicted Answer]\n\n[System]\nWe would like to request your feedback on the performance of the AI assistant in response to the user question displayed above according to the gold answer. Please use the following listed aspects and their descriptions as evaluation criteria:\n    - Accuracy and Hallucinations: The assistant's answer is semantically consistent with the gold answer; The numerical value and order need to be accurate, and there should be no hallucinations.\n    - Completeness: Referring to the reference answers, the assistant's answer should contain all the key points needed to answer the user's question; further elaboration on these key points can be omitted.\nPlease rate whether this answer is suitable for the question. Please note that the gold answer can be considered as a correct answer to the question.\n\nThe assistant receives an overall score on a scale of 1 to 100, where a higher score indicates better overall performance.\nPlease note that if the assistant's answer and the gold answer fully meet the above criteria, its overall rating should be the full marks (100).\nPlease first provide a comprehensive explanation of your evaluation, avoiding any potential bias.\nThen, output a line indicating the score of the Assistant.\n\nPLEASE OUTPUT WITH THE FOLLOWING FORMAT, WHERE THE SCORE IS A SCALE OF 1 TO 100 BY STRICTLY FOLLOWING THIS FORMAT: \"[[score]]\", FOR EXAMPLE \"Rating: [[100]]\":\n<start output>\nEvaluation evidence: your evluation explanation here, no more than 100 words\nRating: [[score]]\n<end output>\n\nNow, start your evaluation:",
        "generate_response": "{\"Reference\": [\"A Survey on Evaluation of Large Language Models\", \"Vicuna: An Open-Source Chatbot Impressing GPT-4 with $90 \\% *$ ChatGPT Quality\", \"ChatLaw: Open-Source Legal Large Language Model with Integrated External Knowledge Bases\", \"Compositional Semantic Parsing with Large Language Models\", \"Open LLM Leaderboard\", \"How Close is ChatGPT to Human Experts? Comparison Corpus, Evaluation, and Detection\", \"REALM: Retrieval-Augmented Language Model PreTraining\", \"Rethinking with Retrieval: Faithful Large Language Model Inference\", \"Measuring Massive Multitask Language Understanding\", \"Atlas: Few-shot Learning with Retrieval Augmented Language Models\", \"Fireact: Toward language agent fine-tuning\", \"Scaling instruction-finetuned language models\", \"Tora: A tool-integrated reasoning agent for mathematical problem solving\", \"Autogpt: the heart of the open-source agent ecosystem\", \"Measuring mathematical problem solving with the math dataset\", \"Metagpt: Meta programming for multiagent collaborative framework\", \"Modelscope-agent: Building your customizable agent system with open-source large language models\", \"Babyagi\", \"Webgpt: Browser-assisted questionanswering with human feedback\", \"Gorilla: Large language model connected with massive apis\", \"Communicative agents for software development\", \"Tool learning with foundation models\", \"Toolllm: Facilitating large language models to master 16000+ real-world apis\", \"Llama: Open and efficient foundation language models\", \"Llama 2: Open foundation and fine-tuned chat models\", \"Voyager: An open-ended embodied agent with large language models\", \"Self-consistency improves chain of thought reasoning in language models\", \"Chain-of-thought prompting elicits reasoning in large language models\", \"Multi-party chat: Conversational agents in group settings with humans and models\", \"Visual chatgpt: Talking, drawing and editing with visual foundation models\", \"Autogen: Enabling next-gen llm applications via multiagent conversation framework\", \"Chatgpt is not enough: Enhancing large language models with knowledge graphs for fact-aware language modeling\", \"Gpt4tools: Teaching large language model to use tools via self-instruction\", \"Mmreact: Prompting chatgpt for multimodal reasoning and action\", \"Tree of thoughts: Deliberate problem solving with large language models\", \"React: Synergizing reasoning and acting in language models\", \"Memorybank: Enhancing large language models with long-term memory\", \"Ghost in the minecraft: Generally capable agents for open-world enviroments via large language models with text-based knowledge and memory\"], \"Citation\": [\"METAGPT: META PROGRAMMING FOR A MulTi-AGENT COLLABORATIVE FRAMEWORK\", \"Human-Centered LLM-Agent User Interface: A Position Paper\"]}",
        "eval_response": "Evaluation evidence: The assistant's answer does not align with the gold answer. The gold answer specifies one reference and one citation for the paper \"Small LLMs Are Weak Tool Learners: A Multi-LLM Agent,\" while the assistant's answer lists numerous unrelated references and citations, indicating a significant hallucination issue. This discrepancy shows a lack of accuracy and introduces irrelevant information, failing to meet the completeness criterion as it does not accurately reflect the relationships specified in the gold answer.\n\nRating: [[10]]",
        "score": 10
    },
    {
        "level": 3,
        "set": 2,
        "length": 66504,
        "type": "paper",
        "language": "en",
        "question": "TravelPlanner: A Benchmark for Real-World Planning with Language Agents",
        "instruction": "We hope you will carefully study the provided papers and determine the citation relationships between them. Please follow the instructions below strictly to complete the task:\n\n#Specific Requirements:\n1. Reference: When a given paper mentions other provided papers, those other papers are considered as \"references\" for the given paper. To summarize in this specific context, references are about what the given paper is using.\n2. Citation: Conversely, when other provided papers mention the given paper in their works, the given paper is being \"cited\" by those other papers. To summarize in this specific context, citations are about who is using the given paper.\n3. Given a paper, you need to determine the citation or reference relationship between this paper and the other papers. Do not consider papers that are not provided.\n3. Please present the paper titles in a json format as follows: {{\"Reference\":[\"Reference Title 1\", \"Reference Title 2\", ..., \"Reference Title n\"], \"Citation\":[\"Citation Title 1\", \"Citation Title 2\", ..., \"Citation Title n\"]}}.\n4. If a paper does not have any references or citations, please leave the corresponding list empty, e.g.{{\"Refernce\":[]}}, {{\"Citation\":[]}}.",
        "prompt_template": "#Papers:\n{docs}\n\n{instruction}\n\n#The paper you need to analyze:\n{question}",
        "doc": [
            "2402.01622.md",
            "2405.18208.md",
            "2405.20625.md",
            "2310.06825.md",
            "2401.04088.md"
        ],
        "answer": {
            "Reference": [
                "# Mistral 7B ",
                "# Mixtral of Experts "
            ],
            "Citation": [
                "# A Human-Like Reasoning Framework for Multi-Phases Planning Task with Large Language Models ",
                "# Robust Planning with LLM-Modulo Framework: Case Study in Travel Planning "
            ]
        },
        "shuffle_doc": true,
        "id": "807bbcaf-5471-4f3d-baac-e691180b7edc",
        "prompt": "[Question]\n#Papers:\n{docs}\n\nWe hope you will carefully study the provided papers and determine the citation relationships between them. Please follow the instructions below strictly to complete the task:\n\n#Specific Requirements:\n1. Reference: When a given paper mentions other provided papers, those other papers are considered as \"references\" for the given paper. To summarize in this specific context, references are about what the given paper is using.\n2. Citation: Conversely, when other provided papers mention the given paper in their works, the given paper is being \"cited\" by those other papers. To summarize in this specific context, citations are about who is using the given paper.\n3. Given a paper, you need to determine the citation or reference relationship between this paper and the other papers. Do not consider papers that are not provided.\n3. Please present the paper titles in a json format as follows: {{\"Reference\":[\"Reference Title 1\", \"Reference Title 2\", ..., \"Reference Title n\"], \"Citation\":[\"Citation Title 1\", \"Citation Title 2\", ..., \"Citation Title n\"]}}.\n4. If a paper does not have any references or citations, please leave the corresponding list empty, e.g.{{\"Refernce\":[]}}, {{\"Citation\":[]}}.\n\n#The paper you need to analyze:\nTravelPlanner: A Benchmark for Real-World Planning with Language Agents\n\n[Gold Answer]\n{'Reference': ['# Mistral 7B ', '# Mixtral of Experts '], 'Citation': ['# A Human-Like Reasoning Framework for Multi-Phases Planning Task with Large Language Models ', '# Robust Planning with LLM-Modulo Framework: Case Study in Travel Planning ']}\n\n[The Start of Assistant's Predicted Answer]\n{\"Reference\":[\"A cognitive model of planning\", \"Language models as agent models\", \"Autogpt\", \"Graph of thoughts: Solving elaborate problems with large language models\", \"Language models are few-shot learners\", \"Training verifiers to solve math word problems\", \"Dart: an example of accelerated evolutionary development\", \"Mind2web: Towards a generalist agent for the web\", \"Gemini: a family of highly capable multimodal models\", \"Openagi: When llm meets domain experts\", \"HTN planning: Overview, comparison, and beyond\", \"Planning and the brain\", \"AI Magazine\", \"Solving quantitative reasoning problems with language models\", \"API-Bank: A Comprehensive Benchmark for Tool-Augmented LLMs\", \"Unleashing infinite-length input capacity for large-scale language models with self-controlled memory system\", \"LLM+P: Empowering Large Language Models with Optimal Planning Proficiency\", \"Lost in the Middle: How Language Models Use Long Contexts\", \"AgentBench: Evaluating LLMs as Agents\", \"Chameleon: Plug-and-Play Compositional Reasoning with Large Language Models\", \"Planning in the brain\", \"Robot planning\", \"Task-driven autonomous agent utilizing gpt-4, pinecone, and langchain for diverse applications\", \"WebGPT: Browser-Assisted Question-Answering with Human Feedback\", \"ChatGPT\", \"GPT-4 Technical Report\", \"Generative Agents: Interactive Simulacra of Human Behavior\", \"Are NLP Models Really Able to Solve Simple Math Word Problems?\", \"Gorilla: Large Language Model Connected with Massive APIs\", \"Planning and scheduling in manufacturing and services\", \"ToolLLM: Facilitating Large Language Models to Master $16000+$ Real-World APIs\", \"Solving general arithmetic word problems\", \"Artificial intelligence a modern approach\", \"Toolformer: Language Models Can Teach Themselves to Use Tools\", \"HuggingGPT: Solving AI Tasks with ChatGPT and its Friends in Hugging Face\", \"Reflexion: Language Agents with Verbal Reinforcement Learning\", \"Mastering the game of go with deep neural networks and tree search\", \"The Predictron: End-to-end Learning and Planning\", \"LLM-Planner: Few-shot Grounded Planning for Embodied Agents with Large Language Models\", \"Language agents: a critical evolutionary step of artificial intelligence\", \"Cognitive architectures for language agents\", \"Llama: Open and efficient foundation language models\", \"Llama 2: Open Foundation and Fine-Tuned Chat Models\", \"PlanBench: An Extensible Benchmark for Evaluating Large Language Models on Planning and Reasoning About Change\", \"Voyager: An Open-Ended Embodied Agent with Large Language Models\", \"Chain-of-Thought Prompting Elicits Reasoning in Large Language Models\", \"LLM-Powered Autonomous Agents\", \"On the Tool Manipulation Capability of Open-Source Large Language Models\", \"React: Synergizing Reasoning and Acting in Language Models\", \"Tree of Thoughts: Deliberate Problem Solving with Large Language Models\", \"Distilling Script Knowledge from Large Language Models for Constrained Language Planning\", \"GPT-4V(ision) is a Generalist Web Agent, if Grounded\", \"MemoryBank: Enhancing Large Language Models with Long-Term Memory\", \"WebARENA: A Realistic Web Environment for Building Autonomous Agents\", \"RecurrentGPT: Interactive Generation of (Arbitrarily) Long Text\", \"ToolQA: A Dataset for LLM Question Answering with External Tools\", \"ToolChain*: Efficient Action Space Navigation in Large Language Models with $\\mathrm{A}^{*}$ Search\"], \"Citation\":[\"A Survey on Evaluation of Large Language Models\", \"task success is not enough: Investigating the use of video-language models as behavior critics for catching undesirable agent behaviors\", \"Tool documentation enables zero-shot tool-usage with large language models\", \"Thinking, fast and slow\", \"Can large language models reason and plan?\", \"Llms can't plan, but can help planning in llm-modulo frameworks\", \"Gpt-3.5: Language model\", \"Gpt-4: Language model\", \"Art: Automatic multi-step reasoning and tool-use for large language models\", \"Toolformer: Language models can teach themselves to use tools\", \"Reflexion: Language agents with verbal reinforcement learning\", \"Chain of thoughtlessness: An analysis of cot in planning\", \"On the self-verification limitations of large language models on reasoning and planning tasks\", \"Large language models still can't plan (a benchmark for llms on planning and reasoning about change)\", \"Theory of mind abilities of large language models in human-robot interaction: An illusion?\", \"A survey on large language model based autonomous agents\", \"Chain-of-thought prompting elicits reasoning in large language models\", \"The rise and potential of large language model based agents: A survey\", \"Travelplanner: A benchmark for real-world planning with language agents\", \"React: Synergizing reasoning and acting in language models\"]}\n[The End of Assistant's Predicted Answer]\n\n[System]\nWe would like to request your feedback on the performance of the AI assistant in response to the user question displayed above according to the gold answer. Please use the following listed aspects and their descriptions as evaluation criteria:\n    - Accuracy and Hallucinations: The assistant's answer is semantically consistent with the gold answer; The numerical value and order need to be accurate, and there should be no hallucinations.\n    - Completeness: Referring to the reference answers, the assistant's answer should contain all the key points needed to answer the user's question; further elaboration on these key points can be omitted.\nPlease rate whether this answer is suitable for the question. Please note that the gold answer can be considered as a correct answer to the question.\n\nThe assistant receives an overall score on a scale of 1 to 100, where a higher score indicates better overall performance.\nPlease note that if the assistant's answer and the gold answer fully meet the above criteria, its overall rating should be the full marks (100).\nPlease first provide a comprehensive explanation of your evaluation, avoiding any potential bias.\nThen, output a line indicating the score of the Assistant.\n\nPLEASE OUTPUT WITH THE FOLLOWING FORMAT, WHERE THE SCORE IS A SCALE OF 1 TO 100 BY STRICTLY FOLLOWING THIS FORMAT: \"[[score]]\", FOR EXAMPLE \"Rating: [[100]]\":\n<start output>\nEvaluation evidence: your evluation explanation here, no more than 100 words\nRating: [[score]]\n<end output>\n\nNow, start your evaluation:",
        "generate_response": "{\"Reference\":[\"A cognitive model of planning\", \"Language models as agent models\", \"Autogpt\", \"Graph of thoughts: Solving elaborate problems with large language models\", \"Language models are few-shot learners\", \"Training verifiers to solve math word problems\", \"Dart: an example of accelerated evolutionary development\", \"Mind2web: Towards a generalist agent for the web\", \"Gemini: a family of highly capable multimodal models\", \"Openagi: When llm meets domain experts\", \"HTN planning: Overview, comparison, and beyond\", \"Planning and the brain\", \"AI Magazine\", \"Solving quantitative reasoning problems with language models\", \"API-Bank: A Comprehensive Benchmark for Tool-Augmented LLMs\", \"Unleashing infinite-length input capacity for large-scale language models with self-controlled memory system\", \"LLM+P: Empowering Large Language Models with Optimal Planning Proficiency\", \"Lost in the Middle: How Language Models Use Long Contexts\", \"AgentBench: Evaluating LLMs as Agents\", \"Chameleon: Plug-and-Play Compositional Reasoning with Large Language Models\", \"Planning in the brain\", \"Robot planning\", \"Task-driven autonomous agent utilizing gpt-4, pinecone, and langchain for diverse applications\", \"WebGPT: Browser-Assisted Question-Answering with Human Feedback\", \"ChatGPT\", \"GPT-4 Technical Report\", \"Generative Agents: Interactive Simulacra of Human Behavior\", \"Are NLP Models Really Able to Solve Simple Math Word Problems?\", \"Gorilla: Large Language Model Connected with Massive APIs\", \"Planning and scheduling in manufacturing and services\", \"ToolLLM: Facilitating Large Language Models to Master $16000+$ Real-World APIs\", \"Solving general arithmetic word problems\", \"Artificial intelligence a modern approach\", \"Toolformer: Language Models Can Teach Themselves to Use Tools\", \"HuggingGPT: Solving AI Tasks with ChatGPT and its Friends in Hugging Face\", \"Reflexion: Language Agents with Verbal Reinforcement Learning\", \"Mastering the game of go with deep neural networks and tree search\", \"The Predictron: End-to-end Learning and Planning\", \"LLM-Planner: Few-shot Grounded Planning for Embodied Agents with Large Language Models\", \"Language agents: a critical evolutionary step of artificial intelligence\", \"Cognitive architectures for language agents\", \"Llama: Open and efficient foundation language models\", \"Llama 2: Open Foundation and Fine-Tuned Chat Models\", \"PlanBench: An Extensible Benchmark for Evaluating Large Language Models on Planning and Reasoning About Change\", \"Voyager: An Open-Ended Embodied Agent with Large Language Models\", \"Chain-of-Thought Prompting Elicits Reasoning in Large Language Models\", \"LLM-Powered Autonomous Agents\", \"On the Tool Manipulation Capability of Open-Source Large Language Models\", \"React: Synergizing Reasoning and Acting in Language Models\", \"Tree of Thoughts: Deliberate Problem Solving with Large Language Models\", \"Distilling Script Knowledge from Large Language Models for Constrained Language Planning\", \"GPT-4V(ision) is a Generalist Web Agent, if Grounded\", \"MemoryBank: Enhancing Large Language Models with Long-Term Memory\", \"WebARENA: A Realistic Web Environment for Building Autonomous Agents\", \"RecurrentGPT: Interactive Generation of (Arbitrarily) Long Text\", \"ToolQA: A Dataset for LLM Question Answering with External Tools\", \"ToolChain*: Efficient Action Space Navigation in Large Language Models with $\\mathrm{A}^{*}$ Search\"], \"Citation\":[\"A Survey on Evaluation of Large Language Models\", \"task success is not enough: Investigating the use of video-language models as behavior critics for catching undesirable agent behaviors\", \"Tool documentation enables zero-shot tool-usage with large language models\", \"Thinking, fast and slow\", \"Can large language models reason and plan?\", \"Llms can't plan, but can help planning in llm-modulo frameworks\", \"Gpt-3.5: Language model\", \"Gpt-4: Language model\", \"Art: Automatic multi-step reasoning and tool-use for large language models\", \"Toolformer: Language models can teach themselves to use tools\", \"Reflexion: Language agents with verbal reinforcement learning\", \"Chain of thoughtlessness: An analysis of cot in planning\", \"On the self-verification limitations of large language models on reasoning and planning tasks\", \"Large language models still can't plan (a benchmark for llms on planning and reasoning about change)\", \"Theory of mind abilities of large language models in human-robot interaction: An illusion?\", \"A survey on large language model based autonomous agents\", \"Chain-of-thought prompting elicits reasoning in large language models\", \"The rise and potential of large language model based agents: A survey\", \"Travelplanner: A benchmark for real-world planning with language agents\", \"React: Synergizing reasoning and acting in language models\"]}",
        "eval_response": "Evaluation evidence: The assistant's answer does not align with the gold answer in terms of accuracy and completeness. The gold answer specifies two references and two citations, which are not mentioned in the assistant's extensive list. Instead, the assistant's response includes a large number of unrelated titles, indicating a significant hallucination issue. The response fails to accurately identify the specific references and citations relevant to the \"TravelPlanner: A Benchmark for Real-World Planning with Language Agents\" paper, thus not meeting the task requirements.\n\nRating: [[10]]",
        "score": 10
    },
    {
        "level": 3,
        "set": 2,
        "length": 69984,
        "type": "paper",
        "language": "en",
        "question": "WebVoyager : Building an End-to-End Web Agent with Large Multimodal Models",
        "instruction": "We hope you will carefully study the provided papers and determine the citation relationships between them. Please follow the instructions below strictly to complete the task:\n\n#Specific Requirements:\n1. Reference: When a given paper mentions other provided papers, those other papers are considered as \"references\" for the given paper. To summarize in this specific context, references are about what the given paper is using.\n2. Citation: Conversely, when other provided papers mention the given paper in their works, the given paper is being \"cited\" by those other papers. To summarize in this specific context, citations are about who is using the given paper.\n3. Given a paper, you need to determine the citation or reference relationship between this paper and the other papers. Do not consider papers that are not provided.\n3. Please present the paper titles in a json format as follows: {{\"Reference\":[\"Reference Title 1\", \"Reference Title 2\", ..., \"Reference Title n\"], \"Citation\":[\"Citation Title 1\", \"Citation Title 2\", ..., \"Citation Title n\"]}}.\n4. If a paper does not have any references or citations, please leave the corresponding list empty, e.g.{{\"Refernce\":[]}}, {{\"Citation\":[]}}.",
        "prompt_template": "#Papers:\n{docs}\n\n{instruction}\n\n#The paper you need to analyze:\n{question}",
        "doc": [
            "2401.13919.md",
            "2404.05955.md",
            "2405.03710.md",
            "2405.14573.md"
        ],
        "answer": {
            "Reference": [],
            "Citation": [
                "# VisualWebBench: How Far Have Multimodal LLMs Evolved in Web Page Understanding and Grounding? ",
                "# Automating the Enterprise with Foundation Models ",
                "# AndroidWorld: A Dynamic Benchmarking Environment for Autonomous Agents "
            ]
        },
        "shuffle_doc": true,
        "id": "e65cfea4-8ade-49a8-bf69-83549bdfb890",
        "prompt": "[Question]\n#Papers:\n{docs}\n\nWe hope you will carefully study the provided papers and determine the citation relationships between them. Please follow the instructions below strictly to complete the task:\n\n#Specific Requirements:\n1. Reference: When a given paper mentions other provided papers, those other papers are considered as \"references\" for the given paper. To summarize in this specific context, references are about what the given paper is using.\n2. Citation: Conversely, when other provided papers mention the given paper in their works, the given paper is being \"cited\" by those other papers. To summarize in this specific context, citations are about who is using the given paper.\n3. Given a paper, you need to determine the citation or reference relationship between this paper and the other papers. Do not consider papers that are not provided.\n3. Please present the paper titles in a json format as follows: {{\"Reference\":[\"Reference Title 1\", \"Reference Title 2\", ..., \"Reference Title n\"], \"Citation\":[\"Citation Title 1\", \"Citation Title 2\", ..., \"Citation Title n\"]}}.\n4. If a paper does not have any references or citations, please leave the corresponding list empty, e.g.{{\"Refernce\":[]}}, {{\"Citation\":[]}}.\n\n#The paper you need to analyze:\nWebVoyager : Building an End-to-End Web Agent with Large Multimodal Models\n\n[Gold Answer]\n{'Reference': [], 'Citation': ['# VisualWebBench: How Far Have Multimodal LLMs Evolved in Web Page Understanding and Grounding? ', '# Automating the Enterprise with Foundation Models ', '# AndroidWorld: A Dynamic Benchmarking Environment for Autonomous Agents ']}\n\n[The Start of Assistant's Predicted Answer]\n{\"Reference\": [\"Cm3: A causal masked multimodal model of the internet\", \"AutoGPT\", \"Language models are few-shot learners\", \"Evaluating large language models trained on code\", \"Webvln: Vision-and-language navigation on websites\", \"Seeclick: Harnessing gui grounding for advanced visual gui agents\", \"Palm: Scaling language modeling with pathways\", \"A coefficient of agreement for nominal scales\", \"One model, multiple modalities: A sparsely activated approach for text, sound, image, video, and code\", \"Mind2web: Towards a generalist agent for the web\", \"A survey for in-context learning\", \"An image is worth $16 \\\\times 16$ words: Transformers for image recognition at scale\", \"Measuring nominal scale agreement among many raters\", \"Multimodal web navigation with instruction-finetuned foundation models\", \"Making the v in vqa matter: Elevating the role of image understanding in visual question answering\", \"A real-world webagent with planning, long context understanding, and program synthesis\", \"The abduction of sherlock holmes: A dataset for visual abductive reasoning\", \"Visualwebarena: Evaluating multimodal agents on realistic visual web tasks\", \"Pix2struct: Screenshot parsing as pretraining for visual language understanding\", \"Visualbert: A simple and performant baseline for vision and language\", \"Learn to explain: Multimodal reasoning via thought chains for science question answering\", \"Chameleon: Plug-and-play compositional reasoning with large language models\", \"Laser: Llm agent with state-space exploration for web navigation\", \"Gaia: a benchmark for general ai assistants\", \"Webgpt: Browser-assisted questionanswering with human feedback\", \"Gpt-4 technical report\", \"Training language models to follow instructions with human feedback\", \"Toolllm: Facilitating large language models to master 16000+ real-world apis\", \"Exploring the limits of transfer learning with a unified text-to-text transformer\", \"Toolformer: Language models can teach themselves to use tools\", \"From pixels to ui actions: Learning to follow instructions via graphical user interfaces\", \"World of bits: An open-domain platform for web-based agents\", \"Reflexion: Language agents with verbal reinforcement learning\", \"Gemini: a family of highly capable multimodal models\", \"Self-instruct: Aligning language model with self generated instructions\", \"Simvlm: Simple visual language model pretraining with weak supervision\", \"Llm-powered autonomous agents\", \"Gpt-4v in wonderland: Large multimodal models for zero-shot smartphone gui navigation\", \"Set-of-mark prompting unleashes extraordinary visual grounding in gpt-4v\", \"The dawn of $1 mms$ : Preliminary explorations with gpt-4v (ision)\", \"Webshop: Towards scalable real-world web interaction with grounded language agents\", \"React: Synergizing reasoning and acting in language models\", \"Lumos: Learning agents with unified data, modular design, and open-source llms\", \"From recognition to cognition: Visual commonsense reasoning\", \"Appagent: Multimodal agents as smartphone users\", \"Gpt-4v (ision) is a generalist web agent, if grounded\", \"Webarena: A realistic web environment for building autonomous agents\", \"Object detection in 20 years: A survey\"], \"Citation\": []}\n[The End of Assistant's Predicted Answer]\n\n[System]\nWe would like to request your feedback on the performance of the AI assistant in response to the user question displayed above according to the gold answer. Please use the following listed aspects and their descriptions as evaluation criteria:\n    - Accuracy and Hallucinations: The assistant's answer is semantically consistent with the gold answer; The numerical value and order need to be accurate, and there should be no hallucinations.\n    - Completeness: Referring to the reference answers, the assistant's answer should contain all the key points needed to answer the user's question; further elaboration on these key points can be omitted.\nPlease rate whether this answer is suitable for the question. Please note that the gold answer can be considered as a correct answer to the question.\n\nThe assistant receives an overall score on a scale of 1 to 100, where a higher score indicates better overall performance.\nPlease note that if the assistant's answer and the gold answer fully meet the above criteria, its overall rating should be the full marks (100).\nPlease first provide a comprehensive explanation of your evaluation, avoiding any potential bias.\nThen, output a line indicating the score of the Assistant.\n\nPLEASE OUTPUT WITH THE FOLLOWING FORMAT, WHERE THE SCORE IS A SCALE OF 1 TO 100 BY STRICTLY FOLLOWING THIS FORMAT: \"[[score]]\", FOR EXAMPLE \"Rating: [[100]]\":\n<start output>\nEvaluation evidence: your evluation explanation here, no more than 100 words\nRating: [[score]]\n<end output>\n\nNow, start your evaluation:",
        "generate_response": "{\"Reference\": [\"Cm3: A causal masked multimodal model of the internet\", \"AutoGPT\", \"Language models are few-shot learners\", \"Evaluating large language models trained on code\", \"Webvln: Vision-and-language navigation on websites\", \"Seeclick: Harnessing gui grounding for advanced visual gui agents\", \"Palm: Scaling language modeling with pathways\", \"A coefficient of agreement for nominal scales\", \"One model, multiple modalities: A sparsely activated approach for text, sound, image, video, and code\", \"Mind2web: Towards a generalist agent for the web\", \"A survey for in-context learning\", \"An image is worth $16 \\\\times 16$ words: Transformers for image recognition at scale\", \"Measuring nominal scale agreement among many raters\", \"Multimodal web navigation with instruction-finetuned foundation models\", \"Making the v in vqa matter: Elevating the role of image understanding in visual question answering\", \"A real-world webagent with planning, long context understanding, and program synthesis\", \"The abduction of sherlock holmes: A dataset for visual abductive reasoning\", \"Visualwebarena: Evaluating multimodal agents on realistic visual web tasks\", \"Pix2struct: Screenshot parsing as pretraining for visual language understanding\", \"Visualbert: A simple and performant baseline for vision and language\", \"Learn to explain: Multimodal reasoning via thought chains for science question answering\", \"Chameleon: Plug-and-play compositional reasoning with large language models\", \"Laser: Llm agent with state-space exploration for web navigation\", \"Gaia: a benchmark for general ai assistants\", \"Webgpt: Browser-assisted questionanswering with human feedback\", \"Gpt-4 technical report\", \"Training language models to follow instructions with human feedback\", \"Toolllm: Facilitating large language models to master 16000+ real-world apis\", \"Exploring the limits of transfer learning with a unified text-to-text transformer\", \"Toolformer: Language models can teach themselves to use tools\", \"From pixels to ui actions: Learning to follow instructions via graphical user interfaces\", \"World of bits: An open-domain platform for web-based agents\", \"Reflexion: Language agents with verbal reinforcement learning\", \"Gemini: a family of highly capable multimodal models\", \"Self-instruct: Aligning language model with self generated instructions\", \"Simvlm: Simple visual language model pretraining with weak supervision\", \"Llm-powered autonomous agents\", \"Gpt-4v in wonderland: Large multimodal models for zero-shot smartphone gui navigation\", \"Set-of-mark prompting unleashes extraordinary visual grounding in gpt-4v\", \"The dawn of $1 mms$ : Preliminary explorations with gpt-4v (ision)\", \"Webshop: Towards scalable real-world web interaction with grounded language agents\", \"React: Synergizing reasoning and acting in language models\", \"Lumos: Learning agents with unified data, modular design, and open-source llms\", \"From recognition to cognition: Visual commonsense reasoning\", \"Appagent: Multimodal agents as smartphone users\", \"Gpt-4v (ision) is a generalist web agent, if grounded\", \"Webarena: A realistic web environment for building autonomous agents\", \"Object detection in 20 years: A survey\"], \"Citation\": []}",
        "eval_response": "Evaluation evidence: The assistant's answer does not align with the gold answer in terms of accuracy and completeness. The assistant lists a large number of references which are not mentioned in the gold answer, indicating a significant hallucination issue. Furthermore, the assistant fails to list any citations, whereas the gold answer specifies three papers that cite the analyzed paper. This discrepancy shows a lack of accuracy in identifying citation relationships and an overgeneration of irrelevant references, leading to a failure in meeting the task requirements.\n\nRating: [[10]]",
        "score": 10
    },
    {
        "level": 4,
        "set": 3,
        "length": 122464,
        "type": "paper",
        "language": "en",
        "question": "",
        "instruction": "We kindly ask you to thoroughly review the provided papers and construct a citation chain from them. Please adhere to the following instructions strictly while completing the task:\n\n#Task Instructions:\nGiven several papers, you are required to identify and list the longest citation chain, which demonstrates the citation relationship among the provided papers.\n\n#Specific Requirements:\n1.Please present the titles of the papers in the form of a list, as follows: [\"Title of Paper 1\", \"Title of Paper 2\", ..., \"Title of Paper n\"].\n2.Ensure that the citation chain in the list is linear and continuous, meaning that the first paper title in the list (Paper 1) should not cite any other works. Instead, it should be cited by the next paper in the list (Paper 2); subsequently, each paper should then be cited by the next one in the list, continuing up to the last paper (Paper n).\n3.Consider only the citation relationships within the supplied collection of papers, and ensure that the citation chain accurately reflects the sequential citation order among these documents.\n4.Do not take into account any articles not provided, and disregard other non-linear citation relationships.",
        "prompt_template": "{instruction}\n\n#Papers Provided:\n{docs}",
        "doc": [
            "2210.16724.md",
            "2302.09751.md",
            "2312.15791.md",
            "2403.07059.md",
            "2405.07770.md"
        ],
        "answer": [
            "# QuEst: Graph Transformer for Quantum Circuit Reliability Estimation ",
            "# VQE-generated Quantum Circuit Dataset for Machine Learning ",
            "# SantaQlaus: A resource-efficient method to leverage quantum shot-noise for optimization of variational quantum algorithms ",
            "# Better than classical? The subtle art of benchmarking quantum machine learning models ",
            "# Hype or Heuristic? Quantum Reinforcement Learning for Join Order Optimisation "
        ],
        "shuffle_doc": true,
        "id": "1f576521-dbe4-4b01-b55c-6f5490629596",
        "prompt": "[Question]\nWe kindly ask you to thoroughly review the provided papers and construct a citation chain from them. Please adhere to the following instructions strictly while completing the task:\n\n#Task Instructions:\nGiven several papers, you are required to identify and list the longest citation chain, which demonstrates the citation relationship among the provided papers.\n\n#Specific Requirements:\n1.Please present the titles of the papers in the form of a list, as follows: [\"Title of Paper 1\", \"Title of Paper 2\", ..., \"Title of Paper n\"].\n2.Ensure that the citation chain in the list is linear and continuous, meaning that the first paper title in the list (Paper 1) should not cite any other works. Instead, it should be cited by the next paper in the list (Paper 2); subsequently, each paper should then be cited by the next one in the list, continuing up to the last paper (Paper n).\n3.Consider only the citation relationships within the supplied collection of papers, and ensure that the citation chain accurately reflects the sequential citation order among these documents.\n4.Do not take into account any articles not provided, and disregard other non-linear citation relationships.\n\n#Papers Provided:\n{docs}\n\n[Gold Answer]\n['# QuEst: Graph Transformer for Quantum Circuit Reliability Estimation ', '# VQE-generated Quantum Circuit Dataset for Machine Learning ', '# SantaQlaus: A resource-efficient method to leverage quantum shot-noise for optimization of variational quantum algorithms ', '# Better than classical? The subtle art of benchmarking quantum machine learning models ', '# Hype or Heuristic? Quantum Reinforcement Learning for Join Order Optimisation ']\n\n[The Start of Assistant's Predicted Answer]\n{\"head\": \"VQE-generated Quantum Circuit Dataset for Machine Learning\", \"relation\": \"reference\", \"tail\": [\"A variational eigenvalue solver on a photonic quantum processor\", \"The variational quantum eigensolver: a review of methods and best practices\", \"OpenQASM 3: A Broader and Deeper Quantum Assembly Language\", \"Generative modeling approach for benchmarking and training shallow quantum circuits\", \"Qdataset, quantum datasets for machine learning\", \"Qulacs: a fast and versatile quantum circuit simulator for research purpose\", \"Quest: Graph transformer for quantum circuit reliability estimation\", \"Quantum machine learning\", \"Variational Quantum Eigensolver (VQE)\", \"Quantum computation of electronic transitions using a variational quantum eigensolver\", \"Unitary block optimization for variational quantum algorithms\"]}\n[The End of Assistant's Predicted Answer]\n\n[System]\nWe would like to request your feedback on the performance of the AI assistant in response to the user question displayed above according to the gold answer. Please use the following listed aspects and their descriptions as evaluation criteria:\n    - Accuracy and Hallucinations: The assistant's answer is semantically consistent with the gold answer; The numerical value and order need to be accurate, and there should be no hallucinations.\n    - Completeness: Referring to the reference answers, the assistant's answer should contain all the key points needed to answer the user's question; further elaboration on these key points can be omitted.\nPlease rate whether this answer is suitable for the question. Please note that the gold answer can be considered as a correct answer to the question.\n\nThe assistant receives an overall score on a scale of 1 to 100, where a higher score indicates better overall performance.\nPlease note that if the assistant's answer and the gold answer fully meet the above criteria, its overall rating should be the full marks (100).\nPlease first provide a comprehensive explanation of your evaluation, avoiding any potential bias.\nThen, output a line indicating the score of the Assistant.\n\nPLEASE OUTPUT WITH THE FOLLOWING FORMAT, WHERE THE SCORE IS A SCALE OF 1 TO 100 BY STRICTLY FOLLOWING THIS FORMAT: \"[[score]]\", FOR EXAMPLE \"Rating: [[100]]\":\n<start output>\nEvaluation evidence: your evluation explanation here, no more than 100 words\nRating: [[score]]\n<end output>\n\nNow, start your evaluation:",
        "generate_response": "{\"head\": \"VQE-generated Quantum Circuit Dataset for Machine Learning\", \"relation\": \"reference\", \"tail\": [\"A variational eigenvalue solver on a photonic quantum processor\", \"The variational quantum eigensolver: a review of methods and best practices\", \"OpenQASM 3: A Broader and Deeper Quantum Assembly Language\", \"Generative modeling approach for benchmarking and training shallow quantum circuits\", \"Qdataset, quantum datasets for machine learning\", \"Qulacs: a fast and versatile quantum circuit simulator for research purpose\", \"Quest: Graph transformer for quantum circuit reliability estimation\", \"Quantum machine learning\", \"Variational Quantum Eigensolver (VQE)\", \"Quantum computation of electronic transitions using a variational quantum eigensolver\", \"Unitary block optimization for variational quantum algorithms\"]}",
        "eval_response": "Evaluation evidence: The assistant's response does not align with the task requirements. The task was to identify and list the longest citation chain among the provided papers, presented in a linear and continuous list format. Instead, the assistant provided a structured JSON-like output detailing references for a specific paper, which is not in accordance with the task of forming a citation chain. The response does not match the format or content requirements specified in the task instructions, leading to a significant deviation from the expected answer.\n\nRating: [[10]]",
        "score": 10
    },
    {
        "level": 3,
        "set": 2,
        "length": 97608,
        "type": "paper",
        "language": "en",
        "question": "On Prompt-Driven Safeguarding for Large Language Models",
        "instruction": "We hope you will carefully study the provided papers and determine the citation relationships between them. Please follow the instructions below strictly to complete the task:\n\n#Specific Requirements:\n1. Reference: When a given paper mentions other provided papers, those other papers are considered as \"references\" for the given paper. To summarize in this specific context, references are about what the given paper is using.\n2. Citation: Conversely, when other provided papers mention the given paper in their works, the given paper is being \"cited\" by those other papers. To summarize in this specific context, citations are about who is using the given paper.\n3. Given a paper, you need to determine the citation or reference relationship between this paper and the other papers. Do not consider papers that are not provided.\n3. Please present the paper titles in a json format as follows: {{\"Reference\":[\"Reference Title 1\", \"Reference Title 2\", ..., \"Reference Title n\"], \"Citation\":[\"Citation Title 1\", \"Citation Title 2\", ..., \"Citation Title n\"]}}.\n4. If a paper does not have any references or citations, please leave the corresponding list empty, e.g.{{\"Refernce\":[]}}, {{\"Citation\":[]}}.",
        "prompt_template": "#Papers:\n{docs}\n\n{instruction}\n\n#The paper you need to analyze:\n{question}",
        "doc": [
            "2401.18018.md",
            "2405.20947.md",
            "2311.10702.md",
            "2401.06373.md"
        ],
        "answer": {
            "Reference": [
                "# Camels in a Changing Climate: Enhancing LM Adaptation with TLU 2 ",
                "# How Johnny Can Persuade LLMs to Jailbreak Them: Rethinking Persuasion to Challenge AI Safety by Humanizing LLMs "
            ],
            "Citation": [
                "# OR-Bench: An Over-Refusal Benchmark for Large Language Models "
            ]
        },
        "shuffle_doc": true,
        "id": "e9039d18-4ce9-47c3-95e6-b6c01a403d26",
        "prompt": "[Question]\n#Papers:\n{docs}\n\nWe hope you will carefully study the provided papers and determine the citation relationships between them. Please follow the instructions below strictly to complete the task:\n\n#Specific Requirements:\n1. Reference: When a given paper mentions other provided papers, those other papers are considered as \"references\" for the given paper. To summarize in this specific context, references are about what the given paper is using.\n2. Citation: Conversely, when other provided papers mention the given paper in their works, the given paper is being \"cited\" by those other papers. To summarize in this specific context, citations are about who is using the given paper.\n3. Given a paper, you need to determine the citation or reference relationship between this paper and the other papers. Do not consider papers that are not provided.\n3. Please present the paper titles in a json format as follows: {{\"Reference\":[\"Reference Title 1\", \"Reference Title 2\", ..., \"Reference Title n\"], \"Citation\":[\"Citation Title 1\", \"Citation Title 2\", ..., \"Citation Title n\"]}}.\n4. If a paper does not have any references or citations, please leave the corresponding list empty, e.g.{{\"Refernce\":[]}}, {{\"Citation\":[]}}.\n\n#The paper you need to analyze:\nOn Prompt-Driven Safeguarding for Large Language Models\n\n[Gold Answer]\n{'Reference': ['# Camels in a Changing Climate: Enhancing LM Adaptation with TLU 2 ', '# How Johnny Can Persuade LLMs to Jailbreak Them: Rethinking Persuasion to Challenge AI Safety by Humanizing LLMs '], 'Citation': ['# OR-Bench: An Over-Refusal Benchmark for Large Language Models ']}\n\n[The Start of Assistant's Predicted Answer]\n{\"Reference\": [\"The CRINGE loss: Learning what language not to model\", \"Training a helpful and harmless assistant with reinforcement learning from human feedback\", \"Constitutional ai: Harmlessness from ai feedback\", \"Purple llama cyberseceval: A secure coding benchmark for language models\", \"RLPrompt: Optimizing discrete text prompts with reinforcement learning\", \"The curious case of neural text degeneration\", \"Catastrophic jailbreak of open-source LLMs via exploiting generation\", \"Camels in a changing climate: Enhancing lm adaptation with tulu 2\", \"Mistral 7b\", \"The power of scale for parameter-efficient prompt tuning\", \"Alpacaeval: An automatic evaluator of instruction-following models\", \"Self-alignment with instruction backtranslation\", \"Prefix-tuning: Optimizing continuous prompts for generation\", \"AutoDAN: Generating stealthy jailbreak prompts on aligned large language models\", \"Code llama: Open foundation models for code\", \"Gpt-4 technical report\", \"On second thought, let's not think step by step! bias and toxicity in zero-shot reasoning\", \"Towards Controllable Biases in Language Generation\", \"AutoPrompt: Eliciting Knowledge from Language Models with Automatically Generated Prompts\", \"On the safety of conversational models: Taxonomy, dataset, and benchmark\", \"Llama 2: Open foundation and finetuned chat models\", \"Openchat: Advancing open-source language models with mixed-quality data\", \"Jailbroken: How does LLM safety training fail?\", \"Ethical and social risks of harm from language models\", \"Neural text generation with unlikelihood training\", \"Recipes for safety in open-domain chatbots\", \"Large language models as optimizers\", \"How johnny can persuade llms to jailbreak them: Rethinking persuasion to challenge ai safety by humanizing llms\", \"Ethical-advice taker: Do language models understand natural language interventions?\", \"Chat templates for huggingface large language models\", \"Exploring prompt-based few-shot learning for grounded dialog generation\", \"Click: Controllable text generation with sequence likelihood contrastive learning\", \"Weak-to-strong extrapolation expedites alignment\", \"Large language models are not robust multiple choice selectors\", \"Large language models are humanlevel prompt engineers\", \"Universal and transferable adversarial attacks on aligned language models\"], \"Citation\": []}\n[The End of Assistant's Predicted Answer]\n\n[System]\nWe would like to request your feedback on the performance of the AI assistant in response to the user question displayed above according to the gold answer. Please use the following listed aspects and their descriptions as evaluation criteria:\n    - Accuracy and Hallucinations: The assistant's answer is semantically consistent with the gold answer; The numerical value and order need to be accurate, and there should be no hallucinations.\n    - Completeness: Referring to the reference answers, the assistant's answer should contain all the key points needed to answer the user's question; further elaboration on these key points can be omitted.\nPlease rate whether this answer is suitable for the question. Please note that the gold answer can be considered as a correct answer to the question.\n\nThe assistant receives an overall score on a scale of 1 to 100, where a higher score indicates better overall performance.\nPlease note that if the assistant's answer and the gold answer fully meet the above criteria, its overall rating should be the full marks (100).\nPlease first provide a comprehensive explanation of your evaluation, avoiding any potential bias.\nThen, output a line indicating the score of the Assistant.\n\nPLEASE OUTPUT WITH THE FOLLOWING FORMAT, WHERE THE SCORE IS A SCALE OF 1 TO 100 BY STRICTLY FOLLOWING THIS FORMAT: \"[[score]]\", FOR EXAMPLE \"Rating: [[100]]\":\n<start output>\nEvaluation evidence: your evluation explanation here, no more than 100 words\nRating: [[score]]\n<end output>\n\nNow, start your evaluation:",
        "generate_response": "{\"Reference\": [\"The CRINGE loss: Learning what language not to model\", \"Training a helpful and harmless assistant with reinforcement learning from human feedback\", \"Constitutional ai: Harmlessness from ai feedback\", \"Purple llama cyberseceval: A secure coding benchmark for language models\", \"RLPrompt: Optimizing discrete text prompts with reinforcement learning\", \"The curious case of neural text degeneration\", \"Catastrophic jailbreak of open-source LLMs via exploiting generation\", \"Camels in a changing climate: Enhancing lm adaptation with tulu 2\", \"Mistral 7b\", \"The power of scale for parameter-efficient prompt tuning\", \"Alpacaeval: An automatic evaluator of instruction-following models\", \"Self-alignment with instruction backtranslation\", \"Prefix-tuning: Optimizing continuous prompts for generation\", \"AutoDAN: Generating stealthy jailbreak prompts on aligned large language models\", \"Code llama: Open foundation models for code\", \"Gpt-4 technical report\", \"On second thought, let's not think step by step! bias and toxicity in zero-shot reasoning\", \"Towards Controllable Biases in Language Generation\", \"AutoPrompt: Eliciting Knowledge from Language Models with Automatically Generated Prompts\", \"On the safety of conversational models: Taxonomy, dataset, and benchmark\", \"Llama 2: Open foundation and finetuned chat models\", \"Openchat: Advancing open-source language models with mixed-quality data\", \"Jailbroken: How does LLM safety training fail?\", \"Ethical and social risks of harm from language models\", \"Neural text generation with unlikelihood training\", \"Recipes for safety in open-domain chatbots\", \"Large language models as optimizers\", \"How johnny can persuade llms to jailbreak them: Rethinking persuasion to challenge ai safety by humanizing llms\", \"Ethical-advice taker: Do language models understand natural language interventions?\", \"Chat templates for huggingface large language models\", \"Exploring prompt-based few-shot learning for grounded dialog generation\", \"Click: Controllable text generation with sequence likelihood contrastive learning\", \"Weak-to-strong extrapolation expedites alignment\", \"Large language models are not robust multiple choice selectors\", \"Large language models are humanlevel prompt engineers\", \"Universal and transferable adversarial attacks on aligned language models\"], \"Citation\": []}",
        "eval_response": "Evaluation evidence: The assistant's answer inaccurately lists numerous references that are not mentioned in the gold answer, indicating a significant deviation and hallucination. The gold answer specifies two references and one citation, but the assistant's answer includes a long list of references and no citations, failing to match the gold answer's content. This discrepancy shows issues with accuracy and completeness, as the assistant neither correctly identifies the references nor includes the citation.\n\nRating: [[20]]",
        "score": 20
    },
    {
        "level": 3,
        "set": 3,
        "length": 190906,
        "type": "paper",
        "language": "en",
        "question": "RAG VS Fine-TUNing: PiPELINES, TRADEOFFS, AND A CASE STUDY ON AGRICULTURE",
        "instruction": "We hope you will carefully study the provided papers and determine the citation relationships between them. Please follow the instructions below strictly to complete the task:\n\n#Specific Requirements:\n1. Reference: When a given paper mentions other provided papers, those other papers are considered as \"references\" for the given paper. To summarize in this specific context, references are about what the given paper is using.\n2. Citation: Conversely, when other provided papers mention the given paper in their works, the given paper is being \"cited\" by those other papers. To summarize in this specific context, citations are about who is using the given paper.\n3. Given a paper, you need to determine the citation or reference relationship between this paper and the other papers. Do not consider papers that are not provided.\n3. Please present the paper titles in a json format as follows: {{\"Reference\":[\"Reference Title 1\", \"Reference Title 2\", ..., \"Reference Title n\"], \"Citation\":[\"Citation Title 1\", \"Citation Title 2\", ..., \"Citation Title n\"]}}.\n4. If a paper does not have any references or citations, please leave the corresponding list empty, e.g.{{\"Refernce\":[]}}, {{\"Citation\":[]}}.",
        "prompt_template": "#Papers:\n{docs}\n\n{instruction}\n\n#The paper you need to analyze:\n{question}",
        "doc": [
            "2401.08406.md",
            "2404.15939.md",
            "2405.03085.md",
            "2405.07437.md",
            "2405.17147.md",
            "2405.20347.md",
            "2306.05685.md",
            "2307.03875.md",
            "2307.16877.md",
            "2310.06225.md",
            "2310.17631.md"
        ],
        "answer": {
            "Reference": [
                "# Judging LLM-as-a-Judge with MT-Bench and Chatbot Arena ",
                "# Large Language Models for Supply Chain Optimization ",
                "# Evaluating Correctness and Faithfulness of Instruction-Following Models for Question Answering ",
                "# GPT-4 AS AN AGRONOMIST ASSISTANT? ANSWERING AGRICULTURE QUESTIONS USING LARGE LANGUAGE MODELS ",
                "# JUDGELM : FINE-TUNED LARGE LANGUAGE MODELS ARE SCALABLE JUDGES "
            ],
            "Citation": [
                "# Telco-RAG: Navigating the Challenges of Retrieval-Augmented Language Models for Telecommunications ",
                "# Compressing Long Context for Enhancing RAG with AMR-based Concept Distillation ",
                "# Evaluation of Retrieval-Augmented Generation: A Survey ",
                "# Large Language Models (LLMs): Deployment, Tokenomics and Sustainability ",
                "# Small Language Models for Application Interactions: A Case Study "
            ]
        },
        "shuffle_doc": true,
        "id": "6e3079ba-812e-4059-883c-cc699d8f9cd1",
        "prompt": "[Question]\n#Papers:\n{docs}\n\nWe hope you will carefully study the provided papers and determine the citation relationships between them. Please follow the instructions below strictly to complete the task:\n\n#Specific Requirements:\n1. Reference: When a given paper mentions other provided papers, those other papers are considered as \"references\" for the given paper. To summarize in this specific context, references are about what the given paper is using.\n2. Citation: Conversely, when other provided papers mention the given paper in their works, the given paper is being \"cited\" by those other papers. To summarize in this specific context, citations are about who is using the given paper.\n3. Given a paper, you need to determine the citation or reference relationship between this paper and the other papers. Do not consider papers that are not provided.\n3. Please present the paper titles in a json format as follows: {{\"Reference\":[\"Reference Title 1\", \"Reference Title 2\", ..., \"Reference Title n\"], \"Citation\":[\"Citation Title 1\", \"Citation Title 2\", ..., \"Citation Title n\"]}}.\n4. If a paper does not have any references or citations, please leave the corresponding list empty, e.g.{{\"Refernce\":[]}}, {{\"Citation\":[]}}.\n\n#The paper you need to analyze:\nRAG VS Fine-TUNing: PiPELINES, TRADEOFFS, AND A CASE STUDY ON AGRICULTURE\n\n[Gold Answer]\n{'Reference': ['# Judging LLM-as-a-Judge with MT-Bench and Chatbot Arena ', '# Large Language Models for Supply Chain Optimization ', '# Evaluating Correctness and Faithfulness of Instruction-Following Models for Question Answering ', '# GPT-4 AS AN AGRONOMIST ASSISTANT? ANSWERING AGRICULTURE QUESTIONS USING LARGE LANGUAGE MODELS ', '# JUDGELM : FINE-TUNED LARGE LANGUAGE MODELS ARE SCALABLE JUDGES '], 'Citation': ['# Telco-RAG: Navigating the Challenges of Retrieval-Augmented Language Models for Telecommunications ', '# Compressing Long Context for Enhancing RAG with AMR-based Concept Distillation ', '# Evaluation of Retrieval-Augmented Generation: A Survey ', '# Large Language Models (LLMs): Deployment, Tokenomics and Sustainability ', '# Small Language Models for Application Interactions: A Case Study ']}\n\n[The Start of Assistant's Predicted Answer]\n{\"Reference\":[\"A Comparison of Statistical Downscaling Methods Suited for Wildfire Applications\", \"International Journal of Climatology\", \"Vicuna: An Open-Source Chatbot Impressing GPT-4 with $90 \\% *$ ChatGPT Quality\", \"500 Questions 500 Answers - Embrapa/SCT\", \"Farmer advisory (KVK Q&A portal)\", \"Vikaspedia\", \"Lora: Low-rank adaptation of large language models\", \"Billion-scale similarity search with GPUs\", \"From word embeddings to document distances\", \"Retrieval-augmented generation for knowledge-intensive nlp tasks\", \"Large language models for supply chain optimization\", \"FActScore: Fine-grained Atomic Evaluation of Factual Precision in Long Form Text Generation\", \"Large Dual Encoders Are Generalizable Retrievers\", \"Capabilities of gpt-4 on medical challenge problems\", \"Gpt-4 technical report\", \"Pdf2text\", \"Pypdf\", \"ZeRO: Memory Optimizations Toward Training Trillion Parameter Models\", \"Sentence-BERT: Sentence Embeddings using Siamese BERT-Networks\", \"Knowledge guided representation learning and causal structure learning in soil science\", \"GPT-4 as an agronomist assistant? answering agriculture exams using large language models\", \"Microsoft copilot impact on finance\", \"Large language models in medicine\", \"LLaMA: Open and Efficient Foundation Language Models\", \"Llama 2: Open Foundation and Fine-Tuned Chat Models\", \"How llm applications are revolutionizing the manufacturing industry\", \"FarmBeats: An IoT platform for data-driven agriculture\", \"Seeing through clouds in satellite images\", \"Judging LLM-as-a-Judge with MT-Bench and Chatbot Arena\", \"JudgeLM: Fine-tuned Large Language Models are Scalable Judges\", \"Scrapy\"], \"Citation\":[\"Attention is all you need\", \"Large Language Models for Telecom: Forthcoming Impact on the Industry\", \"TeleQnA: A Benchmark Dataset to Assess Large Language Models Telecommunications Knowledge\", \"The computational limits of deep learning\", \"Fine-Tuning or Retrieval? Comparing Knowledge Injection in LLMs\", \"Telecom language models: Must they be large?\", \"Evaluating the Ideal Chunk Size for a RAG System Using LlamaIndex\", \"The Chronicles of RAG: The Retriever, the Chunk and the Generator\", \"RQRAG: Learning to Refine Queries for Retrieval Augmented Generation\", \"Unleashing the potential of prompt engineering in Large Language Models: a comprehensive review\", \"Improving the Domain Adaptation of Retrieval Augmented Generation (RAG) Models for Open Domain Question Answering\", \"Faiss: Facebook ai similarity search\", \"TR 21.905, Vocabulary for 3GPP Specifications\", \"Specifications by series\", \"ChatGPT Outperforms CrowdWorkers for Text-Annotation Tasks\", \"Neural Approaches to Conversational AI\", \"New embedding models and api updates\", \"Matryoshka Representation Learning\"]}\n[The End of Assistant's Predicted Answer]\n\n[System]\nWe would like to request your feedback on the performance of the AI assistant in response to the user question displayed above according to the gold answer. Please use the following listed aspects and their descriptions as evaluation criteria:\n    - Accuracy and Hallucinations: The assistant's answer is semantically consistent with the gold answer; The numerical value and order need to be accurate, and there should be no hallucinations.\n    - Completeness: Referring to the reference answers, the assistant's answer should contain all the key points needed to answer the user's question; further elaboration on these key points can be omitted.\nPlease rate whether this answer is suitable for the question. Please note that the gold answer can be considered as a correct answer to the question.\n\nThe assistant receives an overall score on a scale of 1 to 100, where a higher score indicates better overall performance.\nPlease note that if the assistant's answer and the gold answer fully meet the above criteria, its overall rating should be the full marks (100).\nPlease first provide a comprehensive explanation of your evaluation, avoiding any potential bias.\nThen, output a line indicating the score of the Assistant.\n\nPLEASE OUTPUT WITH THE FOLLOWING FORMAT, WHERE THE SCORE IS A SCALE OF 1 TO 100 BY STRICTLY FOLLOWING THIS FORMAT: \"[[score]]\", FOR EXAMPLE \"Rating: [[100]]\":\n<start output>\nEvaluation evidence: your evluation explanation here, no more than 100 words\nRating: [[score]]\n<end output>\n\nNow, start your evaluation:",
        "generate_response": "{\"Reference\":[\"A Comparison of Statistical Downscaling Methods Suited for Wildfire Applications\", \"International Journal of Climatology\", \"Vicuna: An Open-Source Chatbot Impressing GPT-4 with $90 \\% *$ ChatGPT Quality\", \"500 Questions 500 Answers - Embrapa/SCT\", \"Farmer advisory (KVK Q&A portal)\", \"Vikaspedia\", \"Lora: Low-rank adaptation of large language models\", \"Billion-scale similarity search with GPUs\", \"From word embeddings to document distances\", \"Retrieval-augmented generation for knowledge-intensive nlp tasks\", \"Large language models for supply chain optimization\", \"FActScore: Fine-grained Atomic Evaluation of Factual Precision in Long Form Text Generation\", \"Large Dual Encoders Are Generalizable Retrievers\", \"Capabilities of gpt-4 on medical challenge problems\", \"Gpt-4 technical report\", \"Pdf2text\", \"Pypdf\", \"ZeRO: Memory Optimizations Toward Training Trillion Parameter Models\", \"Sentence-BERT: Sentence Embeddings using Siamese BERT-Networks\", \"Knowledge guided representation learning and causal structure learning in soil science\", \"GPT-4 as an agronomist assistant? answering agriculture exams using large language models\", \"Microsoft copilot impact on finance\", \"Large language models in medicine\", \"LLaMA: Open and Efficient Foundation Language Models\", \"Llama 2: Open Foundation and Fine-Tuned Chat Models\", \"How llm applications are revolutionizing the manufacturing industry\", \"FarmBeats: An IoT platform for data-driven agriculture\", \"Seeing through clouds in satellite images\", \"Judging LLM-as-a-Judge with MT-Bench and Chatbot Arena\", \"JudgeLM: Fine-tuned Large Language Models are Scalable Judges\", \"Scrapy\"], \"Citation\":[\"Attention is all you need\", \"Large Language Models for Telecom: Forthcoming Impact on the Industry\", \"TeleQnA: A Benchmark Dataset to Assess Large Language Models Telecommunications Knowledge\", \"The computational limits of deep learning\", \"Fine-Tuning or Retrieval? Comparing Knowledge Injection in LLMs\", \"Telecom language models: Must they be large?\", \"Evaluating the Ideal Chunk Size for a RAG System Using LlamaIndex\", \"The Chronicles of RAG: The Retriever, the Chunk and the Generator\", \"RQRAG: Learning to Refine Queries for Retrieval Augmented Generation\", \"Unleashing the potential of prompt engineering in Large Language Models: a comprehensive review\", \"Improving the Domain Adaptation of Retrieval Augmented Generation (RAG) Models for Open Domain Question Answering\", \"Faiss: Facebook ai similarity search\", \"TR 21.905, Vocabulary for 3GPP Specifications\", \"Specifications by series\", \"ChatGPT Outperforms CrowdWorkers for Text-Annotation Tasks\", \"Neural Approaches to Conversational AI\", \"New embedding models and api updates\", \"Matryoshka Representation Learning\"]}",
        "eval_response": "Evaluation evidence: The assistant's answer does not align with the gold answer, indicating a significant discrepancy in accuracy and completeness. The references and citations listed in the assistant's response do not match those in the gold answer, suggesting hallucinations and inaccuracies. The assistant's response includes titles not present in the gold answer, and it misses several key references and citations that are crucial for a correct response. This misalignment shows a failure to accurately identify the relationships between the specified paper and others provided.\n\nRating: [[20]]",
        "score": 20
    },
    {
        "level": 3,
        "set": 2,
        "length": 56826,
        "type": "paper",
        "language": "en",
        "question": "Patchscopes: A Unifying Framework for Inspecting Hidden Representations of Language Models",
        "instruction": "We hope you will carefully study the provided papers and determine the citation relationships between them. Please follow the instructions below strictly to complete the task:\n\n#Specific Requirements:\n1. Reference: When a given paper mentions other provided papers, those other papers are considered as \"references\" for the given paper. To summarize in this specific context, references are about what the given paper is using.\n2. Citation: Conversely, when other provided papers mention the given paper in their works, the given paper is being \"cited\" by those other papers. To summarize in this specific context, citations are about who is using the given paper.\n3. Given a paper, you need to determine the citation or reference relationship between this paper and the other papers. Do not consider papers that are not provided.\n3. Please present the paper titles in a json format as follows: {{\"Reference\":[\"Reference Title 1\", \"Reference Title 2\", ..., \"Reference Title n\"], \"Citation\":[\"Citation Title 1\", \"Citation Title 2\", ..., \"Citation Title n\"]}}.\n4. If a paper does not have any references or citations, please leave the corresponding list empty, e.g.{{\"Refernce\":[]}}, {{\"Citation\":[]}}.",
        "prompt_template": "#Papers:\n{docs}\n\n{instruction}\n\n#The paper you need to analyze:\n{question}",
        "doc": [
            "2401.06102.md",
            "2404.05971.md",
            "2310.15916.md"
        ],
        "answer": {
            "Reference": [
                "# In-Context Learning Creates Task Vectors "
            ],
            "Citation": [
                "# Does Transformer Interpretability Transfer to RNNs? "
            ]
        },
        "shuffle_doc": true,
        "id": "a47da151-06cf-4888-9636-90491d9150ce",
        "prompt": "[Question]\n#Papers:\n{docs}\n\nWe hope you will carefully study the provided papers and determine the citation relationships between them. Please follow the instructions below strictly to complete the task:\n\n#Specific Requirements:\n1. Reference: When a given paper mentions other provided papers, those other papers are considered as \"references\" for the given paper. To summarize in this specific context, references are about what the given paper is using.\n2. Citation: Conversely, when other provided papers mention the given paper in their works, the given paper is being \"cited\" by those other papers. To summarize in this specific context, citations are about who is using the given paper.\n3. Given a paper, you need to determine the citation or reference relationship between this paper and the other papers. Do not consider papers that are not provided.\n3. Please present the paper titles in a json format as follows: {{\"Reference\":[\"Reference Title 1\", \"Reference Title 2\", ..., \"Reference Title n\"], \"Citation\":[\"Citation Title 1\", \"Citation Title 2\", ..., \"Citation Title n\"]}}.\n4. If a paper does not have any references or citations, please leave the corresponding list empty, e.g.{{\"Refernce\":[]}}, {{\"Citation\":[]}}.\n\n#The paper you need to analyze:\nPatchscopes: A Unifying Framework for Inspecting Hidden Representations of Language Models\n\n[Gold Answer]\n{'Reference': ['# In-Context Learning Creates Task Vectors '], 'Citation': ['# Does Transformer Interpretability Transfer to RNNs? ']}\n\n[The Start of Assistant's Predicted Answer]\n{\"Reference\": [\"Understanding intermediate layers using linear classifier probes\", \"Revisiting model stitching to compare neural representations\", \"Probing classifiers: Promises, shortcomings, and advances\", \"Analysis methods in neural language processing: A survey\", \"A Survey on Evaluation of Large Language Models\", \"Vicuna: An open-source chatbot impressing GPT-4 with $90 \\% *$ ChatGPT quality\", \"Eliciting latent predictions from transformers with the tuned lens\", \"Graph of thoughts: Solving elaborate problems with large language models\", \"Pythia: A suite for analyzing large language models across training and scaling\", \"Language models can explain neurons in language models\", \"Sok: Toward transparent AI: A survey on interpreting the inner structures of deep neural networks\", \"Transformer feed-forward layers build predictions by promoting concepts in the vocabulary space\", \"Dissecting recall of factual associations in auto-regressive language models\", \"Dissect: Disentangled simultaneous explanations via concept traversals\", \"Locating and editing factual associations in gpt\", \"Mass-editing memory in a transformer\", \"The expressive power of transformers with chain of thought\", \"Linearly mapping from image to text space\", \"A mechanism for solving relational tasks in transformer language models\", \"Can llms facilitate interpretation of pre-trained language models?\", \"Mapping language models to grounded conceptual spaces\", \"Sentence-bert: Sentence embeddings using siamese bert-networks\", \"Multimodal neurons in pretrained text-only transformers\", \"Explaining black box text modules in natural language with language models\", \"A mechanistic interpretation of arithmetic reasoning in language models using causal mediation analysis\", \"Lstmvis: A tool for visual analysis of hidden state dynamics in recurrent neural networks\", \"Bert rediscovers the classical nlp pipeline\", \"Investigating gender bias in language models using causal mediation analysis\", \"Interpretability in the wild: a circuit for indirect object identification in GPT-2 small\", \"Gaussian Process Probes (GPP) for uncertainty-aware probing\", \"Chain-of-thought prompting elicits reasoning in large language models\", \"Tree of thoughts: Deliberate problem solving with large language models\", \"Give me the facts! a survey on factual knowledge probing in pre-trained language models\", \"Towards best practices of activation patching in language models: Metrics and methods\", \"Interpreting deep visual representations via network dissection\"], \"Citation\": []}\n[The End of Assistant's Predicted Answer]\n\n[System]\nWe would like to request your feedback on the performance of the AI assistant in response to the user question displayed above according to the gold answer. Please use the following listed aspects and their descriptions as evaluation criteria:\n    - Accuracy and Hallucinations: The assistant's answer is semantically consistent with the gold answer; The numerical value and order need to be accurate, and there should be no hallucinations.\n    - Completeness: Referring to the reference answers, the assistant's answer should contain all the key points needed to answer the user's question; further elaboration on these key points can be omitted.\nPlease rate whether this answer is suitable for the question. Please note that the gold answer can be considered as a correct answer to the question.\n\nThe assistant receives an overall score on a scale of 1 to 100, where a higher score indicates better overall performance.\nPlease note that if the assistant's answer and the gold answer fully meet the above criteria, its overall rating should be the full marks (100).\nPlease first provide a comprehensive explanation of your evaluation, avoiding any potential bias.\nThen, output a line indicating the score of the Assistant.\n\nPLEASE OUTPUT WITH THE FOLLOWING FORMAT, WHERE THE SCORE IS A SCALE OF 1 TO 100 BY STRICTLY FOLLOWING THIS FORMAT: \"[[score]]\", FOR EXAMPLE \"Rating: [[100]]\":\n<start output>\nEvaluation evidence: your evluation explanation here, no more than 100 words\nRating: [[score]]\n<end output>\n\nNow, start your evaluation:",
        "generate_response": "{\"Reference\": [\"Understanding intermediate layers using linear classifier probes\", \"Revisiting model stitching to compare neural representations\", \"Probing classifiers: Promises, shortcomings, and advances\", \"Analysis methods in neural language processing: A survey\", \"A Survey on Evaluation of Large Language Models\", \"Vicuna: An open-source chatbot impressing GPT-4 with $90 \\% *$ ChatGPT quality\", \"Eliciting latent predictions from transformers with the tuned lens\", \"Graph of thoughts: Solving elaborate problems with large language models\", \"Pythia: A suite for analyzing large language models across training and scaling\", \"Language models can explain neurons in language models\", \"Sok: Toward transparent AI: A survey on interpreting the inner structures of deep neural networks\", \"Transformer feed-forward layers build predictions by promoting concepts in the vocabulary space\", \"Dissecting recall of factual associations in auto-regressive language models\", \"Dissect: Disentangled simultaneous explanations via concept traversals\", \"Locating and editing factual associations in gpt\", \"Mass-editing memory in a transformer\", \"The expressive power of transformers with chain of thought\", \"Linearly mapping from image to text space\", \"A mechanism for solving relational tasks in transformer language models\", \"Can llms facilitate interpretation of pre-trained language models?\", \"Mapping language models to grounded conceptual spaces\", \"Sentence-bert: Sentence embeddings using siamese bert-networks\", \"Multimodal neurons in pretrained text-only transformers\", \"Explaining black box text modules in natural language with language models\", \"A mechanistic interpretation of arithmetic reasoning in language models using causal mediation analysis\", \"Lstmvis: A tool for visual analysis of hidden state dynamics in recurrent neural networks\", \"Bert rediscovers the classical nlp pipeline\", \"Investigating gender bias in language models using causal mediation analysis\", \"Interpretability in the wild: a circuit for indirect object identification in GPT-2 small\", \"Gaussian Process Probes (GPP) for uncertainty-aware probing\", \"Chain-of-thought prompting elicits reasoning in large language models\", \"Tree of thoughts: Deliberate problem solving with large language models\", \"Give me the facts! a survey on factual knowledge probing in pre-trained language models\", \"Towards best practices of activation patching in language models: Metrics and methods\", \"Interpreting deep visual representations via network dissection\"], \"Citation\": []}",
        "eval_response": "Evaluation evidence: The assistant's answer contains a significant number of hallucinated references that are not mentioned in the gold answer, indicating a lack of accuracy and the presence of hallucinations. The gold answer lists one reference and one citation, but the assistant's answer includes a long list of references that are not corroborated by the gold answer and fails to include the correct citation. This discrepancy shows a failure in both accuracy and completeness.\n\nRating: [[10]]",
        "score": 10
    },
    {
        "level": 3,
        "set": 4,
        "length": 218267,
        "type": "paper",
        "language": "en",
        "question": "LLMs Can't Plan, But Can Help Planning in LLM-Modulo Frameworks",
        "instruction": "We hope you will carefully study the provided papers and determine the citation relationships between them. Please follow the instructions below strictly to complete the task:\n\n#Specific Requirements:\n1. Reference: When a given paper mentions other provided papers, those other papers are considered as \"references\" for the given paper. To summarize in this specific context, references are about what the given paper is using.\n2. Citation: Conversely, when other provided papers mention the given paper in their works, the given paper is being \"cited\" by those other papers. To summarize in this specific context, citations are about who is using the given paper.\n3. Given a paper, you need to determine the citation or reference relationship between this paper and the other papers. Do not consider papers that are not provided.\n3. Please present the paper titles in a json format as follows: {{\"Reference\":[\"Reference Title 1\", \"Reference Title 2\", ..., \"Reference Title n\"], \"Citation\":[\"Citation Title 1\", \"Citation Title 2\", ..., \"Citation Title n\"]}}.\n4. If a paper does not have any references or citations, please leave the corresponding list empty, e.g.{{\"Refernce\":[]}}, {{\"Citation\":[]}}.",
        "prompt_template": "#Papers:\n{docs}\n\n{instruction}\n\n#The paper you need to analyze:\n{question}",
        "doc": [
            "2402.01817.md",
            "2404.18311.md",
            "2405.05885.md",
            "2405.13966.md",
            "2405.15194.md",
            "2405.16661.md",
            "2310.01798.md",
            "2310.08118.md",
            "2310.12397.md",
            "2310.12931.md",
            "2401.05302.md"
        ],
        "answer": {
            "Reference": [
                "# LARGE LANGUAGE MoDELS CANNOT SELF-CorRECT REASONING YET ",
                "# Can Large Language Models Really Improve by Self-critiquing Their Own Plans? ",
                "# GPT-4 Doesn't Know It's Wrong: An Analysis of Iterative Prompting for Reasoning Problems ",
                "# EUREKA: HUMAN-LEVEL REWARD DESIGN VIA CODING LARGE LANGUAGE MODELS ",
                "# Theory of Mind abilities of Large Language Models in Human-Robot Interaction : An Illusion? "
            ],
            "Citation": [
                "# Towards Incremental Learning in Large Language Models: A Critical Review ",
                "# Co-driver: VLM-based Autonomous Driving Assistant with Human-like Behavior and Understanding for Complex Road Scenes ",
                "# On the Brittle Foundations of ReAct Prompting for Agentic Large Language Models ",
                "# Efficient Reinforcement Learning via Large Language Model-based Search ",
                "# RLSF: Reinforcement Learning via Symbolic Feedback "
            ]
        },
        "shuffle_doc": true,
        "id": "4f6afc9e-e1c8-4d32-ae5d-4ba7deac73cb",
        "prompt": "[Question]\n#Papers:\n{docs}\n\nWe hope you will carefully study the provided papers and determine the citation relationships between them. Please follow the instructions below strictly to complete the task:\n\n#Specific Requirements:\n1. Reference: When a given paper mentions other provided papers, those other papers are considered as \"references\" for the given paper. To summarize in this specific context, references are about what the given paper is using.\n2. Citation: Conversely, when other provided papers mention the given paper in their works, the given paper is being \"cited\" by those other papers. To summarize in this specific context, citations are about who is using the given paper.\n3. Given a paper, you need to determine the citation or reference relationship between this paper and the other papers. Do not consider papers that are not provided.\n3. Please present the paper titles in a json format as follows: {{\"Reference\":[\"Reference Title 1\", \"Reference Title 2\", ..., \"Reference Title n\"], \"Citation\":[\"Citation Title 1\", \"Citation Title 2\", ..., \"Citation Title n\"]}}.\n4. If a paper does not have any references or citations, please leave the corresponding list empty, e.g.{{\"Refernce\":[]}}, {{\"Citation\":[]}}.\n\n#The paper you need to analyze:\nLLMs Can't Plan, But Can Help Planning in LLM-Modulo Frameworks\n\n[Gold Answer]\n{'Reference': ['# LARGE LANGUAGE MoDELS CANNOT SELF-CorRECT REASONING YET ', '# Can Large Language Models Really Improve by Self-critiquing Their Own Plans? ', \"# GPT-4 Doesn't Know It's Wrong: An Analysis of Iterative Prompting for Reasoning Problems \", '# EUREKA: HUMAN-LEVEL REWARD DESIGN VIA CODING LARGE LANGUAGE MODELS ', '# Theory of Mind abilities of Large Language Models in Human-Robot Interaction : An Illusion? '], 'Citation': ['# Towards Incremental Learning in Large Language Models: A Critical Review ', '# Co-driver: VLM-based Autonomous Driving Assistant with Human-like Behavior and Understanding for Complex Road Scenes ', '# On the Brittle Foundations of ReAct Prompting for Agentic Large Language Models ', '# Efficient Reinforcement Learning via Large Language Model-based Search ', '# RLSF: Reinforcement Learning via Symbolic Feedback ']}\n\n[The Start of Assistant's Predicted Answer]\n{\"Reference\":[\"The Alchemy of yore is not that Chemistry is useless, but that people wanted to delude themselves that Chemistry can be Nuclear Physics if you prompt it just so\", \"Alchemy of yore\", \"Chemistry\", \"Nuclear Physics\", \"automated planning community (Ghallab et al. 2004)\", \"Ghallab et al. 2004\", \"SAT Modulo Theories (Nieuwenhuis \\& Oliveras, 2006)\", \"Nieuwenhuis \\& Oliveras, 2006\", \"International Planning Competition (IPC, 1998)\", \"IPC, 1998\", \"PDDL specification (McDermott et al. 1998)\", \"McDermott et al. 1998\", \"Deep space network planning\", \"mars rover task planning\", \"Johnston et al., 2014\", \"Bresina et al., 2004\", \"Clever Hans effect (cle)\", \"Clever Hans\", \"fine tune the LLM on blocks world planning problem-solution pairs\", \"Pallagani et al. 2023\", \"use a classical planner like FF (Hoffmann \\& Nebel 2001)\", \"Hoffmann \\& Nebel 2001\", \"LLM-Modulo framework\", \"AlphaGeometry\", \"FunSearch\", \"Reinforcement Learning with Simulators\", \"RL systems\", \"AlphaGeometry (Trinh et al. (2024)\", \"Trinh et al. (2024)\", \"FunSearch (Romera-Paredes et al., 2023)\", \"Romera-Paredes et al., 2023\", \"Polanyi's Revenge\", \"Kambhampati, 2021\", \"multi-modal LLMs (e.g. GPT4V)\", \"GPT4V\", \"LLM's can help come up with approximate quasi-symbolic transition models\", \"AlphaGeometry\", \"FunSearch\", \"SayCan (Ahn et al. 2022)\", \"Ahn et al. 2022\", \"On the planning abilities of large language models - a critical investigation (Valmeekam et al. 2023c)\", \"Valmeekam et al. 2023c\"], \"Citation\":[\"Guan et al. 2023\", \"Guan et al. 2022\", \"Howey et al. 2004\", \"Olmo et al. 2021\", \"Wang et al. 2023\", \"Valmeekam et al. 2023c\", \"Stechly et al. 2023\", \"Yao et al. 2023a\", \"Yao et al. 2023b\", \"Hao et al. 2023\", \"Liang et al. 2023\", \"Ma et al. 2023\", \"McCoy et al. 2023\", \"Pallagani et al. 2023\", \"Pan et al. 2023\", \"Romera-Paredes et al. 2023\", \"Shinn et al. 2023\", \"Shridhar et al. 2021\", \"Silver et al. 2022\", \"Stechly et al. 2023\", \"Trinh et al. 2024\", \"Ullman 2023\", \"Valmeekam et al. 2023a\", \"Valmeekam et al. 2023b\", \"Verma et al. 2024\", \"Wang et al. 2023\", \"Weng et al. 2023\", \"Xie et al. 2023\", \"Yao et al. 2023a\", \"Yao et al. 2023b\", \"Yao et al. 2023c\", \"Yao et al. 2023d\", \"Yao et al. 2023e\", \"Yao et al. 2023f\", \"Yao et al. 2023g\", \"Yao et al. 2023h\", \"Yao et al. 2023i\"]}\n[The End of Assistant's Predicted Answer]\n\n[System]\nWe would like to request your feedback on the performance of the AI assistant in response to the user question displayed above according to the gold answer. Please use the following listed aspects and their descriptions as evaluation criteria:\n    - Accuracy and Hallucinations: The assistant's answer is semantically consistent with the gold answer; The numerical value and order need to be accurate, and there should be no hallucinations.\n    - Completeness: Referring to the reference answers, the assistant's answer should contain all the key points needed to answer the user's question; further elaboration on these key points can be omitted.\nPlease rate whether this answer is suitable for the question. Please note that the gold answer can be considered as a correct answer to the question.\n\nThe assistant receives an overall score on a scale of 1 to 100, where a higher score indicates better overall performance.\nPlease note that if the assistant's answer and the gold answer fully meet the above criteria, its overall rating should be the full marks (100).\nPlease first provide a comprehensive explanation of your evaluation, avoiding any potential bias.\nThen, output a line indicating the score of the Assistant.\n\nPLEASE OUTPUT WITH THE FOLLOWING FORMAT, WHERE THE SCORE IS A SCALE OF 1 TO 100 BY STRICTLY FOLLOWING THIS FORMAT: \"[[score]]\", FOR EXAMPLE \"Rating: [[100]]\":\n<start output>\nEvaluation evidence: your evluation explanation here, no more than 100 words\nRating: [[score]]\n<end output>\n\nNow, start your evaluation:",
        "generate_response": "{\"Reference\":[\"The Alchemy of yore is not that Chemistry is useless, but that people wanted to delude themselves that Chemistry can be Nuclear Physics if you prompt it just so\", \"Alchemy of yore\", \"Chemistry\", \"Nuclear Physics\", \"automated planning community (Ghallab et al. 2004)\", \"Ghallab et al. 2004\", \"SAT Modulo Theories (Nieuwenhuis \\& Oliveras, 2006)\", \"Nieuwenhuis \\& Oliveras, 2006\", \"International Planning Competition (IPC, 1998)\", \"IPC, 1998\", \"PDDL specification (McDermott et al. 1998)\", \"McDermott et al. 1998\", \"Deep space network planning\", \"mars rover task planning\", \"Johnston et al., 2014\", \"Bresina et al., 2004\", \"Clever Hans effect (cle)\", \"Clever Hans\", \"fine tune the LLM on blocks world planning problem-solution pairs\", \"Pallagani et al. 2023\", \"use a classical planner like FF (Hoffmann \\& Nebel 2001)\", \"Hoffmann \\& Nebel 2001\", \"LLM-Modulo framework\", \"AlphaGeometry\", \"FunSearch\", \"Reinforcement Learning with Simulators\", \"RL systems\", \"AlphaGeometry (Trinh et al. (2024)\", \"Trinh et al. (2024)\", \"FunSearch (Romera-Paredes et al., 2023)\", \"Romera-Paredes et al., 2023\", \"Polanyi's Revenge\", \"Kambhampati, 2021\", \"multi-modal LLMs (e.g. GPT4V)\", \"GPT4V\", \"LLM's can help come up with approximate quasi-symbolic transition models\", \"AlphaGeometry\", \"FunSearch\", \"SayCan (Ahn et al. 2022)\", \"Ahn et al. 2022\", \"On the planning abilities of large language models - a critical investigation (Valmeekam et al. 2023c)\", \"Valmeekam et al. 2023c\"], \"Citation\":[\"Guan et al. 2023\", \"Guan et al. 2022\", \"Howey et al. 2004\", \"Olmo et al. 2021\", \"Wang et al. 2023\", \"Valmeekam et al. 2023c\", \"Stechly et al. 2023\", \"Yao et al. 2023a\", \"Yao et al. 2023b\", \"Hao et al. 2023\", \"Liang et al. 2023\", \"Ma et al. 2023\", \"McCoy et al. 2023\", \"Pallagani et al. 2023\", \"Pan et al. 2023\", \"Romera-Paredes et al. 2023\", \"Shinn et al. 2023\", \"Shridhar et al. 2021\", \"Silver et al. 2022\", \"Stechly et al. 2023\", \"Trinh et al. 2024\", \"Ullman 2023\", \"Valmeekam et al. 2023a\", \"Valmeekam et al. 2023b\", \"Verma et al. 2024\", \"Wang et al. 2023\", \"Weng et al. 2023\", \"Xie et al. 2023\", \"Yao et al. 2023a\", \"Yao et al. 2023b\", \"Yao et al. 2023c\", \"Yao et al. 2023d\", \"Yao et al. 2023e\", \"Yao et al. 2023f\", \"Yao et al. 2023g\", \"Yao et al. 2023h\", \"Yao et al. 2023i\"]}",
        "eval_response": "Evaluation evidence: The assistant's response does not align with the gold answer in terms of accuracy and completeness. The references and citations listed in the assistant's response do not match those in the gold answer, indicating significant hallucinations and inaccuracies. The assistant's response includes numerous titles and references that are not present in the gold answer, which suggests a misunderstanding of the task requirements and the provided documents. This leads to a failure in accurately identifying the citation and reference relationships for the specified paper.\n\nRating: [[10]]",
        "score": 10
    },
    {
        "level": 4,
        "set": 2,
        "length": 59281,
        "type": "paper",
        "language": "en",
        "question": "",
        "instruction": "We kindly ask you to thoroughly review the provided papers and construct a citation chain from them. Please adhere to the following instructions strictly while completing the task:\n\n#Task Instructions:\nGiven several papers, you are required to identify and list the longest citation chain, which demonstrates the citation relationship among the provided papers.\n\n#Specific Requirements:\n1.Please present the titles of the papers in the form of a list, as follows: [\"Title of Paper 1\", \"Title of Paper 2\", ..., \"Title of Paper n\"].\n2.Ensure that the citation chain in the list is linear and continuous, meaning that the first paper title in the list (Paper 1) should not cite any other works. Instead, it should be cited by the next paper in the list (Paper 2); subsequently, each paper should then be cited by the next one in the list, continuing up to the last paper (Paper n).\n3.Consider only the citation relationships within the supplied collection of papers, and ensure that the citation chain accurately reflects the sequential citation order among these documents.\n4.Do not take into account any articles not provided, and disregard other non-linear citation relationships.",
        "prompt_template": "{instruction}\n\n#Papers Provided:\n{docs}",
        "doc": [
            "2401.00368.md",
            "2310.20329.md",
            "2401.04621.md"
        ],
        "answer": [
            "# Improving Text Embeddings with Large Language Models ",
            "# InstructCoder: Instruction Tuning Large Language Models for Code Editing ",
            "#  DebugBench: <br> Evaluating Debugging Capability of Large Language Models "
        ],
        "shuffle_doc": true,
        "id": "59fc6ed6-6750-4b52-916d-cd466b856922",
        "prompt": "[Question]\nWe kindly ask you to thoroughly review the provided papers and construct a citation chain from them. Please adhere to the following instructions strictly while completing the task:\n\n#Task Instructions:\nGiven several papers, you are required to identify and list the longest citation chain, which demonstrates the citation relationship among the provided papers.\n\n#Specific Requirements:\n1.Please present the titles of the papers in the form of a list, as follows: [\"Title of Paper 1\", \"Title of Paper 2\", ..., \"Title of Paper n\"].\n2.Ensure that the citation chain in the list is linear and continuous, meaning that the first paper title in the list (Paper 1) should not cite any other works. Instead, it should be cited by the next paper in the list (Paper 2); subsequently, each paper should then be cited by the next one in the list, continuing up to the last paper (Paper n).\n3.Consider only the citation relationships within the supplied collection of papers, and ensure that the citation chain accurately reflects the sequential citation order among these documents.\n4.Do not take into account any articles not provided, and disregard other non-linear citation relationships.\n\n#Papers Provided:\n{docs}\n\n[Gold Answer]\n['# Improving Text Embeddings with Large Language Models ', '# InstructCoder: Instruction Tuning Large Language Models for Code Editing ', '#  DebugBench: <br> Evaluating Debugging Capability of Large Language Models ']\n\n[The Start of Assistant's Predicted Answer]\n[\"InstructCoder: Instruction Tuning Large Language Models for Code Editing\"]\n[The End of Assistant's Predicted Answer]\n\n[System]\nWe would like to request your feedback on the performance of the AI assistant in response to the user question displayed above according to the gold answer. Please use the following listed aspects and their descriptions as evaluation criteria:\n    - Accuracy and Hallucinations: The assistant's answer is semantically consistent with the gold answer; The numerical value and order need to be accurate, and there should be no hallucinations.\n    - Completeness: Referring to the reference answers, the assistant's answer should contain all the key points needed to answer the user's question; further elaboration on these key points can be omitted.\nPlease rate whether this answer is suitable for the question. Please note that the gold answer can be considered as a correct answer to the question.\n\nThe assistant receives an overall score on a scale of 1 to 100, where a higher score indicates better overall performance.\nPlease note that if the assistant's answer and the gold answer fully meet the above criteria, its overall rating should be the full marks (100).\nPlease first provide a comprehensive explanation of your evaluation, avoiding any potential bias.\nThen, output a line indicating the score of the Assistant.\n\nPLEASE OUTPUT WITH THE FOLLOWING FORMAT, WHERE THE SCORE IS A SCALE OF 1 TO 100 BY STRICTLY FOLLOWING THIS FORMAT: \"[[score]]\", FOR EXAMPLE \"Rating: [[100]]\":\n<start output>\nEvaluation evidence: your evluation explanation here, no more than 100 words\nRating: [[score]]\n<end output>\n\nNow, start your evaluation:",
        "generate_response": "[\"InstructCoder: Instruction Tuning Large Language Models for Code Editing\"]",
        "eval_response": "Evaluation evidence: The assistant's answer is incomplete and inaccurate compared to the gold answer. The gold answer lists a citation chain of three papers, while the assistant's answer only includes one paper title, missing the other two. This omission significantly impacts the accuracy and completeness of the response, as it fails to provide the longest citation chain as requested. There are no hallucinations in the provided answer, but the lack of completeness and accuracy in forming the citation chain leads to a lower score.\n\nRating: [[30]]",
        "score": 30
    },
    {
        "level": 3,
        "set": 2,
        "length": 78289,
        "type": "paper",
        "language": "en",
        "question": "GPT-4V(ision) is a Generalist Web Agent, if Grounded",
        "instruction": "We hope you will carefully study the provided papers and determine the citation relationships between them. Please follow the instructions below strictly to complete the task:\n\n#Specific Requirements:\n1. Reference: When a given paper mentions other provided papers, those other papers are considered as \"references\" for the given paper. To summarize in this specific context, references are about what the given paper is using.\n2. Citation: Conversely, when other provided papers mention the given paper in their works, the given paper is being \"cited\" by those other papers. To summarize in this specific context, citations are about who is using the given paper.\n3. Given a paper, you need to determine the citation or reference relationship between this paper and the other papers. Do not consider papers that are not provided.\n3. Please present the paper titles in a json format as follows: {{\"Reference\":[\"Reference Title 1\", \"Reference Title 2\", ..., \"Reference Title n\"], \"Citation\":[\"Citation Title 1\", \"Citation Title 2\", ..., \"Citation Title n\"]}}.\n4. If a paper does not have any references or citations, please leave the corresponding list empty, e.g.{{\"Refernce\":[]}}, {{\"Citation\":[]}}.",
        "prompt_template": "#Papers:\n{docs}\n\n{instruction}\n\n#The paper you need to analyze:\n{question}",
        "doc": [
            "2401.01614.md",
            "2405.04497.md",
            "2312.08914.md",
            "2401.13649.md"
        ],
        "answer": {
            "Reference": [
                "# CogAgent: A Visual Language Model for GUI Agents ",
                "# VisualWebArena: EvalUATING MULTIMODAL AGENTS ON REALISTIC VISUAL WEB TASKS "
            ],
            "Citation": [
                "# Unveiling Disparities in Web Task Handling Between Human and Web Agent "
            ]
        },
        "shuffle_doc": true,
        "id": "d988b4fc-3fe8-44da-8f7b-50484b62c1ee",
        "prompt": "[Question]\n#Papers:\n{docs}\n\nWe hope you will carefully study the provided papers and determine the citation relationships between them. Please follow the instructions below strictly to complete the task:\n\n#Specific Requirements:\n1. Reference: When a given paper mentions other provided papers, those other papers are considered as \"references\" for the given paper. To summarize in this specific context, references are about what the given paper is using.\n2. Citation: Conversely, when other provided papers mention the given paper in their works, the given paper is being \"cited\" by those other papers. To summarize in this specific context, citations are about who is using the given paper.\n3. Given a paper, you need to determine the citation or reference relationship between this paper and the other papers. Do not consider papers that are not provided.\n3. Please present the paper titles in a json format as follows: {{\"Reference\":[\"Reference Title 1\", \"Reference Title 2\", ..., \"Reference Title n\"], \"Citation\":[\"Citation Title 1\", \"Citation Title 2\", ..., \"Citation Title n\"]}}.\n4. If a paper does not have any references or citations, please leave the corresponding list empty, e.g.{{\"Refernce\":[]}}, {{\"Citation\":[]}}.\n\n#The paper you need to analyze:\nGPT-4V(ision) is a Generalist Web Agent, if Grounded\n\n[Gold Answer]\n{'Reference': ['# CogAgent: A Visual Language Model for GUI Agents ', '# VisualWebArena: EvalUATING MULTIMODAL AGENTS ON REALISTIC VISUAL WEB TASKS '], 'Citation': ['# Unveiling Disparities in Web Task Handling Between Human and Web Agent ']}\n\n[The Start of Assistant's Predicted Answer]\n{\"Reference\":[\"A Survey on Evaluation of Large Language Models\", \"Vicuna: An Open-Source Chatbot Impressing GPT-4 with $90 \\% *$ ChatGPT Quality\", \"ChatLaw: Open-Source Legal Large Language Model with Integrated External Knowledge Bases\", \"Compositional Semantic Parsing with Large Language Models\", \"Open LLM Leaderboard\", \"How Close is ChatGPT to Human Experts? Comparison Corpus, Evaluation, and Detection\", \"REALM: Retrieval-Augmented Language Model PreTraining\", \"Rethinking with Retrieval: Faithful Large Language Model Inference\", \"Measuring Massive Multitask Language Understanding\", \"Pix2Act: From Pixels to UI Actions: Learning to Follow Instructions via Graphical User Interfaces\", \"Multimodal Web Navigation with Instruction-Finetuned Foundation Models\", \"Cogagent: A Visual Language Model for GUI Agents\", \"GPT-4 Technical Report\", \"Kosmos-2: Grounding Multimodal Large Language Models to the World\", \"Flickr30K Entities: Collecting Region-to-Phrase Correspondences for Richer Image-to-Sentence Models\", \"Learning Transferable Visual Models from Natural Language Supervision\", \"FLin: A Flexible Natural Language Interface for Web Navigation\", \"Mind2Web: Towards a Generalist Agent for the Web\", \"ScienceQA: A Novel Resource for Question Answering on Scholarly Articles\", \"World of Bits: An Open-Domain Platform for Web-Based Agents\", \"What Does CLIP Know about a Red Circle? Visual Prompt Engineering for VLMS\", \"Hierarchical Prompting Assists Large Language Model on Web Navigation\", \"GPT-4V in Wonderland: Large Multimodal Models for Zero-Shot Smartphone GUI Navigation\", \"Set-of-mark Prompting Unleashes Extraordinary Visual Grounding in GPT-4V\", \"Fine-Grained Visual Prompting\", \"The Dawn of LMMs: Preliminary Explorations with GPT-4V(ision)\", \"Ferret: Refer and Ground Anything Anywhere at Any Granularity\", \"MMMU: A Massive Multi-Discipline Multimodal Understanding and Reasoning Benchmark for Expert AGI\", \"GPT-4V(ision) as a Generalist Evaluator for Vision-Language Tasks\", \"BuboGPT: Enabling Visual Grounding in Multi-Modal LLMs\", \"AGiEval: A Human-Centric Benchmark for Evaluating Foundation Models\", \"Scene Parsing through ADE20K Dataset\"], \"Citation\":[]}\n[The End of Assistant's Predicted Answer]\n\n[System]\nWe would like to request your feedback on the performance of the AI assistant in response to the user question displayed above according to the gold answer. Please use the following listed aspects and their descriptions as evaluation criteria:\n    - Accuracy and Hallucinations: The assistant's answer is semantically consistent with the gold answer; The numerical value and order need to be accurate, and there should be no hallucinations.\n    - Completeness: Referring to the reference answers, the assistant's answer should contain all the key points needed to answer the user's question; further elaboration on these key points can be omitted.\nPlease rate whether this answer is suitable for the question. Please note that the gold answer can be considered as a correct answer to the question.\n\nThe assistant receives an overall score on a scale of 1 to 100, where a higher score indicates better overall performance.\nPlease note that if the assistant's answer and the gold answer fully meet the above criteria, its overall rating should be the full marks (100).\nPlease first provide a comprehensive explanation of your evaluation, avoiding any potential bias.\nThen, output a line indicating the score of the Assistant.\n\nPLEASE OUTPUT WITH THE FOLLOWING FORMAT, WHERE THE SCORE IS A SCALE OF 1 TO 100 BY STRICTLY FOLLOWING THIS FORMAT: \"[[score]]\", FOR EXAMPLE \"Rating: [[100]]\":\n<start output>\nEvaluation evidence: your evluation explanation here, no more than 100 words\nRating: [[score]]\n<end output>\n\nNow, start your evaluation:",
        "generate_response": "{\"Reference\":[\"A Survey on Evaluation of Large Language Models\", \"Vicuna: An Open-Source Chatbot Impressing GPT-4 with $90 \\% *$ ChatGPT Quality\", \"ChatLaw: Open-Source Legal Large Language Model with Integrated External Knowledge Bases\", \"Compositional Semantic Parsing with Large Language Models\", \"Open LLM Leaderboard\", \"How Close is ChatGPT to Human Experts? Comparison Corpus, Evaluation, and Detection\", \"REALM: Retrieval-Augmented Language Model PreTraining\", \"Rethinking with Retrieval: Faithful Large Language Model Inference\", \"Measuring Massive Multitask Language Understanding\", \"Pix2Act: From Pixels to UI Actions: Learning to Follow Instructions via Graphical User Interfaces\", \"Multimodal Web Navigation with Instruction-Finetuned Foundation Models\", \"Cogagent: A Visual Language Model for GUI Agents\", \"GPT-4 Technical Report\", \"Kosmos-2: Grounding Multimodal Large Language Models to the World\", \"Flickr30K Entities: Collecting Region-to-Phrase Correspondences for Richer Image-to-Sentence Models\", \"Learning Transferable Visual Models from Natural Language Supervision\", \"FLin: A Flexible Natural Language Interface for Web Navigation\", \"Mind2Web: Towards a Generalist Agent for the Web\", \"ScienceQA: A Novel Resource for Question Answering on Scholarly Articles\", \"World of Bits: An Open-Domain Platform for Web-Based Agents\", \"What Does CLIP Know about a Red Circle? Visual Prompt Engineering for VLMS\", \"Hierarchical Prompting Assists Large Language Model on Web Navigation\", \"GPT-4V in Wonderland: Large Multimodal Models for Zero-Shot Smartphone GUI Navigation\", \"Set-of-mark Prompting Unleashes Extraordinary Visual Grounding in GPT-4V\", \"Fine-Grained Visual Prompting\", \"The Dawn of LMMs: Preliminary Explorations with GPT-4V(ision)\", \"Ferret: Refer and Ground Anything Anywhere at Any Granularity\", \"MMMU: A Massive Multi-Discipline Multimodal Understanding and Reasoning Benchmark for Expert AGI\", \"GPT-4V(ision) as a Generalist Evaluator for Vision-Language Tasks\", \"BuboGPT: Enabling Visual Grounding in Multi-Modal LLMs\", \"AGiEval: A Human-Centric Benchmark for Evaluating Foundation Models\", \"Scene Parsing through ADE20K Dataset\"], \"Citation\":[]}",
        "eval_response": "Evaluation evidence: The assistant's response does not align with the gold answer. The references listed in the assistant's response do not match those in the gold answer, which specifically mentions two papers. Additionally, the assistant's response fails to identify any citations, whereas the gold answer lists one citing paper. This discrepancy indicates issues with accuracy and completeness, as the assistant neither correctly identifies the references nor the citations for the given paper.\n\nRating: [[20]]",
        "score": 20
    },
    {
        "level": 3,
        "set": 3,
        "length": 107441,
        "type": "paper",
        "language": "en",
        "question": "LLM Maybe LongLM: SelfExtend LLM Context Window Without Tuning",
        "instruction": "We hope you will carefully study the provided papers and determine the citation relationships between them. Please follow the instructions below strictly to complete the task:\n\n#Specific Requirements:\n1. Reference: When a given paper mentions other provided papers, those other papers are considered as \"references\" for the given paper. To summarize in this specific context, references are about what the given paper is using.\n2. Citation: Conversely, when other provided papers mention the given paper in their works, the given paper is being \"cited\" by those other papers. To summarize in this specific context, citations are about who is using the given paper.\n3. Given a paper, you need to determine the citation or reference relationship between this paper and the other papers. Do not consider papers that are not provided.\n3. Please present the paper titles in a json format as follows: {{\"Reference\":[\"Reference Title 1\", \"Reference Title 2\", ..., \"Reference Title n\"], \"Citation\":[\"Citation Title 1\", \"Citation Title 2\", ..., \"Citation Title n\"]}}.\n4. If a paper does not have any references or citations, please leave the corresponding list empty, e.g.{{\"Refernce\":[]}}, {{\"Citation\":[]}}.",
        "prompt_template": "#Papers:\n{docs}\n\n{instruction}\n\n#The paper you need to analyze:\n{question}",
        "doc": [
            "2401.01325.md",
            "2404.18311.md",
            "2405.14722.md",
            "2310.16450.md",
            "2312.15166.md"
        ],
        "answer": {
            "Reference": [
                "# CLEX: CONTINUOUS LENGTH EXTRAPOLATION FOR LARGE LANGUAGE MODELS ",
                "# SOLAR 10.7B: Scaling Large Language Models with Simple yet Effective Depth Up-Scaling "
            ],
            "Citation": [
                "# Towards Incremental Learning in Large Language Models: A Critical Review ",
                "# CAPE: Context-Adaptive Positional Encoding for Length Extrapolation "
            ]
        },
        "shuffle_doc": true,
        "id": "d591f349-7474-4715-8b66-9180fc7a26bf",
        "prompt": "[Question]\n#Papers:\n{docs}\n\nWe hope you will carefully study the provided papers and determine the citation relationships between them. Please follow the instructions below strictly to complete the task:\n\n#Specific Requirements:\n1. Reference: When a given paper mentions other provided papers, those other papers are considered as \"references\" for the given paper. To summarize in this specific context, references are about what the given paper is using.\n2. Citation: Conversely, when other provided papers mention the given paper in their works, the given paper is being \"cited\" by those other papers. To summarize in this specific context, citations are about who is using the given paper.\n3. Given a paper, you need to determine the citation or reference relationship between this paper and the other papers. Do not consider papers that are not provided.\n3. Please present the paper titles in a json format as follows: {{\"Reference\":[\"Reference Title 1\", \"Reference Title 2\", ..., \"Reference Title n\"], \"Citation\":[\"Citation Title 1\", \"Citation Title 2\", ..., \"Citation Title n\"]}}.\n4. If a paper does not have any references or citations, please leave the corresponding list empty, e.g.{{\"Refernce\":[]}}, {{\"Citation\":[]}}.\n\n#The paper you need to analyze:\nLLM Maybe LongLM: SelfExtend LLM Context Window Without Tuning\n\n[Gold Answer]\n{'Reference': ['# CLEX: CONTINUOUS LENGTH EXTRAPOLATION FOR LARGE LANGUAGE MODELS ', '# SOLAR 10.7B: Scaling Large Language Models with Simple yet Effective Depth Up-Scaling '], 'Citation': ['# Towards Incremental Learning in Large Language Models: A Critical Review ', '# CAPE: Context-Adaptive Positional Encoding for Length Extrapolation ']}\n\n[The Start of Assistant's Predicted Answer]\n{\"Citation\": [], \"Reference\": [\"A Survey on Evaluation of Large Language Models\", \"Vicuna: An Open-Source Chatbot Impressing GPT-4 with $90 \\% *$ ChatGPT Quality\", \"ChatLaw: Open-Source Legal Large Language Model with Integrated External Knowledge Bases\", \"Compositional Semantic Parsing with Large Language Models\", \"Open LLM Leaderboard\", \"How Close is ChatGPT to Human Experts? Comparison Corpus, Evaluation, and Detection\", \"REALM: Retrieval-Augmented Language Model PreTraining\", \"Rethinking with Retrieval: Faithful Large Language Model Inference\", \"Measuring Massive Multitask Language Understanding\", \"Atlas: Few-shot Learning with Retrieval Augmented Language Models\", \"Mistrallite model\", \"Longbench: A bilingual, multitask benchmark for long context understanding\", \"Language models are few-shot learners\", \"Continuous length extrapolation for large language models\", \"Extending context window of large language models via positional interpolation\", \"Efficient fine-tuning of long-context large language models\", \"Phi-2: The surprising power of small language models\", \"Mistral 7b\", \"Rethinking positional encoding in language pre-training\", \"Scaling large language models with simple yet effective depth up-scaling\", \"Truthfulqa: Measuring how models mimic human falsehoods\", \"Towards out-of-distribution generalization: A survey\", \"Landmark attention: Random-access infinite context length for transformers\", \"Giraffe: Adventures in expanding context lengths in llms\", \"Yarn: Efficient context window extension of large language models\", \"Train short, test long: Attention with linear biases enables input length extrapolation\", \"Compressive transformers for long-range sequence modelling\", \"Exploring the limits of transfer learning with a unified text-to-text transformer\", \"Code llama: Open foundation models for code\", \"Sparsebert: Rethinking the importance analysis in self-attention\", \"Rectified rotary position embeddings\", \"RoFormer: Enhanced transformer with rotary position embedding\", \"A lengthextrapolatable transformer\", \"Llama 2: Open foundation and finetuned chat models\", \"Attention is all you need\", \"mt5: A massively multilingual pre-trained text-to-text transformer\", \"Big bird: Transformers for longer sequences\", \"Hellaswag: Can a machine really finish your sentence?\", \"When neural networks fail to generalize? a model sensitivity perspective\", \"Opt: Open pre-trained transformer language models\", \"A survey of large language models\"]}\n[The End of Assistant's Predicted Answer]\n\n[System]\nWe would like to request your feedback on the performance of the AI assistant in response to the user question displayed above according to the gold answer. Please use the following listed aspects and their descriptions as evaluation criteria:\n    - Accuracy and Hallucinations: The assistant's answer is semantically consistent with the gold answer; The numerical value and order need to be accurate, and there should be no hallucinations.\n    - Completeness: Referring to the reference answers, the assistant's answer should contain all the key points needed to answer the user's question; further elaboration on these key points can be omitted.\nPlease rate whether this answer is suitable for the question. Please note that the gold answer can be considered as a correct answer to the question.\n\nThe assistant receives an overall score on a scale of 1 to 100, where a higher score indicates better overall performance.\nPlease note that if the assistant's answer and the gold answer fully meet the above criteria, its overall rating should be the full marks (100).\nPlease first provide a comprehensive explanation of your evaluation, avoiding any potential bias.\nThen, output a line indicating the score of the Assistant.\n\nPLEASE OUTPUT WITH THE FOLLOWING FORMAT, WHERE THE SCORE IS A SCALE OF 1 TO 100 BY STRICTLY FOLLOWING THIS FORMAT: \"[[score]]\", FOR EXAMPLE \"Rating: [[100]]\":\n<start output>\nEvaluation evidence: your evluation explanation here, no more than 100 words\nRating: [[score]]\n<end output>\n\nNow, start your evaluation:",
        "generate_response": "{\"Citation\": [], \"Reference\": [\"A Survey on Evaluation of Large Language Models\", \"Vicuna: An Open-Source Chatbot Impressing GPT-4 with $90 \\% *$ ChatGPT Quality\", \"ChatLaw: Open-Source Legal Large Language Model with Integrated External Knowledge Bases\", \"Compositional Semantic Parsing with Large Language Models\", \"Open LLM Leaderboard\", \"How Close is ChatGPT to Human Experts? Comparison Corpus, Evaluation, and Detection\", \"REALM: Retrieval-Augmented Language Model PreTraining\", \"Rethinking with Retrieval: Faithful Large Language Model Inference\", \"Measuring Massive Multitask Language Understanding\", \"Atlas: Few-shot Learning with Retrieval Augmented Language Models\", \"Mistrallite model\", \"Longbench: A bilingual, multitask benchmark for long context understanding\", \"Language models are few-shot learners\", \"Continuous length extrapolation for large language models\", \"Extending context window of large language models via positional interpolation\", \"Efficient fine-tuning of long-context large language models\", \"Phi-2: The surprising power of small language models\", \"Mistral 7b\", \"Rethinking positional encoding in language pre-training\", \"Scaling large language models with simple yet effective depth up-scaling\", \"Truthfulqa: Measuring how models mimic human falsehoods\", \"Towards out-of-distribution generalization: A survey\", \"Landmark attention: Random-access infinite context length for transformers\", \"Giraffe: Adventures in expanding context lengths in llms\", \"Yarn: Efficient context window extension of large language models\", \"Train short, test long: Attention with linear biases enables input length extrapolation\", \"Compressive transformers for long-range sequence modelling\", \"Exploring the limits of transfer learning with a unified text-to-text transformer\", \"Code llama: Open foundation models for code\", \"Sparsebert: Rethinking the importance analysis in self-attention\", \"Rectified rotary position embeddings\", \"RoFormer: Enhanced transformer with rotary position embedding\", \"A lengthextrapolatable transformer\", \"Llama 2: Open foundation and finetuned chat models\", \"Attention is all you need\", \"mt5: A massively multilingual pre-trained text-to-text transformer\", \"Big bird: Transformers for longer sequences\", \"Hellaswag: Can a machine really finish your sentence?\", \"When neural networks fail to generalize? a model sensitivity perspective\", \"Opt: Open pre-trained transformer language models\", \"A survey of large language models\"]}",
        "eval_response": "Evaluation evidence: The assistant's answer does not align with the gold answer, indicating a significant error in identifying the correct references and citations for the given paper. The assistant listed numerous unrelated papers, showing a lack of accuracy and a high degree of hallucination. The answer also fails in completeness as it does not include any of the correct titles from the gold answer, missing both the required references and citations.\n\nRating: [[10]]",
        "score": 10
    },
    {
        "level": 3,
        "set": 2,
        "length": 85396,
        "type": "paper",
        "language": "en",
        "question": "LLM Maybe LongLM: SelfExtend LLM Context Window Without Tuning",
        "instruction": "We hope you will carefully study the provided papers and determine the citation relationships between them. Please follow the instructions below strictly to complete the task:\n\n#Specific Requirements:\n1. Reference: When a given paper mentions other provided papers, those other papers are considered as \"references\" for the given paper. To summarize in this specific context, references are about what the given paper is using.\n2. Citation: Conversely, when other provided papers mention the given paper in their works, the given paper is being \"cited\" by those other papers. To summarize in this specific context, citations are about who is using the given paper.\n3. Given a paper, you need to determine the citation or reference relationship between this paper and the other papers. Do not consider papers that are not provided.\n3. Please present the paper titles in a json format as follows: {{\"Reference\":[\"Reference Title 1\", \"Reference Title 2\", ..., \"Reference Title n\"], \"Citation\":[\"Citation Title 1\", \"Citation Title 2\", ..., \"Citation Title n\"]}}.\n4. If a paper does not have any references or citations, please leave the corresponding list empty, e.g.{{\"Refernce\":[]}}, {{\"Citation\":[]}}.",
        "prompt_template": "#Papers:\n{docs}\n\n{instruction}\n\n#The paper you need to analyze:\n{question}",
        "doc": [
            "2401.01325.md",
            "2404.18311.md",
            "2405.04028.md",
            "2405.14722.md"
        ],
        "answer": {
            "Reference": [],
            "Citation": [
                "# Towards Incremental Learning in Large Language Models: A Critical Review ",
                "# Masked Graph Transformer for Large-Scale Recommendation ",
                "# CAPE: Context-Adaptive Positional Encoding for Length Extrapolation "
            ]
        },
        "shuffle_doc": true,
        "id": "c506849b-7a23-45f4-8c0b-0188f127dc16",
        "prompt": "[Question]\n#Papers:\n{docs}\n\nWe hope you will carefully study the provided papers and determine the citation relationships between them. Please follow the instructions below strictly to complete the task:\n\n#Specific Requirements:\n1. Reference: When a given paper mentions other provided papers, those other papers are considered as \"references\" for the given paper. To summarize in this specific context, references are about what the given paper is using.\n2. Citation: Conversely, when other provided papers mention the given paper in their works, the given paper is being \"cited\" by those other papers. To summarize in this specific context, citations are about who is using the given paper.\n3. Given a paper, you need to determine the citation or reference relationship between this paper and the other papers. Do not consider papers that are not provided.\n3. Please present the paper titles in a json format as follows: {{\"Reference\":[\"Reference Title 1\", \"Reference Title 2\", ..., \"Reference Title n\"], \"Citation\":[\"Citation Title 1\", \"Citation Title 2\", ..., \"Citation Title n\"]}}.\n4. If a paper does not have any references or citations, please leave the corresponding list empty, e.g.{{\"Refernce\":[]}}, {{\"Citation\":[]}}.\n\n#The paper you need to analyze:\nLLM Maybe LongLM: SelfExtend LLM Context Window Without Tuning\n\n[Gold Answer]\n{'Reference': [], 'Citation': ['# Towards Incremental Learning in Large Language Models: A Critical Review ', '# Masked Graph Transformer for Large-Scale Recommendation ', '# CAPE: Context-Adaptive Positional Encoding for Length Extrapolation ']}\n\n[The Start of Assistant's Predicted Answer]\n{\"Reference\":[\"A Survey on Evaluation of Large Language Models\", \"Vicuna: An Open-Source Chatbot Impressing GPT-4 with $90 \\% *$ ChatGPT Quality\", \"ChatLaw: Open-Source Legal Large Language Model with Integrated External Knowledge Bases\", \"Compositional Semantic Parsing with Large Language Models\", \"Open LLM Leaderboard\", \"How Close is ChatGPT to Human Experts? Comparison Corpus, Evaluation, and Detection\", \"REALM: Retrieval-Augmented Language Model PreTraining\", \"Rethinking with Retrieval: Faithful Large Language Model Inference\", \"Measuring Massive Multitask Language Understanding\", \"Atlas: Few-shot Learning with Retrieval Augmented Language Models\", \"Mistrallite model\", \"Longbench: A bilingual, multitask benchmark for long context understanding\", \"Language models are few-shot learners\", \"Continuous length extrapolation for large language models\", \"Extending context window of large language models via positional interpolation\", \"Efficient fine-tuning of long-context large language models\", \"The surprising power of small language models\", \"Mistral 7b\", \"Rethinking positional encoding in language pre-training\", \"Scaling large language models with simple yet effective depth up-scaling\", \"TruthfulQA: Measuring how models mimic human falsehoods\", \"Towards out-of-distribution generalization: A survey\", \"Landmark attention: Random-access infinite context length for transformers\", \"Giraffe: Adventures in expanding context lengths in LLMs\", \"YARN: Efficient context window extension of large language models\", \"Train Short, Test Long: Attention with Linear Biases Enables Input Length Extrapolation\", \"Compressive Transformers for Long-Range Sequence Modelling\", \"Exploring the Limits of Transfer Learning with a Unified Text-to-Text Transformer\", \"CodeLlama: Open Foundation Models for Code\", \"RoFormer: Enhanced Transformer with Rotary Position Embedding\", \"A Length-Extrapolatable Transformer\", \"Llama 2: Open Foundation and Fine-Tuned Chat Models\", \"Attention Is All You Need\", \"MT5: A Massively Multilingual Pre-Trained Text-to-Text Transformer\", \"Harnessing the Power of LLMs in Practice: A Survey on ChatGPT and Beyond\", \"Amazon/MistralLite\", \"BigBird: Transformers for Longer Sequences\", \"HellaswAG: Can a Machine Really Finish Your Sentence?\", \"When Neural Networks Fail to Generalize? A Model Sensitivity Perspective\", \"OPT: Open Pre-Trained Transformer Language Models\", \"A Survey of Large Language Models\"], \"Citation\":[]}\n[The End of Assistant's Predicted Answer]\n\n[System]\nWe would like to request your feedback on the performance of the AI assistant in response to the user question displayed above according to the gold answer. Please use the following listed aspects and their descriptions as evaluation criteria:\n    - Accuracy and Hallucinations: The assistant's answer is semantically consistent with the gold answer; The numerical value and order need to be accurate, and there should be no hallucinations.\n    - Completeness: Referring to the reference answers, the assistant's answer should contain all the key points needed to answer the user's question; further elaboration on these key points can be omitted.\nPlease rate whether this answer is suitable for the question. Please note that the gold answer can be considered as a correct answer to the question.\n\nThe assistant receives an overall score on a scale of 1 to 100, where a higher score indicates better overall performance.\nPlease note that if the assistant's answer and the gold answer fully meet the above criteria, its overall rating should be the full marks (100).\nPlease first provide a comprehensive explanation of your evaluation, avoiding any potential bias.\nThen, output a line indicating the score of the Assistant.\n\nPLEASE OUTPUT WITH THE FOLLOWING FORMAT, WHERE THE SCORE IS A SCALE OF 1 TO 100 BY STRICTLY FOLLOWING THIS FORMAT: \"[[score]]\", FOR EXAMPLE \"Rating: [[100]]\":\n<start output>\nEvaluation evidence: your evluation explanation here, no more than 100 words\nRating: [[score]]\n<end output>\n\nNow, start your evaluation:",
        "generate_response": "{\"Reference\":[\"A Survey on Evaluation of Large Language Models\", \"Vicuna: An Open-Source Chatbot Impressing GPT-4 with $90 \\% *$ ChatGPT Quality\", \"ChatLaw: Open-Source Legal Large Language Model with Integrated External Knowledge Bases\", \"Compositional Semantic Parsing with Large Language Models\", \"Open LLM Leaderboard\", \"How Close is ChatGPT to Human Experts? Comparison Corpus, Evaluation, and Detection\", \"REALM: Retrieval-Augmented Language Model PreTraining\", \"Rethinking with Retrieval: Faithful Large Language Model Inference\", \"Measuring Massive Multitask Language Understanding\", \"Atlas: Few-shot Learning with Retrieval Augmented Language Models\", \"Mistrallite model\", \"Longbench: A bilingual, multitask benchmark for long context understanding\", \"Language models are few-shot learners\", \"Continuous length extrapolation for large language models\", \"Extending context window of large language models via positional interpolation\", \"Efficient fine-tuning of long-context large language models\", \"The surprising power of small language models\", \"Mistral 7b\", \"Rethinking positional encoding in language pre-training\", \"Scaling large language models with simple yet effective depth up-scaling\", \"TruthfulQA: Measuring how models mimic human falsehoods\", \"Towards out-of-distribution generalization: A survey\", \"Landmark attention: Random-access infinite context length for transformers\", \"Giraffe: Adventures in expanding context lengths in LLMs\", \"YARN: Efficient context window extension of large language models\", \"Train Short, Test Long: Attention with Linear Biases Enables Input Length Extrapolation\", \"Compressive Transformers for Long-Range Sequence Modelling\", \"Exploring the Limits of Transfer Learning with a Unified Text-to-Text Transformer\", \"CodeLlama: Open Foundation Models for Code\", \"RoFormer: Enhanced Transformer with Rotary Position Embedding\", \"A Length-Extrapolatable Transformer\", \"Llama 2: Open Foundation and Fine-Tuned Chat Models\", \"Attention Is All You Need\", \"MT5: A Massively Multilingual Pre-Trained Text-to-Text Transformer\", \"Harnessing the Power of LLMs in Practice: A Survey on ChatGPT and Beyond\", \"Amazon/MistralLite\", \"BigBird: Transformers for Longer Sequences\", \"HellaswAG: Can a Machine Really Finish Your Sentence?\", \"When Neural Networks Fail to Generalize? A Model Sensitivity Perspective\", \"OPT: Open Pre-Trained Transformer Language Models\", \"A Survey of Large Language Models\"], \"Citation\":[]}",
        "eval_response": "Evaluation evidence: The assistant's answer is incorrect as it lists references that are not mentioned in the gold answer, indicating hallucinations. The gold answer specifies that there are no references for the analyzed paper, only citations. The assistant incorrectly lists numerous references, which is a significant error. Additionally, the assistant fails to list any citations, whereas the gold answer lists three specific citations. This shows a lack of accuracy and completeness in the assistant's response.\n\nRating: [[10]]",
        "score": 10
    },
    {
        "level": 3,
        "set": 2,
        "length": 69286,
        "type": "paper",
        "language": "en",
        "question": "VisualWebArena: EvalUATING MULTIMODAL AGENTS ON REALISTIC VISUAL WEB TASKS",
        "instruction": "We hope you will carefully study the provided papers and determine the citation relationships between them. Please follow the instructions below strictly to complete the task:\n\n#Specific Requirements:\n1. Reference: When a given paper mentions other provided papers, those other papers are considered as \"references\" for the given paper. To summarize in this specific context, references are about what the given paper is using.\n2. Citation: Conversely, when other provided papers mention the given paper in their works, the given paper is being \"cited\" by those other papers. To summarize in this specific context, citations are about who is using the given paper.\n3. Given a paper, you need to determine the citation or reference relationship between this paper and the other papers. Do not consider papers that are not provided.\n3. Please present the paper titles in a json format as follows: {{\"Reference\":[\"Reference Title 1\", \"Reference Title 2\", ..., \"Reference Title n\"], \"Citation\":[\"Citation Title 1\", \"Citation Title 2\", ..., \"Citation Title n\"]}}.\n4. If a paper does not have any references or citations, please leave the corresponding list empty, e.g.{{\"Refernce\":[]}}, {{\"Citation\":[]}}.",
        "prompt_template": "#Papers:\n{docs}\n\n{instruction}\n\n#The paper you need to analyze:\n{question}",
        "doc": [
            "2401.13649.md",
            "2405.15341.md",
            "2405.17438.md",
            "2312.13771.md",
            "2401.04088.md"
        ],
        "answer": {
            "Reference": [
                "# AppAgent: Multimodal Agents as Smartphone Users ",
                "# Mixtral of Experts "
            ],
            "Citation": [
                "# V-Zen: Efficient GUI Understanding and Precise Grounding With A Novel Multimodal LLM ",
                "# An LLM-Tool Compiler for Fused Parallel Function Calling "
            ]
        },
        "shuffle_doc": true,
        "id": "01cfb28d-5f53-4124-a942-0b4c48a60b69",
        "prompt": "[Question]\n#Papers:\n{docs}\n\nWe hope you will carefully study the provided papers and determine the citation relationships between them. Please follow the instructions below strictly to complete the task:\n\n#Specific Requirements:\n1. Reference: When a given paper mentions other provided papers, those other papers are considered as \"references\" for the given paper. To summarize in this specific context, references are about what the given paper is using.\n2. Citation: Conversely, when other provided papers mention the given paper in their works, the given paper is being \"cited\" by those other papers. To summarize in this specific context, citations are about who is using the given paper.\n3. Given a paper, you need to determine the citation or reference relationship between this paper and the other papers. Do not consider papers that are not provided.\n3. Please present the paper titles in a json format as follows: {{\"Reference\":[\"Reference Title 1\", \"Reference Title 2\", ..., \"Reference Title n\"], \"Citation\":[\"Citation Title 1\", \"Citation Title 2\", ..., \"Citation Title n\"]}}.\n4. If a paper does not have any references or citations, please leave the corresponding list empty, e.g.{{\"Refernce\":[]}}, {{\"Citation\":[]}}.\n\n#The paper you need to analyze:\nVisualWebArena: EvalUATING MULTIMODAL AGENTS ON REALISTIC VISUAL WEB TASKS\n\n[Gold Answer]\n{'Reference': ['# AppAgent: Multimodal Agents as Smartphone Users ', '# Mixtral of Experts '], 'Citation': ['# V-Zen: Efficient GUI Understanding and Precise Grounding With A Novel Multimodal LLM ', '# An LLM-Tool Compiler for Fused Parallel Function Calling ']}\n\n[The Start of Assistant's Predicted Answer]\n{\"Reference\":[\"A Survey on Evaluation of Large Language Models\", \"Vicuna: An Open-Source Chatbot Impressing GPT-4 with $90 \\% *$ ChatGPT Quality\", \"Frozen: Multimodal few-shot learning with frozen language models\", \"Conversational Multimodal Agent for Complex Tasks\", \"Gemini: A Family of Highly Capable Multimodal Models\", \"A Real-World WebAgent with Planning, Long Context Understanding, and Program Synthesis\", \"Improved Baselines with Visual Instruction Tuning\", \"Visual Instruction Tuning\", \"AgentBench: Evaluating LLMs as Agents\", \"Bolaa: Benchmarking and Orchestrating LLM-Augmented Autonomous Agents\", \"Gaia: A Benchmark for General AI Assistants\", \"GPT-4 Technical Report\", \"Training Language Models to Follow Instructions with Human Feedback\", \"Scaling Language Models: Methods, Analysis and Insights from Training Gopher\", \"World of Bits: An Open-Domain Platform for Web-Based Agents\", \"Long Range Arena: A Benchmark for Efficient Transformers\", \"Eyes Wide Shut? Exploring the Visual Shortcomings of Multimodal LLMs\", \"Llama: Open and Efficient Foundation Language Models\", \"Llama 2: Open Foundation and Fine-Tuned Chat Models\", \"Multimodal Few-Shot Learning with Frozen Language Models\", \"Focused Transformer: Contrastive Training for Context Scaling\", \"Attention is All You Need\", \"Show and Tell: A Neural Image Caption Generator\", \"A Survey on Large Language Model Based Autonomous Agents\", \"GPT-4V in Wonderland: Large Multimodal Models for Zero-Shot Smartphone GUI Navigation\", \"Set-of-Marks Prompting Unleashes Extraordinary Visual Grounding in GPT-4V\", \"AppAgent: Multimodal Agents as Smartphone Users\", \"The Dawn of LMMs: Preliminary Explorations with GPT-4V (Vision)\", \"WebShop: Towards Scalable Real-World Web Interaction with Grounded Language Agents\", \"Tree of Thoughts: Deliberate Problem Solving with Large Language Models\", \"MMMU: A Massive Multi-Discipline Multimodal Understanding and Reasoning Benchmark for Expert AGI\", \"AgentTuning: Enabling Generalized Agent Abilities for LLMs\", \"You Only Look at Screens: Multimodal Chain-of-Action Agents\", \"OPT: Open Pre-Trained Transformer Language Models\", \"GPT-4V (Vision) is a Generalist Web Agent, if Grounded\", \"WebArena: A Realistic Web Environment for Building Autonomous Agents\"], \"Citation\":[\"Mixtral of Experts\"]}\n[The End of Assistant's Predicted Answer]\n\n[System]\nWe would like to request your feedback on the performance of the AI assistant in response to the user question displayed above according to the gold answer. Please use the following listed aspects and their descriptions as evaluation criteria:\n    - Accuracy and Hallucinations: The assistant's answer is semantically consistent with the gold answer; The numerical value and order need to be accurate, and there should be no hallucinations.\n    - Completeness: Referring to the reference answers, the assistant's answer should contain all the key points needed to answer the user's question; further elaboration on these key points can be omitted.\nPlease rate whether this answer is suitable for the question. Please note that the gold answer can be considered as a correct answer to the question.\n\nThe assistant receives an overall score on a scale of 1 to 100, where a higher score indicates better overall performance.\nPlease note that if the assistant's answer and the gold answer fully meet the above criteria, its overall rating should be the full marks (100).\nPlease first provide a comprehensive explanation of your evaluation, avoiding any potential bias.\nThen, output a line indicating the score of the Assistant.\n\nPLEASE OUTPUT WITH THE FOLLOWING FORMAT, WHERE THE SCORE IS A SCALE OF 1 TO 100 BY STRICTLY FOLLOWING THIS FORMAT: \"[[score]]\", FOR EXAMPLE \"Rating: [[100]]\":\n<start output>\nEvaluation evidence: your evluation explanation here, no more than 100 words\nRating: [[score]]\n<end output>\n\nNow, start your evaluation:",
        "generate_response": "{\"Reference\":[\"A Survey on Evaluation of Large Language Models\", \"Vicuna: An Open-Source Chatbot Impressing GPT-4 with $90 \\% *$ ChatGPT Quality\", \"Frozen: Multimodal few-shot learning with frozen language models\", \"Conversational Multimodal Agent for Complex Tasks\", \"Gemini: A Family of Highly Capable Multimodal Models\", \"A Real-World WebAgent with Planning, Long Context Understanding, and Program Synthesis\", \"Improved Baselines with Visual Instruction Tuning\", \"Visual Instruction Tuning\", \"AgentBench: Evaluating LLMs as Agents\", \"Bolaa: Benchmarking and Orchestrating LLM-Augmented Autonomous Agents\", \"Gaia: A Benchmark for General AI Assistants\", \"GPT-4 Technical Report\", \"Training Language Models to Follow Instructions with Human Feedback\", \"Scaling Language Models: Methods, Analysis and Insights from Training Gopher\", \"World of Bits: An Open-Domain Platform for Web-Based Agents\", \"Long Range Arena: A Benchmark for Efficient Transformers\", \"Eyes Wide Shut? Exploring the Visual Shortcomings of Multimodal LLMs\", \"Llama: Open and Efficient Foundation Language Models\", \"Llama 2: Open Foundation and Fine-Tuned Chat Models\", \"Multimodal Few-Shot Learning with Frozen Language Models\", \"Focused Transformer: Contrastive Training for Context Scaling\", \"Attention is All You Need\", \"Show and Tell: A Neural Image Caption Generator\", \"A Survey on Large Language Model Based Autonomous Agents\", \"GPT-4V in Wonderland: Large Multimodal Models for Zero-Shot Smartphone GUI Navigation\", \"Set-of-Marks Prompting Unleashes Extraordinary Visual Grounding in GPT-4V\", \"AppAgent: Multimodal Agents as Smartphone Users\", \"The Dawn of LMMs: Preliminary Explorations with GPT-4V (Vision)\", \"WebShop: Towards Scalable Real-World Web Interaction with Grounded Language Agents\", \"Tree of Thoughts: Deliberate Problem Solving with Large Language Models\", \"MMMU: A Massive Multi-Discipline Multimodal Understanding and Reasoning Benchmark for Expert AGI\", \"AgentTuning: Enabling Generalized Agent Abilities for LLMs\", \"You Only Look at Screens: Multimodal Chain-of-Action Agents\", \"OPT: Open Pre-Trained Transformer Language Models\", \"GPT-4V (Vision) is a Generalist Web Agent, if Grounded\", \"WebArena: A Realistic Web Environment for Building Autonomous Agents\"], \"Citation\":[\"Mixtral of Experts\"]}",
        "eval_response": "Evaluation evidence: The assistant's answer does not match the gold answer in terms of accuracy and completeness. The references and citations listed are largely incorrect or hallucinated, not aligning with the provided gold answer. The gold answer specifies two references and two citations, which are entirely missing from the assistant's response, except for one correct citation. This indicates a significant deviation from the required task.\n\nRating: [[10]]",
        "score": 10
    },
    {
        "level": 3,
        "set": 2,
        "length": 78820,
        "type": "paper",
        "language": "en",
        "question": "DistServe: Disaggregating Prefill and Decoding for Goodput-optimized Large Language Model Serving",
        "instruction": "We hope you will carefully study the provided papers and determine the citation relationships between them. Please follow the instructions below strictly to complete the task:\n\n#Specific Requirements:\n1. Reference: When a given paper mentions other provided papers, those other papers are considered as \"references\" for the given paper. To summarize in this specific context, references are about what the given paper is using.\n2. Citation: Conversely, when other provided papers mention the given paper in their works, the given paper is being \"cited\" by those other papers. To summarize in this specific context, citations are about who is using the given paper.\n3. Given a paper, you need to determine the citation or reference relationship between this paper and the other papers. Do not consider papers that are not provided.\n3. Please present the paper titles in a json format as follows: {{\"Reference\":[\"Reference Title 1\", \"Reference Title 2\", ..., \"Reference Title n\"], \"Citation\":[\"Citation Title 1\", \"Citation Title 2\", ..., \"Citation Title n\"]}}.\n4. If a paper does not have any references or citations, please leave the corresponding list empty, e.g.{{\"Refernce\":[]}}, {{\"Citation\":[]}}.",
        "prompt_template": "#Papers:\n{docs}\n\n{instruction}\n\n#The paper you need to analyze:\n{question}",
        "doc": [
            "2401.09670.md",
            "2405.19888.md",
            "2401.11181.md",
            "2403.01876.md"
        ],
        "answer": {
            "Reference": [
                "# Inference without Interference: Disaggregate LLM Inference for Mixed Downstream Workloads ",
                "# DjVu: KV-cache Streaming for Fast, Fault-tolerant Generative LLM Serving "
            ],
            "Citation": [
                "# Parrot: Efficient Serving of LLM-based Applications with Semantic Variable "
            ]
        },
        "shuffle_doc": true,
        "id": "5e05231b-0c48-4fc0-b5b5-01fe714d9f91",
        "prompt": "[Question]\n#Papers:\n{docs}\n\nWe hope you will carefully study the provided papers and determine the citation relationships between them. Please follow the instructions below strictly to complete the task:\n\n#Specific Requirements:\n1. Reference: When a given paper mentions other provided papers, those other papers are considered as \"references\" for the given paper. To summarize in this specific context, references are about what the given paper is using.\n2. Citation: Conversely, when other provided papers mention the given paper in their works, the given paper is being \"cited\" by those other papers. To summarize in this specific context, citations are about who is using the given paper.\n3. Given a paper, you need to determine the citation or reference relationship between this paper and the other papers. Do not consider papers that are not provided.\n3. Please present the paper titles in a json format as follows: {{\"Reference\":[\"Reference Title 1\", \"Reference Title 2\", ..., \"Reference Title n\"], \"Citation\":[\"Citation Title 1\", \"Citation Title 2\", ..., \"Citation Title n\"]}}.\n4. If a paper does not have any references or citations, please leave the corresponding list empty, e.g.{{\"Refernce\":[]}}, {{\"Citation\":[]}}.\n\n#The paper you need to analyze:\nDistServe: Disaggregating Prefill and Decoding for Goodput-optimized Large Language Model Serving\n\n[Gold Answer]\n{'Reference': ['# Inference without Interference: Disaggregate LLM Inference for Mixed Downstream Workloads ', '# DjVu: KV-cache Streaming for Fast, Fault-tolerant Generative LLM Serving '], 'Citation': ['# Parrot: Efficient Serving of LLM-based Applications with Semantic Variable ']}\n\n[The Start of Assistant's Predicted Answer]\n{\"Reference\":[\"TorchServe\", \"NVIDIA Triton\", \"Orca\", \"vLLM\", \"SARATHI\", \"FastServe\", \"Splitwise\", \"TetriInfer\", \"DjVu\", \"Pollux\", \"Sia\", \"Clockwork\", \"Shepherd\", \"AlpaServe\", \"Compute Express Link\", \"Gpipe\", \"Pipedream\", \"Zero\", \"Megatron-lm\", \"Djvu\", \"HotGPT\", \"Llama\", \"Fast distributed inference serving for large language models\", \"Orca: A distributed serving system for Transformer-Based generative models\", \"Shepherd: Serving dnns in the wild\", \"OPT: Open pre-trained transformer language models\", \"Make it real: An end-to-end implementation of a physically disaggregated data center\", \"FT-CNN: Algorithm-based fault tolerance for convolutional neural networks\", \"Alpa: Automating inter- and Intra-Operator parallelism for distributed deep learning\", \"PetS: A unified framework for ParameterEfficient transformers serving\"], \"Citation\":[]}\n[The End of Assistant's Predicted Answer]\n\n[System]\nWe would like to request your feedback on the performance of the AI assistant in response to the user question displayed above according to the gold answer. Please use the following listed aspects and their descriptions as evaluation criteria:\n    - Accuracy and Hallucinations: The assistant's answer is semantically consistent with the gold answer; The numerical value and order need to be accurate, and there should be no hallucinations.\n    - Completeness: Referring to the reference answers, the assistant's answer should contain all the key points needed to answer the user's question; further elaboration on these key points can be omitted.\nPlease rate whether this answer is suitable for the question. Please note that the gold answer can be considered as a correct answer to the question.\n\nThe assistant receives an overall score on a scale of 1 to 100, where a higher score indicates better overall performance.\nPlease note that if the assistant's answer and the gold answer fully meet the above criteria, its overall rating should be the full marks (100).\nPlease first provide a comprehensive explanation of your evaluation, avoiding any potential bias.\nThen, output a line indicating the score of the Assistant.\n\nPLEASE OUTPUT WITH THE FOLLOWING FORMAT, WHERE THE SCORE IS A SCALE OF 1 TO 100 BY STRICTLY FOLLOWING THIS FORMAT: \"[[score]]\", FOR EXAMPLE \"Rating: [[100]]\":\n<start output>\nEvaluation evidence: your evluation explanation here, no more than 100 words\nRating: [[score]]\n<end output>\n\nNow, start your evaluation:",
        "generate_response": "{\"Reference\":[\"TorchServe\", \"NVIDIA Triton\", \"Orca\", \"vLLM\", \"SARATHI\", \"FastServe\", \"Splitwise\", \"TetriInfer\", \"DjVu\", \"Pollux\", \"Sia\", \"Clockwork\", \"Shepherd\", \"AlpaServe\", \"Compute Express Link\", \"Gpipe\", \"Pipedream\", \"Zero\", \"Megatron-lm\", \"Djvu\", \"HotGPT\", \"Llama\", \"Fast distributed inference serving for large language models\", \"Orca: A distributed serving system for Transformer-Based generative models\", \"Shepherd: Serving dnns in the wild\", \"OPT: Open pre-trained transformer language models\", \"Make it real: An end-to-end implementation of a physically disaggregated data center\", \"FT-CNN: Algorithm-based fault tolerance for convolutional neural networks\", \"Alpa: Automating inter- and Intra-Operator parallelism for distributed deep learning\", \"PetS: A unified framework for ParameterEfficient transformers serving\"], \"Citation\":[]}",
        "eval_response": "Evaluation evidence: The assistant's answer includes a long list of references that are not mentioned in the gold answer, indicating significant hallucination. The gold answer specifies only two references and one citation, which the assistant fails to accurately identify. Instead, the assistant lists numerous unrelated papers, showing a lack of accuracy and a misunderstanding of the task requirements. This results in both inaccurate content and a failure to meet the completeness criterion as per the user's specific instructions.\n\nRating: [[10]]",
        "score": 10
    },
    {
        "level": 3,
        "set": 3,
        "length": 103496,
        "type": "paper",
        "language": "en",
        "question": "A Comprehensive Study of Knowledge Editing for Large Language Models",
        "instruction": "We hope you will carefully study the provided papers and determine the citation relationships between them. Please follow the instructions below strictly to complete the task:\n\n#Specific Requirements:\n1. Reference: When a given paper mentions other provided papers, those other papers are considered as \"references\" for the given paper. To summarize in this specific context, references are about what the given paper is using.\n2. Citation: Conversely, when other provided papers mention the given paper in their works, the given paper is being \"cited\" by those other papers. To summarize in this specific context, citations are about who is using the given paper.\n3. Given a paper, you need to determine the citation or reference relationship between this paper and the other papers. Do not consider papers that are not provided.\n3. Please present the paper titles in a json format as follows: {{\"Reference\":[\"Reference Title 1\", \"Reference Title 2\", ..., \"Reference Title n\"], \"Citation\":[\"Citation Title 1\", \"Citation Title 2\", ..., \"Citation Title n\"]}}.\n4. If a paper does not have any references or citations, please leave the corresponding list empty, e.g.{{\"Refernce\":[]}}, {{\"Citation\":[]}}.",
        "prompt_template": "#Papers:\n{docs}\n\n{instruction}\n\n#The paper you need to analyze:\n{question}",
        "doc": [
            "2401.01286.md",
            "2405.16821.md",
            "2405.19086.md"
        ],
        "answer": {
            "Reference": [],
            "Citation": [
                "# Perturbation-Restrained Sequential Model Editing ",
                "# MEMoE: Enhancing Model Editing with Mixture of Experts Adaptors "
            ]
        },
        "shuffle_doc": true,
        "id": "5b5b0394-d6d4-4655-8a63-aeafabd43ac7",
        "prompt": "[Question]\n#Papers:\n{docs}\n\nWe hope you will carefully study the provided papers and determine the citation relationships between them. Please follow the instructions below strictly to complete the task:\n\n#Specific Requirements:\n1. Reference: When a given paper mentions other provided papers, those other papers are considered as \"references\" for the given paper. To summarize in this specific context, references are about what the given paper is using.\n2. Citation: Conversely, when other provided papers mention the given paper in their works, the given paper is being \"cited\" by those other papers. To summarize in this specific context, citations are about who is using the given paper.\n3. Given a paper, you need to determine the citation or reference relationship between this paper and the other papers. Do not consider papers that are not provided.\n3. Please present the paper titles in a json format as follows: {{\"Reference\":[\"Reference Title 1\", \"Reference Title 2\", ..., \"Reference Title n\"], \"Citation\":[\"Citation Title 1\", \"Citation Title 2\", ..., \"Citation Title n\"]}}.\n4. If a paper does not have any references or citations, please leave the corresponding list empty, e.g.{{\"Refernce\":[]}}, {{\"Citation\":[]}}.\n\n#The paper you need to analyze:\nA Comprehensive Study of Knowledge Editing for Large Language Models\n\n[Gold Answer]\n{'Reference': [], 'Citation': ['# Perturbation-Restrained Sequential Model Editing ', '# MEMoE: Enhancing Model Editing with Mixture of Experts Adaptors ']}\n\n[The Start of Assistant's Predicted Answer]\n{\"Reference\": [\"Language Neurons in Pretrained Transformers\", \"Revealing a Core Linguistic Region in Large Language Models\", \"Editing Factual Associations in GPT\", \"Linearity of Relation Decoding in Transformer Language Models\", \"Linguistic Knowledge and Transferability of Contextual Representations\", \"Evaluating Commonsense in Pretrained Language Models\", \"Symbolic Knowledge Distillation: From General Language Models to Commonsense Models\", \"A Systematic Investigation of Commonsense Knowledge in Large Language Models\", \"Retentive or Forgetful? Diving into the Knowledge Memorizing Mechanism of Language Models\", \"Finetuning Language Models for Factuality\", \"VISIT: Visualizing and Interpreting the Semantic Information Flow of Transformers\", \"Generative Models as a Complex Systems Science: How Can We Make Sense of Large Language Model Behavior?\", \"Attention Lens: A Tool for Mechanistically Interpreting the Attention Head Information Retrieval Mechanism\", \"Seeking Neural Nuggets: Knowledge Transfer in Large Language Models from a Parametric Perspective\", \"Discovering Knowledge-Critical Subnetworks in Pretrained Language Models\", \"Composing Parameter-Efficient Modules with Arithmetic Operations\", \"Adding Conditional Control to Text-to-Image Diffusion Models\", \"Can We Edit Multimodal Large Language Models?\", \"RefacT: Updating Text-to-Image Models by Editing the Text Encoder\", \"Finding and Editing Multi-Modal Neurons in Pre-Trained Transformer\", \"Erasing Concepts from Diffusion Models\", \"Do Localization Methods Actually Localize Memorized Data in LLMs?\", \"A Benchmark for Assessing Knowledge Locating Methods in Language Models\"], \"Citation\": [\"Language Neurons in Pretrained Transformers\", \"Revealing a Core Linguistic Region in Large Language Models\", \"Editing Factual Associations in GPT\", \"Linearity of Relation Decoding in Transformer Language Models\", \"Linguistic Knowledge and Transferability of Contextual Representations\", \"Evaluating Commonsense in Pretrained Language Models\", \"Symbolic Knowledge Distillation: From General Language Models to Commonsense Models\", \"A Systematic Investigation of Commonsense Knowledge in Large Language Models\", \"Retentive or Forgetful? Diving into the Knowledge Memorizing Mechanism of Language Models\", \"Finetuning Language Models for Factuality\", \"VISIT: Visualizing and Interpreting the Semantic Information Flow of Transformers\", \"Generative Models as a Complex Systems Science: How Can We Make Sense of Large Language Model Behavior?\", \"Attention Lens: A Tool for Mechanistically Interpreting the Attention Head Information Retrieval Mechanism\", \"Seeking Neural Nuggets: Knowledge Transfer in Large Language Models from a Parametric Perspective\", \"Discovering Knowledge-Critical Subnetworks in Pretrained Language Models\", \"Composing Parameter-Efficient Modules with Arithmetic Operations\", \"Adding Conditional Control to Text-to-Image Diffusion Models\", \"Can We Edit Multimodal Large Language Models?\", \"RefacT: Updating Text-to-Image Models by Editing the Text Encoder\", \"Finding and Editing Multi-Modal Neurons in Pre-Trained Transformer\", \"Erasing Concepts from Diffusion Models\", \"Do Localization Methods Actually Localize Memorized Data in LLMs?\", \"A Benchmark for Assessing Knowledge Locating Methods in Language Models\"]}\n[The End of Assistant's Predicted Answer]\n\n[System]\nWe would like to request your feedback on the performance of the AI assistant in response to the user question displayed above according to the gold answer. Please use the following listed aspects and their descriptions as evaluation criteria:\n    - Accuracy and Hallucinations: The assistant's answer is semantically consistent with the gold answer; The numerical value and order need to be accurate, and there should be no hallucinations.\n    - Completeness: Referring to the reference answers, the assistant's answer should contain all the key points needed to answer the user's question; further elaboration on these key points can be omitted.\nPlease rate whether this answer is suitable for the question. Please note that the gold answer can be considered as a correct answer to the question.\n\nThe assistant receives an overall score on a scale of 1 to 100, where a higher score indicates better overall performance.\nPlease note that if the assistant's answer and the gold answer fully meet the above criteria, its overall rating should be the full marks (100).\nPlease first provide a comprehensive explanation of your evaluation, avoiding any potential bias.\nThen, output a line indicating the score of the Assistant.\n\nPLEASE OUTPUT WITH THE FOLLOWING FORMAT, WHERE THE SCORE IS A SCALE OF 1 TO 100 BY STRICTLY FOLLOWING THIS FORMAT: \"[[score]]\", FOR EXAMPLE \"Rating: [[100]]\":\n<start output>\nEvaluation evidence: your evluation explanation here, no more than 100 words\nRating: [[score]]\n<end output>\n\nNow, start your evaluation:",
        "generate_response": "{\"Reference\": [\"Language Neurons in Pretrained Transformers\", \"Revealing a Core Linguistic Region in Large Language Models\", \"Editing Factual Associations in GPT\", \"Linearity of Relation Decoding in Transformer Language Models\", \"Linguistic Knowledge and Transferability of Contextual Representations\", \"Evaluating Commonsense in Pretrained Language Models\", \"Symbolic Knowledge Distillation: From General Language Models to Commonsense Models\", \"A Systematic Investigation of Commonsense Knowledge in Large Language Models\", \"Retentive or Forgetful? Diving into the Knowledge Memorizing Mechanism of Language Models\", \"Finetuning Language Models for Factuality\", \"VISIT: Visualizing and Interpreting the Semantic Information Flow of Transformers\", \"Generative Models as a Complex Systems Science: How Can We Make Sense of Large Language Model Behavior?\", \"Attention Lens: A Tool for Mechanistically Interpreting the Attention Head Information Retrieval Mechanism\", \"Seeking Neural Nuggets: Knowledge Transfer in Large Language Models from a Parametric Perspective\", \"Discovering Knowledge-Critical Subnetworks in Pretrained Language Models\", \"Composing Parameter-Efficient Modules with Arithmetic Operations\", \"Adding Conditional Control to Text-to-Image Diffusion Models\", \"Can We Edit Multimodal Large Language Models?\", \"RefacT: Updating Text-to-Image Models by Editing the Text Encoder\", \"Finding and Editing Multi-Modal Neurons in Pre-Trained Transformer\", \"Erasing Concepts from Diffusion Models\", \"Do Localization Methods Actually Localize Memorized Data in LLMs?\", \"A Benchmark for Assessing Knowledge Locating Methods in Language Models\"], \"Citation\": [\"Language Neurons in Pretrained Transformers\", \"Revealing a Core Linguistic Region in Large Language Models\", \"Editing Factual Associations in GPT\", \"Linearity of Relation Decoding in Transformer Language Models\", \"Linguistic Knowledge and Transferability of Contextual Representations\", \"Evaluating Commonsense in Pretrained Language Models\", \"Symbolic Knowledge Distillation: From General Language Models to Commonsense Models\", \"A Systematic Investigation of Commonsense Knowledge in Large Language Models\", \"Retentive or Forgetful? Diving into the Knowledge Memorizing Mechanism of Language Models\", \"Finetuning Language Models for Factuality\", \"VISIT: Visualizing and Interpreting the Semantic Information Flow of Transformers\", \"Generative Models as a Complex Systems Science: How Can We Make Sense of Large Language Model Behavior?\", \"Attention Lens: A Tool for Mechanistically Interpreting the Attention Head Information Retrieval Mechanism\", \"Seeking Neural Nuggets: Knowledge Transfer in Large Language Models from a Parametric Perspective\", \"Discovering Knowledge-Critical Subnetworks in Pretrained Language Models\", \"Composing Parameter-Efficient Modules with Arithmetic Operations\", \"Adding Conditional Control to Text-to-Image Diffusion Models\", \"Can We Edit Multimodal Large Language Models?\", \"RefacT: Updating Text-to-Image Models by Editing the Text Encoder\", \"Finding and Editing Multi-Modal Neurons in Pre-Trained Transformer\", \"Erasing Concepts from Diffusion Models\", \"Do Localization Methods Actually Localize Memorized Data in LLMs?\", \"A Benchmark for Assessing Knowledge Locating Methods in Language Models\"]}",
        "eval_response": "Evaluation evidence: The assistant's answer is incorrect and does not align with the gold answer. The assistant lists numerous references and citations that are not mentioned in the gold answer, indicating a significant hallucination issue. The gold answer specifies two citations and no references, whereas the assistant's answer includes a long list of both, which are not relevant to the provided task. This discrepancy shows a lack of accuracy and completeness in understanding and executing the task requirements.\n\nRating: [[10]]",
        "score": 10
    },
    {
        "level": 3,
        "set": 3,
        "length": 106285,
        "type": "paper",
        "language": "en",
        "question": "DeepSeek LLM <br> Scaling Open-Source Language Models with Longtermism",
        "instruction": "We hope you will carefully study the provided papers and determine the citation relationships between them. Please follow the instructions below strictly to complete the task:\n\n#Specific Requirements:\n1. Reference: When a given paper mentions other provided papers, those other papers are considered as \"references\" for the given paper. To summarize in this specific context, references are about what the given paper is using.\n2. Citation: Conversely, when other provided papers mention the given paper in their works, the given paper is being \"cited\" by those other papers. To summarize in this specific context, citations are about who is using the given paper.\n3. Given a paper, you need to determine the citation or reference relationship between this paper and the other papers. Do not consider papers that are not provided.\n3. Please present the paper titles in a json format as follows: {{\"Reference\":[\"Reference Title 1\", \"Reference Title 2\", ..., \"Reference Title n\"], \"Citation\":[\"Citation Title 1\", \"Citation Title 2\", ..., \"Citation Title n\"]}}.\n4. If a paper does not have any references or citations, please leave the corresponding list empty, e.g.{{\"Refernce\":[]}}, {{\"Citation\":[]}}.",
        "prompt_template": "#Papers:\n{docs}\n\n{instruction}\n\n#The paper you need to analyze:\n{question}",
        "doc": [
            "2401.02954.md",
            "2405.18392.md",
            "2405.19327.md"
        ],
        "answer": {
            "Reference": [],
            "Citation": [
                "# Scaling Laws and Compute-Optimal Training Beyond Fixed Training Durations ",
                "# MAP-Neo: Highly Capable and Transparent Bilingual Large Language Model Series "
            ]
        },
        "shuffle_doc": true,
        "id": "061f53b5-cbb9-4c3d-b00f-0a4369a55de1",
        "prompt": "[Question]\n#Papers:\n{docs}\n\nWe hope you will carefully study the provided papers and determine the citation relationships between them. Please follow the instructions below strictly to complete the task:\n\n#Specific Requirements:\n1. Reference: When a given paper mentions other provided papers, those other papers are considered as \"references\" for the given paper. To summarize in this specific context, references are about what the given paper is using.\n2. Citation: Conversely, when other provided papers mention the given paper in their works, the given paper is being \"cited\" by those other papers. To summarize in this specific context, citations are about who is using the given paper.\n3. Given a paper, you need to determine the citation or reference relationship between this paper and the other papers. Do not consider papers that are not provided.\n3. Please present the paper titles in a json format as follows: {{\"Reference\":[\"Reference Title 1\", \"Reference Title 2\", ..., \"Reference Title n\"], \"Citation\":[\"Citation Title 1\", \"Citation Title 2\", ..., \"Citation Title n\"]}}.\n4. If a paper does not have any references or citations, please leave the corresponding list empty, e.g.{{\"Refernce\":[]}}, {{\"Citation\":[]}}.\n\n#The paper you need to analyze:\nDeepSeek LLM <br> Scaling Open-Source Language Models with Longtermism\n\n[Gold Answer]\n{'Reference': [], 'Citation': ['# Scaling Laws and Compute-Optimal Training Beyond Fixed Training Durations ', '# MAP-Neo: Highly Capable and Transparent Bilingual Large Language Model Series ']}\n\n[The Start of Assistant's Predicted Answer]\n{\"Reference\":[\"A Survey on Evaluation of Large Language Models\", \"Vicuna: An Open-Source Chatbot Impressing GPT-4 with $90 \\% *$ ChatGPT Quality\", \"ChatLaw: Open-Source Legal Large Language Model with Integrated External Knowledge Bases\", \"Compositional Semantic Parsing with Large Language Models\", \"Open LLM Leaderboard\", \"How Close is ChatGPT to Human Experts? Comparison Corpus, Evaluation, and Detection\", \"REALM: Retrieval-Augmented Language Model PreTraining\", \"Rethinking with Retrieval: Faithful Large Language Model Inference\", \"Measuring Massive Multitask Language Understanding\", \"GQA: Training generalized multi-query transformer models from multi-head checkpoints\", \"Introducing Claude\", \"Program synthesis with large language models\", \"Qwen technical report\", \"RACE: large-scale reading comprehension dataset from examinations\", \"CMMLU: Measuring massive multitask language understanding in Chinese\", \"Ccpm: A chinese classical poetry matching dataset\", \"The refinedweb dataset for falcon llm: outperforming curated corpora with web data, and web data only\", \"Don't decay the learning rate, increase the batch size\", \"Using deepspeed and megatron to train megatron-turing nlg 530b, a large-scale generative language model\", \"Llama 2: Open foundation and fine-tuned chat models\", \"Attention is all you need\", \"Chain-of-thought prompting elicits reasoning in large language models\", \"Cmath: Can your language model pass chinese elementary school math test?\", \"CLUE: A chinese language understanding evaluation benchmark\", \"Do-not-answer: A dataset for evaluating safeguards in llms\", \"Metamath: Bootstrap your own mathematical questions for large language models\", \"HellaSwag: Can a machine really finish your sentence?\", \"Root mean square layer normalization\", \"Which algorithmic choices matter at which batch sizes? insights from a noisy quadratic model\", \"Chid: A large-scale chinese idiom dataset for cloze test\", \"Instruction-following evaluation for large language models\"], \"Citation\":[]}\n[The End of Assistant's Predicted Answer]\n\n[System]\nWe would like to request your feedback on the performance of the AI assistant in response to the user question displayed above according to the gold answer. Please use the following listed aspects and their descriptions as evaluation criteria:\n    - Accuracy and Hallucinations: The assistant's answer is semantically consistent with the gold answer; The numerical value and order need to be accurate, and there should be no hallucinations.\n    - Completeness: Referring to the reference answers, the assistant's answer should contain all the key points needed to answer the user's question; further elaboration on these key points can be omitted.\nPlease rate whether this answer is suitable for the question. Please note that the gold answer can be considered as a correct answer to the question.\n\nThe assistant receives an overall score on a scale of 1 to 100, where a higher score indicates better overall performance.\nPlease note that if the assistant's answer and the gold answer fully meet the above criteria, its overall rating should be the full marks (100).\nPlease first provide a comprehensive explanation of your evaluation, avoiding any potential bias.\nThen, output a line indicating the score of the Assistant.\n\nPLEASE OUTPUT WITH THE FOLLOWING FORMAT, WHERE THE SCORE IS A SCALE OF 1 TO 100 BY STRICTLY FOLLOWING THIS FORMAT: \"[[score]]\", FOR EXAMPLE \"Rating: [[100]]\":\n<start output>\nEvaluation evidence: your evluation explanation here, no more than 100 words\nRating: [[score]]\n<end output>\n\nNow, start your evaluation:",
        "generate_response": "{\"Reference\":[\"A Survey on Evaluation of Large Language Models\", \"Vicuna: An Open-Source Chatbot Impressing GPT-4 with $90 \\% *$ ChatGPT Quality\", \"ChatLaw: Open-Source Legal Large Language Model with Integrated External Knowledge Bases\", \"Compositional Semantic Parsing with Large Language Models\", \"Open LLM Leaderboard\", \"How Close is ChatGPT to Human Experts? Comparison Corpus, Evaluation, and Detection\", \"REALM: Retrieval-Augmented Language Model PreTraining\", \"Rethinking with Retrieval: Faithful Large Language Model Inference\", \"Measuring Massive Multitask Language Understanding\", \"GQA: Training generalized multi-query transformer models from multi-head checkpoints\", \"Introducing Claude\", \"Program synthesis with large language models\", \"Qwen technical report\", \"RACE: large-scale reading comprehension dataset from examinations\", \"CMMLU: Measuring massive multitask language understanding in Chinese\", \"Ccpm: A chinese classical poetry matching dataset\", \"The refinedweb dataset for falcon llm: outperforming curated corpora with web data, and web data only\", \"Don't decay the learning rate, increase the batch size\", \"Using deepspeed and megatron to train megatron-turing nlg 530b, a large-scale generative language model\", \"Llama 2: Open foundation and fine-tuned chat models\", \"Attention is all you need\", \"Chain-of-thought prompting elicits reasoning in large language models\", \"Cmath: Can your language model pass chinese elementary school math test?\", \"CLUE: A chinese language understanding evaluation benchmark\", \"Do-not-answer: A dataset for evaluating safeguards in llms\", \"Metamath: Bootstrap your own mathematical questions for large language models\", \"HellaSwag: Can a machine really finish your sentence?\", \"Root mean square layer normalization\", \"Which algorithmic choices matter at which batch sizes? insights from a noisy quadratic model\", \"Chid: A large-scale chinese idiom dataset for cloze test\", \"Instruction-following evaluation for large language models\"], \"Citation\":[]}",
        "eval_response": "Evaluation evidence: The assistant's answer inaccurately lists numerous references that are not mentioned in the gold answer, indicating a significant hallucination issue. The gold answer specifies no references and two citations, which the assistant fails to include, showing a lack of accuracy and completeness. The assistant's response does not align with the gold answer's citation details, further detracting from its accuracy.\n\nRating: [[10]]",
        "score": 10
    },
    {
        "level": 4,
        "set": 1,
        "length": 47554,
        "type": "paper",
        "language": "en",
        "question": "",
        "instruction": "We kindly ask you to thoroughly review the provided papers and construct a citation chain from them. Please adhere to the following instructions strictly while completing the task:\n\n#Task Instructions:\nGiven several papers, you are required to identify and list the longest citation chain, which demonstrates the citation relationship among the provided papers.\n\n#Specific Requirements:\n1.Please present the titles of the papers in the form of a list, as follows: [\"Title of Paper 1\", \"Title of Paper 2\", ..., \"Title of Paper n\"].\n2.Ensure that the citation chain in the list is linear and continuous, meaning that the first paper title in the list (Paper 1) should not cite any other works. Instead, it should be cited by the next paper in the list (Paper 2); subsequently, each paper should then be cited by the next one in the list, continuing up to the last paper (Paper n).\n3.Consider only the citation relationships within the supplied collection of papers, and ensure that the citation chain accurately reflects the sequential citation order among these documents.\n4.Do not take into account any articles not provided, and disregard other non-linear citation relationships.",
        "prompt_template": "{instruction}\n\n#Papers Provided:\n{docs}",
        "doc": [
            "2309.15088.md",
            "2310.08319.md",
            "2401.00368.md"
        ],
        "answer": [
            "# RankVicuna: Zero-Shot Listwise Document Reranking with Open-Source Large Language Models ",
            "# Fine-Tuning LLaMA for Multi-Stage Text Retrieval ",
            "# Improving Text Embeddings with Large Language Models "
        ],
        "shuffle_doc": true,
        "id": "9f29f552-c817-4c2a-8910-b5e8ae7c014f",
        "prompt": "[Question]\nWe kindly ask you to thoroughly review the provided papers and construct a citation chain from them. Please adhere to the following instructions strictly while completing the task:\n\n#Task Instructions:\nGiven several papers, you are required to identify and list the longest citation chain, which demonstrates the citation relationship among the provided papers.\n\n#Specific Requirements:\n1.Please present the titles of the papers in the form of a list, as follows: [\"Title of Paper 1\", \"Title of Paper 2\", ..., \"Title of Paper n\"].\n2.Ensure that the citation chain in the list is linear and continuous, meaning that the first paper title in the list (Paper 1) should not cite any other works. Instead, it should be cited by the next paper in the list (Paper 2); subsequently, each paper should then be cited by the next one in the list, continuing up to the last paper (Paper n).\n3.Consider only the citation relationships within the supplied collection of papers, and ensure that the citation chain accurately reflects the sequential citation order among these documents.\n4.Do not take into account any articles not provided, and disregard other non-linear citation relationships.\n\n#Papers Provided:\n{docs}\n\n[Gold Answer]\n['# RankVicuna: Zero-Shot Listwise Document Reranking with Open-Source Large Language Models ', '# Fine-Tuning LLaMA for Multi-Stage Text Retrieval ', '# Improving Text Embeddings with Large Language Models ']\n\n[The Start of Assistant's Predicted Answer]\n[\"Title of Paper 1\", \"Title of Paper 2\", \"Title of Paper 3\", \"Title of Paper 4\", \"Title of Paper 5\"]\n[The End of Assistant's Predicted Answer]\n\n[System]\nWe would like to request your feedback on the performance of the AI assistant in response to the user question displayed above according to the gold answer. Please use the following listed aspects and their descriptions as evaluation criteria:\n    - Accuracy and Hallucinations: The assistant's answer is semantically consistent with the gold answer; The numerical value and order need to be accurate, and there should be no hallucinations.\n    - Completeness: Referring to the reference answers, the assistant's answer should contain all the key points needed to answer the user's question; further elaboration on these key points can be omitted.\nPlease rate whether this answer is suitable for the question. Please note that the gold answer can be considered as a correct answer to the question.\n\nThe assistant receives an overall score on a scale of 1 to 100, where a higher score indicates better overall performance.\nPlease note that if the assistant's answer and the gold answer fully meet the above criteria, its overall rating should be the full marks (100).\nPlease first provide a comprehensive explanation of your evaluation, avoiding any potential bias.\nThen, output a line indicating the score of the Assistant.\n\nPLEASE OUTPUT WITH THE FOLLOWING FORMAT, WHERE THE SCORE IS A SCALE OF 1 TO 100 BY STRICTLY FOLLOWING THIS FORMAT: \"[[score]]\", FOR EXAMPLE \"Rating: [[100]]\":\n<start output>\nEvaluation evidence: your evluation explanation here, no more than 100 words\nRating: [[score]]\n<end output>\n\nNow, start your evaluation:",
        "generate_response": "[\"Title of Paper 1\", \"Title of Paper 2\", \"Title of Paper 3\", \"Title of Paper 4\", \"Title of Paper 5\"]",
        "eval_response": "Evaluation evidence: The assistant's response does not match the gold answer, indicating a failure in accurately identifying the longest citation chain among the provided papers. The assistant's response lists five generic paper titles, which do not correspond to any specific titles from the provided documents. This shows a lack of accuracy and the presence of hallucinations, as the titles listed do not exist in the provided context. The response also fails in completeness as it does not reflect the actual citation relationships among the provided papers.\n\nRating: [[10]]",
        "score": 10
    },
    {
        "level": 4,
        "set": 2,
        "length": 74096,
        "type": "paper",
        "language": "en",
        "question": "",
        "instruction": "We kindly ask you to thoroughly review the provided papers and construct a citation chain from them. Please adhere to the following instructions strictly while completing the task:\n\n#Task Instructions:\nGiven several papers, you are required to identify and list the longest citation chain, which demonstrates the citation relationship among the provided papers.\n\n#Specific Requirements:\n1.Please present the titles of the papers in the form of a list, as follows: [\"Title of Paper 1\", \"Title of Paper 2\", ..., \"Title of Paper n\"].\n2.Ensure that the citation chain in the list is linear and continuous, meaning that the first paper title in the list (Paper 1) should not cite any other works. Instead, it should be cited by the next paper in the list (Paper 2); subsequently, each paper should then be cited by the next one in the list, continuing up to the last paper (Paper n).\n3.Consider only the citation relationships within the supplied collection of papers, and ensure that the citation chain accurately reflects the sequential citation order among these documents.\n4.Do not take into account any articles not provided, and disregard other non-linear citation relationships.",
        "prompt_template": "{instruction}\n\n#Papers Provided:\n{docs}",
        "doc": [
            "2310.08319.md",
            "2401.00368.md",
            "2310.20329.md",
            "2401.04621.md"
        ],
        "answer": [
            "# Fine-Tuning LLaMA for Multi-Stage Text Retrieval ",
            "# Improving Text Embeddings with Large Language Models ",
            "# InstructCoder: Instruction Tuning Large Language Models for Code Editing ",
            "#  DebugBench: <br> Evaluating Debugging Capability of Large Language Models "
        ],
        "shuffle_doc": true,
        "id": "dfcd1962-3307-4022-a6ed-e4fd2b023de5",
        "prompt": "[Question]\nWe kindly ask you to thoroughly review the provided papers and construct a citation chain from them. Please adhere to the following instructions strictly while completing the task:\n\n#Task Instructions:\nGiven several papers, you are required to identify and list the longest citation chain, which demonstrates the citation relationship among the provided papers.\n\n#Specific Requirements:\n1.Please present the titles of the papers in the form of a list, as follows: [\"Title of Paper 1\", \"Title of Paper 2\", ..., \"Title of Paper n\"].\n2.Ensure that the citation chain in the list is linear and continuous, meaning that the first paper title in the list (Paper 1) should not cite any other works. Instead, it should be cited by the next paper in the list (Paper 2); subsequently, each paper should then be cited by the next one in the list, continuing up to the last paper (Paper n).\n3.Consider only the citation relationships within the supplied collection of papers, and ensure that the citation chain accurately reflects the sequential citation order among these documents.\n4.Do not take into account any articles not provided, and disregard other non-linear citation relationships.\n\n#Papers Provided:\n{docs}\n\n[Gold Answer]\n['# Fine-Tuning LLaMA for Multi-Stage Text Retrieval ', '# Improving Text Embeddings with Large Language Models ', '# InstructCoder: Instruction Tuning Large Language Models for Code Editing ', '#  DebugBench: <br> Evaluating Debugging Capability of Large Language Models ']\n\n[The Start of Assistant's Predicted Answer]\n[\"DebugBench: Evaluating Debugging Capability of Large Language Models\", \"Program synthesis with large language models\", \"The pile: An 800 gb dataset of diverse text for language modeling\", \"Training language models to follow instructions with human feedback\", \"A Survey of large language models\"]\n[The End of Assistant's Predicted Answer]\n\n[System]\nWe would like to request your feedback on the performance of the AI assistant in response to the user question displayed above according to the gold answer. Please use the following listed aspects and their descriptions as evaluation criteria:\n    - Accuracy and Hallucinations: The assistant's answer is semantically consistent with the gold answer; The numerical value and order need to be accurate, and there should be no hallucinations.\n    - Completeness: Referring to the reference answers, the assistant's answer should contain all the key points needed to answer the user's question; further elaboration on these key points can be omitted.\nPlease rate whether this answer is suitable for the question. Please note that the gold answer can be considered as a correct answer to the question.\n\nThe assistant receives an overall score on a scale of 1 to 100, where a higher score indicates better overall performance.\nPlease note that if the assistant's answer and the gold answer fully meet the above criteria, its overall rating should be the full marks (100).\nPlease first provide a comprehensive explanation of your evaluation, avoiding any potential bias.\nThen, output a line indicating the score of the Assistant.\n\nPLEASE OUTPUT WITH THE FOLLOWING FORMAT, WHERE THE SCORE IS A SCALE OF 1 TO 100 BY STRICTLY FOLLOWING THIS FORMAT: \"[[score]]\", FOR EXAMPLE \"Rating: [[100]]\":\n<start output>\nEvaluation evidence: your evluation explanation here, no more than 100 words\nRating: [[score]]\n<end output>\n\nNow, start your evaluation:",
        "generate_response": "[\"DebugBench: Evaluating Debugging Capability of Large Language Models\", \"Program synthesis with large language models\", \"The pile: An 800 gb dataset of diverse text for language modeling\", \"Training language models to follow instructions with human feedback\", \"A Survey of large language models\"]",
        "eval_response": "Evaluation evidence: The assistant's answer does not match the gold answer in terms of the titles of the papers and their citation order. The titles listed in the assistant's answer do not appear in the gold answer, indicating a significant deviation from the required task. This shows inaccuracies and hallucinations as the titles and citation chain presented do not align with the provided gold answer. Therefore, the response fails to meet the accuracy and completeness criteria.\n\nRating: [[10]]",
        "score": 10
    },
    {
        "level": 3,
        "set": 3,
        "length": 177714,
        "type": "paper",
        "language": "en",
        "question": "RAG VS Fine-TUNing: PiPELINES, TRADEOFFS, AND A CASE STUDY ON AGRICULTURE",
        "instruction": "We hope you will carefully study the provided papers and determine the citation relationships between them. Please follow the instructions below strictly to complete the task:\n\n#Specific Requirements:\n1. Reference: When a given paper mentions other provided papers, those other papers are considered as \"references\" for the given paper. To summarize in this specific context, references are about what the given paper is using.\n2. Citation: Conversely, when other provided papers mention the given paper in their works, the given paper is being \"cited\" by those other papers. To summarize in this specific context, citations are about who is using the given paper.\n3. Given a paper, you need to determine the citation or reference relationship between this paper and the other papers. Do not consider papers that are not provided.\n3. Please present the paper titles in a json format as follows: {{\"Reference\":[\"Reference Title 1\", \"Reference Title 2\", ..., \"Reference Title n\"], \"Citation\":[\"Citation Title 1\", \"Citation Title 2\", ..., \"Citation Title n\"]}}.\n4. If a paper does not have any references or citations, please leave the corresponding list empty, e.g.{{\"Refernce\":[]}}, {{\"Citation\":[]}}.",
        "prompt_template": "#Papers:\n{docs}\n\n{instruction}\n\n#The paper you need to analyze:\n{question}",
        "doc": [
            "2401.08406.md",
            "2404.15939.md",
            "2405.03085.md",
            "2405.07437.md",
            "2405.17147.md",
            "2405.20347.md",
            "2306.05685.md",
            "2307.03875.md",
            "2307.16877.md",
            "2310.06225.md",
            "2310.17631.md"
        ],
        "answer": {
            "Reference": [
                "# Judging LLM-as-a-Judge with MT-Bench and Chatbot Arena ",
                "# Large Language Models for Supply Chain Optimization ",
                "# Evaluating Correctness and Faithfulness of Instruction-Following Models for Question Answering ",
                "# GPT-4 AS AN AGRONOMIST ASSISTANT? ANSWERING AGRICULTURE QUESTIONS USING LARGE LANGUAGE MODELS ",
                "# JUDGELM : FINE-TUNED LARGE LANGUAGE MODELS ARE SCALABLE JUDGES "
            ],
            "Citation": [
                "# Telco-RAG: Navigating the Challenges of Retrieval-Augmented Language Models for Telecommunications ",
                "# Compressing Long Context for Enhancing RAG with AMR-based Concept Distillation ",
                "# Large Language Models (LLMs): Deployment, Tokenomics and Sustainability ",
                "# Small Language Models for Application Interactions: A Case Study "
            ]
        },
        "shuffle_doc": true,
        "id": "cca25baf-ee39-4c29-833d-cbbe6a9feda6",
        "prompt": "[Question]\n#Papers:\n{docs}\n\nWe hope you will carefully study the provided papers and determine the citation relationships between them. Please follow the instructions below strictly to complete the task:\n\n#Specific Requirements:\n1. Reference: When a given paper mentions other provided papers, those other papers are considered as \"references\" for the given paper. To summarize in this specific context, references are about what the given paper is using.\n2. Citation: Conversely, when other provided papers mention the given paper in their works, the given paper is being \"cited\" by those other papers. To summarize in this specific context, citations are about who is using the given paper.\n3. Given a paper, you need to determine the citation or reference relationship between this paper and the other papers. Do not consider papers that are not provided.\n3. Please present the paper titles in a json format as follows: {{\"Reference\":[\"Reference Title 1\", \"Reference Title 2\", ..., \"Reference Title n\"], \"Citation\":[\"Citation Title 1\", \"Citation Title 2\", ..., \"Citation Title n\"]}}.\n4. If a paper does not have any references or citations, please leave the corresponding list empty, e.g.{{\"Refernce\":[]}}, {{\"Citation\":[]}}.\n\n#The paper you need to analyze:\nRAG VS Fine-TUNing: PiPELINES, TRADEOFFS, AND A CASE STUDY ON AGRICULTURE\n\n[Gold Answer]\n{'Reference': ['# Judging LLM-as-a-Judge with MT-Bench and Chatbot Arena ', '# Large Language Models for Supply Chain Optimization ', '# Evaluating Correctness and Faithfulness of Instruction-Following Models for Question Answering ', '# GPT-4 AS AN AGRONOMIST ASSISTANT? ANSWERING AGRICULTURE QUESTIONS USING LARGE LANGUAGE MODELS ', '# JUDGELM : FINE-TUNED LARGE LANGUAGE MODELS ARE SCALABLE JUDGES '], 'Citation': ['# Telco-RAG: Navigating the Challenges of Retrieval-Augmented Language Models for Telecommunications ', '# Compressing Long Context for Enhancing RAG with AMR-based Concept Distillation ', '# Large Language Models (LLMs): Deployment, Tokenomics and Sustainability ', '# Small Language Models for Application Interactions: A Case Study ']}\n\n[The Start of Assistant's Predicted Answer]\n{\"Reference\": [\"A Comparison of Statistical Downscaling Methods Suited for Wildfire Applications\", \"International Journal of Climatology\", \"Vikaspediahttps://vikaspedia.in/agriculture/portal\", \"500 Questions 500 Answers - Embrapa/SCT\", \"GPT-4 technical report\", \"LLaMA: Open and Efficient Foundation Language Models\", \"Llama 2: Open Foundation and Fine-Tuned Chat Models\"], \"Citation\": [\"Telco-RAG: Navigating the Challenges of Retrieval-Augmented Language Models for Telecommunications\"]}\n[The End of Assistant's Predicted Answer]\n\n[System]\nWe would like to request your feedback on the performance of the AI assistant in response to the user question displayed above according to the gold answer. Please use the following listed aspects and their descriptions as evaluation criteria:\n    - Accuracy and Hallucinations: The assistant's answer is semantically consistent with the gold answer; The numerical value and order need to be accurate, and there should be no hallucinations.\n    - Completeness: Referring to the reference answers, the assistant's answer should contain all the key points needed to answer the user's question; further elaboration on these key points can be omitted.\nPlease rate whether this answer is suitable for the question. Please note that the gold answer can be considered as a correct answer to the question.\n\nThe assistant receives an overall score on a scale of 1 to 100, where a higher score indicates better overall performance.\nPlease note that if the assistant's answer and the gold answer fully meet the above criteria, its overall rating should be the full marks (100).\nPlease first provide a comprehensive explanation of your evaluation, avoiding any potential bias.\nThen, output a line indicating the score of the Assistant.\n\nPLEASE OUTPUT WITH THE FOLLOWING FORMAT, WHERE THE SCORE IS A SCALE OF 1 TO 100 BY STRICTLY FOLLOWING THIS FORMAT: \"[[score]]\", FOR EXAMPLE \"Rating: [[100]]\":\n<start output>\nEvaluation evidence: your evluation explanation here, no more than 100 words\nRating: [[score]]\n<end output>\n\nNow, start your evaluation:",
        "generate_response": "{\"Reference\": [\"A Comparison of Statistical Downscaling Methods Suited for Wildfire Applications\", \"International Journal of Climatology\", \"Vikaspediahttps://vikaspedia.in/agriculture/portal\", \"500 Questions 500 Answers - Embrapa/SCT\", \"GPT-4 technical report\", \"LLaMA: Open and Efficient Foundation Language Models\", \"Llama 2: Open Foundation and Fine-Tuned Chat Models\"], \"Citation\": [\"Telco-RAG: Navigating the Challenges of Retrieval-Augmented Language Models for Telecommunications\"]}",
        "eval_response": "Evaluation evidence: The assistant's answer does not match the gold answer in terms of accuracy and completeness. The references and citations listed are entirely different from those in the gold answer, indicating a significant deviation and possible hallucination of content. The assistant fails to identify any of the correct references or citations as per the gold answer, which suggests a misunderstanding or misinterpretation of the source material or task requirements.\n\nRating: [[10]]",
        "score": 10
    },
    {
        "level": 3,
        "set": 3,
        "length": 101658,
        "type": "paper",
        "language": "en",
        "question": "WebVoyager : Building an End-to-End Web Agent with Large Multimodal Models",
        "instruction": "We hope you will carefully study the provided papers and determine the citation relationships between them. Please follow the instructions below strictly to complete the task:\n\n#Specific Requirements:\n1. Reference: When a given paper mentions other provided papers, those other papers are considered as \"references\" for the given paper. To summarize in this specific context, references are about what the given paper is using.\n2. Citation: Conversely, when other provided papers mention the given paper in their works, the given paper is being \"cited\" by those other papers. To summarize in this specific context, citations are about who is using the given paper.\n3. Given a paper, you need to determine the citation or reference relationship between this paper and the other papers. Do not consider papers that are not provided.\n3. Please present the paper titles in a json format as follows: {{\"Reference\":[\"Reference Title 1\", \"Reference Title 2\", ..., \"Reference Title n\"], \"Citation\":[\"Citation Title 1\", \"Citation Title 2\", ..., \"Citation Title n\"]}}.\n4. If a paper does not have any references or citations, please leave the corresponding list empty, e.g.{{\"Refernce\":[]}}, {{\"Citation\":[]}}.",
        "prompt_template": "#Papers:\n{docs}\n\n{instruction}\n\n#The paper you need to analyze:\n{question}",
        "doc": [
            "2401.13919.md",
            "2405.11120.md",
            "2405.14573.md",
            "2401.10935.md",
            "2401.13649.md"
        ],
        "answer": {
            "Reference": [
                "# SeeClick: Harnessing GUI Grounding for Advanced Visual GUI Agents ",
                "# VisualWebArena: EvalUATING MULTIMODAL AGENTS ON REALISTIC VISUAL WEB TASKS "
            ],
            "Citation": [
                "# Latent State Estimation Helps UI Agents to Reason ",
                "# AndroidWorld: A Dynamic Benchmarking Environment for Autonomous Agents "
            ]
        },
        "shuffle_doc": true,
        "id": "2822e16c-6518-4ba5-9ea6-33c57b8beb31",
        "prompt": "[Question]\n#Papers:\n{docs}\n\nWe hope you will carefully study the provided papers and determine the citation relationships between them. Please follow the instructions below strictly to complete the task:\n\n#Specific Requirements:\n1. Reference: When a given paper mentions other provided papers, those other papers are considered as \"references\" for the given paper. To summarize in this specific context, references are about what the given paper is using.\n2. Citation: Conversely, when other provided papers mention the given paper in their works, the given paper is being \"cited\" by those other papers. To summarize in this specific context, citations are about who is using the given paper.\n3. Given a paper, you need to determine the citation or reference relationship between this paper and the other papers. Do not consider papers that are not provided.\n3. Please present the paper titles in a json format as follows: {{\"Reference\":[\"Reference Title 1\", \"Reference Title 2\", ..., \"Reference Title n\"], \"Citation\":[\"Citation Title 1\", \"Citation Title 2\", ..., \"Citation Title n\"]}}.\n4. If a paper does not have any references or citations, please leave the corresponding list empty, e.g.{{\"Refernce\":[]}}, {{\"Citation\":[]}}.\n\n#The paper you need to analyze:\nWebVoyager : Building an End-to-End Web Agent with Large Multimodal Models\n\n[Gold Answer]\n{'Reference': ['# SeeClick: Harnessing GUI Grounding for Advanced Visual GUI Agents ', '# VisualWebArena: EvalUATING MULTIMODAL AGENTS ON REALISTIC VISUAL WEB TASKS '], 'Citation': ['# Latent State Estimation Helps UI Agents to Reason ', '# AndroidWorld: A Dynamic Benchmarking Environment for Autonomous Agents ']}\n\n[The Start of Assistant's Predicted Answer]\n{\"Reference\": [\"Cm3: A causal masked multimodal model of the internet\", \"AutoGPT\", \"Language models are few-shot learners\", \"Evaluating large language models trained on code\", \"Webvln: Vision-and-language navigation on websites\", \"Seeclick: Harnessing gui grounding for advanced visual gui agents\", \"Palm: Scaling language modeling with pathways\", \"A coefficient of agreement for nominal scales\", \"Mind2web: Towards a generalist agent for the web\", \"A survey for in-context learning\", \"An image is worth $16 \\times 16$ words: Transformers for image recognition at scale\", \"Measuring nominal scale agreement among many raters\", \"Multimodal web navigation with instruction-finetuned foundation models\", \"Making the v in vqa matter: Elevating the role of image understanding in visual question answering\", \"A real-world webagent with planning, long context understanding, and program synthesis\", \"The abduction of sherlock holmes: A dataset for visual abductive reasoning\", \"Visualwebarena: Evaluating multimodal agents on realistic visual web tasks\", \"Pix2struct: Screenshot parsing as pretraining for visual language understanding\", \"Visualbert: A simple and performant baseline for vision and language\", \"Learn to explain: Multimodal reasoning via thought chains for science question answering\", \"Chameleon: Plug-and-play compositional reasoning with large language models\", \"Laser: Llm agent with state-space exploration for web navigation\", \"Gaia: a benchmark for general ai assistants\", \"Webgpt: Browser-assisted questionanswering with human feedback\", \"Gpt-4 technical report\", \"Training language models to follow instructions with human feedback\", \"Toolllm: Facilitating large language models to master 16000+ real-world apis\", \"Exploring the limits of transfer learning with a unified text-to-text transformer\", \"Toolformer: Language models can teach themselves to use tools\", \"From pixels to ui actions: Learning to follow instructions via graphical user interfaces\", \"World of bits: An open-domain platform for web-based agents\", \"Reflexion: Language agents with verbal reinforcement learning\", \"Gemini: A family of highly capable multimodal models\", \"Self-instruct: Aligning language model with self generated instructions\", \"Simple visual language model pretraining with weak supervision\", \"Llm-powered autonomous agents\", \"Gpt-4v in wonderland: Large multimodal models for zero-shot smartphone gui navigation\", \"Set-of-mark prompting unleashes extraordinary visual grounding in gpt-4v\", \"The dawn of $1 \\mathrm{mms}$ : Preliminary explorations with gpt-4v (ision)\", \"Webshop: Towards scalable real-world web interaction with grounded language agents\", \"React: Synergizing reasoning and acting in language models\", \"Lumos: Learning agents with unified data, modular design, and open-source llms\", \"From recognition to cognition: Visual commonsense reasoning\", \"Appagent: Multimodal agents as smartphone users\", \"Gpt-4v (ision) is a generalist web agent, if grounded\", \"Webarena: A realistic web environment for building autonomous agents\", \"Object detection in 20 years: A survey\"], \"Citation\": []}\n[The End of Assistant's Predicted Answer]\n\n[System]\nWe would like to request your feedback on the performance of the AI assistant in response to the user question displayed above according to the gold answer. Please use the following listed aspects and their descriptions as evaluation criteria:\n    - Accuracy and Hallucinations: The assistant's answer is semantically consistent with the gold answer; The numerical value and order need to be accurate, and there should be no hallucinations.\n    - Completeness: Referring to the reference answers, the assistant's answer should contain all the key points needed to answer the user's question; further elaboration on these key points can be omitted.\nPlease rate whether this answer is suitable for the question. Please note that the gold answer can be considered as a correct answer to the question.\n\nThe assistant receives an overall score on a scale of 1 to 100, where a higher score indicates better overall performance.\nPlease note that if the assistant's answer and the gold answer fully meet the above criteria, its overall rating should be the full marks (100).\nPlease first provide a comprehensive explanation of your evaluation, avoiding any potential bias.\nThen, output a line indicating the score of the Assistant.\n\nPLEASE OUTPUT WITH THE FOLLOWING FORMAT, WHERE THE SCORE IS A SCALE OF 1 TO 100 BY STRICTLY FOLLOWING THIS FORMAT: \"[[score]]\", FOR EXAMPLE \"Rating: [[100]]\":\n<start output>\nEvaluation evidence: your evluation explanation here, no more than 100 words\nRating: [[score]]\n<end output>\n\nNow, start your evaluation:",
        "generate_response": "{\"Reference\": [\"Cm3: A causal masked multimodal model of the internet\", \"AutoGPT\", \"Language models are few-shot learners\", \"Evaluating large language models trained on code\", \"Webvln: Vision-and-language navigation on websites\", \"Seeclick: Harnessing gui grounding for advanced visual gui agents\", \"Palm: Scaling language modeling with pathways\", \"A coefficient of agreement for nominal scales\", \"Mind2web: Towards a generalist agent for the web\", \"A survey for in-context learning\", \"An image is worth $16 \\times 16$ words: Transformers for image recognition at scale\", \"Measuring nominal scale agreement among many raters\", \"Multimodal web navigation with instruction-finetuned foundation models\", \"Making the v in vqa matter: Elevating the role of image understanding in visual question answering\", \"A real-world webagent with planning, long context understanding, and program synthesis\", \"The abduction of sherlock holmes: A dataset for visual abductive reasoning\", \"Visualwebarena: Evaluating multimodal agents on realistic visual web tasks\", \"Pix2struct: Screenshot parsing as pretraining for visual language understanding\", \"Visualbert: A simple and performant baseline for vision and language\", \"Learn to explain: Multimodal reasoning via thought chains for science question answering\", \"Chameleon: Plug-and-play compositional reasoning with large language models\", \"Laser: Llm agent with state-space exploration for web navigation\", \"Gaia: a benchmark for general ai assistants\", \"Webgpt: Browser-assisted questionanswering with human feedback\", \"Gpt-4 technical report\", \"Training language models to follow instructions with human feedback\", \"Toolllm: Facilitating large language models to master 16000+ real-world apis\", \"Exploring the limits of transfer learning with a unified text-to-text transformer\", \"Toolformer: Language models can teach themselves to use tools\", \"From pixels to ui actions: Learning to follow instructions via graphical user interfaces\", \"World of bits: An open-domain platform for web-based agents\", \"Reflexion: Language agents with verbal reinforcement learning\", \"Gemini: A family of highly capable multimodal models\", \"Self-instruct: Aligning language model with self generated instructions\", \"Simple visual language model pretraining with weak supervision\", \"Llm-powered autonomous agents\", \"Gpt-4v in wonderland: Large multimodal models for zero-shot smartphone gui navigation\", \"Set-of-mark prompting unleashes extraordinary visual grounding in gpt-4v\", \"The dawn of $1 \\mathrm{mms}$ : Preliminary explorations with gpt-4v (ision)\", \"Webshop: Towards scalable real-world web interaction with grounded language agents\", \"React: Synergizing reasoning and acting in language models\", \"Lumos: Learning agents with unified data, modular design, and open-source llms\", \"From recognition to cognition: Visual commonsense reasoning\", \"Appagent: Multimodal agents as smartphone users\", \"Gpt-4v (ision) is a generalist web agent, if grounded\", \"Webarena: A realistic web environment for building autonomous agents\", \"Object detection in 20 years: A survey\"], \"Citation\": []}",
        "eval_response": "Evaluation evidence: The assistant's response contains a significant number of hallucinations, listing numerous papers that are not mentioned in the gold answer. The only correct references from the gold answer are \"Seeclick: Harnessing gui grounding for advanced visual gui agents\" and \"Visualwebarena: Evaluating multimodal agents on realistic visual web tasks.\" The citations section is entirely missing, whereas the gold answer specifies two citing papers. This indicates issues with both accuracy and completeness.\n\nRating: [[20]]",
        "score": 20
    },
    {
        "level": 3,
        "set": 3,
        "length": 113316,
        "type": "paper",
        "language": "en",
        "question": "Scalable Pre-training of Large Autoregressive Image Models",
        "instruction": "We hope you will carefully study the provided papers and determine the citation relationships between them. Please follow the instructions below strictly to complete the task:\n\n#Specific Requirements:\n1. Reference: When a given paper mentions other provided papers, those other papers are considered as \"references\" for the given paper. To summarize in this specific context, references are about what the given paper is using.\n2. Citation: Conversely, when other provided papers mention the given paper in their works, the given paper is being \"cited\" by those other papers. To summarize in this specific context, citations are about who is using the given paper.\n3. Given a paper, you need to determine the citation or reference relationship between this paper and the other papers. Do not consider papers that are not provided.\n3. Please present the paper titles in a json format as follows: {{\"Reference\":[\"Reference Title 1\", \"Reference Title 2\", ..., \"Reference Title n\"], \"Citation\":[\"Citation Title 1\", \"Citation Title 2\", ..., \"Citation Title n\"]}}.\n4. If a paper does not have any references or citations, please leave the corresponding list empty, e.g.{{\"Refernce\":[]}}, {{\"Citation\":[]}}.",
        "prompt_template": "#Papers:\n{docs}\n\n{instruction}\n\n#The paper you need to analyze:\n{question}",
        "doc": [
            "2401.08541.md",
            "2405.15160.md",
            "2304.14108.md",
            "2309.17425.md"
        ],
        "answer": {
            "Reference": [
                "# DATACOMP: In search of the next generation of multimodal datasets ",
                "# Data Filtering Networks "
            ],
            "Citation": [
                "# ARVideo: Autoregressive Pretraining for Self-Supervised Video Representation Learning "
            ]
        },
        "shuffle_doc": true,
        "id": "4824819f-2886-48a5-a665-4b1283835cab",
        "prompt": "[Question]\n#Papers:\n{docs}\n\nWe hope you will carefully study the provided papers and determine the citation relationships between them. Please follow the instructions below strictly to complete the task:\n\n#Specific Requirements:\n1. Reference: When a given paper mentions other provided papers, those other papers are considered as \"references\" for the given paper. To summarize in this specific context, references are about what the given paper is using.\n2. Citation: Conversely, when other provided papers mention the given paper in their works, the given paper is being \"cited\" by those other papers. To summarize in this specific context, citations are about who is using the given paper.\n3. Given a paper, you need to determine the citation or reference relationship between this paper and the other papers. Do not consider papers that are not provided.\n3. Please present the paper titles in a json format as follows: {{\"Reference\":[\"Reference Title 1\", \"Reference Title 2\", ..., \"Reference Title n\"], \"Citation\":[\"Citation Title 1\", \"Citation Title 2\", ..., \"Citation Title n\"]}}.\n4. If a paper does not have any references or citations, please leave the corresponding list empty, e.g.{{\"Refernce\":[]}}, {{\"Citation\":[]}}.\n\n#The paper you need to analyze:\nScalable Pre-training of Large Autoregressive Image Models\n\n[Gold Answer]\n{'Reference': ['# DATACOMP: In search of the next generation of multimodal datasets ', '# Data Filtering Networks '], 'Citation': ['# ARVideo: Autoregressive Pretraining for Self-Supervised Video Representation Learning ']}\n\n[The Start of Assistant's Predicted Answer]\n{\"Reference\": [\"V-JEPA: Latent video prediction for visual representation learning\", \"Self-supervised learning from images with a joint-embedding predictive architecture\", \"Sequential modeling enables scalable learning for large vision models\", \"From detection of individual metastases to classification of lymph node status at the patient level: the camelyon17 challenge\", \"BEiT: Bert pretraining of image transformers\", \"Vicreg: Variance-invariance-covariance regularization for selfsupervised learning\", \"Cliquecnn: Deep unsupervised exemplar learning\", \"The iwildcam 2020 competition dataset\", \"A neural probabilistic language model\", \"Unsupervised learning by predicting noise\", \"Unsupervised pre-training of image features on non-curated data\", \"Unsupervised learning of visual representations by contrasting cluster assignments\", \"Emerging properties in self-supervised vision transformers\", \"Generative pretraining from pixels\", \"A simple framework for contrastive learning of visual representations\", \"An empirical study of training self-supervised vision transformers\", \"Are large-scale datasets necessary for self-supervised pre-training?\", \"Finding structure in time\", \"Data filtering networks\", \"Datacomp: In search of the next generation of multimodal datasets\", \"Unsupervised representation learning by predicting image rotations\", \"Generative adversarial nets\", \"Scaling and benchmarking self-supervised visual representation learning\", \"Vision models are more robust and fair when pretrained on uncurated images without supervision\", \"Bootstrap your own latent-a new approach to self-supervised learning\", \"Mask r-cnn\", \"Momentum contrast for unsupervised visual representation learning\", \"Masked autoencoders are scalable vision learners\", \"Training compute-optimal large language models\", \"Low-rank adaptation of large language models\", \"Deep networks with stochastic depth\", \"The neural autoregressive distribution estimator\", \"Set transformer: A framework for attention-based permutation-invariant neural networks\", \"Dinov2: Learning robust visual features without supervision\", \"Training language models to follow instructions with human feedback\", \"Exploring the limits of transfer learning with a unified text-to-text transformer\", \"Zero-shot text-to-image generation\", \"Pixelcnn++: Improving the pixelcnn with discretized logistic mixture likelihood and other modifications\", \"Prediction and entropy of printed english\", \"The effectiveness of mae pre-pretraining for billionscale pretraining\", \"Divide and contrast: Self-supervised learning from uncurated data\", \"Training data-efficient image transformers \\u2026 distillation through attention\", \"Going deeper with image transformers\", \"Llama: Open and efficient foundation language models\", \"Conditional image generation with pixelcnn decoders\", \"Pixel recurrent neural networks\", \"Neurips\", \"Attention is all you need\", \"Rotation equivariant cnns for digital pathology\", \"Stacked denoising autoencoders: Learning useful representations in a deep network with a local denoising criterion\", \"Diffusion models as masked autoencoders\", \"ClusterFit: Improving Generalization of Visual Representations\", \"Coca: Contrastive captioners are image-text foundation models\", \"Barlow twins: Self-supervised learning via redundancy reduction\", \"mixup: Beyond empirical risk minimization\", \"Colorful image colorization\", \"Image bert pre-training with online tokenizer\"], \"Citation\": []}\n[The End of Assistant's Predicted Answer]\n\n[System]\nWe would like to request your feedback on the performance of the AI assistant in response to the user question displayed above according to the gold answer. Please use the following listed aspects and their descriptions as evaluation criteria:\n    - Accuracy and Hallucinations: The assistant's answer is semantically consistent with the gold answer; The numerical value and order need to be accurate, and there should be no hallucinations.\n    - Completeness: Referring to the reference answers, the assistant's answer should contain all the key points needed to answer the user's question; further elaboration on these key points can be omitted.\nPlease rate whether this answer is suitable for the question. Please note that the gold answer can be considered as a correct answer to the question.\n\nThe assistant receives an overall score on a scale of 1 to 100, where a higher score indicates better overall performance.\nPlease note that if the assistant's answer and the gold answer fully meet the above criteria, its overall rating should be the full marks (100).\nPlease first provide a comprehensive explanation of your evaluation, avoiding any potential bias.\nThen, output a line indicating the score of the Assistant.\n\nPLEASE OUTPUT WITH THE FOLLOWING FORMAT, WHERE THE SCORE IS A SCALE OF 1 TO 100 BY STRICTLY FOLLOWING THIS FORMAT: \"[[score]]\", FOR EXAMPLE \"Rating: [[100]]\":\n<start output>\nEvaluation evidence: your evluation explanation here, no more than 100 words\nRating: [[score]]\n<end output>\n\nNow, start your evaluation:",
        "generate_response": "{\"Reference\": [\"V-JEPA: Latent video prediction for visual representation learning\", \"Self-supervised learning from images with a joint-embedding predictive architecture\", \"Sequential modeling enables scalable learning for large vision models\", \"From detection of individual metastases to classification of lymph node status at the patient level: the camelyon17 challenge\", \"BEiT: Bert pretraining of image transformers\", \"Vicreg: Variance-invariance-covariance regularization for selfsupervised learning\", \"Cliquecnn: Deep unsupervised exemplar learning\", \"The iwildcam 2020 competition dataset\", \"A neural probabilistic language model\", \"Unsupervised learning by predicting noise\", \"Unsupervised pre-training of image features on non-curated data\", \"Unsupervised learning of visual representations by contrasting cluster assignments\", \"Emerging properties in self-supervised vision transformers\", \"Generative pretraining from pixels\", \"A simple framework for contrastive learning of visual representations\", \"An empirical study of training self-supervised vision transformers\", \"Are large-scale datasets necessary for self-supervised pre-training?\", \"Finding structure in time\", \"Data filtering networks\", \"Datacomp: In search of the next generation of multimodal datasets\", \"Unsupervised representation learning by predicting image rotations\", \"Generative adversarial nets\", \"Scaling and benchmarking self-supervised visual representation learning\", \"Vision models are more robust and fair when pretrained on uncurated images without supervision\", \"Bootstrap your own latent-a new approach to self-supervised learning\", \"Mask r-cnn\", \"Momentum contrast for unsupervised visual representation learning\", \"Masked autoencoders are scalable vision learners\", \"Training compute-optimal large language models\", \"Low-rank adaptation of large language models\", \"Deep networks with stochastic depth\", \"The neural autoregressive distribution estimator\", \"Set transformer: A framework for attention-based permutation-invariant neural networks\", \"Dinov2: Learning robust visual features without supervision\", \"Training language models to follow instructions with human feedback\", \"Exploring the limits of transfer learning with a unified text-to-text transformer\", \"Zero-shot text-to-image generation\", \"Pixelcnn++: Improving the pixelcnn with discretized logistic mixture likelihood and other modifications\", \"Prediction and entropy of printed english\", \"The effectiveness of mae pre-pretraining for billionscale pretraining\", \"Divide and contrast: Self-supervised learning from uncurated data\", \"Training data-efficient image transformers \\u2026 distillation through attention\", \"Going deeper with image transformers\", \"Llama: Open and efficient foundation language models\", \"Conditional image generation with pixelcnn decoders\", \"Pixel recurrent neural networks\", \"Neurips\", \"Attention is all you need\", \"Rotation equivariant cnns for digital pathology\", \"Stacked denoising autoencoders: Learning useful representations in a deep network with a local denoising criterion\", \"Diffusion models as masked autoencoders\", \"ClusterFit: Improving Generalization of Visual Representations\", \"Coca: Contrastive captioners are image-text foundation models\", \"Barlow twins: Self-supervised learning via redundancy reduction\", \"mixup: Beyond empirical risk minimization\", \"Colorful image colorization\", \"Image bert pre-training with online tokenizer\"], \"Citation\": []}",
        "eval_response": "Evaluation evidence: The assistant's answer does not match the gold answer in terms of accuracy and completeness. The assistant listed numerous references that are not mentioned in the gold answer, indicating a significant hallucination issue. Additionally, the assistant failed to identify any citations, whereas the gold answer lists one citation. This discrepancy shows a lack of accuracy and completeness in the assistant's response.\n\nRating: [[10]]",
        "score": 10
    },
    {
        "level": 3,
        "set": 2,
        "length": 73799,
        "type": "paper",
        "language": "en",
        "question": "Self-Discover: Large Language Models Self-Compose Reasoning Structures",
        "instruction": "We hope you will carefully study the provided papers and determine the citation relationships between them. Please follow the instructions below strictly to complete the task:\n\n#Specific Requirements:\n1. Reference: When a given paper mentions other provided papers, those other papers are considered as \"references\" for the given paper. To summarize in this specific context, references are about what the given paper is using.\n2. Citation: Conversely, when other provided papers mention the given paper in their works, the given paper is being \"cited\" by those other papers. To summarize in this specific context, citations are about who is using the given paper.\n3. Given a paper, you need to determine the citation or reference relationship between this paper and the other papers. Do not consider papers that are not provided.\n3. Please present the paper titles in a json format as follows: {{\"Reference\":[\"Reference Title 1\", \"Reference Title 2\", ..., \"Reference Title n\"], \"Citation\":[\"Citation Title 1\", \"Citation Title 2\", ..., \"Citation Title n\"]}}.\n4. If a paper does not have any references or citations, please leave the corresponding list empty, e.g.{{\"Refernce\":[]}}, {{\"Citation\":[]}}.",
        "prompt_template": "#Papers:\n{docs}\n\n{instruction}\n\n#The paper you need to analyze:\n{question}",
        "doc": [
            "2402.03620.md",
            "2402.15205.md",
            "2403.03101.md",
            "2405.14804.md"
        ],
        "answer": {
            "Reference": [],
            "Citation": [
                "# Enhancing ICU Patient Recovery: Using LLMs to Assist Nurses in Diary Writing ",
                "# KnowAgent: Knowledge-Augmented Planning for LLM-Based Agents ",
                "# Can LLMs Solve Longer Math Word Problems Better? "
            ]
        },
        "shuffle_doc": true,
        "id": "0acccefc-0b0e-4758-adcb-dc5d18627ac3",
        "prompt": "[Question]\n#Papers:\n{docs}\n\nWe hope you will carefully study the provided papers and determine the citation relationships between them. Please follow the instructions below strictly to complete the task:\n\n#Specific Requirements:\n1. Reference: When a given paper mentions other provided papers, those other papers are considered as \"references\" for the given paper. To summarize in this specific context, references are about what the given paper is using.\n2. Citation: Conversely, when other provided papers mention the given paper in their works, the given paper is being \"cited\" by those other papers. To summarize in this specific context, citations are about who is using the given paper.\n3. Given a paper, you need to determine the citation or reference relationship between this paper and the other papers. Do not consider papers that are not provided.\n3. Please present the paper titles in a json format as follows: {{\"Reference\":[\"Reference Title 1\", \"Reference Title 2\", ..., \"Reference Title n\"], \"Citation\":[\"Citation Title 1\", \"Citation Title 2\", ..., \"Citation Title n\"]}}.\n4. If a paper does not have any references or citations, please leave the corresponding list empty, e.g.{{\"Refernce\":[]}}, {{\"Citation\":[]}}.\n\n#The paper you need to analyze:\nSelf-Discover: Large Language Models Self-Compose Reasoning Structures\n\n[Gold Answer]\n{'Reference': [], 'Citation': ['# Enhancing ICU Patient Recovery: Using LLMs to Assist Nurses in Diary Writing ', '# KnowAgent: Knowledge-Augmented Planning for LLM-Based Agents ', '# Can LLMs Solve Longer Math Word Problems Better? ']}\n\n[The Start of Assistant's Predicted Answer]\n{\"Reference\":[\"A Survey on Evaluation of Large Language Models\", \"Vicuna: An Open-Source Chatbot Impressing GPT-4 with $90 \\% *$ ChatGPT Quality\", \"ChatLaw: Open-Source Legal Large Language Model with Integrated External Knowledge Bases\", \"Compositional Semantic Parsing with Large Language Models\", \"Open LLM Leaderboard\", \"How Close is ChatGPT to Human Experts? Comparison Corpus, Evaluation, and Detection\", \"REALM: Retrieval-Augmented Language Model PreTraining\", \"Rethinking with Retrieval: Faithful Large Language Model Inference\", \"Measuring Massive Multitask Language Understanding\", \"Atlas: Few-shot Learning with Retrieval Augmented Language Models\", \"BIG-Bench Hard (BBH) (Suzgun et al., 2022)\", \"Thinking for Doing (T4D) (Zhou et al., 2023)\", \"MATH (Hendrycks et al., 2021)\", \"Chain-of-Thought prompting (CoT) (Nye et al., 2021; Wei et al., 2022)\", \"Leastto-most prompting (Zhou et al., 2022a; Drozdov et al., 2022)\", \"Decomposed prompting (Khot et al., 2022)\", \"Reframing (Mishra et al., 2022b)\", \"Help Me Think Prompting (Mishra & Nouri, 2023)\", \"Stepback Prompting (Zheng et al., 2023)\", \"Tree-of-Thought (ToT) (Yao et al., 2023a)\", \"Graph-of-Thought (Besta et al., 2023; Yao et al., 2023b)\", \"Branch-solve-merge (Saha et al., 2023)\", \"RAP (Hao et al., 2023)\", \"Palm 2 technical report\", \"Graph of thoughts: Solving elaborate problems with large language models\", \"Language models are few-shot learners\", \"Skills-in-context prompting: Unlocking compositionality in large language models\", \"Program of thoughts prompting: Disentangling computation from reasoning for numerical reasoning tasks\", \"Palm: Scaling language modeling with pathways\", \"Scaling instruction-finetuned language models\", \"Training verifiers to solve math word problems\", \"Compositional semantic parsing with large language models\", \"Promptbreeder: Self-referential self-improvement via prompt evolution\", \"StrategyLLM: Large language models as strategy generators, executors, optimizers, and evaluators for problem solving\", \"Reasoning with language model is planning with world model\", \"Measuring mathematical problem solving with the math dataset\", \"Decomposed prompting: A modular approach for solving complex tasks\", \"Large language models are zero-shot reasoners\", \"Less is more: Summary of long instructions is better for program synthesis\", \"Plan, verify and switch: Integrated reasoning with diverse x-of-thoughts\", \"HELP ME THINK: A simple prompting strategy for non-experts to create customized content with models\", \"LILA: A unified benchmark for mathematical reasoning\", \"Cross-task generalization via natural language crowdsourcing instructions\", \"Elements of a theory of human problem solving\", \"Show your work: Scratchpads for intermediate computation with language models\", \"ChatGPT: Optimizing language models for dialogue\", \"JSON generation mode\", \"GPT-4 technical report\", \"Training language models to follow instructions with human feedback\", \"Is a question decomposition unit all we need?\", \"How to solve it: A new aspect of mathematical method\", \"Skills, rules, and knowledge; signals, signs, and symbols, and other distinctions in human performance models\", \"Branch-solve-merge improves large language model evaluation and generation\", \"Beyond the imitation game: Quantifying and extrapolating the capabilities of language models\", \"Challenging big-bench tasks and whether chain-of-thought can solve them\", \"LLAMA 2: Open foundation and fine-tuned chat models\", \"Attention is all you need\", \"Plan-and-solve prompting: Improving zero-shot chain-of-thought reasoning by large language models\", \"Self-consistency improves chain of thought reasoning in large language models\", \"Fine-tuned language models are zero-shot learners\", \"Chain-of-thought prompting elicits reasoning in large language models\", \"Large language models as optimizers\", \"Tree of thoughts: Deliberate problem solving with large language models\", \"Beyond chain-of-thought, effective graph-of-thought reasoning in large language models\", \"Large language models as analogical reasoners\", \"Adapting language models for zero-shot learning by metatuning on dataset and prompt collections\", \"Least-to-most prompting enables complex reasoning in large language models\", \"How far are large language models from agents with theory-of-mind?\", \"Large language models are human-level prompt engineers\"], \"Citation\":[]}\n[The End of Assistant's Predicted Answer]\n\n[System]\nWe would like to request your feedback on the performance of the AI assistant in response to the user question displayed above according to the gold answer. Please use the following listed aspects and their descriptions as evaluation criteria:\n    - Accuracy and Hallucinations: The assistant's answer is semantically consistent with the gold answer; The numerical value and order need to be accurate, and there should be no hallucinations.\n    - Completeness: Referring to the reference answers, the assistant's answer should contain all the key points needed to answer the user's question; further elaboration on these key points can be omitted.\nPlease rate whether this answer is suitable for the question. Please note that the gold answer can be considered as a correct answer to the question.\n\nThe assistant receives an overall score on a scale of 1 to 100, where a higher score indicates better overall performance.\nPlease note that if the assistant's answer and the gold answer fully meet the above criteria, its overall rating should be the full marks (100).\nPlease first provide a comprehensive explanation of your evaluation, avoiding any potential bias.\nThen, output a line indicating the score of the Assistant.\n\nPLEASE OUTPUT WITH THE FOLLOWING FORMAT, WHERE THE SCORE IS A SCALE OF 1 TO 100 BY STRICTLY FOLLOWING THIS FORMAT: \"[[score]]\", FOR EXAMPLE \"Rating: [[100]]\":\n<start output>\nEvaluation evidence: your evluation explanation here, no more than 100 words\nRating: [[score]]\n<end output>\n\nNow, start your evaluation:",
        "generate_response": "{\"Reference\":[\"A Survey on Evaluation of Large Language Models\", \"Vicuna: An Open-Source Chatbot Impressing GPT-4 with $90 \\% *$ ChatGPT Quality\", \"ChatLaw: Open-Source Legal Large Language Model with Integrated External Knowledge Bases\", \"Compositional Semantic Parsing with Large Language Models\", \"Open LLM Leaderboard\", \"How Close is ChatGPT to Human Experts? Comparison Corpus, Evaluation, and Detection\", \"REALM: Retrieval-Augmented Language Model PreTraining\", \"Rethinking with Retrieval: Faithful Large Language Model Inference\", \"Measuring Massive Multitask Language Understanding\", \"Atlas: Few-shot Learning with Retrieval Augmented Language Models\", \"BIG-Bench Hard (BBH) (Suzgun et al., 2022)\", \"Thinking for Doing (T4D) (Zhou et al., 2023)\", \"MATH (Hendrycks et al., 2021)\", \"Chain-of-Thought prompting (CoT) (Nye et al., 2021; Wei et al., 2022)\", \"Leastto-most prompting (Zhou et al., 2022a; Drozdov et al., 2022)\", \"Decomposed prompting (Khot et al., 2022)\", \"Reframing (Mishra et al., 2022b)\", \"Help Me Think Prompting (Mishra & Nouri, 2023)\", \"Stepback Prompting (Zheng et al., 2023)\", \"Tree-of-Thought (ToT) (Yao et al., 2023a)\", \"Graph-of-Thought (Besta et al., 2023; Yao et al., 2023b)\", \"Branch-solve-merge (Saha et al., 2023)\", \"RAP (Hao et al., 2023)\", \"Palm 2 technical report\", \"Graph of thoughts: Solving elaborate problems with large language models\", \"Language models are few-shot learners\", \"Skills-in-context prompting: Unlocking compositionality in large language models\", \"Program of thoughts prompting: Disentangling computation from reasoning for numerical reasoning tasks\", \"Palm: Scaling language modeling with pathways\", \"Scaling instruction-finetuned language models\", \"Training verifiers to solve math word problems\", \"Compositional semantic parsing with large language models\", \"Promptbreeder: Self-referential self-improvement via prompt evolution\", \"StrategyLLM: Large language models as strategy generators, executors, optimizers, and evaluators for problem solving\", \"Reasoning with language model is planning with world model\", \"Measuring mathematical problem solving with the math dataset\", \"Decomposed prompting: A modular approach for solving complex tasks\", \"Large language models are zero-shot reasoners\", \"Less is more: Summary of long instructions is better for program synthesis\", \"Plan, verify and switch: Integrated reasoning with diverse x-of-thoughts\", \"HELP ME THINK: A simple prompting strategy for non-experts to create customized content with models\", \"LILA: A unified benchmark for mathematical reasoning\", \"Cross-task generalization via natural language crowdsourcing instructions\", \"Elements of a theory of human problem solving\", \"Show your work: Scratchpads for intermediate computation with language models\", \"ChatGPT: Optimizing language models for dialogue\", \"JSON generation mode\", \"GPT-4 technical report\", \"Training language models to follow instructions with human feedback\", \"Is a question decomposition unit all we need?\", \"How to solve it: A new aspect of mathematical method\", \"Skills, rules, and knowledge; signals, signs, and symbols, and other distinctions in human performance models\", \"Branch-solve-merge improves large language model evaluation and generation\", \"Beyond the imitation game: Quantifying and extrapolating the capabilities of language models\", \"Challenging big-bench tasks and whether chain-of-thought can solve them\", \"LLAMA 2: Open foundation and fine-tuned chat models\", \"Attention is all you need\", \"Plan-and-solve prompting: Improving zero-shot chain-of-thought reasoning by large language models\", \"Self-consistency improves chain of thought reasoning in large language models\", \"Fine-tuned language models are zero-shot learners\", \"Chain-of-thought prompting elicits reasoning in large language models\", \"Large language models as optimizers\", \"Tree of thoughts: Deliberate problem solving with large language models\", \"Beyond chain-of-thought, effective graph-of-thought reasoning in large language models\", \"Large language models as analogical reasoners\", \"Adapting language models for zero-shot learning by metatuning on dataset and prompt collections\", \"Least-to-most prompting enables complex reasoning in large language models\", \"How far are large language models from agents with theory-of-mind?\", \"Large language models are human-level prompt engineers\"], \"Citation\":[]}",
        "eval_response": "Evaluation evidence: The assistant's response does not align with the gold answer. The gold answer specifies that the paper \"Self-Discover: Large Language Models Self-Compose Reasoning Structures\" has no references but is cited by three specific papers. The assistant's response incorrectly lists numerous references that are not mentioned in the gold answer and fails to list any citations, directly contradicting the gold answer's information. This indicates issues with both accuracy and completeness.\n\nRating: [[10]]",
        "score": 10
    },
    {
        "level": 3,
        "set": 3,
        "length": 153947,
        "type": "paper",
        "language": "en",
        "question": "Eyes Wide Shut? Exploring the Visual Shortcomings of Multimodal LLMs",
        "instruction": "We hope you will carefully study the provided papers and determine the citation relationships between them. Please follow the instructions below strictly to complete the task:\n\n#Specific Requirements:\n1. Reference: When a given paper mentions other provided papers, those other papers are considered as \"references\" for the given paper. To summarize in this specific context, references are about what the given paper is using.\n2. Citation: Conversely, when other provided papers mention the given paper in their works, the given paper is being \"cited\" by those other papers. To summarize in this specific context, citations are about who is using the given paper.\n3. Given a paper, you need to determine the citation or reference relationship between this paper and the other papers. Do not consider papers that are not provided.\n3. Please present the paper titles in a json format as follows: {{\"Reference\":[\"Reference Title 1\", \"Reference Title 2\", ..., \"Reference Title n\"], \"Citation\":[\"Citation Title 1\", \"Citation Title 2\", ..., \"Citation Title n\"]}}.\n4. If a paper does not have any references or citations, please leave the corresponding list empty, e.g.{{\"Refernce\":[]}}, {{\"Citation\":[]}}.",
        "prompt_template": "#Papers:\n{docs}\n\n{instruction}\n\n#The paper you need to analyze:\n{question}",
        "doc": [
            "2401.06209.md",
            "2309.17421.md",
            "2310.03744.md"
        ],
        "answer": {
            "Reference": [
                "# The Dawn of LMMs: Preliminary Explorations with GPT-4V(ision) ",
                "# Improved Baselines with Visual Instruction Tuning "
            ],
            "Citation": []
        },
        "shuffle_doc": true,
        "id": "8449ae24-332f-485a-af72-de543bedb06d",
        "prompt": "[Question]\n#Papers:\n{docs}\n\nWe hope you will carefully study the provided papers and determine the citation relationships between them. Please follow the instructions below strictly to complete the task:\n\n#Specific Requirements:\n1. Reference: When a given paper mentions other provided papers, those other papers are considered as \"references\" for the given paper. To summarize in this specific context, references are about what the given paper is using.\n2. Citation: Conversely, when other provided papers mention the given paper in their works, the given paper is being \"cited\" by those other papers. To summarize in this specific context, citations are about who is using the given paper.\n3. Given a paper, you need to determine the citation or reference relationship between this paper and the other papers. Do not consider papers that are not provided.\n3. Please present the paper titles in a json format as follows: {{\"Reference\":[\"Reference Title 1\", \"Reference Title 2\", ..., \"Reference Title n\"], \"Citation\":[\"Citation Title 1\", \"Citation Title 2\", ..., \"Citation Title n\"]}}.\n4. If a paper does not have any references or citations, please leave the corresponding list empty, e.g.{{\"Refernce\":[]}}, {{\"Citation\":[]}}.\n\n#The paper you need to analyze:\nEyes Wide Shut? Exploring the Visual Shortcomings of Multimodal LLMs\n\n[Gold Answer]\n{'Reference': ['# The Dawn of LMMs: Preliminary Explorations with GPT-4V(ision) ', '# Improved Baselines with Visual Instruction Tuning '], 'Citation': []}\n\n[The Start of Assistant's Predicted Answer]\n{\"Reference\":[\"GPT-4V [41]\", \"GPT-4 [41]\", \"LLaVA-1.5 [30]\", \"InstructBLIP [8]\", \"Mini-GPT4 [71]\", \"Gemini [14]\", \"Bard [13]\", \"OpenAI ViT-L-14 [43]\", \"SigL\", \"EVA02 ViT-bigE-14+ [54]\", \"CLIP-ViT-L-14 [43]\", \"DINOV2-ViT-L-14 [42]\", \"DINOv2 [42]\", \"MAE\", \"MoCoV3\", \"SLIP [39]\", \"LLaVA [30, 31]\", \"Thrush et al. [56]\", \"Yuksekgonul et al. [65]\", \"Hsieh et al. [19]\", \"Tschannen et al. [60]\", \"Tong et al. [57]\", \"ImageNet [45, 47]\", \"LAIONAesthetic [48]\", \"TextVQA [52]\", \"VQAv2 [15]\", \"GQA [21]\", \"MM-Vet [64]\", \"POPE [27]\", \"MM-Bench [32]\"], \"Citation\":[\"[41]\", \"[57]\", \"[65]\", \"[43]\", \"[42]\", \"[54]\", \"[62]\", \"[66]\", \"[10]\", \"[47]\", \"[48]\"]}\n[The End of Assistant's Predicted Answer]\n\n[System]\nWe would like to request your feedback on the performance of the AI assistant in response to the user question displayed above according to the gold answer. Please use the following listed aspects and their descriptions as evaluation criteria:\n    - Accuracy and Hallucinations: The assistant's answer is semantically consistent with the gold answer; The numerical value and order need to be accurate, and there should be no hallucinations.\n    - Completeness: Referring to the reference answers, the assistant's answer should contain all the key points needed to answer the user's question; further elaboration on these key points can be omitted.\nPlease rate whether this answer is suitable for the question. Please note that the gold answer can be considered as a correct answer to the question.\n\nThe assistant receives an overall score on a scale of 1 to 100, where a higher score indicates better overall performance.\nPlease note that if the assistant's answer and the gold answer fully meet the above criteria, its overall rating should be the full marks (100).\nPlease first provide a comprehensive explanation of your evaluation, avoiding any potential bias.\nThen, output a line indicating the score of the Assistant.\n\nPLEASE OUTPUT WITH THE FOLLOWING FORMAT, WHERE THE SCORE IS A SCALE OF 1 TO 100 BY STRICTLY FOLLOWING THIS FORMAT: \"[[score]]\", FOR EXAMPLE \"Rating: [[100]]\":\n<start output>\nEvaluation evidence: your evluation explanation here, no more than 100 words\nRating: [[score]]\n<end output>\n\nNow, start your evaluation:",
        "generate_response": "{\"Reference\":[\"GPT-4V [41]\", \"GPT-4 [41]\", \"LLaVA-1.5 [30]\", \"InstructBLIP [8]\", \"Mini-GPT4 [71]\", \"Gemini [14]\", \"Bard [13]\", \"OpenAI ViT-L-14 [43]\", \"SigL\", \"EVA02 ViT-bigE-14+ [54]\", \"CLIP-ViT-L-14 [43]\", \"DINOV2-ViT-L-14 [42]\", \"DINOv2 [42]\", \"MAE\", \"MoCoV3\", \"SLIP [39]\", \"LLaVA [30, 31]\", \"Thrush et al. [56]\", \"Yuksekgonul et al. [65]\", \"Hsieh et al. [19]\", \"Tschannen et al. [60]\", \"Tong et al. [57]\", \"ImageNet [45, 47]\", \"LAIONAesthetic [48]\", \"TextVQA [52]\", \"VQAv2 [15]\", \"GQA [21]\", \"MM-Vet [64]\", \"POPE [27]\", \"MM-Bench [32]\"], \"Citation\":[\"[41]\", \"[57]\", \"[65]\", \"[43]\", \"[42]\", \"[54]\", \"[62]\", \"[66]\", \"[10]\", \"[47]\", \"[48]\"]}",
        "eval_response": "Evaluation evidence: The assistant's answer does not align with the gold answer in terms of accuracy and completeness. The gold answer lists specific paper titles as references, while the assistant's answer includes a mix of paper titles and citation formats, which introduces inaccuracies and hallucinations. Additionally, the assistant's answer includes a citation list, which is not present in the gold answer, indicating a misunderstanding of the task requirements. The response fails to accurately reflect the relationships as specified in the gold answer.\n\nRating: [[20]]",
        "score": 20
    },
    {
        "level": 3,
        "set": 2,
        "length": 76116,
        "type": "paper",
        "language": "en",
        "question": "EasyTool: Enhancing LLM-based Agents with Concise Tool Instruction",
        "instruction": "We hope you will carefully study the provided papers and determine the citation relationships between them. Please follow the instructions below strictly to complete the task:\n\n#Specific Requirements:\n1. Reference: When a given paper mentions other provided papers, those other papers are considered as \"references\" for the given paper. To summarize in this specific context, references are about what the given paper is using.\n2. Citation: Conversely, when other provided papers mention the given paper in their works, the given paper is being \"cited\" by those other papers. To summarize in this specific context, citations are about who is using the given paper.\n3. Given a paper, you need to determine the citation or reference relationship between this paper and the other papers. Do not consider papers that are not provided.\n3. Please present the paper titles in a json format as follows: {{\"Reference\":[\"Reference Title 1\", \"Reference Title 2\", ..., \"Reference Title n\"], \"Citation\":[\"Citation Title 1\", \"Citation Title 2\", ..., \"Citation Title n\"]}}.\n4. If a paper does not have any references or citations, please leave the corresponding list empty, e.g.{{\"Refernce\":[]}}, {{\"Citation\":[]}}.",
        "prompt_template": "#Papers:\n{docs}\n\n{instruction}\n\n#The paper you need to analyze:\n{question}",
        "doc": [
            "2401.06201.md",
            "2405.05955.md",
            "2405.18208.md",
            "2306.06624.md",
            "2310.07075.md"
        ],
        "answer": {
            "Reference": [
                "# RestGPT: Connecting Large Language Models with Real-World RESTful APIs ",
                "# Don't Fine-Tune, Decode: Syntax Error-Free Tool Use via Constrained Decoding "
            ],
            "Citation": [
                "# Smurfs: Leveraging Multiple Proficiency Agents with Context-Efficiency for Tool Planning ",
                "# A Human-Like Reasoning Framework for Multi-Phases Planning Task with Large Language Models "
            ]
        },
        "shuffle_doc": true,
        "id": "0425cdc9-1f78-458e-975b-c90c0346cf31",
        "prompt": "[Question]\n#Papers:\n{docs}\n\nWe hope you will carefully study the provided papers and determine the citation relationships between them. Please follow the instructions below strictly to complete the task:\n\n#Specific Requirements:\n1. Reference: When a given paper mentions other provided papers, those other papers are considered as \"references\" for the given paper. To summarize in this specific context, references are about what the given paper is using.\n2. Citation: Conversely, when other provided papers mention the given paper in their works, the given paper is being \"cited\" by those other papers. To summarize in this specific context, citations are about who is using the given paper.\n3. Given a paper, you need to determine the citation or reference relationship between this paper and the other papers. Do not consider papers that are not provided.\n3. Please present the paper titles in a json format as follows: {{\"Reference\":[\"Reference Title 1\", \"Reference Title 2\", ..., \"Reference Title n\"], \"Citation\":[\"Citation Title 1\", \"Citation Title 2\", ..., \"Citation Title n\"]}}.\n4. If a paper does not have any references or citations, please leave the corresponding list empty, e.g.{{\"Refernce\":[]}}, {{\"Citation\":[]}}.\n\n#The paper you need to analyze:\nEasyTool: Enhancing LLM-based Agents with Concise Tool Instruction\n\n[Gold Answer]\n{'Reference': ['# RestGPT: Connecting Large Language Models with Real-World RESTful APIs ', \"# Don't Fine-Tune, Decode: Syntax Error-Free Tool Use via Constrained Decoding \"], 'Citation': ['# Smurfs: Leveraging Multiple Proficiency Agents with Context-Efficiency for Tool Planning ', '# A Human-Like Reasoning Framework for Multi-Phases Planning Task with Large Language Models ']}\n\n[The Start of Assistant's Predicted Answer]\n{\"Reference\":[\"A Survey on Evaluation of Large Language Models\", \"Vicuna: An Open-Source Chatbot Impressing GPT-4 with $90 \\% *$ ChatGPT Quality\", \"Compositional Semantic Parsing with Large Language Models\", \"Training language models to follow instructions with human feedback\", \"REALM: Retrieval-Augmented Language Model PreTraining\", \"Rethinking with Retrieval: Faithful Large Language Model Inference\", \"Measuring Massive Multitask Language Understanding\", \"Atlas: Few-shot Learning with Retrieval Augmented Language Models\", \"Tool utilization in LLM-based agents: Inconsistency, Redundancy, Incompleteness\", \"Previous tool utilization methods and their limitations\", \"EASYTOOL: Tool Description Generation and Tool Functionality Guidelines Construction\", \"Evaluation of EASYTOOL-generated tool descriptions\", \"ToolBench dataset for real-world question answering evaluation\", \"RestBench dataset for real-world web services evaluation\", \"FuncQA dataset for numerical reasoning evaluation\", \"BERT Retriever\", \"GPT Ada\", \"RestGPT\", \"Vicuna-13B-based RestGPT\", \"Vicuna-30B\", \"ChatGPT\", \"Mistral-Instruct-7B\", \"ToolDec\", \"ReAct\", \"Chain-of-thought (CoT) prompting\", \"Error analysis of tool calls in different LLMs\", \"EASYTool's effectiveness across different real-world applications\", \"Limitations of EASYTOOL\", \"Ethics Statement regarding human annotations and tool benchmarks\", \"Tool documentation and tool instruction generation for LLMs using EASYTool\", \"Improvement in retrieval performance with EASYTool-generated tool descriptions\", \"Enhancement of LLMs' tool utilization ability on complex math problems with EASYTool\", \"TALM: Tool Augmented Language Models\", \"Gorilla: Large Language Model connected with massive APIs\", \"Toolformer: Language models can teach themselves to use tools\", \"ToolLLM: Facilitating large language models to master 16000+ real-world APIs\", \"ToolAlpaca: Generalized tool learning for language models with 3000 simulated cases\", \"Llama: Open and efficient foundation language models\", \"Llama 2: Open foundation and fine-tuned chat models\", \"On the tool manipulation capability of open-source large language models\", \"React: Synergizing reasoning and acting in language models\", \"Syntax error-free and generalizable tool use for LLMS via finite-state decoding\", \"Toolchain*: Efficient action space navigation in large language models with $a^{*}$ search\"], \"Citation\":[\"Smurfs: Leveraging Multiple Proficiency Agents with Context-Efficiency for Tool Planning\"]}\n[The End of Assistant's Predicted Answer]\n\n[System]\nWe would like to request your feedback on the performance of the AI assistant in response to the user question displayed above according to the gold answer. Please use the following listed aspects and their descriptions as evaluation criteria:\n    - Accuracy and Hallucinations: The assistant's answer is semantically consistent with the gold answer; The numerical value and order need to be accurate, and there should be no hallucinations.\n    - Completeness: Referring to the reference answers, the assistant's answer should contain all the key points needed to answer the user's question; further elaboration on these key points can be omitted.\nPlease rate whether this answer is suitable for the question. Please note that the gold answer can be considered as a correct answer to the question.\n\nThe assistant receives an overall score on a scale of 1 to 100, where a higher score indicates better overall performance.\nPlease note that if the assistant's answer and the gold answer fully meet the above criteria, its overall rating should be the full marks (100).\nPlease first provide a comprehensive explanation of your evaluation, avoiding any potential bias.\nThen, output a line indicating the score of the Assistant.\n\nPLEASE OUTPUT WITH THE FOLLOWING FORMAT, WHERE THE SCORE IS A SCALE OF 1 TO 100 BY STRICTLY FOLLOWING THIS FORMAT: \"[[score]]\", FOR EXAMPLE \"Rating: [[100]]\":\n<start output>\nEvaluation evidence: your evluation explanation here, no more than 100 words\nRating: [[score]]\n<end output>\n\nNow, start your evaluation:",
        "generate_response": "{\"Reference\":[\"A Survey on Evaluation of Large Language Models\", \"Vicuna: An Open-Source Chatbot Impressing GPT-4 with $90 \\% *$ ChatGPT Quality\", \"Compositional Semantic Parsing with Large Language Models\", \"Training language models to follow instructions with human feedback\", \"REALM: Retrieval-Augmented Language Model PreTraining\", \"Rethinking with Retrieval: Faithful Large Language Model Inference\", \"Measuring Massive Multitask Language Understanding\", \"Atlas: Few-shot Learning with Retrieval Augmented Language Models\", \"Tool utilization in LLM-based agents: Inconsistency, Redundancy, Incompleteness\", \"Previous tool utilization methods and their limitations\", \"EASYTOOL: Tool Description Generation and Tool Functionality Guidelines Construction\", \"Evaluation of EASYTOOL-generated tool descriptions\", \"ToolBench dataset for real-world question answering evaluation\", \"RestBench dataset for real-world web services evaluation\", \"FuncQA dataset for numerical reasoning evaluation\", \"BERT Retriever\", \"GPT Ada\", \"RestGPT\", \"Vicuna-13B-based RestGPT\", \"Vicuna-30B\", \"ChatGPT\", \"Mistral-Instruct-7B\", \"ToolDec\", \"ReAct\", \"Chain-of-thought (CoT) prompting\", \"Error analysis of tool calls in different LLMs\", \"EASYTool's effectiveness across different real-world applications\", \"Limitations of EASYTOOL\", \"Ethics Statement regarding human annotations and tool benchmarks\", \"Tool documentation and tool instruction generation for LLMs using EASYTool\", \"Improvement in retrieval performance with EASYTool-generated tool descriptions\", \"Enhancement of LLMs' tool utilization ability on complex math problems with EASYTool\", \"TALM: Tool Augmented Language Models\", \"Gorilla: Large Language Model connected with massive APIs\", \"Toolformer: Language models can teach themselves to use tools\", \"ToolLLM: Facilitating large language models to master 16000+ real-world APIs\", \"ToolAlpaca: Generalized tool learning for language models with 3000 simulated cases\", \"Llama: Open and efficient foundation language models\", \"Llama 2: Open foundation and fine-tuned chat models\", \"On the tool manipulation capability of open-source large language models\", \"React: Synergizing reasoning and acting in language models\", \"Syntax error-free and generalizable tool use for LLMS via finite-state decoding\", \"Toolchain*: Efficient action space navigation in large language models with $a^{*}$ search\"], \"Citation\":[\"Smurfs: Leveraging Multiple Proficiency Agents with Context-Efficiency for Tool Planning\"]}",
        "eval_response": "Evaluation evidence: The assistant's answer does not match the gold answer in terms of accuracy and completeness. The references and citations listed are largely incorrect or hallucinated, not aligning with the provided gold answer. The gold answer specifies two references and two citations, which are not accurately reflected in the assistant's response. The assistant's answer includes a long list of unrelated titles, indicating a significant deviation from the required task.\n\nRating: [[10]]",
        "score": 10
    },
    {
        "level": 4,
        "set": 2,
        "length": 66699,
        "type": "paper",
        "language": "en",
        "question": "",
        "instruction": "We kindly ask you to thoroughly review the provided papers and construct a citation chain from them. Please adhere to the following instructions strictly while completing the task:\n\n#Task Instructions:\nGiven several papers, you are required to identify and list the longest citation chain, which demonstrates the citation relationship among the provided papers.\n\n#Specific Requirements:\n1.Please present the titles of the papers in the form of a list, as follows: [\"Title of Paper 1\", \"Title of Paper 2\", ..., \"Title of Paper n\"].\n2.Ensure that the citation chain in the list is linear and continuous, meaning that the first paper title in the list (Paper 1) should not cite any other works. Instead, it should be cited by the next paper in the list (Paper 2); subsequently, each paper should then be cited by the next one in the list, continuing up to the last paper (Paper n).\n3.Consider only the citation relationships within the supplied collection of papers, and ensure that the citation chain accurately reflects the sequential citation order among these documents.\n4.Do not take into account any articles not provided, and disregard other non-linear citation relationships.",
        "prompt_template": "{instruction}\n\n#Papers Provided:\n{docs}",
        "doc": [
            "2312.07395.md",
            "2312.17235.md",
            "2402.05861.md"
        ],
        "answer": [
            "# A Simple Recipe for Contrastively Pre-training Video-First Encoders Beyond 16 Frames ",
            "# A Simple LLM Framework for Long-Range Video Question-Answering ",
            "# Memory Consolidation Enables Long-Context Video Understanding "
        ],
        "shuffle_doc": true,
        "id": "51c625a5-a791-483a-a690-16b959115841",
        "prompt": "[Question]\nWe kindly ask you to thoroughly review the provided papers and construct a citation chain from them. Please adhere to the following instructions strictly while completing the task:\n\n#Task Instructions:\nGiven several papers, you are required to identify and list the longest citation chain, which demonstrates the citation relationship among the provided papers.\n\n#Specific Requirements:\n1.Please present the titles of the papers in the form of a list, as follows: [\"Title of Paper 1\", \"Title of Paper 2\", ..., \"Title of Paper n\"].\n2.Ensure that the citation chain in the list is linear and continuous, meaning that the first paper title in the list (Paper 1) should not cite any other works. Instead, it should be cited by the next paper in the list (Paper 2); subsequently, each paper should then be cited by the next one in the list, continuing up to the last paper (Paper n).\n3.Consider only the citation relationships within the supplied collection of papers, and ensure that the citation chain accurately reflects the sequential citation order among these documents.\n4.Do not take into account any articles not provided, and disregard other non-linear citation relationships.\n\n#Papers Provided:\n{docs}\n\n[Gold Answer]\n['# A Simple Recipe for Contrastively Pre-training Video-First Encoders Beyond 16 Frames ', '# A Simple LLM Framework for Long-Range Video Question-Answering ', '# Memory Consolidation Enables Long-Context Video Understanding ']\n\n[The Start of Assistant's Predicted Answer]\n[\"A Simple LLM Framework for Long-Range Video Question-Answering\", \"Aanisha Bhattacharya, Yaman K Singla, Balaji Krishnamurthy, Rajiv Ratn Shah, and Changyou Chen. 2023. A video is worth 4096 tokens: Verbalize story videos to understand them in zero shot\", \"Takeshi Kojima, Shixiang Shane Gu, Machel Reid, Yutaka Matsuo, and Yusuke Iwasawa. 2022. Large language models are zero-shot reasoners\", \"Diederik P. Kingma and Jimmy Ba. 2014. Adam: A method for stochastic optimization\", \"Zhenhailong Wang, Manling Li, Ruochen Xu, Luowei Zhou, Jie Lei, Xudong Lin, Shuohang Wang, Ziyi Yang, Chenguang Zhu, Derek Hoiem, et al. 2022b. Language models with image descriptors are strong few-shot video-language learners\", \"Chao-Yuan Wu, Yanghao Li, Karttikeya Mangalam, Haoqi Fan, Bo Xiong, Jitendra Malik, and Christoph Feichtenhofer. 2022. Memvit: Memory-augmented multiscale vision transformer for efficient long-term video recognition\", \"Kevin Qinghong Lin, Alex Jinpeng Wang, Mattia Soldan, Michael Wray, Rui Yan, Eric Zhongcong Xu, Difei Gao, Rongbin Ouyang, Fei Wu, Tianwei Zhang, Jiwei Li, and Guoyin Wang. 2023c. BLIP-2: bootstrapping language-image pretraining with frozen image encoders and large language models\", \"Jiapeng Li, Ping Wei, Wenjuan Han, and Lifeng Fan. 2023b. Intentqa: Context-aware video intent reasoning\", \"Junnan Li, Dongxu Li, Silvio Savarese, and Steven Hoi. 2023c. BLIP-2: bootstrapping language-image pretraining with frozen image encoders and large language models\", \"Jiajun Shen, Antoine Miech, Andrew Zisserman, and Aida Nematzdeh. 2023. A simple recipe for contrastively pre-training video-first encoders beyond 16 frames\", \"Qinghao Ye, Haiyang Xu, Guohai Xu, Jiabo Ye, Ming Yan, Yiyang Zhou, Junyang Wang, Anwen Hu, Pengcheng Shi, Yaya Shi, et al. 2023. mplug-owl: Modularization empowers large language models with multimodality\", \"Shoubin Yu, Jaemin Cho, Prateek Yadav, and Mohit Bansal. 2023. Self-chained image-language model for video localization and question answering\", \"Junbin Xiao, Angela Yao, Zhiyuan Liu, Yicong Li, Wei Ji, and Tat-Seng Chua. 2022a. Video as conditional graph hierarchy for multi-granular question answering\", \"Junbin Xiao, Angela Yao, Yicong Li, and Tat Seng Chua. 2022b. Video graph transformer for video question answering\", \"Antoine Yang, Antoine Miech, Josef Sivic, Ivan Laptev, and Cordelia Schmid. 2021. Just ask: Learning to answer questions from millions of narrated videos\", \"Antoine Yang, Antoine Miech, Josef Sivic, Ivan Laptev, and Cordelia Schmid. 2022a. Zero-shot video question answering via frozen bidirectional language models\", \"Antoine Yang, Antoine Miech, Josef Sivic, Ivan Laptev, and Cordelia Schmid. 2022b. Zero-shot video question answering via frozen bidirectional language models\", \"Jianing Yang, Yuying Zhu, Yongxin Wang, Ruitao Yi, Amir Zadeh, and Louis-Philippe Morency. 2020 What gives the answer away? question answering bias analysis on video qa datasets\", \"Xitong Yang, Fu-Jen Chu, Matt Feiszli, Raghav Goyal, Lorenzo Torresani, and Du Tran. 2023. Relational space-time query in long-form videos\", \"Shunyu Yao, Jeffrey Zhao, Dian Yu, Nan Du, Izhak Shafran, Karthik Narasimhan, and Yuan Cao. 2022. React: Synergizing reasoning and acting in language models\", \"Yue Zhao, Ishan Misra, Philipp Krhenbhl, and Rohit Girdhar. 2023. Learning video representations from large language models\", \"Denny Zhou, Nathanael Schrli, Le Hou, Jason Wei, Nathan Scales, Xuezhi Wang, Dale Schuurmans, Claire Cui, Olivier Bousquet, Quoc Le, et al. 2023 Least-to-most prompting enables complex reasoning in large language models\", \"Yongchao Zhou, Andrei Ioan Muresanu, Ziwen Han, Keiran Paster, Silviu Pitis, Harris Chan, and Jimmy Ba. 2022. Large language models are human-level prompt engineers\", \"Andy Zeng, Maria Attarian, Brian Ichter, Krzysztof Choromanski, Adrian Wong, Stefan Welker, Federico Tombari, Aveek Purohit, Michael Ryoo, Vikas Sindhwani, Johnny Lee, Vincent Vanhoucke, and Pete Florence. 2022. Socratic models: Composing zeroshot multimodal reasoning with language\", \"Chuhan Zhang, Ankush Gupta, and Andrew Zisserman. 2021. Temporal query networks for fine-grained video understanding\", \"Zhuosheng Zhang, Aston Zhang, Mu Li, and Alex Smola. 2022. Automatic chain of thought prompting in large language models\", \"Zhou Yu, Dejing Xu, Jun Yu, Ting Yu, Zhou Zhao, Yueting Zhuang, and Dacheng Tao. 2019. Activitynet-qa: A dataset for understanding complex web videos via question answering\"]\n[The End of Assistant's Predicted Answer]\n\n[System]\nWe would like to request your feedback on the performance of the AI assistant in response to the user question displayed above according to the gold answer. Please use the following listed aspects and their descriptions as evaluation criteria:\n    - Accuracy and Hallucinations: The assistant's answer is semantically consistent with the gold answer; The numerical value and order need to be accurate, and there should be no hallucinations.\n    - Completeness: Referring to the reference answers, the assistant's answer should contain all the key points needed to answer the user's question; further elaboration on these key points can be omitted.\nPlease rate whether this answer is suitable for the question. Please note that the gold answer can be considered as a correct answer to the question.\n\nThe assistant receives an overall score on a scale of 1 to 100, where a higher score indicates better overall performance.\nPlease note that if the assistant's answer and the gold answer fully meet the above criteria, its overall rating should be the full marks (100).\nPlease first provide a comprehensive explanation of your evaluation, avoiding any potential bias.\nThen, output a line indicating the score of the Assistant.\n\nPLEASE OUTPUT WITH THE FOLLOWING FORMAT, WHERE THE SCORE IS A SCALE OF 1 TO 100 BY STRICTLY FOLLOWING THIS FORMAT: \"[[score]]\", FOR EXAMPLE \"Rating: [[100]]\":\n<start output>\nEvaluation evidence: your evluation explanation here, no more than 100 words\nRating: [[score]]\n<end output>\n\nNow, start your evaluation:",
        "generate_response": "[\"A Simple LLM Framework for Long-Range Video Question-Answering\", \"Aanisha Bhattacharya, Yaman K Singla, Balaji Krishnamurthy, Rajiv Ratn Shah, and Changyou Chen. 2023. A video is worth 4096 tokens: Verbalize story videos to understand them in zero shot\", \"Takeshi Kojima, Shixiang Shane Gu, Machel Reid, Yutaka Matsuo, and Yusuke Iwasawa. 2022. Large language models are zero-shot reasoners\", \"Diederik P. Kingma and Jimmy Ba. 2014. Adam: A method for stochastic optimization\", \"Zhenhailong Wang, Manling Li, Ruochen Xu, Luowei Zhou, Jie Lei, Xudong Lin, Shuohang Wang, Ziyi Yang, Chenguang Zhu, Derek Hoiem, et al. 2022b. Language models with image descriptors are strong few-shot video-language learners\", \"Chao-Yuan Wu, Yanghao Li, Karttikeya Mangalam, Haoqi Fan, Bo Xiong, Jitendra Malik, and Christoph Feichtenhofer. 2022. Memvit: Memory-augmented multiscale vision transformer for efficient long-term video recognition\", \"Kevin Qinghong Lin, Alex Jinpeng Wang, Mattia Soldan, Michael Wray, Rui Yan, Eric Zhongcong Xu, Difei Gao, Rongbin Ouyang, Fei Wu, Tianwei Zhang, Jiwei Li, and Guoyin Wang. 2023c. BLIP-2: bootstrapping language-image pretraining with frozen image encoders and large language models\", \"Jiapeng Li, Ping Wei, Wenjuan Han, and Lifeng Fan. 2023b. Intentqa: Context-aware video intent reasoning\", \"Junnan Li, Dongxu Li, Silvio Savarese, and Steven Hoi. 2023c. BLIP-2: bootstrapping language-image pretraining with frozen image encoders and large language models\", \"Jiajun Shen, Antoine Miech, Andrew Zisserman, and Aida Nematzdeh. 2023. A simple recipe for contrastively pre-training video-first encoders beyond 16 frames\", \"Qinghao Ye, Haiyang Xu, Guohai Xu, Jiabo Ye, Ming Yan, Yiyang Zhou, Junyang Wang, Anwen Hu, Pengcheng Shi, Yaya Shi, et al. 2023. mplug-owl: Modularization empowers large language models with multimodality\", \"Shoubin Yu, Jaemin Cho, Prateek Yadav, and Mohit Bansal. 2023. Self-chained image-language model for video localization and question answering\", \"Junbin Xiao, Angela Yao, Zhiyuan Liu, Yicong Li, Wei Ji, and Tat-Seng Chua. 2022a. Video as conditional graph hierarchy for multi-granular question answering\", \"Junbin Xiao, Angela Yao, Yicong Li, and Tat Seng Chua. 2022b. Video graph transformer for video question answering\", \"Antoine Yang, Antoine Miech, Josef Sivic, Ivan Laptev, and Cordelia Schmid. 2021. Just ask: Learning to answer questions from millions of narrated videos\", \"Antoine Yang, Antoine Miech, Josef Sivic, Ivan Laptev, and Cordelia Schmid. 2022a. Zero-shot video question answering via frozen bidirectional language models\", \"Antoine Yang, Antoine Miech, Josef Sivic, Ivan Laptev, and Cordelia Schmid. 2022b. Zero-shot video question answering via frozen bidirectional language models\", \"Jianing Yang, Yuying Zhu, Yongxin Wang, Ruitao Yi, Amir Zadeh, and Louis-Philippe Morency. 2020 What gives the answer away? question answering bias analysis on video qa datasets\", \"Xitong Yang, Fu-Jen Chu, Matt Feiszli, Raghav Goyal, Lorenzo Torresani, and Du Tran. 2023. Relational space-time query in long-form videos\", \"Shunyu Yao, Jeffrey Zhao, Dian Yu, Nan Du, Izhak Shafran, Karthik Narasimhan, and Yuan Cao. 2022. React: Synergizing reasoning and acting in language models\", \"Yue Zhao, Ishan Misra, Philipp Krhenbhl, and Rohit Girdhar. 2023. Learning video representations from large language models\", \"Denny Zhou, Nathanael Schrli, Le Hou, Jason Wei, Nathan Scales, Xuezhi Wang, Dale Schuurmans, Claire Cui, Olivier Bousquet, Quoc Le, et al. 2023 Least-to-most prompting enables complex reasoning in large language models\", \"Yongchao Zhou, Andrei Ioan Muresanu, Ziwen Han, Keiran Paster, Silviu Pitis, Harris Chan, and Jimmy Ba. 2022. Large language models are human-level prompt engineers\", \"Andy Zeng, Maria Attarian, Brian Ichter, Krzysztof Choromanski, Adrian Wong, Stefan Welker, Federico Tombari, Aveek Purohit, Michael Ryoo, Vikas Sindhwani, Johnny Lee, Vincent Vanhoucke, and Pete Florence. 2022. Socratic models: Composing zeroshot multimodal reasoning with language\", \"Chuhan Zhang, Ankush Gupta, and Andrew Zisserman. 2021. Temporal query networks for fine-grained video understanding\", \"Zhuosheng Zhang, Aston Zhang, Mu Li, and Alex Smola. 2022. Automatic chain of thought prompting in large language models\", \"Zhou Yu, Dejing Xu, Jun Yu, Ting Yu, Zhou Zhao, Yueting Zhuang, and Dacheng Tao. 2019. Activitynet-qa: A dataset for understanding complex web videos via question answering\"]",
        "eval_response": "Evaluation evidence: The assistant's response does not align with the gold answer or the task instructions. The gold answer provided a specific citation chain, while the assistant's response listed a series of paper titles without establishing a clear, linear citation relationship among them. The response included numerous titles not relevant to constructing a citation chain, indicating a lack of accuracy and completeness in addressing the task requirements.\n\nRating: [[20]]",
        "score": 20
    },
    {
        "level": 3,
        "set": 1,
        "length": 48100,
        "type": "paper",
        "language": "en",
        "question": "LLMs Can't Plan, But Can Help Planning in LLM-Modulo Frameworks",
        "instruction": "We hope you will carefully study the provided papers and determine the citation relationships between them. Please follow the instructions below strictly to complete the task:\n\n#Specific Requirements:\n1. Reference: When a given paper mentions other provided papers, those other papers are considered as \"references\" for the given paper. To summarize in this specific context, references are about what the given paper is using.\n2. Citation: Conversely, when other provided papers mention the given paper in their works, the given paper is being \"cited\" by those other papers. To summarize in this specific context, citations are about who is using the given paper.\n3. Given a paper, you need to determine the citation or reference relationship between this paper and the other papers. Do not consider papers that are not provided.\n3. Please present the paper titles in a json format as follows: {{\"Reference\":[\"Reference Title 1\", \"Reference Title 2\", ..., \"Reference Title n\"], \"Citation\":[\"Citation Title 1\", \"Citation Title 2\", ..., \"Citation Title n\"]}}.\n4. If a paper does not have any references or citations, please leave the corresponding list empty, e.g.{{\"Refernce\":[]}}, {{\"Citation\":[]}}.",
        "prompt_template": "#Papers:\n{docs}\n\n{instruction}\n\n#The paper you need to analyze:\n{question}",
        "doc": [
            "2402.01817.md",
            "2405.13966.md",
            "2405.16661.md",
            "2310.08118.md"
        ],
        "answer": {
            "Reference": [
                "# Can Large Language Models Really Improve by Self-critiquing Their Own Plans? "
            ],
            "Citation": [
                "# On the Brittle Foundations of ReAct Prompting for Agentic Large Language Models ",
                "# RLSF: Reinforcement Learning via Symbolic Feedback "
            ]
        },
        "shuffle_doc": true,
        "id": "09b3204a-4cd3-49b7-97b3-54a20864e2b8",
        "prompt": "[Question]\n#Papers:\n{docs}\n\nWe hope you will carefully study the provided papers and determine the citation relationships between them. Please follow the instructions below strictly to complete the task:\n\n#Specific Requirements:\n1. Reference: When a given paper mentions other provided papers, those other papers are considered as \"references\" for the given paper. To summarize in this specific context, references are about what the given paper is using.\n2. Citation: Conversely, when other provided papers mention the given paper in their works, the given paper is being \"cited\" by those other papers. To summarize in this specific context, citations are about who is using the given paper.\n3. Given a paper, you need to determine the citation or reference relationship between this paper and the other papers. Do not consider papers that are not provided.\n3. Please present the paper titles in a json format as follows: {{\"Reference\":[\"Reference Title 1\", \"Reference Title 2\", ..., \"Reference Title n\"], \"Citation\":[\"Citation Title 1\", \"Citation Title 2\", ..., \"Citation Title n\"]}}.\n4. If a paper does not have any references or citations, please leave the corresponding list empty, e.g.{{\"Refernce\":[]}}, {{\"Citation\":[]}}.\n\n#The paper you need to analyze:\nLLMs Can't Plan, But Can Help Planning in LLM-Modulo Frameworks\n\n[Gold Answer]\n{'Reference': ['# Can Large Language Models Really Improve by Self-critiquing Their Own Plans? '], 'Citation': ['# On the Brittle Foundations of ReAct Prompting for Agentic Large Language Models ', '# RLSF: Reinforcement Learning via Symbolic Feedback ']}\n\n[The Start of Assistant's Predicted Answer]\n{\"Reference\":[\"Alchemy of yore\", \"Chemistry\", \"Nuclear Physics\", \"SAT Modulo Theories (Nieuwenhuis \\& Oliveras, 2006)\", \"Knowledge Representation community of yore (Doyle \\& Patil, 1991)\", \"NASA mission planning\", \"deep space network planning\", \"mars rover task planning\", \"Clever Hans effect (cle)\", \"Automated Planning: theory and practice (Ghallab, Nau, and Traverso, 2004)\", \"Retrieval-Augmented Generation Benchmark (RGB)\", \"PDDL planning problems\", \"AlphaGeometry\", \"FunSearch\", \"Reinforcement Learning with Simulators\", \"Gipo: an integrated graphical tool to support knowledge engineering in ai planning (ECP-01)\", \"Codeplan: Repository-level coding using llms and planning (Bairi et al., 2023)\", \"Language models are few-shot learners (Brown et al., 2020)\", \"Sparks of artificial general intelligence: Early experiments with gpt-4 (Bubeck et al., 2023)\", \"Two theses of knowledge representation: Language restrictions, taxonomic classification, and the utility of representation services (Doyle and Patil, 1991)\", \"Faith and fate: Limits of transformers on compositionality (Dziri et al., 2023)\", \"Large language models are not abstract reasoners (Gendron et al., 2023)\", \"The FF planning system: fast plan generation through heuristic search (Hoffmann and Nebel, 2001)\", \"VAL: Automatic plan validation, continuous effects and mixed initiative planning using PDDL (Howey et al., 2004)\", \"Large language models cannot self-correct reasoning yet (Huang et al., 2023a)\", \"Large language models can self-improve (Huang et al., 2023b)\", \"Inner monologue: Embodied reasoning through planning with language models (Huang, Xia, Xiao, Chan, Liang, Florence, Zeng, Tompson, Mordatch, Chebotar, et al., 2022)\", \"Automated scheduling for nasa's deep space network (Johnston, Tran, Arroyo, Sorensen, Tay, Carruth, Coffman, and Wallace, 2014)\", \"Polanyi's revenge and ai's new romance with tacit knowledge (Kambhampati, 2021)\", \"Can llms really reason and plan? (Kambhampati, 2023)\", \"On the role of large language models in planning (Kambhampati, Valmeekam, Marquez, and Guan, 2023)\", \"Reward design with language models (Kwon et al., 2022)\", \"Code as policies: Language model programs for embodied control (Liang et al., 2023)\", \"LLM+P: Empowering large language models with optimal planning proficiency (Liu et al., 2023)\", \"Eureka: Human-level reward design via coding large language models (Ma et al., 2023)\", \"Embers of autoregression: Understanding large language models through the problem they are trained to solve (McCoy et al., 2023)\", \"PDDL-the planning domain definition language (McDermott, Ghallab, Howe, Knoblock, Ram, Veloso, Weld, and Wilkins, 1998)\", \"On sat modulo theories and optimization problems (Nieuwenhuis and Oliveras, 2006)\", \"GPT3-to-plan: Extracting plans from text using gpt-3 (Olmo et al., 2021)\", \"Introducing chatgpt by openai (OpenAI, 2022)\", \"Gpt-4 technical report (OpenAI, 2023)\", \"Training language models to follow instructions with human feedback (Ouyang et al., 2022)\", \"Understanding the capabilities of large language models for automated planning (Pallagani et al., 2023)\", \"Logic-LM: Empowering large language models with symbolic solvers for faithful logical reasoning (Pan et al., 2023)\", \"Saynav: Grounding large language models for dynamic planning to navigation in new environments (Rajvanshi et al., 2023)\", \"Mathematical discoveries from program search with large language models (Romera-Paredes et al., 2023)\", \"Artificial intelligence a modern approach (Russell and Norvig, 2010)\", \"Reflexion: Language agents with verbal reinforcement learning (Shinn, Cassano, Gopinath, Narasimhan, and Yao, 2023)\", \"ALIGNing Text and Embodied Environments for Interactive Learning (Shridhar et al., 2021)\", \"PDDL planning with pretrained large language models (Silver et al., 2022)\", \"GPT-4 doesn't know it's wrong: An analysis of iterative prompting for reasoning problems (Stechly et al., 2023)\", \"Solving olympiad geometry without human demonstrations (Trinh et al., 2024)\", \"Large language models fail on trivial alterations to theory-of-mind tasks (Ullman, 2023)\", \"On the planning abilities of large language models - a critical investigation (Valmeekam et al., 2023c)\", \"Theory of mind abilities of large language models in human-robot interaction: An illusion? (Verma, Bhambri, and Kambhampati, 2024)\", \"Voyager: An open-ended embodied agent with large language models (Wang et al., 2023)\", \"Self-instruct: Aligning language model with self generated instructions (Wang, Kordi, Mishra, Liu, Smith, Khashabi, and Hajishirzi, 2022)\", \"Large language models are better reasoners with self-verification (Weng, Zhu, Xia, He, Liu, Sun, Liu, and Zhao, 2023)\", \"Translating natural language to planning goals with large-language models (Xie et al., 2023)\", \"Tree of thoughts: Deliberate problem solving with large language models (Yao et al., 2023a)\", \"React: Synergizing reasoning and acting in language models (Yao et al., 2023b)\", \"A Survey on Evaluation of Large Language Models (Chang et al., 2023)\", \"Vicuna: An Open-Source Chatbot Impressing GPT-4 with $90 \\% *$ ChatGPT Quality (Chiang et al., 2023)\", \"ChatLaw: Open-Source Legal Large Language Model with Integrated External Knowledge Bases (Cui et al., 2023)\", \"Compositional Semantic Parsing with Large Language Models (Drozdov et al., 2023)\", \"How Close is ChatGPT to Human Experts? Comparison Corpus, Evaluation, and Detection (Guo et al., 2023)\", \"Rethinking with Retrieval: Faithful Large Language Model Inference (He, Zhang, and Roth, 2022)\", \"Measuring Massive Multitask Language Understanding (Hendrycks et al., 2021)\", \"Atlas: Few-shot Learning with Retrieval Augmented Language Models (Izacard et al., 2022)\"]}, \"Citation\":[\"Clever Hans\", \"Gipo: an integrated graphical tool to support knowledge engineering in ai planning\", \"Do as i can, not as i say: Grounding language in robotic affordances\", \"Language models are few-shot learners\", \"Sparks of artificial general intelligence: Early experiments with gpt-4\", \"Two theses of knowledge representation: Language restrictions, taxonomic classification, and the utility of representation services\", \"Faith and fate: Limits of transformers on compositionality\", \"Large language models are not abstract reasoners\", \"Automated Planning: theory and practice\", \"Leveraging approximate symbolic models for reinforcement learning via skill diversity\", \"Leveraging pre-trained large language models to construct and utilize world models for model-based task planning\", \"Reasoning with language model is planning with world model\", \"The FF planning system: fast plan generation through heuristic search\", \"VAL: Automatic plan validation, continuous effects and mixed initiative planning using PDDL\", \"Large language models cannot self-correct reasoning yet\", \"Large language models can self-improve\", \"Inner monologue: Embodied reasoning through planning with language models\", \"PDDL-the planning domain definition language\", \"On sat modulo theories and optimization problems\", \"Gpt3-to-plan: Extracting plans from text using gpt-3\", \"Introducing chatgpt by openai\", \"Gpt-4 technical report\", \"Training language models to follow instructions with human feedback\", \"Understanding the capabilities of large language models for automated planning\", \"Logic-lm: Empowering large language models with symbolic solvers for faithful logical reasoning\", \"Saynav: Grounding large language models for dynamic planning to navigation in new environments\", \"Mathematical discoveries from program search with large language models\", \"Artificial intelligence a modern approach\", \"Reflexion: Language agents with verbal reinforcement learning\", \"PDDL planning with pretrained large language models\", \"GPT-4 doesn't know it's wrong: An analysis of iterative prompting for reasoning problems\", \"Solving olympiad geometry without human demonstrations\", \"Large language models fail on trivial alterations to theory-of-mind tasks\", \"On the planning abilities of large language models - a critical investigation\", \"Theory of mind abilities of large language models in human-robot interaction: An illusion?\", \"Voyager: An open-ended embodied agent with large language models\", \"Self-instruct: Aligning language model with self generated instructions\", \"Large language models are better reasoners with self-verification\", \"Translating natural language to planning goals with large-language models\", \"Tree of thoughts: Deliberate problem solving with large language models\", \"React: Synergizing reasoning and acting in language models\", \"Atlas: Few-shot Learning with Retrieval Augmented Language Models\"]}\n[The End of Assistant's Predicted Answer]\n\n[System]\nWe would like to request your feedback on the performance of the AI assistant in response to the user question displayed above according to the gold answer. Please use the following listed aspects and their descriptions as evaluation criteria:\n    - Accuracy and Hallucinations: The assistant's answer is semantically consistent with the gold answer; The numerical value and order need to be accurate, and there should be no hallucinations.\n    - Completeness: Referring to the reference answers, the assistant's answer should contain all the key points needed to answer the user's question; further elaboration on these key points can be omitted.\nPlease rate whether this answer is suitable for the question. Please note that the gold answer can be considered as a correct answer to the question.\n\nThe assistant receives an overall score on a scale of 1 to 100, where a higher score indicates better overall performance.\nPlease note that if the assistant's answer and the gold answer fully meet the above criteria, its overall rating should be the full marks (100).\nPlease first provide a comprehensive explanation of your evaluation, avoiding any potential bias.\nThen, output a line indicating the score of the Assistant.\n\nPLEASE OUTPUT WITH THE FOLLOWING FORMAT, WHERE THE SCORE IS A SCALE OF 1 TO 100 BY STRICTLY FOLLOWING THIS FORMAT: \"[[score]]\", FOR EXAMPLE \"Rating: [[100]]\":\n<start output>\nEvaluation evidence: your evluation explanation here, no more than 100 words\nRating: [[score]]\n<end output>\n\nNow, start your evaluation:",
        "generate_response": "{\"Reference\":[\"Alchemy of yore\", \"Chemistry\", \"Nuclear Physics\", \"SAT Modulo Theories (Nieuwenhuis \\& Oliveras, 2006)\", \"Knowledge Representation community of yore (Doyle \\& Patil, 1991)\", \"NASA mission planning\", \"deep space network planning\", \"mars rover task planning\", \"Clever Hans effect (cle)\", \"Automated Planning: theory and practice (Ghallab, Nau, and Traverso, 2004)\", \"Retrieval-Augmented Generation Benchmark (RGB)\", \"PDDL planning problems\", \"AlphaGeometry\", \"FunSearch\", \"Reinforcement Learning with Simulators\", \"Gipo: an integrated graphical tool to support knowledge engineering in ai planning (ECP-01)\", \"Codeplan: Repository-level coding using llms and planning (Bairi et al., 2023)\", \"Language models are few-shot learners (Brown et al., 2020)\", \"Sparks of artificial general intelligence: Early experiments with gpt-4 (Bubeck et al., 2023)\", \"Two theses of knowledge representation: Language restrictions, taxonomic classification, and the utility of representation services (Doyle and Patil, 1991)\", \"Faith and fate: Limits of transformers on compositionality (Dziri et al., 2023)\", \"Large language models are not abstract reasoners (Gendron et al., 2023)\", \"The FF planning system: fast plan generation through heuristic search (Hoffmann and Nebel, 2001)\", \"VAL: Automatic plan validation, continuous effects and mixed initiative planning using PDDL (Howey et al., 2004)\", \"Large language models cannot self-correct reasoning yet (Huang et al., 2023a)\", \"Large language models can self-improve (Huang et al., 2023b)\", \"Inner monologue: Embodied reasoning through planning with language models (Huang, Xia, Xiao, Chan, Liang, Florence, Zeng, Tompson, Mordatch, Chebotar, et al., 2022)\", \"Automated scheduling for nasa's deep space network (Johnston, Tran, Arroyo, Sorensen, Tay, Carruth, Coffman, and Wallace, 2014)\", \"Polanyi's revenge and ai's new romance with tacit knowledge (Kambhampati, 2021)\", \"Can llms really reason and plan? (Kambhampati, 2023)\", \"On the role of large language models in planning (Kambhampati, Valmeekam, Marquez, and Guan, 2023)\", \"Reward design with language models (Kwon et al., 2022)\", \"Code as policies: Language model programs for embodied control (Liang et al., 2023)\", \"LLM+P: Empowering large language models with optimal planning proficiency (Liu et al., 2023)\", \"Eureka: Human-level reward design via coding large language models (Ma et al., 2023)\", \"Embers of autoregression: Understanding large language models through the problem they are trained to solve (McCoy et al., 2023)\", \"PDDL-the planning domain definition language (McDermott, Ghallab, Howe, Knoblock, Ram, Veloso, Weld, and Wilkins, 1998)\", \"On sat modulo theories and optimization problems (Nieuwenhuis and Oliveras, 2006)\", \"GPT3-to-plan: Extracting plans from text using gpt-3 (Olmo et al., 2021)\", \"Introducing chatgpt by openai (OpenAI, 2022)\", \"Gpt-4 technical report (OpenAI, 2023)\", \"Training language models to follow instructions with human feedback (Ouyang et al., 2022)\", \"Understanding the capabilities of large language models for automated planning (Pallagani et al., 2023)\", \"Logic-LM: Empowering large language models with symbolic solvers for faithful logical reasoning (Pan et al., 2023)\", \"Saynav: Grounding large language models for dynamic planning to navigation in new environments (Rajvanshi et al., 2023)\", \"Mathematical discoveries from program search with large language models (Romera-Paredes et al., 2023)\", \"Artificial intelligence a modern approach (Russell and Norvig, 2010)\", \"Reflexion: Language agents with verbal reinforcement learning (Shinn, Cassano, Gopinath, Narasimhan, and Yao, 2023)\", \"ALIGNing Text and Embodied Environments for Interactive Learning (Shridhar et al., 2021)\", \"PDDL planning with pretrained large language models (Silver et al., 2022)\", \"GPT-4 doesn't know it's wrong: An analysis of iterative prompting for reasoning problems (Stechly et al., 2023)\", \"Solving olympiad geometry without human demonstrations (Trinh et al., 2024)\", \"Large language models fail on trivial alterations to theory-of-mind tasks (Ullman, 2023)\", \"On the planning abilities of large language models - a critical investigation (Valmeekam et al., 2023c)\", \"Theory of mind abilities of large language models in human-robot interaction: An illusion? (Verma, Bhambri, and Kambhampati, 2024)\", \"Voyager: An open-ended embodied agent with large language models (Wang et al., 2023)\", \"Self-instruct: Aligning language model with self generated instructions (Wang, Kordi, Mishra, Liu, Smith, Khashabi, and Hajishirzi, 2022)\", \"Large language models are better reasoners with self-verification (Weng, Zhu, Xia, He, Liu, Sun, Liu, and Zhao, 2023)\", \"Translating natural language to planning goals with large-language models (Xie et al., 2023)\", \"Tree of thoughts: Deliberate problem solving with large language models (Yao et al., 2023a)\", \"React: Synergizing reasoning and acting in language models (Yao et al., 2023b)\", \"A Survey on Evaluation of Large Language Models (Chang et al., 2023)\", \"Vicuna: An Open-Source Chatbot Impressing GPT-4 with $90 \\% *$ ChatGPT Quality (Chiang et al., 2023)\", \"ChatLaw: Open-Source Legal Large Language Model with Integrated External Knowledge Bases (Cui et al., 2023)\", \"Compositional Semantic Parsing with Large Language Models (Drozdov et al., 2023)\", \"How Close is ChatGPT to Human Experts? Comparison Corpus, Evaluation, and Detection (Guo et al., 2023)\", \"Rethinking with Retrieval: Faithful Large Language Model Inference (He, Zhang, and Roth, 2022)\", \"Measuring Massive Multitask Language Understanding (Hendrycks et al., 2021)\", \"Atlas: Few-shot Learning with Retrieval Augmented Language Models (Izacard et al., 2022)\"]}, \"Citation\":[\"Clever Hans\", \"Gipo: an integrated graphical tool to support knowledge engineering in ai planning\", \"Do as i can, not as i say: Grounding language in robotic affordances\", \"Language models are few-shot learners\", \"Sparks of artificial general intelligence: Early experiments with gpt-4\", \"Two theses of knowledge representation: Language restrictions, taxonomic classification, and the utility of representation services\", \"Faith and fate: Limits of transformers on compositionality\", \"Large language models are not abstract reasoners\", \"Automated Planning: theory and practice\", \"Leveraging approximate symbolic models for reinforcement learning via skill diversity\", \"Leveraging pre-trained large language models to construct and utilize world models for model-based task planning\", \"Reasoning with language model is planning with world model\", \"The FF planning system: fast plan generation through heuristic search\", \"VAL: Automatic plan validation, continuous effects and mixed initiative planning using PDDL\", \"Large language models cannot self-correct reasoning yet\", \"Large language models can self-improve\", \"Inner monologue: Embodied reasoning through planning with language models\", \"PDDL-the planning domain definition language\", \"On sat modulo theories and optimization problems\", \"Gpt3-to-plan: Extracting plans from text using gpt-3\", \"Introducing chatgpt by openai\", \"Gpt-4 technical report\", \"Training language models to follow instructions with human feedback\", \"Understanding the capabilities of large language models for automated planning\", \"Logic-lm: Empowering large language models with symbolic solvers for faithful logical reasoning\", \"Saynav: Grounding large language models for dynamic planning to navigation in new environments\", \"Mathematical discoveries from program search with large language models\", \"Artificial intelligence a modern approach\", \"Reflexion: Language agents with verbal reinforcement learning\", \"PDDL planning with pretrained large language models\", \"GPT-4 doesn't know it's wrong: An analysis of iterative prompting for reasoning problems\", \"Solving olympiad geometry without human demonstrations\", \"Large language models fail on trivial alterations to theory-of-mind tasks\", \"On the planning abilities of large language models - a critical investigation\", \"Theory of mind abilities of large language models in human-robot interaction: An illusion?\", \"Voyager: An open-ended embodied agent with large language models\", \"Self-instruct: Aligning language model with self generated instructions\", \"Large language models are better reasoners with self-verification\", \"Translating natural language to planning goals with large-language models\", \"Tree of thoughts: Deliberate problem solving with large language models\", \"React: Synergizing reasoning and acting in language models\", \"Atlas: Few-shot Learning with Retrieval Augmented Language Models\"]}",
        "eval_response": "Evaluation evidence: The assistant's response does not align with the gold answer, indicating a significant deviation in accuracy and completeness. The references and citations listed in the assistant's response do not match those in the gold answer, suggesting hallucinations and inaccuracies. The assistant's answer includes an extensive list of unrelated titles, which are not relevant to the specific task of identifying citation relationships for the given paper.\n\nRating: [[10]]",
        "score": 10
    },
    {
        "level": 3,
        "set": 1,
        "length": 36817,
        "type": "paper",
        "language": "en",
        "question": "Large Language Models for Mathematical Reasoning: Progresses and Challenges",
        "instruction": "We hope you will carefully study the provided papers and determine the citation relationships between them. Please follow the instructions below strictly to complete the task:\n\n#Specific Requirements:\n1. Reference: When a given paper mentions other provided papers, those other papers are considered as \"references\" for the given paper. To summarize in this specific context, references are about what the given paper is using.\n2. Citation: Conversely, when other provided papers mention the given paper in their works, the given paper is being \"cited\" by those other papers. To summarize in this specific context, citations are about who is using the given paper.\n3. Given a paper, you need to determine the citation or reference relationship between this paper and the other papers. Do not consider papers that are not provided.\n3. Please present the paper titles in a json format as follows: {{\"Reference\":[\"Reference Title 1\", \"Reference Title 2\", ..., \"Reference Title n\"], \"Citation\":[\"Citation Title 1\", \"Citation Title 2\", ..., \"Citation Title n\"]}}.\n4. If a paper does not have any references or citations, please leave the corresponding list empty, e.g.{{\"Refernce\":[]}}, {{\"Citation\":[]}}.",
        "prompt_template": "#Papers:\n{docs}\n\n{instruction}\n\n#The paper you need to analyze:\n{question}",
        "doc": [
            "2402.00157.md",
            "2405.19973.md",
            "2310.09590.md"
        ],
        "answer": {
            "Reference": [
                "# Solving Math Word Problems with Reexamination "
            ],
            "Citation": [
                "# A Triumvirate of AI Driven Theoretical Discovery "
            ]
        },
        "shuffle_doc": true,
        "id": "ef556128-89d2-4abc-a0ae-2595b35a64bc",
        "prompt": "[Question]\n#Papers:\n{docs}\n\nWe hope you will carefully study the provided papers and determine the citation relationships between them. Please follow the instructions below strictly to complete the task:\n\n#Specific Requirements:\n1. Reference: When a given paper mentions other provided papers, those other papers are considered as \"references\" for the given paper. To summarize in this specific context, references are about what the given paper is using.\n2. Citation: Conversely, when other provided papers mention the given paper in their works, the given paper is being \"cited\" by those other papers. To summarize in this specific context, citations are about who is using the given paper.\n3. Given a paper, you need to determine the citation or reference relationship between this paper and the other papers. Do not consider papers that are not provided.\n3. Please present the paper titles in a json format as follows: {{\"Reference\":[\"Reference Title 1\", \"Reference Title 2\", ..., \"Reference Title n\"], \"Citation\":[\"Citation Title 1\", \"Citation Title 2\", ..., \"Citation Title n\"]}}.\n4. If a paper does not have any references or citations, please leave the corresponding list empty, e.g.{{\"Refernce\":[]}}, {{\"Citation\":[]}}.\n\n#The paper you need to analyze:\nLarge Language Models for Mathematical Reasoning: Progresses and Challenges\n\n[Gold Answer]\n{'Reference': ['# Solving Math Word Problems with Reexamination '], 'Citation': ['# A Triumvirate of AI Driven Theoretical Discovery ']}\n\n[The Start of Assistant's Predicted Answer]\n{\"Reference\": [\"A Survey on Evaluation of Large Language Models\", \"Vicuna: An Open-Source Chatbot Impressing GPT-4 with $90 \\% *$ ChatGPT Quality\", \"ChatLaw: Open-Source Legal Large Language Model with Integrated External Knowledge Bases\", \"Compositional Semantic Parsing with Large Language Models\", \"Open LLM Leaderboard\", \"How Close is ChatGPT to Human Experts? Comparison Corpus, Evaluation, and Detection\", \"REALM: Retrieval-Augmented Language Model PreTraining\", \"Rethinking with Retrieval: Faithful Large Language Model Inference\", \"Measuring Massive Multitask Language Understanding\", \"Atlas: Few-shot Learning with Retrieval Augmented Language Models\", \"Large language models for mathematicians\", \"Can neural networks do arithmetic? A survey on the elementary numerical skills of state-of-the-art deep learning models\", \"Llama: Open and efficient foundation language models\", \"Llama 2: Open foundation and fine-tuned chat models\", \"Solving olympiad geometry without human demonstrations\", \"An empirical study on challenging math problem solving with GPT4\", \"LPML: llm-prompting markup language for mathematical reasoning\", \"How well do large language models perform in arithmetic tasks?\", \"Mammoth: Building math generalist models through hybrid instruction tuning\", \"GLM-130B: an open bilingual pre-trained model\", \"Evaluating and improving tool-augmented computation-intensive math reasoning\", \"Interpretable math word problem solution generation via step-by-step planning\", \"Ape210k: A large-scale and template-rich dataset of math word problems\", \"Minif2f: a cross-system benchmark for formal olympiad-level mathematics\", \"Agieval: A human-centric benchmark for evaluating foundation models\", \"Solving challenging math word problems using GPT-4 code interpreter with code-based self-verification\", \"Mathattack: Attacking large language models towards math solving ability\", \"Solving math word problems via cooperative reasoning induced language models\", \"Solving math word problems concerning systems of equations with GPT-3\"], \"Citation\": []}\n[The End of Assistant's Predicted Answer]\n\n[System]\nWe would like to request your feedback on the performance of the AI assistant in response to the user question displayed above according to the gold answer. Please use the following listed aspects and their descriptions as evaluation criteria:\n    - Accuracy and Hallucinations: The assistant's answer is semantically consistent with the gold answer; The numerical value and order need to be accurate, and there should be no hallucinations.\n    - Completeness: Referring to the reference answers, the assistant's answer should contain all the key points needed to answer the user's question; further elaboration on these key points can be omitted.\nPlease rate whether this answer is suitable for the question. Please note that the gold answer can be considered as a correct answer to the question.\n\nThe assistant receives an overall score on a scale of 1 to 100, where a higher score indicates better overall performance.\nPlease note that if the assistant's answer and the gold answer fully meet the above criteria, its overall rating should be the full marks (100).\nPlease first provide a comprehensive explanation of your evaluation, avoiding any potential bias.\nThen, output a line indicating the score of the Assistant.\n\nPLEASE OUTPUT WITH THE FOLLOWING FORMAT, WHERE THE SCORE IS A SCALE OF 1 TO 100 BY STRICTLY FOLLOWING THIS FORMAT: \"[[score]]\", FOR EXAMPLE \"Rating: [[100]]\":\n<start output>\nEvaluation evidence: your evluation explanation here, no more than 100 words\nRating: [[score]]\n<end output>\n\nNow, start your evaluation:",
        "generate_response": "{\"Reference\": [\"A Survey on Evaluation of Large Language Models\", \"Vicuna: An Open-Source Chatbot Impressing GPT-4 with $90 \\% *$ ChatGPT Quality\", \"ChatLaw: Open-Source Legal Large Language Model with Integrated External Knowledge Bases\", \"Compositional Semantic Parsing with Large Language Models\", \"Open LLM Leaderboard\", \"How Close is ChatGPT to Human Experts? Comparison Corpus, Evaluation, and Detection\", \"REALM: Retrieval-Augmented Language Model PreTraining\", \"Rethinking with Retrieval: Faithful Large Language Model Inference\", \"Measuring Massive Multitask Language Understanding\", \"Atlas: Few-shot Learning with Retrieval Augmented Language Models\", \"Large language models for mathematicians\", \"Can neural networks do arithmetic? A survey on the elementary numerical skills of state-of-the-art deep learning models\", \"Llama: Open and efficient foundation language models\", \"Llama 2: Open foundation and fine-tuned chat models\", \"Solving olympiad geometry without human demonstrations\", \"An empirical study on challenging math problem solving with GPT4\", \"LPML: llm-prompting markup language for mathematical reasoning\", \"How well do large language models perform in arithmetic tasks?\", \"Mammoth: Building math generalist models through hybrid instruction tuning\", \"GLM-130B: an open bilingual pre-trained model\", \"Evaluating and improving tool-augmented computation-intensive math reasoning\", \"Interpretable math word problem solution generation via step-by-step planning\", \"Ape210k: A large-scale and template-rich dataset of math word problems\", \"Minif2f: a cross-system benchmark for formal olympiad-level mathematics\", \"Agieval: A human-centric benchmark for evaluating foundation models\", \"Solving challenging math word problems using GPT-4 code interpreter with code-based self-verification\", \"Mathattack: Attacking large language models towards math solving ability\", \"Solving math word problems via cooperative reasoning induced language models\", \"Solving math word problems concerning systems of equations with GPT-3\"], \"Citation\": []}",
        "eval_response": "Evaluation evidence: The assistant's answer significantly deviates from the gold answer in terms of accuracy and completeness. The gold answer lists one specific reference and one citation, while the assistant's answer includes a long list of references and no citations, none of which match the gold answer. This indicates a misunderstanding or misinterpretation of the task, leading to hallucinations (inclusion of irrelevant paper titles) and a lack of accuracy (missing the correct titles from the gold answer).\n\nRating: [[10]]",
        "score": 10
    },
    {
        "level": 3,
        "set": 2,
        "length": 64125,
        "type": "paper",
        "language": "en",
        "question": "MedusA: Simple LLM Inference Acceleration Framework with Multiple Decoding Heads",
        "instruction": "We hope you will carefully study the provided papers and determine the citation relationships between them. Please follow the instructions below strictly to complete the task:\n\n#Specific Requirements:\n1. Reference: When a given paper mentions other provided papers, those other papers are considered as \"references\" for the given paper. To summarize in this specific context, references are about what the given paper is using.\n2. Citation: Conversely, when other provided papers mention the given paper in their works, the given paper is being \"cited\" by those other papers. To summarize in this specific context, citations are about who is using the given paper.\n3. Given a paper, you need to determine the citation or reference relationship between this paper and the other papers. Do not consider papers that are not provided.\n3. Please present the paper titles in a json format as follows: {{\"Reference\":[\"Reference Title 1\", \"Reference Title 2\", ..., \"Reference Title n\"], \"Citation\":[\"Citation Title 1\", \"Citation Title 2\", ..., \"Citation Title n\"]}}.\n4. If a paper does not have any references or citations, please leave the corresponding list empty, e.g.{{\"Refernce\":[]}}, {{\"Citation\":[]}}.",
        "prompt_template": "#Papers:\n{docs}\n\n{instruction}\n\n#The paper you need to analyze:\n{question}",
        "doc": [
            "2401.10774.md",
            "2405.19261.md",
            "2405.19715.md"
        ],
        "answer": {
            "Reference": [],
            "Citation": [
                "# Faster Cascades via Speculative Decoding ",
                "# SpecDec++: Boosting Speculative Decoding via Adaptive Candidate Lengths "
            ]
        },
        "shuffle_doc": true,
        "id": "b8fb8fd5-2756-444f-ad94-74393b5486c5",
        "prompt": "[Question]\n#Papers:\n{docs}\n\nWe hope you will carefully study the provided papers and determine the citation relationships between them. Please follow the instructions below strictly to complete the task:\n\n#Specific Requirements:\n1. Reference: When a given paper mentions other provided papers, those other papers are considered as \"references\" for the given paper. To summarize in this specific context, references are about what the given paper is using.\n2. Citation: Conversely, when other provided papers mention the given paper in their works, the given paper is being \"cited\" by those other papers. To summarize in this specific context, citations are about who is using the given paper.\n3. Given a paper, you need to determine the citation or reference relationship between this paper and the other papers. Do not consider papers that are not provided.\n3. Please present the paper titles in a json format as follows: {{\"Reference\":[\"Reference Title 1\", \"Reference Title 2\", ..., \"Reference Title n\"], \"Citation\":[\"Citation Title 1\", \"Citation Title 2\", ..., \"Citation Title n\"]}}.\n4. If a paper does not have any references or citations, please leave the corresponding list empty, e.g.{{\"Refernce\":[]}}, {{\"Citation\":[]}}.\n\n#The paper you need to analyze:\nMedusA: Simple LLM Inference Acceleration Framework with Multiple Decoding Heads\n\n[Gold Answer]\n{'Reference': [], 'Citation': ['# Faster Cascades via Speculative Decoding ', '# SpecDec++: Boosting Speculative Decoding via Adaptive Candidate Lengths ']}\n\n[The Start of Assistant's Predicted Answer]\n{\"Reference\":[\"Multi-query attention\", \"Axolotl\", \"MIROSTAT: A neural text decoding algorithm that directly controls perplexity\", \"Language models are few-shot learners\", \"Accelerating large language model decoding with speculative sampling\", \"Vicuna: An open-source chatbot impressing gpt-4 with $90 \\% *$ chatgpt quality\", \"Palm: Scaling language modeling with pathways\", \"8-bit optimizers via block-wise quantization\", \"Llm. int8 (): 8-bit matrix multiplication for transformers at scale\", \"Qlora: Efficient finetuning of quantized llms\", \"Enhancing chat language models by scaling high-quality instructional conversations\", \"Alpacafarm: A simulation framework for methods that learn from human feedback\", \"Sigmoid-weighted linear units for neural network function approximation in reinforcement learning\", \"Hierarchical neural story generation\", \"Gptq: Accurate post-training quantization for generative pre-trained transformers\", \"Breaking the sequential dependency of $11 \\mathrm{~m}$ inference using lookahead decoding\", \"Palm 2 technical report\", \"Truncation sampling as language model desmoothing\", \"The curious case of neural text degeneration\", \"Lora: Low-rank adaptation of large language models\", \"Assisted generation: a new direction toward low-latency text generation\", \"Dense-and-sparse quantization\", \"Efficient memory management for large language model serving with pagedattention\", \"Fast inference from transformers via speculative decoding\", \"Sequence-level knowledge distillation\", \"Fine-tuning can distort pretrained features and underperform out-of-distribution\", \"Training language models to follow instructions with human feedback\", \"Efficiently scaling transformer inference\", \"ShareGPT\", \"Fast transformer decoding: one write-head is all you need\", \"Accelerating llm inference with staged speculative decoding\", \"Blockwise parallel decoding for deep autoregressive models\", \"Llama 2: open foundation and fine-tuned chat models\", \"Zephyr: direct distillation of lm alignment\", \"Speculative decoding: lossless speedup of autoregressive translation\", \"Smoothquant: accurate and efficient post-training quantization for large language models\", \"A survey on non-autoregressive generation for neural machine translation and beyond\", \"Do transformers really perform badly for graph representation?\", \"Opt: open pre-trained transformer language models\", \"H _2 o: heavy-hitter oracle for efficient generative inference of large language models\", \"Judging llm-as-a-judge with mt-bench and chatbot arena\", \"Improving speculative decoding via knowledge distillation\"], \"Citation\":[\"A Survey on Evaluation of Large Language Models\", \"Vicuna: An Open-Source Chatbot Impressing GPT-4 with $90 \\% *$ ChatGPT Quality\", \"ChatLaw: Open-Source Legal Large Language Model with Integrated External Knowledge Bases\", \"Compositional Semantic Parsing with Large Language Models\", \"Open LLM Leaderboard\", \"How Close is ChatGPT to Human Experts? Comparison Corpus, Evaluation, and Detection\", \"REALM: Retrieval-Augmented Language Model PreTraining\", \"Rethinking with Retrieval: Faithful Large Language Model Inference\", \"Measuring Massive Multitask Language Understanding\", \"Atlas: Few-shot Learning with Retrieval Augmented Language Models\", \"Gqa: Training Generalized Multi-query Transformer Models from Multi-head Checkpoints\", \"Alpacafarm: A Simulation Framework for Methods that Learn from Human Feedback\", \"Sequence-level Knowledge Distillation\", \"Assisted Generation: A New Direction toward Low-latency Text Generation\", \"Squeezellm: Dense-and-Sparse Quantization\", \"Online Speculative Decoding\", \"MAUVE: Measuring the Gap between Neural Text and Human Text using Divergence Frontiers\", \"Efficiently Scaling Transformer Inference\", \"Specinfer: Accelerating Generative LLM Serving with Speculative Inference and Token Tree Verification\", \"Gpt-4 Technical Report\", \"Training Language Models to Follow Instructions with Human Feedback\", \"Fast Transformer Decoding: One Write-head is All You Need\", \"Staged Speculative Decoding\", \"Llama 2: Open Foundation and Fine-Tuned Chat Models\", \"Zephyr: Direct Distillation of LM Alignment\"]}\n[The End of Assistant's Predicted Answer]\n\n[System]\nWe would like to request your feedback on the performance of the AI assistant in response to the user question displayed above according to the gold answer. Please use the following listed aspects and their descriptions as evaluation criteria:\n    - Accuracy and Hallucinations: The assistant's answer is semantically consistent with the gold answer; The numerical value and order need to be accurate, and there should be no hallucinations.\n    - Completeness: Referring to the reference answers, the assistant's answer should contain all the key points needed to answer the user's question; further elaboration on these key points can be omitted.\nPlease rate whether this answer is suitable for the question. Please note that the gold answer can be considered as a correct answer to the question.\n\nThe assistant receives an overall score on a scale of 1 to 100, where a higher score indicates better overall performance.\nPlease note that if the assistant's answer and the gold answer fully meet the above criteria, its overall rating should be the full marks (100).\nPlease first provide a comprehensive explanation of your evaluation, avoiding any potential bias.\nThen, output a line indicating the score of the Assistant.\n\nPLEASE OUTPUT WITH THE FOLLOWING FORMAT, WHERE THE SCORE IS A SCALE OF 1 TO 100 BY STRICTLY FOLLOWING THIS FORMAT: \"[[score]]\", FOR EXAMPLE \"Rating: [[100]]\":\n<start output>\nEvaluation evidence: your evluation explanation here, no more than 100 words\nRating: [[score]]\n<end output>\n\nNow, start your evaluation:",
        "generate_response": "{\"Reference\":[\"Multi-query attention\", \"Axolotl\", \"MIROSTAT: A neural text decoding algorithm that directly controls perplexity\", \"Language models are few-shot learners\", \"Accelerating large language model decoding with speculative sampling\", \"Vicuna: An open-source chatbot impressing gpt-4 with $90 \\% *$ chatgpt quality\", \"Palm: Scaling language modeling with pathways\", \"8-bit optimizers via block-wise quantization\", \"Llm. int8 (): 8-bit matrix multiplication for transformers at scale\", \"Qlora: Efficient finetuning of quantized llms\", \"Enhancing chat language models by scaling high-quality instructional conversations\", \"Alpacafarm: A simulation framework for methods that learn from human feedback\", \"Sigmoid-weighted linear units for neural network function approximation in reinforcement learning\", \"Hierarchical neural story generation\", \"Gptq: Accurate post-training quantization for generative pre-trained transformers\", \"Breaking the sequential dependency of $11 \\mathrm{~m}$ inference using lookahead decoding\", \"Palm 2 technical report\", \"Truncation sampling as language model desmoothing\", \"The curious case of neural text degeneration\", \"Lora: Low-rank adaptation of large language models\", \"Assisted generation: a new direction toward low-latency text generation\", \"Dense-and-sparse quantization\", \"Efficient memory management for large language model serving with pagedattention\", \"Fast inference from transformers via speculative decoding\", \"Sequence-level knowledge distillation\", \"Fine-tuning can distort pretrained features and underperform out-of-distribution\", \"Training language models to follow instructions with human feedback\", \"Efficiently scaling transformer inference\", \"ShareGPT\", \"Fast transformer decoding: one write-head is all you need\", \"Accelerating llm inference with staged speculative decoding\", \"Blockwise parallel decoding for deep autoregressive models\", \"Llama 2: open foundation and fine-tuned chat models\", \"Zephyr: direct distillation of lm alignment\", \"Speculative decoding: lossless speedup of autoregressive translation\", \"Smoothquant: accurate and efficient post-training quantization for large language models\", \"A survey on non-autoregressive generation for neural machine translation and beyond\", \"Do transformers really perform badly for graph representation?\", \"Opt: open pre-trained transformer language models\", \"H _2 o: heavy-hitter oracle for efficient generative inference of large language models\", \"Judging llm-as-a-judge with mt-bench and chatbot arena\", \"Improving speculative decoding via knowledge distillation\"], \"Citation\":[\"A Survey on Evaluation of Large Language Models\", \"Vicuna: An Open-Source Chatbot Impressing GPT-4 with $90 \\% *$ ChatGPT Quality\", \"ChatLaw: Open-Source Legal Large Language Model with Integrated External Knowledge Bases\", \"Compositional Semantic Parsing with Large Language Models\", \"Open LLM Leaderboard\", \"How Close is ChatGPT to Human Experts? Comparison Corpus, Evaluation, and Detection\", \"REALM: Retrieval-Augmented Language Model PreTraining\", \"Rethinking with Retrieval: Faithful Large Language Model Inference\", \"Measuring Massive Multitask Language Understanding\", \"Atlas: Few-shot Learning with Retrieval Augmented Language Models\", \"Gqa: Training Generalized Multi-query Transformer Models from Multi-head Checkpoints\", \"Alpacafarm: A Simulation Framework for Methods that Learn from Human Feedback\", \"Sequence-level Knowledge Distillation\", \"Assisted Generation: A New Direction toward Low-latency Text Generation\", \"Squeezellm: Dense-and-Sparse Quantization\", \"Online Speculative Decoding\", \"MAUVE: Measuring the Gap between Neural Text and Human Text using Divergence Frontiers\", \"Efficiently Scaling Transformer Inference\", \"Specinfer: Accelerating Generative LLM Serving with Speculative Inference and Token Tree Verification\", \"Gpt-4 Technical Report\", \"Training Language Models to Follow Instructions with Human Feedback\", \"Fast Transformer Decoding: One Write-head is All You Need\", \"Staged Speculative Decoding\", \"Llama 2: Open Foundation and Fine-Tuned Chat Models\", \"Zephyr: Direct Distillation of LM Alignment\"]}",
        "eval_response": "Evaluation evidence: The assistant's answer does not align with the gold answer in terms of accuracy and completeness. The gold answer specifies that there are no references and two citations for the paper \"MedusA: Simple LLM Inference Acceleration Framework with Multiple Decoding Heads.\" However, the assistant's answer lists numerous references and citations that are not mentioned in the gold answer, indicating a significant deviation and hallucination of data. This discrepancy suggests a misunderstanding of the task requirements or an error in processing the correct information.\n\nRating: [[10]]",
        "score": 10
    },
    {
        "level": 3,
        "set": 3,
        "length": 165100,
        "type": "paper",
        "language": "en",
        "question": "Aligner: Achieving Efficient Alignment through Weak-to-Strong Correction",
        "instruction": "We hope you will carefully study the provided papers and determine the citation relationships between them. Please follow the instructions below strictly to complete the task:\n\n#Specific Requirements:\n1. Reference: When a given paper mentions other provided papers, those other papers are considered as \"references\" for the given paper. To summarize in this specific context, references are about what the given paper is using.\n2. Citation: Conversely, when other provided papers mention the given paper in their works, the given paper is being \"cited\" by those other papers. To summarize in this specific context, citations are about who is using the given paper.\n3. Given a paper, you need to determine the citation or reference relationship between this paper and the other papers. Do not consider papers that are not provided.\n3. Please present the paper titles in a json format as follows: {{\"Reference\":[\"Reference Title 1\", \"Reference Title 2\", ..., \"Reference Title n\"], \"Citation\":[\"Citation Title 1\", \"Citation Title 2\", ..., \"Citation Title n\"]}}.\n4. If a paper does not have any references or citations, please leave the corresponding list empty, e.g.{{\"Refernce\":[]}}, {{\"Citation\":[]}}.",
        "prompt_template": "#Papers:\n{docs}\n\n{instruction}\n\n#The paper you need to analyze:\n{question}",
        "doc": [
            "2402.02416.md",
            "2405.11647.md",
            "2311.17035.md",
            "2401.04056.md"
        ],
        "answer": {
            "Reference": [
                "# Scalable Extraction of Training Data from (Production) Language Models ",
                "# A Minimaximalist Approach to Reinforcement Learning from Human Feedback "
            ],
            "Citation": [
                "# Hummer: Towards Limited Competitive Preference Dataset "
            ]
        },
        "shuffle_doc": true,
        "id": "de08511f-897c-40bc-a5ba-fb8dba6dd896",
        "prompt": "[Question]\n#Papers:\n{docs}\n\nWe hope you will carefully study the provided papers and determine the citation relationships between them. Please follow the instructions below strictly to complete the task:\n\n#Specific Requirements:\n1. Reference: When a given paper mentions other provided papers, those other papers are considered as \"references\" for the given paper. To summarize in this specific context, references are about what the given paper is using.\n2. Citation: Conversely, when other provided papers mention the given paper in their works, the given paper is being \"cited\" by those other papers. To summarize in this specific context, citations are about who is using the given paper.\n3. Given a paper, you need to determine the citation or reference relationship between this paper and the other papers. Do not consider papers that are not provided.\n3. Please present the paper titles in a json format as follows: {{\"Reference\":[\"Reference Title 1\", \"Reference Title 2\", ..., \"Reference Title n\"], \"Citation\":[\"Citation Title 1\", \"Citation Title 2\", ..., \"Citation Title n\"]}}.\n4. If a paper does not have any references or citations, please leave the corresponding list empty, e.g.{{\"Refernce\":[]}}, {{\"Citation\":[]}}.\n\n#The paper you need to analyze:\nAligner: Achieving Efficient Alignment through Weak-to-Strong Correction\n\n[Gold Answer]\n{'Reference': ['# Scalable Extraction of Training Data from (Production) Language Models ', '# A Minimaximalist Approach to Reinforcement Learning from Human Feedback '], 'Citation': ['# Hummer: Towards Limited Competitive Preference Dataset ']}\n\n[The Start of Assistant's Predicted Answer]\n{\"Reference\": [\"Gpt-4 technical report\", \"Concrete problems in ai safety and governance\", \"Palm 2 technical report\", \"Claude 2\", \"A general language assistant as a laboratory for alignment\", \"Training a helpful and harmless assistant with reinforcement learning from human feedback\", \"Constitutional ai: Harmlessness from ai feedback\", \"Red-teaming large language models using chain of utterances for safety-alignment\", \"Measuring progress on scalable oversight for large language models\", \"Weak-to-strong generalization: Eliciting strong capabilities with weak supervision\", \"Open problems and fundamental limitations of reinforcement learning from human feedback\", \"Teaching large language models to self-debug\", \"Seq2sick: Evaluating the robustness of sequence-tosequence models with adversarial examples\", \"Vicuna: An open-source chatbot impressing gpt-4 with 90 % * chatgpt quality\", \"Supervising strong learners by amplifying weak experts\", \"Deep reinforcement learning from human preferences\", \"RedPajama: an Open Dataset for Training Large Language Models\", \"Safe RLHF: Safe reinforcement learning from human feedback\", \"A sequence-to-sequence model for semantic role labeling\", \"Toxicity in chatgpt: Analyzing persona-assigned language models\", \"Red teaming language models to reduce harms: Methods, scaling behaviors, and lessons learned\", \"Reinforced self-training (rest) for language modeling\", \"Deep residual learning for image recognition\", \"Large language models can self-improve\", \"Beavertails: Towards improved safety alignment of LLM via a human-preference dataset\", \"Ai alignment: A comprehensive survey\", \"Survey of hallucination in natural language generation\", \"Perspective API\", \"Deep reinforcement learning for sequence-to-sequence models\", \"Efficient memory management for large language model serving with paged-attention\", \"Rlaif: Scaling reinforcement learning from human feedback with ai feedback\", \"Multi-step jailbreaking privacy attacks on chatGPT\", \"Self-refine: Iterative refinement with self-feedback\", \"A self-refinement strategy for noise reduction in grammatical error correction\", \"Scalable extraction of training data from (production) language models\", \"The alignment problem from a deep learning perspective\", \"Introducing Superalignment\", \"Training language models to follow instructions with human feedback\", \"Direct preference optimization: Your language model is secretly a reward model\", \"Towards empathetic open-domain conversation models: A new benchmark and dataset\", \"Learning to model editing processes\", \"Self-critiquing models for assisting human evaluators\", \"Proximal policy optimization algorithms\", \"Reflexion: Language agents with verbal reinforcement learning\", \"A minimaximalist approach to reinforcement learning from human feedback\", \"Stanford alpaca: An instruction-following llama model\", \"Gemini: a family of highly capable multimodal models\", \"Llama 2: Open foundation and finetuned chat models\", \"Small language models improve giants by rewriting their outputs\", \"Emergent abilities of large language models\", \"Finegrained human feedback gives better rewards for language model training\", \"Baichuan 2: Open large-scale language models\", \"Shadow alignment: The ease of subverting safely-aligned language models\", \"Idea2img: Iterative self-refinement with gpt-4v (ision) for automatic image design and generation\", \"Deepspeed-chat: Easy, fast and affordable rlhf training of chatgpt-like models at all scales\", \"RRHF: Rank responses to align language models with human feedback\", \"A constrained sequence-to-sequence neural model for sentence simplification\", \"A survey of large language models\", \"Fine-tuning language models from human preferences\"], \"Citation\": []}\n[The End of Assistant's Predicted Answer]\n\n[System]\nWe would like to request your feedback on the performance of the AI assistant in response to the user question displayed above according to the gold answer. Please use the following listed aspects and their descriptions as evaluation criteria:\n    - Accuracy and Hallucinations: The assistant's answer is semantically consistent with the gold answer; The numerical value and order need to be accurate, and there should be no hallucinations.\n    - Completeness: Referring to the reference answers, the assistant's answer should contain all the key points needed to answer the user's question; further elaboration on these key points can be omitted.\nPlease rate whether this answer is suitable for the question. Please note that the gold answer can be considered as a correct answer to the question.\n\nThe assistant receives an overall score on a scale of 1 to 100, where a higher score indicates better overall performance.\nPlease note that if the assistant's answer and the gold answer fully meet the above criteria, its overall rating should be the full marks (100).\nPlease first provide a comprehensive explanation of your evaluation, avoiding any potential bias.\nThen, output a line indicating the score of the Assistant.\n\nPLEASE OUTPUT WITH THE FOLLOWING FORMAT, WHERE THE SCORE IS A SCALE OF 1 TO 100 BY STRICTLY FOLLOWING THIS FORMAT: \"[[score]]\", FOR EXAMPLE \"Rating: [[100]]\":\n<start output>\nEvaluation evidence: your evluation explanation here, no more than 100 words\nRating: [[score]]\n<end output>\n\nNow, start your evaluation:",
        "generate_response": "{\"Reference\": [\"Gpt-4 technical report\", \"Concrete problems in ai safety and governance\", \"Palm 2 technical report\", \"Claude 2\", \"A general language assistant as a laboratory for alignment\", \"Training a helpful and harmless assistant with reinforcement learning from human feedback\", \"Constitutional ai: Harmlessness from ai feedback\", \"Red-teaming large language models using chain of utterances for safety-alignment\", \"Measuring progress on scalable oversight for large language models\", \"Weak-to-strong generalization: Eliciting strong capabilities with weak supervision\", \"Open problems and fundamental limitations of reinforcement learning from human feedback\", \"Teaching large language models to self-debug\", \"Seq2sick: Evaluating the robustness of sequence-tosequence models with adversarial examples\", \"Vicuna: An open-source chatbot impressing gpt-4 with 90 % * chatgpt quality\", \"Supervising strong learners by amplifying weak experts\", \"Deep reinforcement learning from human preferences\", \"RedPajama: an Open Dataset for Training Large Language Models\", \"Safe RLHF: Safe reinforcement learning from human feedback\", \"A sequence-to-sequence model for semantic role labeling\", \"Toxicity in chatgpt: Analyzing persona-assigned language models\", \"Red teaming language models to reduce harms: Methods, scaling behaviors, and lessons learned\", \"Reinforced self-training (rest) for language modeling\", \"Deep residual learning for image recognition\", \"Large language models can self-improve\", \"Beavertails: Towards improved safety alignment of LLM via a human-preference dataset\", \"Ai alignment: A comprehensive survey\", \"Survey of hallucination in natural language generation\", \"Perspective API\", \"Deep reinforcement learning for sequence-to-sequence models\", \"Efficient memory management for large language model serving with paged-attention\", \"Rlaif: Scaling reinforcement learning from human feedback with ai feedback\", \"Multi-step jailbreaking privacy attacks on chatGPT\", \"Self-refine: Iterative refinement with self-feedback\", \"A self-refinement strategy for noise reduction in grammatical error correction\", \"Scalable extraction of training data from (production) language models\", \"The alignment problem from a deep learning perspective\", \"Introducing Superalignment\", \"Training language models to follow instructions with human feedback\", \"Direct preference optimization: Your language model is secretly a reward model\", \"Towards empathetic open-domain conversation models: A new benchmark and dataset\", \"Learning to model editing processes\", \"Self-critiquing models for assisting human evaluators\", \"Proximal policy optimization algorithms\", \"Reflexion: Language agents with verbal reinforcement learning\", \"A minimaximalist approach to reinforcement learning from human feedback\", \"Stanford alpaca: An instruction-following llama model\", \"Gemini: a family of highly capable multimodal models\", \"Llama 2: Open foundation and finetuned chat models\", \"Small language models improve giants by rewriting their outputs\", \"Emergent abilities of large language models\", \"Finegrained human feedback gives better rewards for language model training\", \"Baichuan 2: Open large-scale language models\", \"Shadow alignment: The ease of subverting safely-aligned language models\", \"Idea2img: Iterative self-refinement with gpt-4v (ision) for automatic image design and generation\", \"Deepspeed-chat: Easy, fast and affordable rlhf training of chatgpt-like models at all scales\", \"RRHF: Rank responses to align language models with human feedback\", \"A constrained sequence-to-sequence neural model for sentence simplification\", \"A survey of large language models\", \"Fine-tuning language models from human preferences\"], \"Citation\": []}",
        "eval_response": "Evaluation evidence: The assistant's answer does not match the gold answer in terms of accuracy and completeness. The references listed in the assistant's answer do not include the two correct references from the gold answer, and it incorrectly lists numerous unrelated papers. Additionally, the assistant's answer fails to identify any citations, whereas the gold answer lists one correct citation. This indicates significant hallucinations and a lack of completeness.\n\nRating: [[10]]",
        "score": 10
    },
    {
        "level": 4,
        "set": 3,
        "length": 104568,
        "type": "paper",
        "language": "en",
        "question": "",
        "instruction": "We kindly ask you to thoroughly review the provided papers and construct a citation chain from them. Please adhere to the following instructions strictly while completing the task:\n\n#Task Instructions:\nGiven several papers, you are required to identify and list the longest citation chain, which demonstrates the citation relationship among the provided papers.\n\n#Specific Requirements:\n1.Please present the titles of the papers in the form of a list, as follows: [\"Title of Paper 1\", \"Title of Paper 2\", ..., \"Title of Paper n\"].\n2.Ensure that the citation chain in the list is linear and continuous, meaning that the first paper title in the list (Paper 1) should not cite any other works. Instead, it should be cited by the next paper in the list (Paper 2); subsequently, each paper should then be cited by the next one in the list, continuing up to the last paper (Paper n).\n3.Consider only the citation relationships within the supplied collection of papers, and ensure that the citation chain accurately reflects the sequential citation order among these documents.\n4.Do not take into account any articles not provided, and disregard other non-linear citation relationships.",
        "prompt_template": "{instruction}\n\n#Papers Provided:\n{docs}",
        "doc": [
            "2306.17653.md",
            "2402.19113.md",
            "2403.01979.md",
            "2403.08570.md",
            "2405.08480.md"
        ],
        "answer": [
            "# Evidence of free-bound transitions in warm dense matter and their impact on equation-of-state measurements ",
            "# Unraveling electronic correlations in warm dense quantum plasmas ",
            "# Ab initio path integral Monte Carlo simulations of warm dense two-component systems without fixed nodes: structural properties ",
            "# Ab initio Density Response and Local Field Factor of Warm Dense Hydrogen ",
            "# Dynamic exchange-correlation effects in the strongly coupled electron liquid "
        ],
        "shuffle_doc": true,
        "id": "a362e3c9-f2ca-4073-b92d-2aa8a70d355c",
        "prompt": "[Question]\nWe kindly ask you to thoroughly review the provided papers and construct a citation chain from them. Please adhere to the following instructions strictly while completing the task:\n\n#Task Instructions:\nGiven several papers, you are required to identify and list the longest citation chain, which demonstrates the citation relationship among the provided papers.\n\n#Specific Requirements:\n1.Please present the titles of the papers in the form of a list, as follows: [\"Title of Paper 1\", \"Title of Paper 2\", ..., \"Title of Paper n\"].\n2.Ensure that the citation chain in the list is linear and continuous, meaning that the first paper title in the list (Paper 1) should not cite any other works. Instead, it should be cited by the next paper in the list (Paper 2); subsequently, each paper should then be cited by the next one in the list, continuing up to the last paper (Paper n).\n3.Consider only the citation relationships within the supplied collection of papers, and ensure that the citation chain accurately reflects the sequential citation order among these documents.\n4.Do not take into account any articles not provided, and disregard other non-linear citation relationships.\n\n#Papers Provided:\n{docs}\n\n[Gold Answer]\n['# Evidence of free-bound transitions in warm dense matter and their impact on equation-of-state measurements ', '# Unraveling electronic correlations in warm dense quantum plasmas ', '# Ab initio path integral Monte Carlo simulations of warm dense two-component systems without fixed nodes: structural properties ', '# Ab initio Density Response and Local Field Factor of Warm Dense Hydrogen ', '# Dynamic exchange-correlation effects in the strongly coupled electron liquid ']\n\n[The Start of Assistant's Predicted Answer]\n[\"Ab initio path integral Monte Carlo simulations of warm dense two-component systems without fixed nodes: structural properties\", \"Path integrals in the theory of condensed helium\", \"Monte Carlo Calculation of Quantum Systems\", \"On path integral Monte Carlo simulations\", \"Path-integral computation of superfluid densities\", \"Superfluidity of strongly correlated bosons in two- and threedimensional traps\", \"Bose-Einstein condensation temperature of a homogeneous weakly interacting Bose gas: Path integral Monte Carlo study\", \"Path-Integral Monte Carlo Study on a Droplet of a Dipolar Bose-Einstein Condensate Stabilized by Quantum Fluctuation\", \"Fermion sign problem in path integral Monte Carlo simulations: Quantum dots, ultracold atoms, and warm dense matter\", \"Fermion sign problem in path integral monte carlo simulations: grand-canonical ensemble\", \"Computational complexity and fundamental limitations to fermionic quantum Monte Carlo simulations\", \"Path-Integral Monte Carlo Simulation of the Warm Dense Homogeneous Electron Gas\", \"Ab initio thermodynamic results for the degenerate electron gas at finite temperature\", \"Permutation blocking path integral Monte Carlo: a highly efficient approach to the simulation of strongly degenerate non-ideal fermions\", \"Ab initio quantum Monte Carlo simulation of the warm dense electron gas in the thermodynamic limit\", \"Ab initio simulation of warm dense matter\", \"Accurate homogeneous electron gas exchange-correlation free energy for local spin-density calculations\", \"Ab initio exchangecorrelation free energy of the uniform electron gas at warm dense matter conditions\", \"Status of free-energy representations for the homogeneous electron gas\", \"Virial coefficients of the uniform electron gas from path integral Monte Carlo simulations\"]\n[The End of Assistant's Predicted Answer]\n\n[System]\nWe would like to request your feedback on the performance of the AI assistant in response to the user question displayed above according to the gold answer. Please use the following listed aspects and their descriptions as evaluation criteria:\n    - Accuracy and Hallucinations: The assistant's answer is semantically consistent with the gold answer; The numerical value and order need to be accurate, and there should be no hallucinations.\n    - Completeness: Referring to the reference answers, the assistant's answer should contain all the key points needed to answer the user's question; further elaboration on these key points can be omitted.\nPlease rate whether this answer is suitable for the question. Please note that the gold answer can be considered as a correct answer to the question.\n\nThe assistant receives an overall score on a scale of 1 to 100, where a higher score indicates better overall performance.\nPlease note that if the assistant's answer and the gold answer fully meet the above criteria, its overall rating should be the full marks (100).\nPlease first provide a comprehensive explanation of your evaluation, avoiding any potential bias.\nThen, output a line indicating the score of the Assistant.\n\nPLEASE OUTPUT WITH THE FOLLOWING FORMAT, WHERE THE SCORE IS A SCALE OF 1 TO 100 BY STRICTLY FOLLOWING THIS FORMAT: \"[[score]]\", FOR EXAMPLE \"Rating: [[100]]\":\n<start output>\nEvaluation evidence: your evluation explanation here, no more than 100 words\nRating: [[score]]\n<end output>\n\nNow, start your evaluation:",
        "generate_response": "[\"Ab initio path integral Monte Carlo simulations of warm dense two-component systems without fixed nodes: structural properties\", \"Path integrals in the theory of condensed helium\", \"Monte Carlo Calculation of Quantum Systems\", \"On path integral Monte Carlo simulations\", \"Path-integral computation of superfluid densities\", \"Superfluidity of strongly correlated bosons in two- and threedimensional traps\", \"Bose-Einstein condensation temperature of a homogeneous weakly interacting Bose gas: Path integral Monte Carlo study\", \"Path-Integral Monte Carlo Study on a Droplet of a Dipolar Bose-Einstein Condensate Stabilized by Quantum Fluctuation\", \"Fermion sign problem in path integral Monte Carlo simulations: Quantum dots, ultracold atoms, and warm dense matter\", \"Fermion sign problem in path integral monte carlo simulations: grand-canonical ensemble\", \"Computational complexity and fundamental limitations to fermionic quantum Monte Carlo simulations\", \"Path-Integral Monte Carlo Simulation of the Warm Dense Homogeneous Electron Gas\", \"Ab initio thermodynamic results for the degenerate electron gas at finite temperature\", \"Permutation blocking path integral Monte Carlo: a highly efficient approach to the simulation of strongly degenerate non-ideal fermions\", \"Ab initio quantum Monte Carlo simulation of the warm dense electron gas in the thermodynamic limit\", \"Ab initio simulation of warm dense matter\", \"Accurate homogeneous electron gas exchange-correlation free energy for local spin-density calculations\", \"Ab initio exchangecorrelation free energy of the uniform electron gas at warm dense matter conditions\", \"Status of free-energy representations for the homogeneous electron gas\", \"Virial coefficients of the uniform electron gas from path integral Monte Carlo simulations\"]",
        "eval_response": "Evaluation evidence: The assistant's response does not align with the gold answer, indicating a failure in constructing the correct citation chain from the provided papers. The titles listed in the assistant's response do not match any from the gold answer, suggesting a misunderstanding or misinterpretation of the task. This results in both inaccuracies and hallucinations, as the titles and citation relationships are fabricated or incorrect. The response also lacks completeness as it does not reflect the correct citation chain required by the task instructions.\n\nRating: [[10]]",
        "score": 10
    },
    {
        "level": 4,
        "set": 1,
        "length": 43344,
        "type": "paper",
        "language": "en",
        "question": "",
        "instruction": "We kindly ask you to thoroughly review the provided papers and construct a citation chain from them. Please adhere to the following instructions strictly while completing the task:\n\n#Task Instructions:\nGiven several papers, you are required to identify and list the longest citation chain, which demonstrates the citation relationship among the provided papers.\n\n#Specific Requirements:\n1.Please present the titles of the papers in the form of a list, as follows: [\"Title of Paper 1\", \"Title of Paper 2\", ..., \"Title of Paper n\"].\n2.Ensure that the citation chain in the list is linear and continuous, meaning that the first paper title in the list (Paper 1) should not cite any other works. Instead, it should be cited by the next paper in the list (Paper 2); subsequently, each paper should then be cited by the next one in the list, continuing up to the last paper (Paper n).\n3.Consider only the citation relationships within the supplied collection of papers, and ensure that the citation chain accurately reflects the sequential citation order among these documents.\n4.Do not take into account any articles not provided, and disregard other non-linear citation relationships.",
        "prompt_template": "{instruction}\n\n#Papers Provided:\n{docs}",
        "doc": [
            "2402.12348.md",
            "2404.09699.md",
            "2405.08289.md"
        ],
        "answer": [
            "![](https://cdn.mathpix.com/cropped/2024_06_04_791d7ec1c70f2f7bcd44g-01.jpg?height=135&width=174&top_left_y=320&top_left_x=260)",
            "# Generative AI for Game Theory-based Mobile Networking ",
            "# Exploring Equilibrium Strategies in Network Games with Generative AI "
        ],
        "shuffle_doc": true,
        "id": "2357f6df-fed9-4508-a80c-800fe6995caa",
        "prompt": "[Question]\nWe kindly ask you to thoroughly review the provided papers and construct a citation chain from them. Please adhere to the following instructions strictly while completing the task:\n\n#Task Instructions:\nGiven several papers, you are required to identify and list the longest citation chain, which demonstrates the citation relationship among the provided papers.\n\n#Specific Requirements:\n1.Please present the titles of the papers in the form of a list, as follows: [\"Title of Paper 1\", \"Title of Paper 2\", ..., \"Title of Paper n\"].\n2.Ensure that the citation chain in the list is linear and continuous, meaning that the first paper title in the list (Paper 1) should not cite any other works. Instead, it should be cited by the next paper in the list (Paper 2); subsequently, each paper should then be cited by the next one in the list, continuing up to the last paper (Paper n).\n3.Consider only the citation relationships within the supplied collection of papers, and ensure that the citation chain accurately reflects the sequential citation order among these documents.\n4.Do not take into account any articles not provided, and disregard other non-linear citation relationships.\n\n#Papers Provided:\n{docs}\n\n[Gold Answer]\n['![](https://cdn.mathpix.com/cropped/2024_06_04_791d7ec1c70f2f7bcd44g-01.jpg?height=135&width=174&top_left_y=320&top_left_x=260)', '# Generative AI for Game Theory-based Mobile Networking ', '# Exploring Equilibrium Strategies in Network Games with Generative AI ']\n\n[The Start of Assistant's Predicted Answer]\n[\"GTBENCH: Uncovering the Strategic Reasoning Limitations of LLMs via Game-Theoretic Evaluations\"]\n[The End of Assistant's Predicted Answer]\n\n[System]\nWe would like to request your feedback on the performance of the AI assistant in response to the user question displayed above according to the gold answer. Please use the following listed aspects and their descriptions as evaluation criteria:\n    - Accuracy and Hallucinations: The assistant's answer is semantically consistent with the gold answer; The numerical value and order need to be accurate, and there should be no hallucinations.\n    - Completeness: Referring to the reference answers, the assistant's answer should contain all the key points needed to answer the user's question; further elaboration on these key points can be omitted.\nPlease rate whether this answer is suitable for the question. Please note that the gold answer can be considered as a correct answer to the question.\n\nThe assistant receives an overall score on a scale of 1 to 100, where a higher score indicates better overall performance.\nPlease note that if the assistant's answer and the gold answer fully meet the above criteria, its overall rating should be the full marks (100).\nPlease first provide a comprehensive explanation of your evaluation, avoiding any potential bias.\nThen, output a line indicating the score of the Assistant.\n\nPLEASE OUTPUT WITH THE FOLLOWING FORMAT, WHERE THE SCORE IS A SCALE OF 1 TO 100 BY STRICTLY FOLLOWING THIS FORMAT: \"[[score]]\", FOR EXAMPLE \"Rating: [[100]]\":\n<start output>\nEvaluation evidence: your evluation explanation here, no more than 100 words\nRating: [[score]]\n<end output>\n\nNow, start your evaluation:",
        "generate_response": "[\"GTBENCH: Uncovering the Strategic Reasoning Limitations of LLMs via Game-Theoretic Evaluations\"]",
        "eval_response": "Evaluation evidence: The assistant's answer does not match the gold answer and fails to identify the correct citation chain among the provided papers. The assistant lists only one paper title, which does not form a chain. The gold answer lists a chain of three papers, showing a linear citation relationship. The assistant's response lacks accuracy and completeness as it does not correctly identify or list the citation chain required by the task instructions.\n\nRating: [[10]]",
        "score": 10
    },
    {
        "level": 4,
        "set": 3,
        "length": 121235,
        "type": "paper",
        "language": "en",
        "question": "",
        "instruction": "We kindly ask you to thoroughly review the provided papers and construct a citation chain from them. Please adhere to the following instructions strictly while completing the task:\n\n#Task Instructions:\nGiven several papers, you are required to identify and list the longest citation chain, which demonstrates the citation relationship among the provided papers.\n\n#Specific Requirements:\n1.Please present the titles of the papers in the form of a list, as follows: [\"Title of Paper 1\", \"Title of Paper 2\", ..., \"Title of Paper n\"].\n2.Ensure that the citation chain in the list is linear and continuous, meaning that the first paper title in the list (Paper 1) should not cite any other works. Instead, it should be cited by the next paper in the list (Paper 2); subsequently, each paper should then be cited by the next one in the list, continuing up to the last paper (Paper n).\n3.Consider only the citation relationships within the supplied collection of papers, and ensure that the citation chain accurately reflects the sequential citation order among these documents.\n4.Do not take into account any articles not provided, and disregard other non-linear citation relationships.",
        "prompt_template": "{instruction}\n\n#Papers Provided:\n{docs}",
        "doc": [
            "2210.10749.md",
            "2310.02989.md",
            "2405.06107.md"
        ],
        "answer": [
            "# Transformers Learn Shortcuts to Automata ",
            "# XVAL: A CONTINUOUS NUMBER ENCODING FOR LARGE LANGUAGE MODELS ",
            "# Transforming the Bootstrap: Using Transformers to Compute Scattering Amplitudes in Planar $\\mathcal{N}=4$ Super Yang-Mills Theory "
        ],
        "shuffle_doc": true,
        "id": "4ed5f412-6e70-4d47-ad88-1ffede8b744c",
        "prompt": "[Question]\nWe kindly ask you to thoroughly review the provided papers and construct a citation chain from them. Please adhere to the following instructions strictly while completing the task:\n\n#Task Instructions:\nGiven several papers, you are required to identify and list the longest citation chain, which demonstrates the citation relationship among the provided papers.\n\n#Specific Requirements:\n1.Please present the titles of the papers in the form of a list, as follows: [\"Title of Paper 1\", \"Title of Paper 2\", ..., \"Title of Paper n\"].\n2.Ensure that the citation chain in the list is linear and continuous, meaning that the first paper title in the list (Paper 1) should not cite any other works. Instead, it should be cited by the next paper in the list (Paper 2); subsequently, each paper should then be cited by the next one in the list, continuing up to the last paper (Paper n).\n3.Consider only the citation relationships within the supplied collection of papers, and ensure that the citation chain accurately reflects the sequential citation order among these documents.\n4.Do not take into account any articles not provided, and disregard other non-linear citation relationships.\n\n#Papers Provided:\n{docs}\n\n[Gold Answer]\n['# Transformers Learn Shortcuts to Automata ', '# XVAL: A CONTINUOUS NUMBER ENCODING FOR LARGE LANGUAGE MODELS ', '# Transforming the Bootstrap: Using Transformers to Compute Scattering Amplitudes in Planar $\\\\mathcal{N}=4$ Super Yang-Mills Theory ']\n\n[The Start of Assistant's Predicted Answer]\n[\"Transforming the Bootstrap: Using Transformers to Compute Scattering Amplitudes in Planar $\\\\mathcal{N}=4$ Super Yang-Mills Theory\"]\n[The End of Assistant's Predicted Answer]\n\n[System]\nWe would like to request your feedback on the performance of the AI assistant in response to the user question displayed above according to the gold answer. Please use the following listed aspects and their descriptions as evaluation criteria:\n    - Accuracy and Hallucinations: The assistant's answer is semantically consistent with the gold answer; The numerical value and order need to be accurate, and there should be no hallucinations.\n    - Completeness: Referring to the reference answers, the assistant's answer should contain all the key points needed to answer the user's question; further elaboration on these key points can be omitted.\nPlease rate whether this answer is suitable for the question. Please note that the gold answer can be considered as a correct answer to the question.\n\nThe assistant receives an overall score on a scale of 1 to 100, where a higher score indicates better overall performance.\nPlease note that if the assistant's answer and the gold answer fully meet the above criteria, its overall rating should be the full marks (100).\nPlease first provide a comprehensive explanation of your evaluation, avoiding any potential bias.\nThen, output a line indicating the score of the Assistant.\n\nPLEASE OUTPUT WITH THE FOLLOWING FORMAT, WHERE THE SCORE IS A SCALE OF 1 TO 100 BY STRICTLY FOLLOWING THIS FORMAT: \"[[score]]\", FOR EXAMPLE \"Rating: [[100]]\":\n<start output>\nEvaluation evidence: your evluation explanation here, no more than 100 words\nRating: [[score]]\n<end output>\n\nNow, start your evaluation:",
        "generate_response": "[\"Transforming the Bootstrap: Using Transformers to Compute Scattering Amplitudes in Planar $\\\\mathcal{N}=4$ Super Yang-Mills Theory\"]",
        "eval_response": "Evaluation evidence: The assistant's answer is incomplete and does not match the gold answer. The gold answer lists a citation chain of three papers, while the assistant's answer only includes one paper from the chain. This indicates a lack of completeness and accuracy in constructing the required citation chain from the provided papers.\n\nRating: [[30]]",
        "score": 30
    },
    {
        "level": 3,
        "set": 2,
        "length": 66471,
        "type": "paper",
        "language": "en",
        "question": "MedusA: Simple LLM Inference Acceleration Framework with Multiple Decoding Heads",
        "instruction": "We hope you will carefully study the provided papers and determine the citation relationships between them. Please follow the instructions below strictly to complete the task:\n\n#Specific Requirements:\n1. Reference: When a given paper mentions other provided papers, those other papers are considered as \"references\" for the given paper. To summarize in this specific context, references are about what the given paper is using.\n2. Citation: Conversely, when other provided papers mention the given paper in their works, the given paper is being \"cited\" by those other papers. To summarize in this specific context, citations are about who is using the given paper.\n3. Given a paper, you need to determine the citation or reference relationship between this paper and the other papers. Do not consider papers that are not provided.\n3. Please present the paper titles in a json format as follows: {{\"Reference\":[\"Reference Title 1\", \"Reference Title 2\", ..., \"Reference Title n\"], \"Citation\":[\"Citation Title 1\", \"Citation Title 2\", ..., \"Citation Title n\"]}}.\n4. If a paper does not have any references or citations, please leave the corresponding list empty, e.g.{{\"Refernce\":[]}}, {{\"Citation\":[]}}.",
        "prompt_template": "#Papers:\n{docs}\n\n{instruction}\n\n#The paper you need to analyze:\n{question}",
        "doc": [
            "2401.10774.md",
            "2405.17741.md",
            "2405.18628.md",
            "2402.02057.md"
        ],
        "answer": {
            "Reference": [
                "# Break the Sequential Dependency of LLM Inference Using LOOKAHEAD DECODING "
            ],
            "Citation": [
                "# LoRA-Switch: Boosting the Efficiency of Dynamic LLM Adapters via System-Algorithm Co-design ",
                "# Hardware-Aware Parallel Prompt Decoding for Memory-Efficient Acceleration of LLM Inference "
            ]
        },
        "shuffle_doc": true,
        "id": "220b92c0-7520-49fc-a16c-8b92b4079565",
        "prompt": "[Question]\n#Papers:\n{docs}\n\nWe hope you will carefully study the provided papers and determine the citation relationships between them. Please follow the instructions below strictly to complete the task:\n\n#Specific Requirements:\n1. Reference: When a given paper mentions other provided papers, those other papers are considered as \"references\" for the given paper. To summarize in this specific context, references are about what the given paper is using.\n2. Citation: Conversely, when other provided papers mention the given paper in their works, the given paper is being \"cited\" by those other papers. To summarize in this specific context, citations are about who is using the given paper.\n3. Given a paper, you need to determine the citation or reference relationship between this paper and the other papers. Do not consider papers that are not provided.\n3. Please present the paper titles in a json format as follows: {{\"Reference\":[\"Reference Title 1\", \"Reference Title 2\", ..., \"Reference Title n\"], \"Citation\":[\"Citation Title 1\", \"Citation Title 2\", ..., \"Citation Title n\"]}}.\n4. If a paper does not have any references or citations, please leave the corresponding list empty, e.g.{{\"Refernce\":[]}}, {{\"Citation\":[]}}.\n\n#The paper you need to analyze:\nMedusA: Simple LLM Inference Acceleration Framework with Multiple Decoding Heads\n\n[Gold Answer]\n{'Reference': ['# Break the Sequential Dependency of LLM Inference Using LOOKAHEAD DECODING '], 'Citation': ['# LoRA-Switch: Boosting the Efficiency of Dynamic LLM Adapters via System-Algorithm Co-design ', '# Hardware-Aware Parallel Prompt Decoding for Memory-Efficient Acceleration of LLM Inference ']}\n\n[The Start of Assistant's Predicted Answer]\n{\"Reference\":[\"A Survey on Evaluation of Large Language Models\", \"Vicuna: An open-source chatbot impressing GPT-4 with $90 \\% *$ ChatGPT Quality\", \"ChatLaw: Open-Source Legal Large Language Model with Integrated External Knowledge Bases\", \"Compositional Semantic Parsing with Large Language Models\", \"Open LLM Leaderboard\", \"How Close is ChatGPT to Human Experts? Comparison Corpus, Evaluation, and Detection\", \"Rethinking with Retrieval: Faithful Large Language Model Inference\", \"Measuring Massive Multitask Language Understanding\", \"Sequence-level knowledge distillation\", \"Fast inference from transformers via speculative decoding\", \"Squeezellm: Dense-and-sparse quantization\", \"Sigmoid-weighted linear units for neural network function approximation in reinforcement learning\", \"Hierarchical neural story generation\", \"Online speculative decoding\", \"Specinfer: Accelerating Generative LLM Serving with Speculative Inference and Token Tree Verification\", \"Training language models to follow instructions with human feedback\", \"Efficiently scaling transformer inference\", \"Smoothquant: Accurate and efficient post-training quantization for large language models\", \"Distillspec: Improving Speculative Decoding via Knowledge Distillation\", \"Llama 2: Open Foundation and Fine-Tuned Chat Models\", \"Zephyr: Direct Distillation of LM Alignment\", \"Judging LLM-as-a-Judge with MT-Bench and Chatbot Arena\"], \"Citation\":[]}\n[The End of Assistant's Predicted Answer]\n\n[System]\nWe would like to request your feedback on the performance of the AI assistant in response to the user question displayed above according to the gold answer. Please use the following listed aspects and their descriptions as evaluation criteria:\n    - Accuracy and Hallucinations: The assistant's answer is semantically consistent with the gold answer; The numerical value and order need to be accurate, and there should be no hallucinations.\n    - Completeness: Referring to the reference answers, the assistant's answer should contain all the key points needed to answer the user's question; further elaboration on these key points can be omitted.\nPlease rate whether this answer is suitable for the question. Please note that the gold answer can be considered as a correct answer to the question.\n\nThe assistant receives an overall score on a scale of 1 to 100, where a higher score indicates better overall performance.\nPlease note that if the assistant's answer and the gold answer fully meet the above criteria, its overall rating should be the full marks (100).\nPlease first provide a comprehensive explanation of your evaluation, avoiding any potential bias.\nThen, output a line indicating the score of the Assistant.\n\nPLEASE OUTPUT WITH THE FOLLOWING FORMAT, WHERE THE SCORE IS A SCALE OF 1 TO 100 BY STRICTLY FOLLOWING THIS FORMAT: \"[[score]]\", FOR EXAMPLE \"Rating: [[100]]\":\n<start output>\nEvaluation evidence: your evluation explanation here, no more than 100 words\nRating: [[score]]\n<end output>\n\nNow, start your evaluation:",
        "generate_response": "{\"Reference\":[\"A Survey on Evaluation of Large Language Models\", \"Vicuna: An open-source chatbot impressing GPT-4 with $90 \\% *$ ChatGPT Quality\", \"ChatLaw: Open-Source Legal Large Language Model with Integrated External Knowledge Bases\", \"Compositional Semantic Parsing with Large Language Models\", \"Open LLM Leaderboard\", \"How Close is ChatGPT to Human Experts? Comparison Corpus, Evaluation, and Detection\", \"Rethinking with Retrieval: Faithful Large Language Model Inference\", \"Measuring Massive Multitask Language Understanding\", \"Sequence-level knowledge distillation\", \"Fast inference from transformers via speculative decoding\", \"Squeezellm: Dense-and-sparse quantization\", \"Sigmoid-weighted linear units for neural network function approximation in reinforcement learning\", \"Hierarchical neural story generation\", \"Online speculative decoding\", \"Specinfer: Accelerating Generative LLM Serving with Speculative Inference and Token Tree Verification\", \"Training language models to follow instructions with human feedback\", \"Efficiently scaling transformer inference\", \"Smoothquant: Accurate and efficient post-training quantization for large language models\", \"Distillspec: Improving Speculative Decoding via Knowledge Distillation\", \"Llama 2: Open Foundation and Fine-Tuned Chat Models\", \"Zephyr: Direct Distillation of LM Alignment\", \"Judging LLM-as-a-Judge with MT-Bench and Chatbot Arena\"], \"Citation\":[]}",
        "eval_response": "Evaluation evidence: The assistant's answer does not align with the gold answer. The gold answer specifies one reference and two citations for the paper \"MedusA: Simple LLM Inference Acceleration Framework with Multiple Decoding Heads.\" However, the assistant's answer lists numerous references that are not mentioned in the gold answer and fails to list any citations, directly contradicting the gold answer's citation details. This indicates issues with accuracy and completeness, as the assistant neither correctly identified the references nor the citations as per the task requirements.\n\nRating: [[10]]",
        "score": 10
    },
    {
        "level": 3,
        "set": 4,
        "length": 200357,
        "type": "paper",
        "language": "en",
        "question": "WebVoyager : Building an End-to-End Web Agent with Large Multimodal Models",
        "instruction": "We hope you will carefully study the provided papers and determine the citation relationships between them. Please follow the instructions below strictly to complete the task:\n\n#Specific Requirements:\n1. Reference: When a given paper mentions other provided papers, those other papers are considered as \"references\" for the given paper. To summarize in this specific context, references are about what the given paper is using.\n2. Citation: Conversely, when other provided papers mention the given paper in their works, the given paper is being \"cited\" by those other papers. To summarize in this specific context, citations are about who is using the given paper.\n3. Given a paper, you need to determine the citation or reference relationship between this paper and the other papers. Do not consider papers that are not provided.\n3. Please present the paper titles in a json format as follows: {{\"Reference\":[\"Reference Title 1\", \"Reference Title 2\", ..., \"Reference Title n\"], \"Citation\":[\"Citation Title 1\", \"Citation Title 2\", ..., \"Citation Title n\"]}}.\n4. If a paper does not have any references or citations, please leave the corresponding list empty, e.g.{{\"Refernce\":[]}}, {{\"Citation\":[]}}.",
        "prompt_template": "#Papers:\n{docs}\n\n{instruction}\n\n#The paper you need to analyze:\n{question}",
        "doc": [
            "2401.13919.md",
            "2404.05955.md",
            "2404.13591.md",
            "2405.03710.md",
            "2405.11120.md",
            "2405.14573.md",
            "2311.12983.md",
            "2312.13771.md",
            "2401.01614.md",
            "2401.10935.md",
            "2401.13649.md"
        ],
        "answer": {
            "Reference": [
                "# GAIA: ",
                "# AppAgent: Multimodal Agents as Smartphone Users ",
                "# GPT-4V(ision) is a Generalist Web Agent, if Grounded ",
                "# SeeClick: Harnessing GUI Grounding for Advanced Visual GUI Agents ",
                "# VisualWebArena: EvalUATING MULTIMODAL AGENTS ON REALISTIC VISUAL WEB TASKS "
            ],
            "Citation": [
                "# VisualWebBench: How Far Have Multimodal LLMs Evolved in Web Page Understanding and Grounding? ",
                "# MARVEL: Multidimensional Abstraction and Reasoning through Visual Evaluation and Learning ",
                "# Automating the Enterprise with Foundation Models ",
                "# Latent State Estimation Helps UI Agents to Reason ",
                "# AndroidWorld: A Dynamic Benchmarking Environment for Autonomous Agents "
            ]
        },
        "shuffle_doc": true,
        "id": "e9552ce6-05cc-470d-9389-5b46ec465654",
        "prompt": "[Question]\n#Papers:\n{docs}\n\nWe hope you will carefully study the provided papers and determine the citation relationships between them. Please follow the instructions below strictly to complete the task:\n\n#Specific Requirements:\n1. Reference: When a given paper mentions other provided papers, those other papers are considered as \"references\" for the given paper. To summarize in this specific context, references are about what the given paper is using.\n2. Citation: Conversely, when other provided papers mention the given paper in their works, the given paper is being \"cited\" by those other papers. To summarize in this specific context, citations are about who is using the given paper.\n3. Given a paper, you need to determine the citation or reference relationship between this paper and the other papers. Do not consider papers that are not provided.\n3. Please present the paper titles in a json format as follows: {{\"Reference\":[\"Reference Title 1\", \"Reference Title 2\", ..., \"Reference Title n\"], \"Citation\":[\"Citation Title 1\", \"Citation Title 2\", ..., \"Citation Title n\"]}}.\n4. If a paper does not have any references or citations, please leave the corresponding list empty, e.g.{{\"Refernce\":[]}}, {{\"Citation\":[]}}.\n\n#The paper you need to analyze:\nWebVoyager : Building an End-to-End Web Agent with Large Multimodal Models\n\n[Gold Answer]\n{'Reference': ['# GAIA: ', '# AppAgent: Multimodal Agents as Smartphone Users ', '# GPT-4V(ision) is a Generalist Web Agent, if Grounded ', '# SeeClick: Harnessing GUI Grounding for Advanced Visual GUI Agents ', '# VisualWebArena: EvalUATING MULTIMODAL AGENTS ON REALISTIC VISUAL WEB TASKS '], 'Citation': ['# VisualWebBench: How Far Have Multimodal LLMs Evolved in Web Page Understanding and Grounding? ', '# MARVEL: Multidimensional Abstraction and Reasoning through Visual Evaluation and Learning ', '# Automating the Enterprise with Foundation Models ', '# Latent State Estimation Helps UI Agents to Reason ', '# AndroidWorld: A Dynamic Benchmarking Environment for Autonomous Agents ']}\n\n[The Start of Assistant's Predicted Answer]\n[{\"Reference\":[\"Cm3: A causal masked multimodal model of the internet\", \"AutoGPT\", \"Language models are few-shot learners\", \"Evaluating large language models trained on code\", \"Webvln: Vision-and-language navigation on websites\", \"Seeclick: Harnessing gui grounding for advanced visual gui agents\", \"Palm: Scaling language modeling with pathways\", \"A coefficient of agreement for nominal scales\", \"Mind2web: Towards a generalist agent for the web\", \"A survey for in-context learning\", \"An image is worth $16 \\\\times 16$ words: Transformers for image recognition at scale\", \"Measuring nominal scale agreement among many raters\", \"Multimodal web navigation with instruction-finetuned foundation models\", \"Making the v in vqa matter: Elevating the role of image understanding in visual question answering\", \"A real-world webagent with planning, long context understanding, and program synthesis\", \"The abduction of sherlock holmes: A dataset for visual abductive reasoning\", \"Visualwebarena: Evaluating multimodal agents on realistic visual web tasks\", \"Pix2struct: Screenshot parsing as pretraining for visual language understanding\", \"Visualbert: A simple and performant baseline for vision and language\", \"Learn to explain: Multimodal reasoning via thought chains for science question answering\", \"Chameleon: Plug-and-play compositional reasoning with large language models\", \"Laser: Llm agent with state-space exploration for web navigation\", \"Gaia: a benchmark for general ai assistants\", \"Webgpt: Browser-assisted question-answering with human feedback\", \"Gpt-4 technical report\", \"Training language models to follow instructions with human feedback\", \"Toolllm: Facilitating large language models to master 16000+ real-world apis\", \"Exploring the limits of transfer learning with a unified text-to-text transformer\", \"Toolformer: Language models can teach themselves to use tools\", \"From pixels to ui actions: Learning to follow instructions via graphical user interfaces\", \"World of bits: An open-domain platform for web-based agents\", \"Reflexion: Language agents with verbal reinforcement learning\", \"Gemini: a family of highly capable multimodal models\", \"Self-instruct: Aligning language model with self generated instructions\", \"Simvlm: Simple visual language model pretraining with weak supervision\", \"Llm-powered autonomous agents\", \"Gpt-4v in wonderland: Large multimodal models for zero-shot smartphone gui navigation\", \"Set-of-mark prompting unleashes extraordinary visual grounding in gpt-4v\", \"The dawn of $1 \\\\mathrm{mms}$ : Preliminary explorations with gpt-4v (ision)\", \"Webshop: Towards scalable real-world web interaction with grounded language agents\", \"React: Synergizing reasoning and acting in language models\", \"Lumos: Learning agents with unified data, modular design, and open-source llms\", \"From recognition to cognition: Visual commonsense reasoning\", \"Appagent: Multimodal agents as smartphone users\", \"Gpt-4v (ision) is a generalist web agent, if grounded\", \"Webarena: A realistic web environment for building autonomous agents\"], \"Citation\":[\"Cm3: A causal masked multimodal model of the internet\", \"AutoGPT\", \"Language models are few-shot learners\", \"Evaluating large language models trained on code\", \"Webvln: Vision-and-language navigation on websites\", \"Seeclick: Harnessing gui grounding for advanced visual gui agents\", \"Palm: Scaling language modeling with pathways\", \"A coefficient of agreement for nominal scales\", \"Mind2web: Towards a generalist agent for the web\", \"A survey for in-context learning\", \"An image is worth $16 \\\\times 16$ words: Transformers for image recognition at scale\", \"Measuring nominal scale agreement among many raters\", \"Multimodal web navigation with instruction-finetuned foundation models\", \"Making the v in vqa matter: Elevating the role of image understanding in visual question answering\", \"A real-world webagent with planning, long context understanding, and program synthesis\", \"The abduction of sherlock holmes: A dataset for visual abductive reasoning\", \"Visualwebarena: Evaluating multimodal agents on realistic visual web tasks\", \"Pix2struct: Screenshot parsing as pretraining for visual language understanding\", \"Visualbert: A simple and performant baseline for vision and language\", \"Learn to explain: Multimodal reasoning via thought chains for science question answering\", \"Chameleon: Plug-and-play compositional reasoning with large language models\", \"Laser: Llm agent with state-space exploration for web navigation\", \"Gaia: a benchmark for general ai assistants\", \"Webgpt: Browser-assisted question-answering with human feedback\", \"Gpt-4 technical report\", \"Training language models to follow instructions with human feedback\", \"Toolllm: Facilitating large language models to master 16000+ real-world apis\", \"Exploring the limits of transfer learning with a unified text-to-text transformer\", \"Toolformer: Language models can teach themselves to use tools\", \"From pixels to ui actions: Learning to follow instructions via graphical user interfaces\", \"World of bits: An open-domain platform for web-based agents\", \"Reflexion: Language agents with verbal reinforcement learning\", \"Gemini: a family of highly capable multimodal models\", \"Self-instruct: Aligning language model with self generated instructions\", \"Simvlm: Simple visual language model pretraining with weak supervision\", \"Llm-powered autonomous agents\", \"Gpt-4v in wonderland: Large multimodal models for zero-shot smartphone gui navigation\", \"Set-of-mark prompting unleashes extraordinary visual grounding in gpt-4v\", \"The dawn of $1 \\\\mathrm{mms}$ : Preliminary explorations with gpt-4v (ision)\", \"Webshop: Towards scalable real-world web interaction with grounded language agents\", \"React: Synergizing reasoning and acting in language models\", \"Lumos: Learning agents with unified data, modular design, and open-source llms\", \"From recognition to cognition: Visual commonsense reasoning\", \"Appagent: Multimodal agents as smartphone users\", \"Gpt-4v (ision) is a generalist web agent, if grounded\", \"Webarena: A realistic web environment for building autonomous agents\"]}]\"\n[The End of Assistant's Predicted Answer]\n\n[System]\nWe would like to request your feedback on the performance of the AI assistant in response to the user question displayed above according to the gold answer. Please use the following listed aspects and their descriptions as evaluation criteria:\n    - Accuracy and Hallucinations: The assistant's answer is semantically consistent with the gold answer; The numerical value and order need to be accurate, and there should be no hallucinations.\n    - Completeness: Referring to the reference answers, the assistant's answer should contain all the key points needed to answer the user's question; further elaboration on these key points can be omitted.\nPlease rate whether this answer is suitable for the question. Please note that the gold answer can be considered as a correct answer to the question.\n\nThe assistant receives an overall score on a scale of 1 to 100, where a higher score indicates better overall performance.\nPlease note that if the assistant's answer and the gold answer fully meet the above criteria, its overall rating should be the full marks (100).\nPlease first provide a comprehensive explanation of your evaluation, avoiding any potential bias.\nThen, output a line indicating the score of the Assistant.\n\nPLEASE OUTPUT WITH THE FOLLOWING FORMAT, WHERE THE SCORE IS A SCALE OF 1 TO 100 BY STRICTLY FOLLOWING THIS FORMAT: \"[[score]]\", FOR EXAMPLE \"Rating: [[100]]\":\n<start output>\nEvaluation evidence: your evluation explanation here, no more than 100 words\nRating: [[score]]\n<end output>\n\nNow, start your evaluation:",
        "generate_response": "[{\"Reference\":[\"Cm3: A causal masked multimodal model of the internet\", \"AutoGPT\", \"Language models are few-shot learners\", \"Evaluating large language models trained on code\", \"Webvln: Vision-and-language navigation on websites\", \"Seeclick: Harnessing gui grounding for advanced visual gui agents\", \"Palm: Scaling language modeling with pathways\", \"A coefficient of agreement for nominal scales\", \"Mind2web: Towards a generalist agent for the web\", \"A survey for in-context learning\", \"An image is worth $16 \\\\times 16$ words: Transformers for image recognition at scale\", \"Measuring nominal scale agreement among many raters\", \"Multimodal web navigation with instruction-finetuned foundation models\", \"Making the v in vqa matter: Elevating the role of image understanding in visual question answering\", \"A real-world webagent with planning, long context understanding, and program synthesis\", \"The abduction of sherlock holmes: A dataset for visual abductive reasoning\", \"Visualwebarena: Evaluating multimodal agents on realistic visual web tasks\", \"Pix2struct: Screenshot parsing as pretraining for visual language understanding\", \"Visualbert: A simple and performant baseline for vision and language\", \"Learn to explain: Multimodal reasoning via thought chains for science question answering\", \"Chameleon: Plug-and-play compositional reasoning with large language models\", \"Laser: Llm agent with state-space exploration for web navigation\", \"Gaia: a benchmark for general ai assistants\", \"Webgpt: Browser-assisted question-answering with human feedback\", \"Gpt-4 technical report\", \"Training language models to follow instructions with human feedback\", \"Toolllm: Facilitating large language models to master 16000+ real-world apis\", \"Exploring the limits of transfer learning with a unified text-to-text transformer\", \"Toolformer: Language models can teach themselves to use tools\", \"From pixels to ui actions: Learning to follow instructions via graphical user interfaces\", \"World of bits: An open-domain platform for web-based agents\", \"Reflexion: Language agents with verbal reinforcement learning\", \"Gemini: a family of highly capable multimodal models\", \"Self-instruct: Aligning language model with self generated instructions\", \"Simvlm: Simple visual language model pretraining with weak supervision\", \"Llm-powered autonomous agents\", \"Gpt-4v in wonderland: Large multimodal models for zero-shot smartphone gui navigation\", \"Set-of-mark prompting unleashes extraordinary visual grounding in gpt-4v\", \"The dawn of $1 \\\\mathrm{mms}$ : Preliminary explorations with gpt-4v (ision)\", \"Webshop: Towards scalable real-world web interaction with grounded language agents\", \"React: Synergizing reasoning and acting in language models\", \"Lumos: Learning agents with unified data, modular design, and open-source llms\", \"From recognition to cognition: Visual commonsense reasoning\", \"Appagent: Multimodal agents as smartphone users\", \"Gpt-4v (ision) is a generalist web agent, if grounded\", \"Webarena: A realistic web environment for building autonomous agents\"], \"Citation\":[\"Cm3: A causal masked multimodal model of the internet\", \"AutoGPT\", \"Language models are few-shot learners\", \"Evaluating large language models trained on code\", \"Webvln: Vision-and-language navigation on websites\", \"Seeclick: Harnessing gui grounding for advanced visual gui agents\", \"Palm: Scaling language modeling with pathways\", \"A coefficient of agreement for nominal scales\", \"Mind2web: Towards a generalist agent for the web\", \"A survey for in-context learning\", \"An image is worth $16 \\\\times 16$ words: Transformers for image recognition at scale\", \"Measuring nominal scale agreement among many raters\", \"Multimodal web navigation with instruction-finetuned foundation models\", \"Making the v in vqa matter: Elevating the role of image understanding in visual question answering\", \"A real-world webagent with planning, long context understanding, and program synthesis\", \"The abduction of sherlock holmes: A dataset for visual abductive reasoning\", \"Visualwebarena: Evaluating multimodal agents on realistic visual web tasks\", \"Pix2struct: Screenshot parsing as pretraining for visual language understanding\", \"Visualbert: A simple and performant baseline for vision and language\", \"Learn to explain: Multimodal reasoning via thought chains for science question answering\", \"Chameleon: Plug-and-play compositional reasoning with large language models\", \"Laser: Llm agent with state-space exploration for web navigation\", \"Gaia: a benchmark for general ai assistants\", \"Webgpt: Browser-assisted question-answering with human feedback\", \"Gpt-4 technical report\", \"Training language models to follow instructions with human feedback\", \"Toolllm: Facilitating large language models to master 16000+ real-world apis\", \"Exploring the limits of transfer learning with a unified text-to-text transformer\", \"Toolformer: Language models can teach themselves to use tools\", \"From pixels to ui actions: Learning to follow instructions via graphical user interfaces\", \"World of bits: An open-domain platform for web-based agents\", \"Reflexion: Language agents with verbal reinforcement learning\", \"Gemini: a family of highly capable multimodal models\", \"Self-instruct: Aligning language model with self generated instructions\", \"Simvlm: Simple visual language model pretraining with weak supervision\", \"Llm-powered autonomous agents\", \"Gpt-4v in wonderland: Large multimodal models for zero-shot smartphone gui navigation\", \"Set-of-mark prompting unleashes extraordinary visual grounding in gpt-4v\", \"The dawn of $1 \\\\mathrm{mms}$ : Preliminary explorations with gpt-4v (ision)\", \"Webshop: Towards scalable real-world web interaction with grounded language agents\", \"React: Synergizing reasoning and acting in language models\", \"Lumos: Learning agents with unified data, modular design, and open-source llms\", \"From recognition to cognition: Visual commonsense reasoning\", \"Appagent: Multimodal agents as smartphone users\", \"Gpt-4v (ision) is a generalist web agent, if grounded\", \"Webarena: A realistic web environment for building autonomous agents\"]}]\"",
        "eval_response": "Evaluation evidence: The assistant's response does not match the gold answer in terms of accuracy and completeness. The assistant provided a long list of references and citations that are not present in the gold answer, indicating significant hallucinations and inaccuracies. The gold answer lists specific papers as references and citations, which are entirely missing from the assistant's response. This discrepancy shows a failure to accurately identify the citation relationships for the given paper, leading to a response that does not meet the requirements of the task.\n\nRating: [[10]]",
        "score": 10
    },
    {
        "level": 3,
        "set": 3,
        "length": 100347,
        "type": "paper",
        "language": "en",
        "question": "WARM: On the Benefits of Weight Averaged Reward Models",
        "instruction": "We hope you will carefully study the provided papers and determine the citation relationships between them. Please follow the instructions below strictly to complete the task:\n\n#Specific Requirements:\n1. Reference: When a given paper mentions other provided papers, those other papers are considered as \"references\" for the given paper. To summarize in this specific context, references are about what the given paper is using.\n2. Citation: Conversely, when other provided papers mention the given paper in their works, the given paper is being \"cited\" by those other papers. To summarize in this specific context, citations are about who is using the given paper.\n3. Given a paper, you need to determine the citation or reference relationship between this paper and the other papers. Do not consider papers that are not provided.\n3. Please present the paper titles in a json format as follows: {{\"Reference\":[\"Reference Title 1\", \"Reference Title 2\", ..., \"Reference Title n\"], \"Citation\":[\"Citation Title 1\", \"Citation Title 2\", ..., \"Citation Title n\"]}}.\n4. If a paper does not have any references or citations, please leave the corresponding list empty, e.g.{{\"Refernce\":[]}}, {{\"Citation\":[]}}.",
        "prompt_template": "#Papers:\n{docs}\n\n{instruction}\n\n#The paper you need to analyze:\n{question}",
        "doc": [
            "2401.12187.md",
            "2405.19107.md",
            "2312.07551.md",
            "2401.00595.md"
        ],
        "answer": {
            "Reference": [
                "# Language Model Alignment with Elastic Reset ",
                "# State of What Art? <br> A Call for Multi-Prompt LLM Evaluation "
            ],
            "Citation": [
                "# Offline Regularised Reinforcement Learning for Large Language Models Alignment "
            ]
        },
        "shuffle_doc": true,
        "id": "9e7bc316-5697-4c37-8f11-f841909c3fa4",
        "prompt": "[Question]\n#Papers:\n{docs}\n\nWe hope you will carefully study the provided papers and determine the citation relationships between them. Please follow the instructions below strictly to complete the task:\n\n#Specific Requirements:\n1. Reference: When a given paper mentions other provided papers, those other papers are considered as \"references\" for the given paper. To summarize in this specific context, references are about what the given paper is using.\n2. Citation: Conversely, when other provided papers mention the given paper in their works, the given paper is being \"cited\" by those other papers. To summarize in this specific context, citations are about who is using the given paper.\n3. Given a paper, you need to determine the citation or reference relationship between this paper and the other papers. Do not consider papers that are not provided.\n3. Please present the paper titles in a json format as follows: {{\"Reference\":[\"Reference Title 1\", \"Reference Title 2\", ..., \"Reference Title n\"], \"Citation\":[\"Citation Title 1\", \"Citation Title 2\", ..., \"Citation Title n\"]}}.\n4. If a paper does not have any references or citations, please leave the corresponding list empty, e.g.{{\"Refernce\":[]}}, {{\"Citation\":[]}}.\n\n#The paper you need to analyze:\nWARM: On the Benefits of Weight Averaged Reward Models\n\n[Gold Answer]\n{'Reference': ['# Language Model Alignment with Elastic Reset ', '# State of What Art? <br> A Call for Multi-Prompt LLM Evaluation '], 'Citation': ['# Offline Regularised Reinforcement Learning for Large Language Models Alignment ']}\n\n[The Start of Assistant's Predicted Answer]\n{\"Reference\": [\"Gemini\", \"GPT-4\", \"Sparks of artificial general intelligence: Early experiments with gpt-4\", \"Improving language understanding by generative pre-training\", \"Bert: Pre-training of deep bidirectional transformers for language understanding\", \"Language models are few-shot learners\", \"Finetuned language models are zero-shot learners\", \"Super-NaturalInstructions: Generalization via declarative instructions on 1600+ NLP tasks\", \"Stanford Alpaca: An instruction-following LLaMA model\", \"Factually consistent summarization via reinforcement learning with textual entailment feedback\", \"On the fragility of learned reward functions\", \"Deep reinforcement learning from human preferences\", \"Fine-tuning language models from human preferences\", \"Learning to summarize with human feedback\", \"Recursively summarizing books with human feedback\", \"LLaMA 2: Open foundation and fine-tuned chat models\", \"Scaling laws for reward model overoptimization\", \"Open problems and fundamental limitations of reinforcement learning from human feedback\", \"Concrete problems in AI safety\", \"Faulty Reward Functions in the Wild\", \"A general language assistant as a laboratory for alignment\", \"Defining and characterizing reward gaming\", \"The effects of reward misspecification: Mapping and mitigating misaligned models\", \"The alignment ceiling: Objective mismatch in reinforcement learning from human feedback\", \"Deal or no deal? end-to-end learning for negotiation dialogues\", \"A long way to go: Investigating length correlations in rlhf\", \"X-risk analysis for AI research\", \"Natural selection favors AIs over humans\", \"Consequences of misaligned AI\", \"Benchmarks and algorithms for offline preference-based reward learning\", \"Measuring progress on scalable oversight for large language models\", \"Rewarded soups: towards pareto-optimal alignment by interpolating weights fine-tuned on diverse rewards\", \"Training language models to follow instructions with human feedback\", \"Helping or herding? reward model ensembles mitigate but do not eliminate reward hacking\", \"Reward model ensembles help mitigate overoptimization\", \"Simple and scalable predictive uncertainty estimation using deep ensembles\", \"Linear mode connectivity and the lottery ticket hypothesis\", \"What is being transferred in transfer learning?\", \"Model soups: averaging weights of multiple fine-tuned models improves accuracy without increasing inference time\", \"Diverse weight averaging for out-of-distribution generalization\", \"Model Ratatouille: Recycling diverse models for out-of-distribution generalization\", \"SWAD: Domain generalization by seeking flat minima\", \"Domain generalization via invariant feature representation\", \"Invariant risk minimization\", \"Fuse to forget: Bias reduction and selective memorization through model fusion\", \"Spurious feature diversification improves out-of-distribution generalization\", \"Attention is all you need\", \"On the opportunities and risks of foundation models\", \"Learning and transferring mid-level image representations using convolutional neural networks\", \"Constitutional AI: Harmlessness from AI feedback\", \"RLAIF: Scaling reinforcement learning from human feedback with ai feedback\", \"Rank analysis of incomplete block designs: I. the method of paired comparisons\", \"Simple statistical gradient-following algorithms for connectionist reinforcement learning\", \"Proximal policy optimization algorithms\", \"Underspecification presents challenges for credibility in modern machine learning\", \"ID and OOD performance are sometimes inversely correlated on real-world datasets\", \"On calibration of modern neural networks\", \"Can you trust your model's uncertainty? evaluating predictive uncertainty under dataset shift\", \"On calibration and out-of-domain generalization\", \"Bounded rationality\", \"On the feasibility of learning, rather than assuming, human biases for reward inference\", \"On the challenges and practices of reinforcement learning from real human feedback\", \"Red teaming language models to reduce harms: Methods, scaling behaviors, and lessons learned\", \"Rewarding chatbots for real-world engagement with millions of users\", \"Sequence tutor: Conservative fine-tuning of sequence generation models with kl-control\", \"A theory of regularized markov decision processes\", \"Multi-agent communication meets natural language: Synergies between functional and structural language learning\", \"Countering language drift with seeded iterated learning\", \"Learning human objectives by evaluating hypothetical behavior\", \"Trial without error: Towards safe reinforcement learning via human intervention\", \"Secrets of rlhf in large language models part ii: Reward modeling\", \"Bias plus variance decomposition for zero-one loss functions\", \"Generalization error of ensemble estimators\", \"Specific versus general principles for constitutional ai\", \"Fine-tuning can distort pretrained features and underperform out-of-distribution\", \"Git re-basin: Merging models modulo permutation symmetries\", \"Knowledge is a region in weight space for fine-tuned language models\", \"No one representation to rule them all: Overlapping features of training methods\", \"Averaging weights leads to wider optima and better generalization\", \"Ensemble of averages: Improving model selection and boosting performance in domain generalization\", \"T1; dr: Mining reddit to learn automatic summarization\", \"PaLM 2 technical report\", \"Chain-of-Thought prompting elicits reasoning in large language models\", \"Fishr: Invariant gradient variances for out-of-distribution generalization\", \"Intriguing properties of neural networks\", \"Adversarial robustness through local lipschitzness\", \"A case for new neural network smoothness constraints\", \"Formal guarantees on the robustness of a classifier against adversarial manipulation\", \"Robust large margin deep neural networks\", \"Certified adversarial robustness via randomized smoothing\", \"Reinforcement learning in feedback control: Challenges and benchmarks from technical process control\", \"Policy gradient in lipschitz markov decision processes\", \"Lipschitzness is all you need to tame off-policy generative adversarial imitation learning\", \"Alpacafarm: A simulation framework for methods that learn from human feedback\", \"Exploring the limits of transfer learning with a unified text-to-text transformer\", \"Momentum-based weight interpolation of strong zero-shot models for continual learning\", \"Weight averaging: A simple yet effective method to overcome catastrophic forgetting in automatic speech recognition\", \"Direct preference optimization: Your language model is secretly a reward model\", \"NeuralBeagle14-7B\", \"Variable generalization performance of a deep learning model to detect pneumonia in chest radiographs: A cross-sectional study\", \"AI for radiographic COVID-19 detection selects shortcuts over signal\", \"Robust fine-tuning of zero-shot models\", \"Patching open-vocabulary models by interpolating weights\", \"ColD fusion: Collaborative descent for distributed multitask finetuning\", \"Pareto manifold learning: Tackling multiple tasks via ensembles of single-task models\", \"Unified model for image, video, audio and language\", \"Seasoning model soups for robustness to adversarial and natural distribution shifts\", \"Linear connectivity reveals generalization strategies\", \"Improving stability in deep reinforcement learning with weight averaging\", \"Learning a subspace of policies for online adaptation in reinforcement learning\", \"Merging decision transformers: Weight averaging for forming multi-task policies\", \"Language model alignment with elastic reset\", \"Editing models with task arithmetic\", \"Elastic weight removal for faithful and abstractive dialogue generation\", \"Learning from noisy labels with deep neural networks: A survey\", \"Understanding deep learning requires rethinking generalization\", \"Learning from noisy labels by regularized estimation of annotator confusion\", \"Neftune: Noisy embeddings improve instruction finetuning\", \"Robust loss functions under label noise for deep neural networks\", \"Sample selection with uncertainty of losses for learning with noisy labels\", \"Mentornet: Learning data-driven curriculum for very deep neural networks on corrupted labels\", \"Co-teaching: Robust training of deep neural networks with extremely noisy labels\", \"Ensemble learning in the presence of noise\", \"Algorithms for inverse reinforcement learning\", \"Learning optimal advantage from preferences and mistaking it for reward\", \"Active reward learning from multiple teachers\", \"The impact of preference agreement in reinforcement learning from human feedback: A case study in summarization\", \"Toolaugmented reward modeling\", \"Aligning large multimodal models with factually augmented rlhf\", \"RIME: Robust preference-based reinforcement learning with noisy human preferences\", \"A general theoretical paradigm to understand learning from human preferences\", \"Adafactor: Adaptive learning rates with sublinear memory cost\", \"Vanishing gradients in reinforcement finetuning of language models\"], \"Citation\": []}\n[The End of Assistant's Predicted Answer]\n\n[System]\nWe would like to request your feedback on the performance of the AI assistant in response to the user question displayed above according to the gold answer. Please use the following listed aspects and their descriptions as evaluation criteria:\n    - Accuracy and Hallucinations: The assistant's answer is semantically consistent with the gold answer; The numerical value and order need to be accurate, and there should be no hallucinations.\n    - Completeness: Referring to the reference answers, the assistant's answer should contain all the key points needed to answer the user's question; further elaboration on these key points can be omitted.\nPlease rate whether this answer is suitable for the question. Please note that the gold answer can be considered as a correct answer to the question.\n\nThe assistant receives an overall score on a scale of 1 to 100, where a higher score indicates better overall performance.\nPlease note that if the assistant's answer and the gold answer fully meet the above criteria, its overall rating should be the full marks (100).\nPlease first provide a comprehensive explanation of your evaluation, avoiding any potential bias.\nThen, output a line indicating the score of the Assistant.\n\nPLEASE OUTPUT WITH THE FOLLOWING FORMAT, WHERE THE SCORE IS A SCALE OF 1 TO 100 BY STRICTLY FOLLOWING THIS FORMAT: \"[[score]]\", FOR EXAMPLE \"Rating: [[100]]\":\n<start output>\nEvaluation evidence: your evluation explanation here, no more than 100 words\nRating: [[score]]\n<end output>\n\nNow, start your evaluation:",
        "generate_response": "{\"Reference\": [\"Gemini\", \"GPT-4\", \"Sparks of artificial general intelligence: Early experiments with gpt-4\", \"Improving language understanding by generative pre-training\", \"Bert: Pre-training of deep bidirectional transformers for language understanding\", \"Language models are few-shot learners\", \"Finetuned language models are zero-shot learners\", \"Super-NaturalInstructions: Generalization via declarative instructions on 1600+ NLP tasks\", \"Stanford Alpaca: An instruction-following LLaMA model\", \"Factually consistent summarization via reinforcement learning with textual entailment feedback\", \"On the fragility of learned reward functions\", \"Deep reinforcement learning from human preferences\", \"Fine-tuning language models from human preferences\", \"Learning to summarize with human feedback\", \"Recursively summarizing books with human feedback\", \"LLaMA 2: Open foundation and fine-tuned chat models\", \"Scaling laws for reward model overoptimization\", \"Open problems and fundamental limitations of reinforcement learning from human feedback\", \"Concrete problems in AI safety\", \"Faulty Reward Functions in the Wild\", \"A general language assistant as a laboratory for alignment\", \"Defining and characterizing reward gaming\", \"The effects of reward misspecification: Mapping and mitigating misaligned models\", \"The alignment ceiling: Objective mismatch in reinforcement learning from human feedback\", \"Deal or no deal? end-to-end learning for negotiation dialogues\", \"A long way to go: Investigating length correlations in rlhf\", \"X-risk analysis for AI research\", \"Natural selection favors AIs over humans\", \"Consequences of misaligned AI\", \"Benchmarks and algorithms for offline preference-based reward learning\", \"Measuring progress on scalable oversight for large language models\", \"Rewarded soups: towards pareto-optimal alignment by interpolating weights fine-tuned on diverse rewards\", \"Training language models to follow instructions with human feedback\", \"Helping or herding? reward model ensembles mitigate but do not eliminate reward hacking\", \"Reward model ensembles help mitigate overoptimization\", \"Simple and scalable predictive uncertainty estimation using deep ensembles\", \"Linear mode connectivity and the lottery ticket hypothesis\", \"What is being transferred in transfer learning?\", \"Model soups: averaging weights of multiple fine-tuned models improves accuracy without increasing inference time\", \"Diverse weight averaging for out-of-distribution generalization\", \"Model Ratatouille: Recycling diverse models for out-of-distribution generalization\", \"SWAD: Domain generalization by seeking flat minima\", \"Domain generalization via invariant feature representation\", \"Invariant risk minimization\", \"Fuse to forget: Bias reduction and selective memorization through model fusion\", \"Spurious feature diversification improves out-of-distribution generalization\", \"Attention is all you need\", \"On the opportunities and risks of foundation models\", \"Learning and transferring mid-level image representations using convolutional neural networks\", \"Constitutional AI: Harmlessness from AI feedback\", \"RLAIF: Scaling reinforcement learning from human feedback with ai feedback\", \"Rank analysis of incomplete block designs: I. the method of paired comparisons\", \"Simple statistical gradient-following algorithms for connectionist reinforcement learning\", \"Proximal policy optimization algorithms\", \"Underspecification presents challenges for credibility in modern machine learning\", \"ID and OOD performance are sometimes inversely correlated on real-world datasets\", \"On calibration of modern neural networks\", \"Can you trust your model's uncertainty? evaluating predictive uncertainty under dataset shift\", \"On calibration and out-of-domain generalization\", \"Bounded rationality\", \"On the feasibility of learning, rather than assuming, human biases for reward inference\", \"On the challenges and practices of reinforcement learning from real human feedback\", \"Red teaming language models to reduce harms: Methods, scaling behaviors, and lessons learned\", \"Rewarding chatbots for real-world engagement with millions of users\", \"Sequence tutor: Conservative fine-tuning of sequence generation models with kl-control\", \"A theory of regularized markov decision processes\", \"Multi-agent communication meets natural language: Synergies between functional and structural language learning\", \"Countering language drift with seeded iterated learning\", \"Learning human objectives by evaluating hypothetical behavior\", \"Trial without error: Towards safe reinforcement learning via human intervention\", \"Secrets of rlhf in large language models part ii: Reward modeling\", \"Bias plus variance decomposition for zero-one loss functions\", \"Generalization error of ensemble estimators\", \"Specific versus general principles for constitutional ai\", \"Fine-tuning can distort pretrained features and underperform out-of-distribution\", \"Git re-basin: Merging models modulo permutation symmetries\", \"Knowledge is a region in weight space for fine-tuned language models\", \"No one representation to rule them all: Overlapping features of training methods\", \"Averaging weights leads to wider optima and better generalization\", \"Ensemble of averages: Improving model selection and boosting performance in domain generalization\", \"T1; dr: Mining reddit to learn automatic summarization\", \"PaLM 2 technical report\", \"Chain-of-Thought prompting elicits reasoning in large language models\", \"Fishr: Invariant gradient variances for out-of-distribution generalization\", \"Intriguing properties of neural networks\", \"Adversarial robustness through local lipschitzness\", \"A case for new neural network smoothness constraints\", \"Formal guarantees on the robustness of a classifier against adversarial manipulation\", \"Robust large margin deep neural networks\", \"Certified adversarial robustness via randomized smoothing\", \"Reinforcement learning in feedback control: Challenges and benchmarks from technical process control\", \"Policy gradient in lipschitz markov decision processes\", \"Lipschitzness is all you need to tame off-policy generative adversarial imitation learning\", \"Alpacafarm: A simulation framework for methods that learn from human feedback\", \"Exploring the limits of transfer learning with a unified text-to-text transformer\", \"Momentum-based weight interpolation of strong zero-shot models for continual learning\", \"Weight averaging: A simple yet effective method to overcome catastrophic forgetting in automatic speech recognition\", \"Direct preference optimization: Your language model is secretly a reward model\", \"NeuralBeagle14-7B\", \"Variable generalization performance of a deep learning model to detect pneumonia in chest radiographs: A cross-sectional study\", \"AI for radiographic COVID-19 detection selects shortcuts over signal\", \"Robust fine-tuning of zero-shot models\", \"Patching open-vocabulary models by interpolating weights\", \"ColD fusion: Collaborative descent for distributed multitask finetuning\", \"Pareto manifold learning: Tackling multiple tasks via ensembles of single-task models\", \"Unified model for image, video, audio and language\", \"Seasoning model soups for robustness to adversarial and natural distribution shifts\", \"Linear connectivity reveals generalization strategies\", \"Improving stability in deep reinforcement learning with weight averaging\", \"Learning a subspace of policies for online adaptation in reinforcement learning\", \"Merging decision transformers: Weight averaging for forming multi-task policies\", \"Language model alignment with elastic reset\", \"Editing models with task arithmetic\", \"Elastic weight removal for faithful and abstractive dialogue generation\", \"Learning from noisy labels with deep neural networks: A survey\", \"Understanding deep learning requires rethinking generalization\", \"Learning from noisy labels by regularized estimation of annotator confusion\", \"Neftune: Noisy embeddings improve instruction finetuning\", \"Robust loss functions under label noise for deep neural networks\", \"Sample selection with uncertainty of losses for learning with noisy labels\", \"Mentornet: Learning data-driven curriculum for very deep neural networks on corrupted labels\", \"Co-teaching: Robust training of deep neural networks with extremely noisy labels\", \"Ensemble learning in the presence of noise\", \"Algorithms for inverse reinforcement learning\", \"Learning optimal advantage from preferences and mistaking it for reward\", \"Active reward learning from multiple teachers\", \"The impact of preference agreement in reinforcement learning from human feedback: A case study in summarization\", \"Toolaugmented reward modeling\", \"Aligning large multimodal models with factually augmented rlhf\", \"RIME: Robust preference-based reinforcement learning with noisy human preferences\", \"A general theoretical paradigm to understand learning from human preferences\", \"Adafactor: Adaptive learning rates with sublinear memory cost\", \"Vanishing gradients in reinforcement finetuning of language models\"], \"Citation\": []}",
        "eval_response": "Evaluation evidence: The assistant's response is inaccurate and contains hallucinations. The provided answer lists numerous papers that are not mentioned in the gold answer, indicating a failure to correctly identify the specific references and citations for the paper in question. The gold answer specifies two references and one citation, none of which appear in the assistant's response. This discrepancy shows a lack of accuracy and the presence of hallucinations, as the assistant lists irrelevant papers.\n\nRating: [[10]]",
        "score": 10
    },
    {
        "level": 3,
        "set": 4,
        "length": 247445,
        "type": "paper",
        "language": "en",
        "question": "CharacterEval: A Chinese Benchmark for Role-Playing Conversational Agent Evaluation",
        "instruction": "We hope you will carefully study the provided papers and determine the citation relationships between them. Please follow the instructions below strictly to complete the task:\n\n#Specific Requirements:\n1. Reference: When a given paper mentions other provided papers, those other papers are considered as \"references\" for the given paper. To summarize in this specific context, references are about what the given paper is using.\n2. Citation: Conversely, when other provided papers mention the given paper in their works, the given paper is being \"cited\" by those other papers. To summarize in this specific context, citations are about who is using the given paper.\n3. Given a paper, you need to determine the citation or reference relationship between this paper and the other papers. Do not consider papers that are not provided.\n3. Please present the paper titles in a json format as follows: {{\"Reference\":[\"Reference Title 1\", \"Reference Title 2\", ..., \"Reference Title n\"], \"Citation\":[\"Citation Title 1\", \"Citation Title 2\", ..., \"Citation Title n\"]}}.\n4. If a paper does not have any references or citations, please leave the corresponding list empty, e.g.{{\"Refernce\":[]}}, {{\"Citation\":[]}}.",
        "prompt_template": "#Papers:\n{docs}\n\n{instruction}\n\n#The paper you need to analyze:\n{question}",
        "doc": [
            "2401.01275.md",
            "2404.07084.md",
            "2404.18231.md",
            "2405.10150.md",
            "2405.14231.md",
            "2405.18027.md",
            "2308.11432.md",
            "2309.05922.md",
            "2310.08372.md",
            "2310.10158.md",
            "2310.17976.md"
        ],
        "answer": {
            "Reference": [
                "Front. Comput. Sci., 2024, 0(0): 1-42",
                "# Improving Factual Consistency for Knowledge-Grounded Dialogue Systems via Knowledge Enhancement and Alignment ",
                "# Character-LLM: A Trainable Agent for Role-Playing ",
                "# InCharacter: Evaluating Personality Fidelity in Role-Playing Agents through Psychological Interviews "
            ],
            "Citation": [
                "# DYNAMIC GENERATION OF PERSONALITIES WITH LARGE LANGUAGE MODELS ",
                "# Speaker Verification in Agent-generated Conversations ",
                "# From Role-Play to Drama-Interaction: An LLM Solution ",
                "# TimeChara: Evaluating Point-in-Time Character Hallucination of Role-Playing Large Language Models "
            ]
        },
        "shuffle_doc": true,
        "id": "6b4575d9-4f18-4661-b43e-1b7d07f0ffee",
        "prompt": "[Question]\n#Papers:\n{docs}\n\nWe hope you will carefully study the provided papers and determine the citation relationships between them. Please follow the instructions below strictly to complete the task:\n\n#Specific Requirements:\n1. Reference: When a given paper mentions other provided papers, those other papers are considered as \"references\" for the given paper. To summarize in this specific context, references are about what the given paper is using.\n2. Citation: Conversely, when other provided papers mention the given paper in their works, the given paper is being \"cited\" by those other papers. To summarize in this specific context, citations are about who is using the given paper.\n3. Given a paper, you need to determine the citation or reference relationship between this paper and the other papers. Do not consider papers that are not provided.\n3. Please present the paper titles in a json format as follows: {{\"Reference\":[\"Reference Title 1\", \"Reference Title 2\", ..., \"Reference Title n\"], \"Citation\":[\"Citation Title 1\", \"Citation Title 2\", ..., \"Citation Title n\"]}}.\n4. If a paper does not have any references or citations, please leave the corresponding list empty, e.g.{{\"Refernce\":[]}}, {{\"Citation\":[]}}.\n\n#The paper you need to analyze:\nCharacterEval: A Chinese Benchmark for Role-Playing Conversational Agent Evaluation\n\n[Gold Answer]\n{'Reference': ['Front. Comput. Sci., 2024, 0(0): 1-42', '# Improving Factual Consistency for Knowledge-Grounded Dialogue Systems via Knowledge Enhancement and Alignment ', '# Character-LLM: A Trainable Agent for Role-Playing ', '# InCharacter: Evaluating Personality Fidelity in Role-Playing Agents through Psychological Interviews '], 'Citation': ['# DYNAMIC GENERATION OF PERSONALITIES WITH LARGE LANGUAGE MODELS ', '# Speaker Verification in Agent-generated Conversations ', '# From Role-Play to Drama-Interaction: An LLM Solution ', '# TimeChara: Evaluating Point-in-Time Character Hallucination of Role-Playing Large Language Models ']}\n\n[The Start of Assistant's Predicted Answer]\n{\"Reference\":[],\"Citation\":[\"DYNAMIC GENERATION OF PERSONALITIES WITH LARGE LANGUAGE MODELS\",\"Speaker Verification in Agent-generated Conversations\",\"InCharacter: Evaluating Personality Fidelity in Role-Playing Agents through Psychological Interviews\"]}\n[The End of Assistant's Predicted Answer]\n\n[System]\nWe would like to request your feedback on the performance of the AI assistant in response to the user question displayed above according to the gold answer. Please use the following listed aspects and their descriptions as evaluation criteria:\n    - Accuracy and Hallucinations: The assistant's answer is semantically consistent with the gold answer; The numerical value and order need to be accurate, and there should be no hallucinations.\n    - Completeness: Referring to the reference answers, the assistant's answer should contain all the key points needed to answer the user's question; further elaboration on these key points can be omitted.\nPlease rate whether this answer is suitable for the question. Please note that the gold answer can be considered as a correct answer to the question.\n\nThe assistant receives an overall score on a scale of 1 to 100, where a higher score indicates better overall performance.\nPlease note that if the assistant's answer and the gold answer fully meet the above criteria, its overall rating should be the full marks (100).\nPlease first provide a comprehensive explanation of your evaluation, avoiding any potential bias.\nThen, output a line indicating the score of the Assistant.\n\nPLEASE OUTPUT WITH THE FOLLOWING FORMAT, WHERE THE SCORE IS A SCALE OF 1 TO 100 BY STRICTLY FOLLOWING THIS FORMAT: \"[[score]]\", FOR EXAMPLE \"Rating: [[100]]\":\n<start output>\nEvaluation evidence: your evluation explanation here, no more than 100 words\nRating: [[score]]\n<end output>\n\nNow, start your evaluation:",
        "generate_response": "{\"Reference\":[],\"Citation\":[\"DYNAMIC GENERATION OF PERSONALITIES WITH LARGE LANGUAGE MODELS\",\"Speaker Verification in Agent-generated Conversations\",\"InCharacter: Evaluating Personality Fidelity in Role-Playing Agents through Psychological Interviews\"]}",
        "eval_response": "Evaluation evidence: The assistant's answer failed to list any references, which is incorrect as per the gold answer that lists four references. Additionally, the citations listed by the assistant are partially incorrect and incomplete, missing two citations and inaccurately listing one that should be a reference. This shows issues in both accuracy and completeness.\n\nRating: [[30]]",
        "score": 30
    },
    {
        "level": 3,
        "set": 2,
        "length": 94641,
        "type": "paper",
        "language": "en",
        "question": "Multilingual Instruction Tuning With Just a Pinch of Multilinguality",
        "instruction": "We hope you will carefully study the provided papers and determine the citation relationships between them. Please follow the instructions below strictly to complete the task:\n\n#Specific Requirements:\n1. Reference: When a given paper mentions other provided papers, those other papers are considered as \"references\" for the given paper. To summarize in this specific context, references are about what the given paper is using.\n2. Citation: Conversely, when other provided papers mention the given paper in their works, the given paper is being \"cited\" by those other papers. To summarize in this specific context, citations are about who is using the given paper.\n3. Given a paper, you need to determine the citation or reference relationship between this paper and the other papers. Do not consider papers that are not provided.\n3. Please present the paper titles in a json format as follows: {{\"Reference\":[\"Reference Title 1\", \"Reference Title 2\", ..., \"Reference Title n\"], \"Citation\":[\"Citation Title 1\", \"Citation Title 2\", ..., \"Citation Title n\"]}}.\n4. If a paper does not have any references or citations, please leave the corresponding list empty, e.g.{{\"Refernce\":[]}}, {{\"Citation\":[]}}.",
        "prompt_template": "#Papers:\n{docs}\n\n{instruction}\n\n#The paper you need to analyze:\n{question}",
        "doc": [
            "2401.01854.md",
            "2404.04925.md",
            "2404.11932.md",
            "2404.12318.md"
        ],
        "answer": {
            "Reference": [],
            "Citation": [
                "# Multilingual Large Language Model: A Survey of Resources, Taxonomy and Frontiers ",
                "# CrossIn: An Efficient Instruction Tuning Approach for Cross-Lingual Knowledge Alignment ",
                "# Reuse Your Rewards: Reward Model Transfer for Zero-Shot Cross-Lingual Alignment "
            ]
        },
        "shuffle_doc": true,
        "id": "712558e0-a0c2-4ced-b729-9b4b42c70447",
        "prompt": "[Question]\n#Papers:\n{docs}\n\nWe hope you will carefully study the provided papers and determine the citation relationships between them. Please follow the instructions below strictly to complete the task:\n\n#Specific Requirements:\n1. Reference: When a given paper mentions other provided papers, those other papers are considered as \"references\" for the given paper. To summarize in this specific context, references are about what the given paper is using.\n2. Citation: Conversely, when other provided papers mention the given paper in their works, the given paper is being \"cited\" by those other papers. To summarize in this specific context, citations are about who is using the given paper.\n3. Given a paper, you need to determine the citation or reference relationship between this paper and the other papers. Do not consider papers that are not provided.\n3. Please present the paper titles in a json format as follows: {{\"Reference\":[\"Reference Title 1\", \"Reference Title 2\", ..., \"Reference Title n\"], \"Citation\":[\"Citation Title 1\", \"Citation Title 2\", ..., \"Citation Title n\"]}}.\n4. If a paper does not have any references or citations, please leave the corresponding list empty, e.g.{{\"Refernce\":[]}}, {{\"Citation\":[]}}.\n\n#The paper you need to analyze:\nMultilingual Instruction Tuning With Just a Pinch of Multilinguality\n\n[Gold Answer]\n{'Reference': [], 'Citation': ['# Multilingual Large Language Model: A Survey of Resources, Taxonomy and Frontiers ', '# CrossIn: An Efficient Instruction Tuning Approach for Cross-Lingual Knowledge Alignment ', '# Reuse Your Rewards: Reward Model Transfer for Zero-Shot Cross-Lingual Alignment ']}\n\n[The Start of Assistant's Predicted Answer]\n{\"Reference\": [\"Mishra et al., 2022\", \"Wei et al., 2022\", \"Sanh et al., 2022\", \"Ouyang et al., 2022\", \"Pires et al., 2019\", \"Wu and Dredze, 2019\", \"Artetxe and Schwenk, 2019\", \"K et al., 2020\", \"Conneau et al., 2020a,b\", \"Anil et al., 2023\", \"Zhou et al., 2023\", \"Kpf et al., 2023\", \"Bubeck et al., 2023\", \"Dubois et al., 2023\", \"Dettmers et al., 2023\", \"Gudibande et al., 2023\", \"Zheng et al., 2023\", \"Zhou et al., 2023\", \"Fernandes et al., 2023\", \"Shaham et al., 2023\", \"Muennighoff et al., 2023\", \"Chen et al., 2023\", \"Taori et al., 2023\", \"Hu et al., 2022\", \"Lai et al., 2023\", \"Zhang et al., 2023\", \"Kew et al., 2023\", \"Touvron et al., 2023a\", \"Anil et al. (2023)\", \"Devlin et al., 2019\", \"Pires et al. (2019)\", \"Wu and Dredze (2019)\", \"Artetxe and Schwenk (2019)\", \"Xue et al. (2021)\", \"Raffel et al., 2020\", \"Ye et al. (2023)\"], \"Citation\": [\"As instruction-tuned large language models (LLMs) gain global adoption, their ability to follow instructions in multiple languages becomes increasingly crucial\", \"Instruction tuning is a fundamental aspect of building modern general-purpose large language models (LLMs), involving fine-tuning a pre-trained model on pairs of instructions and corresponding responses\", \"Cross-lingual transfer has emerged as a promising approach, in which a model is fine-tuned using one language, and acquiring similar abilities in another\", \"We find that replacing even just 40 English training examples with multilingual examples, significantly improves instructionfollowing in those languages\", \"We find that tuning using a few languages enables better performance for languages unseen during tuning, compared to monolingual tuning with the same number of examples\", \"A second trend is that these models often outperform their monolingually-tuned counterparts on the very language the latter were exclusively tuned on\", \"In contrast, Figure 8 suggests this is not necessarily the case for the cross-lingual transfer from a specific language\", \"Our findings across different languages suggest that even monolingual instruction tuning using only one language can result in improved instruction-following capabilities in other languages\", \"We observe a similar trend when exploring the effect of total number of languages in the tuning set, as even splitting the train set to only two languages improves generalization to new languages\", \"Importantly, we unveil the potential of just a small amount of language diversity in the instruction tuning set for this cross-lingual generalization\"]}\n[The End of Assistant's Predicted Answer]\n\n[System]\nWe would like to request your feedback on the performance of the AI assistant in response to the user question displayed above according to the gold answer. Please use the following listed aspects and their descriptions as evaluation criteria:\n    - Accuracy and Hallucinations: The assistant's answer is semantically consistent with the gold answer; The numerical value and order need to be accurate, and there should be no hallucinations.\n    - Completeness: Referring to the reference answers, the assistant's answer should contain all the key points needed to answer the user's question; further elaboration on these key points can be omitted.\nPlease rate whether this answer is suitable for the question. Please note that the gold answer can be considered as a correct answer to the question.\n\nThe assistant receives an overall score on a scale of 1 to 100, where a higher score indicates better overall performance.\nPlease note that if the assistant's answer and the gold answer fully meet the above criteria, its overall rating should be the full marks (100).\nPlease first provide a comprehensive explanation of your evaluation, avoiding any potential bias.\nThen, output a line indicating the score of the Assistant.\n\nPLEASE OUTPUT WITH THE FOLLOWING FORMAT, WHERE THE SCORE IS A SCALE OF 1 TO 100 BY STRICTLY FOLLOWING THIS FORMAT: \"[[score]]\", FOR EXAMPLE \"Rating: [[100]]\":\n<start output>\nEvaluation evidence: your evluation explanation here, no more than 100 words\nRating: [[score]]\n<end output>\n\nNow, start your evaluation:",
        "generate_response": "{\"Reference\": [\"Mishra et al., 2022\", \"Wei et al., 2022\", \"Sanh et al., 2022\", \"Ouyang et al., 2022\", \"Pires et al., 2019\", \"Wu and Dredze, 2019\", \"Artetxe and Schwenk, 2019\", \"K et al., 2020\", \"Conneau et al., 2020a,b\", \"Anil et al., 2023\", \"Zhou et al., 2023\", \"Kpf et al., 2023\", \"Bubeck et al., 2023\", \"Dubois et al., 2023\", \"Dettmers et al., 2023\", \"Gudibande et al., 2023\", \"Zheng et al., 2023\", \"Zhou et al., 2023\", \"Fernandes et al., 2023\", \"Shaham et al., 2023\", \"Muennighoff et al., 2023\", \"Chen et al., 2023\", \"Taori et al., 2023\", \"Hu et al., 2022\", \"Lai et al., 2023\", \"Zhang et al., 2023\", \"Kew et al., 2023\", \"Touvron et al., 2023a\", \"Anil et al. (2023)\", \"Devlin et al., 2019\", \"Pires et al. (2019)\", \"Wu and Dredze (2019)\", \"Artetxe and Schwenk (2019)\", \"Xue et al. (2021)\", \"Raffel et al., 2020\", \"Ye et al. (2023)\"], \"Citation\": [\"As instruction-tuned large language models (LLMs) gain global adoption, their ability to follow instructions in multiple languages becomes increasingly crucial\", \"Instruction tuning is a fundamental aspect of building modern general-purpose large language models (LLMs), involving fine-tuning a pre-trained model on pairs of instructions and corresponding responses\", \"Cross-lingual transfer has emerged as a promising approach, in which a model is fine-tuned using one language, and acquiring similar abilities in another\", \"We find that replacing even just 40 English training examples with multilingual examples, significantly improves instructionfollowing in those languages\", \"We find that tuning using a few languages enables better performance for languages unseen during tuning, compared to monolingual tuning with the same number of examples\", \"A second trend is that these models often outperform their monolingually-tuned counterparts on the very language the latter were exclusively tuned on\", \"In contrast, Figure 8 suggests this is not necessarily the case for the cross-lingual transfer from a specific language\", \"Our findings across different languages suggest that even monolingual instruction tuning using only one language can result in improved instruction-following capabilities in other languages\", \"We observe a similar trend when exploring the effect of total number of languages in the tuning set, as even splitting the train set to only two languages improves generalization to new languages\", \"Importantly, we unveil the potential of just a small amount of language diversity in the instruction tuning set for this cross-lingual generalization\"]}",
        "eval_response": "Evaluation evidence: The assistant's answer does not align with the gold answer in terms of accuracy and hallucinations. The assistant lists numerous references that are not mentioned in the gold answer, indicating a significant hallucination issue. Additionally, the citations provided by the assistant do not match those in the gold answer, further demonstrating inaccuracies. The assistant's response fails to meet the completeness criterion as it does not accurately reflect the citation relationships specified in the gold answer.\n\nRating: [[10]]",
        "score": 10
    },
    {
        "level": 4,
        "set": 3,
        "length": 119375,
        "type": "paper",
        "language": "en",
        "question": "",
        "instruction": "We kindly ask you to thoroughly review the provided papers and construct a citation chain from them. Please adhere to the following instructions strictly while completing the task:\n\n#Task Instructions:\nGiven several papers, you are required to identify and list the longest citation chain, which demonstrates the citation relationship among the provided papers.\n\n#Specific Requirements:\n1.Please present the titles of the papers in the form of a list, as follows: [\"Title of Paper 1\", \"Title of Paper 2\", ..., \"Title of Paper n\"].\n2.Ensure that the citation chain in the list is linear and continuous, meaning that the first paper title in the list (Paper 1) should not cite any other works. Instead, it should be cited by the next paper in the list (Paper 2); subsequently, each paper should then be cited by the next one in the list, continuing up to the last paper (Paper n).\n3.Consider only the citation relationships within the supplied collection of papers, and ensure that the citation chain accurately reflects the sequential citation order among these documents.\n4.Do not take into account any articles not provided, and disregard other non-linear citation relationships.",
        "prompt_template": "{instruction}\n\n#Papers Provided:\n{docs}",
        "doc": [
            "2010.00453.md",
            "2107.04700.md",
            "2207.06731.md",
            "2405.07628.md"
        ],
        "answer": [
            "## Water is a radiation protection agent for ionised pyrrole",
            "# The unreasonable effectiveness of optimal transport in economics ",
            "# Monotone Comparative Statics for Equilibrium Problems ",
            "# Substitutability, equilibrium transport, and matching models "
        ],
        "shuffle_doc": true,
        "id": "7b842d8d-5550-4777-b6ba-18facdacaa03",
        "prompt": "[Question]\nWe kindly ask you to thoroughly review the provided papers and construct a citation chain from them. Please adhere to the following instructions strictly while completing the task:\n\n#Task Instructions:\nGiven several papers, you are required to identify and list the longest citation chain, which demonstrates the citation relationship among the provided papers.\n\n#Specific Requirements:\n1.Please present the titles of the papers in the form of a list, as follows: [\"Title of Paper 1\", \"Title of Paper 2\", ..., \"Title of Paper n\"].\n2.Ensure that the citation chain in the list is linear and continuous, meaning that the first paper title in the list (Paper 1) should not cite any other works. Instead, it should be cited by the next paper in the list (Paper 2); subsequently, each paper should then be cited by the next one in the list, continuing up to the last paper (Paper n).\n3.Consider only the citation relationships within the supplied collection of papers, and ensure that the citation chain accurately reflects the sequential citation order among these documents.\n4.Do not take into account any articles not provided, and disregard other non-linear citation relationships.\n\n#Papers Provided:\n{docs}\n\n[Gold Answer]\n['## Water is a radiation protection agent for ionised pyrrole', '# The unreasonable effectiveness of optimal transport in economics ', '# Monotone Comparative Statics for Equilibrium Problems ', '# Substitutability, equilibrium transport, and matching models ']\n\n[The Start of Assistant's Predicted Answer]\n[\"Substitutability, equilibrium transport, and matching models\", \"Berry, Gandhi and Haile (2013)\", \"Galichon, Kominers and Weber (2019)\", \"Galichon, Samuelson and Vernet (2022)\", \"Galichon and Lger (2023)\"]\n[The End of Assistant's Predicted Answer]\n\n[System]\nWe would like to request your feedback on the performance of the AI assistant in response to the user question displayed above according to the gold answer. Please use the following listed aspects and their descriptions as evaluation criteria:\n    - Accuracy and Hallucinations: The assistant's answer is semantically consistent with the gold answer; The numerical value and order need to be accurate, and there should be no hallucinations.\n    - Completeness: Referring to the reference answers, the assistant's answer should contain all the key points needed to answer the user's question; further elaboration on these key points can be omitted.\nPlease rate whether this answer is suitable for the question. Please note that the gold answer can be considered as a correct answer to the question.\n\nThe assistant receives an overall score on a scale of 1 to 100, where a higher score indicates better overall performance.\nPlease note that if the assistant's answer and the gold answer fully meet the above criteria, its overall rating should be the full marks (100).\nPlease first provide a comprehensive explanation of your evaluation, avoiding any potential bias.\nThen, output a line indicating the score of the Assistant.\n\nPLEASE OUTPUT WITH THE FOLLOWING FORMAT, WHERE THE SCORE IS A SCALE OF 1 TO 100 BY STRICTLY FOLLOWING THIS FORMAT: \"[[score]]\", FOR EXAMPLE \"Rating: [[100]]\":\n<start output>\nEvaluation evidence: your evluation explanation here, no more than 100 words\nRating: [[score]]\n<end output>\n\nNow, start your evaluation:",
        "generate_response": "[\"Substitutability, equilibrium transport, and matching models\", \"Berry, Gandhi and Haile (2013)\", \"Galichon, Kominers and Weber (2019)\", \"Galichon, Samuelson and Vernet (2022)\", \"Galichon and Lger (2023)\"]",
        "eval_response": "Evaluation evidence: The assistant's answer does not match the gold answer in terms of the titles of the papers listed. The titles in the assistant's response are entirely different and do not appear in the gold answer, indicating a significant accuracy issue and hallucination of content not provided in the task. This discrepancy suggests that the assistant did not correctly identify or construct the citation chain from the provided papers, failing the task requirements.\n\nRating: [[10]]",
        "score": 10
    },
    {
        "level": 3,
        "set": 2,
        "length": 56325,
        "type": "paper",
        "language": "en",
        "question": "DistServe: Disaggregating Prefill and Decoding for Goodput-optimized Large Language Model Serving",
        "instruction": "We hope you will carefully study the provided papers and determine the citation relationships between them. Please follow the instructions below strictly to complete the task:\n\n#Specific Requirements:\n1. Reference: When a given paper mentions other provided papers, those other papers are considered as \"references\" for the given paper. To summarize in this specific context, references are about what the given paper is using.\n2. Citation: Conversely, when other provided papers mention the given paper in their works, the given paper is being \"cited\" by those other papers. To summarize in this specific context, citations are about who is using the given paper.\n3. Given a paper, you need to determine the citation or reference relationship between this paper and the other papers. Do not consider papers that are not provided.\n3. Please present the paper titles in a json format as follows: {{\"Reference\":[\"Reference Title 1\", \"Reference Title 2\", ..., \"Reference Title n\"], \"Citation\":[\"Citation Title 1\", \"Citation Title 2\", ..., \"Citation Title n\"]}}.\n4. If a paper does not have any references or citations, please leave the corresponding list empty, e.g.{{\"Refernce\":[]}}, {{\"Citation\":[]}}.",
        "prompt_template": "#Papers:\n{docs}\n\n{instruction}\n\n#The paper you need to analyze:\n{question}",
        "doc": [
            "2401.09670.md",
            "2405.05465.md",
            "2405.16444.md"
        ],
        "answer": {
            "Reference": [],
            "Citation": [
                "# VIDUR: A LARGE-SCALE SIMULATION FRAMEWORK FOR LLM INFERENCE ",
                "# CacheBlend: Fast Large Language Model Serving for RAG with Cached Knowledge Fusion "
            ]
        },
        "shuffle_doc": true,
        "id": "ad9ca28e-c5db-4686-9200-39efaaf4e0e5",
        "prompt": "[Question]\n#Papers:\n{docs}\n\nWe hope you will carefully study the provided papers and determine the citation relationships between them. Please follow the instructions below strictly to complete the task:\n\n#Specific Requirements:\n1. Reference: When a given paper mentions other provided papers, those other papers are considered as \"references\" for the given paper. To summarize in this specific context, references are about what the given paper is using.\n2. Citation: Conversely, when other provided papers mention the given paper in their works, the given paper is being \"cited\" by those other papers. To summarize in this specific context, citations are about who is using the given paper.\n3. Given a paper, you need to determine the citation or reference relationship between this paper and the other papers. Do not consider papers that are not provided.\n3. Please present the paper titles in a json format as follows: {{\"Reference\":[\"Reference Title 1\", \"Reference Title 2\", ..., \"Reference Title n\"], \"Citation\":[\"Citation Title 1\", \"Citation Title 2\", ..., \"Citation Title n\"]}}.\n4. If a paper does not have any references or citations, please leave the corresponding list empty, e.g.{{\"Refernce\":[]}}, {{\"Citation\":[]}}.\n\n#The paper you need to analyze:\nDistServe: Disaggregating Prefill and Decoding for Goodput-optimized Large Language Model Serving\n\n[Gold Answer]\n{'Reference': [], 'Citation': ['# VIDUR: A LARGE-SCALE SIMULATION FRAMEWORK FOR LLM INFERENCE ', '# CacheBlend: Fast Large Language Model Serving for RAG with Cached Knowledge Fusion ']}\n\n[The Start of Assistant's Predicted Answer]\n{\"Reference\": [\"A Survey on Evaluation of Large Language Models\", \"Vicuna: An Open-Source Chatbot Impressing GPT-4 with $90 \\% *$ ChatGPT Quality\", \"ChatLaw: Open-Source Legal Large Language Model with Integrated External Knowledge Bases\", \"Compositional Semantic Parsing with Large Language Models\", \"Open LLM Leaderboard\", \"How Close is ChatGPT to Human Experts? Comparison Corpus, Evaluation, and Detection\", \"REALM: Retrieval-Augmented Language Model PreTraining\", \"Rethinking with Retrieval: Faithful Large Language Model Inference\", \"Measuring Massive Multitask Language Understanding\", \"Atlas: Few-shot Learning with Retrieval Augmented Language Models\", \"Introducing chatgpt\", \"Bard, an experiment by google\", \"Inflection tech memo\", \"Lanchain usecase: Summarization\", \"Nvidia collective communications library (nccl)\", \"Serve, optimize and scale pytorch models in production\", \"Gqa: Training generalized multi-query transformer models from multi-head checkpoints\", \"A case for disaggregation of $\\\\mathrm{ml}$ data processing\", \"Longbench: A bilingual, multitask benchmark for long context understanding\", \"Evaluating large language models trained on code\", \"Vicuna: An open-source chatbot impressing gpt-4 with $90 \\% *$ chatgpt quality\", \"Compute Express Link Consortium\", \"Fastertransformer\", \"Triton inference server: An optimized cloud and edge inferencing solution.\", \"Flashattention: Fast and memoryefficient exact attention with io-awareness\", \"Turbotransformers: an efficient gpu serving system for transformer models\", \"Serving DNNs like clockwork: Performance predictability from the bottom up\", \"Serving DNNs like clockwork: Performance predictability from the bottom up\", \"Mira: A program-behavior-guided far memory system\", \"Microsecond-scale preemption for concurrent GPU-accelerated DNN inferences\", \"Inference without interference: Disaggregate $11 \\\\mathrm{~m}$ inference for mixed downstream workloads\", \"Gpipe: Efficient training of giant neural networks using pipeline parallelism\", \"Sia: Heterogeneity-aware, goodput-optimized ml-cluster scheduling\", \"Efficient memory management for large language model serving with pagedattention\", \"Efficient memory management for large language model serving with pagedattention\", \"Alpaserve: Statistical multiplexing with model parallelism for deep learning serving\", \"Ray: A distributed framework for emerging AI applications\", \"Pipedream: Generalized pipeline parallelism for dnn training\", \"Gpt-4 technical report\", \"Splitwise: Efficient generative llm inference using phase splitting\", \"Pollux: Co-adaptive cluster scheduling for goodput-optimized deep learning\", \"Zero: Memory optimizations toward training trillion parameter models\", \"Code llama: Open foundation models for code\", \"LegoOS : A disseminated, distributed ${\\\\mathrm{OS}}$ for hardware resource disaggregation\", \"Megatron-lm: Training multi-billion parameter language models\", \"Fundamentals of queueing theory\", \"Djvu: Kv-cache streaming for fast, fault-tolerant generative llm serving\", \"Hotgpt: How to make software documentation more useful with a large language model?\", \"Llama: Open and efficient foundation language models\", \"Fast distributed inference serving for large language models\", \"Orca: A distributed serving system for \\\\{Transformer-Based\\\\} generative models\", \"Shepherd: Serving dnns in the wild\", \"Opt: Open pre-trained transformer language models\", \"Make it real: An end-to-end implementation of a physically disaggregated data center\", \"Ft-cnn: Algorithm-based fault tolerance for convolutional neural networks\", \"Alpa: Automating inter- and Intra-Operator parallelism for distributed deep learning\", \"PetS: A unified framework for ParameterEfficient transformers serving\"], \"Citation\": []}\n[The End of Assistant's Predicted Answer]\n\n[System]\nWe would like to request your feedback on the performance of the AI assistant in response to the user question displayed above according to the gold answer. Please use the following listed aspects and their descriptions as evaluation criteria:\n    - Accuracy and Hallucinations: The assistant's answer is semantically consistent with the gold answer; The numerical value and order need to be accurate, and there should be no hallucinations.\n    - Completeness: Referring to the reference answers, the assistant's answer should contain all the key points needed to answer the user's question; further elaboration on these key points can be omitted.\nPlease rate whether this answer is suitable for the question. Please note that the gold answer can be considered as a correct answer to the question.\n\nThe assistant receives an overall score on a scale of 1 to 100, where a higher score indicates better overall performance.\nPlease note that if the assistant's answer and the gold answer fully meet the above criteria, its overall rating should be the full marks (100).\nPlease first provide a comprehensive explanation of your evaluation, avoiding any potential bias.\nThen, output a line indicating the score of the Assistant.\n\nPLEASE OUTPUT WITH THE FOLLOWING FORMAT, WHERE THE SCORE IS A SCALE OF 1 TO 100 BY STRICTLY FOLLOWING THIS FORMAT: \"[[score]]\", FOR EXAMPLE \"Rating: [[100]]\":\n<start output>\nEvaluation evidence: your evluation explanation here, no more than 100 words\nRating: [[score]]\n<end output>\n\nNow, start your evaluation:",
        "generate_response": "{\"Reference\": [\"A Survey on Evaluation of Large Language Models\", \"Vicuna: An Open-Source Chatbot Impressing GPT-4 with $90 \\% *$ ChatGPT Quality\", \"ChatLaw: Open-Source Legal Large Language Model with Integrated External Knowledge Bases\", \"Compositional Semantic Parsing with Large Language Models\", \"Open LLM Leaderboard\", \"How Close is ChatGPT to Human Experts? Comparison Corpus, Evaluation, and Detection\", \"REALM: Retrieval-Augmented Language Model PreTraining\", \"Rethinking with Retrieval: Faithful Large Language Model Inference\", \"Measuring Massive Multitask Language Understanding\", \"Atlas: Few-shot Learning with Retrieval Augmented Language Models\", \"Introducing chatgpt\", \"Bard, an experiment by google\", \"Inflection tech memo\", \"Lanchain usecase: Summarization\", \"Nvidia collective communications library (nccl)\", \"Serve, optimize and scale pytorch models in production\", \"Gqa: Training generalized multi-query transformer models from multi-head checkpoints\", \"A case for disaggregation of $\\\\mathrm{ml}$ data processing\", \"Longbench: A bilingual, multitask benchmark for long context understanding\", \"Evaluating large language models trained on code\", \"Vicuna: An open-source chatbot impressing gpt-4 with $90 \\% *$ chatgpt quality\", \"Compute Express Link Consortium\", \"Fastertransformer\", \"Triton inference server: An optimized cloud and edge inferencing solution.\", \"Flashattention: Fast and memoryefficient exact attention with io-awareness\", \"Turbotransformers: an efficient gpu serving system for transformer models\", \"Serving DNNs like clockwork: Performance predictability from the bottom up\", \"Serving DNNs like clockwork: Performance predictability from the bottom up\", \"Mira: A program-behavior-guided far memory system\", \"Microsecond-scale preemption for concurrent GPU-accelerated DNN inferences\", \"Inference without interference: Disaggregate $11 \\\\mathrm{~m}$ inference for mixed downstream workloads\", \"Gpipe: Efficient training of giant neural networks using pipeline parallelism\", \"Sia: Heterogeneity-aware, goodput-optimized ml-cluster scheduling\", \"Efficient memory management for large language model serving with pagedattention\", \"Efficient memory management for large language model serving with pagedattention\", \"Alpaserve: Statistical multiplexing with model parallelism for deep learning serving\", \"Ray: A distributed framework for emerging AI applications\", \"Pipedream: Generalized pipeline parallelism for dnn training\", \"Gpt-4 technical report\", \"Splitwise: Efficient generative llm inference using phase splitting\", \"Pollux: Co-adaptive cluster scheduling for goodput-optimized deep learning\", \"Zero: Memory optimizations toward training trillion parameter models\", \"Code llama: Open foundation models for code\", \"LegoOS : A disseminated, distributed ${\\\\mathrm{OS}}$ for hardware resource disaggregation\", \"Megatron-lm: Training multi-billion parameter language models\", \"Fundamentals of queueing theory\", \"Djvu: Kv-cache streaming for fast, fault-tolerant generative llm serving\", \"Hotgpt: How to make software documentation more useful with a large language model?\", \"Llama: Open and efficient foundation language models\", \"Fast distributed inference serving for large language models\", \"Orca: A distributed serving system for \\\\{Transformer-Based\\\\} generative models\", \"Shepherd: Serving dnns in the wild\", \"Opt: Open pre-trained transformer language models\", \"Make it real: An end-to-end implementation of a physically disaggregated data center\", \"Ft-cnn: Algorithm-based fault tolerance for convolutional neural networks\", \"Alpa: Automating inter- and Intra-Operator parallelism for distributed deep learning\", \"PetS: A unified framework for ParameterEfficient transformers serving\"], \"Citation\": []}",
        "eval_response": "Evaluation evidence: The assistant's answer is inaccurate and contains hallucinations. The gold answer indicates that there are no references and two citations for the paper \"DistServe: Disaggregating Prefill and Decoding for Goodput-optimized Large Language Model Serving.\" However, the assistant's answer lists numerous references that are not mentioned in the gold answer and incorrectly states there are no citations. This discrepancy shows a lack of accuracy and the presence of hallucinations in the assistant's response.\n\nRating: [[10]]",
        "score": 10
    },
    {
        "level": 4,
        "set": 2,
        "length": 67651,
        "type": "paper",
        "language": "en",
        "question": "",
        "instruction": "We kindly ask you to thoroughly review the provided papers and construct a citation chain from them. Please adhere to the following instructions strictly while completing the task:\n\n#Task Instructions:\nGiven several papers, you are required to identify and list the longest citation chain, which demonstrates the citation relationship among the provided papers.\n\n#Specific Requirements:\n1.Please present the titles of the papers in the form of a list, as follows: [\"Title of Paper 1\", \"Title of Paper 2\", ..., \"Title of Paper n\"].\n2.Ensure that the citation chain in the list is linear and continuous, meaning that the first paper title in the list (Paper 1) should not cite any other works. Instead, it should be cited by the next paper in the list (Paper 2); subsequently, each paper should then be cited by the next one in the list, continuing up to the last paper (Paper n).\n3.Consider only the citation relationships within the supplied collection of papers, and ensure that the citation chain accurately reflects the sequential citation order among these documents.\n4.Do not take into account any articles not provided, and disregard other non-linear citation relationships.",
        "prompt_template": "{instruction}\n\n#Papers Provided:\n{docs}",
        "doc": [
            "2209.02128.md",
            "2307.10719.md",
            "2308.12950.md",
            "2310.06825.md",
            "2405.12532.md"
        ],
        "answer": [
            "# EvaluATING THE SUSCEPTIBILITY OF PRE-TRAINED LANGUAGE MODELS VIA HANDCRAFTED ADVERSARIAL EXAMPLES ",
            "# LLM Censorship: A Machine LEarning Challenge OR A COMPUTER SECURITY PROBLEM? ",
            "# Code Llama: Open Foundation Models for Code ",
            "# Mistral 7B ",
            "# PyramidInfer: Pyramid KV Cache Compression for High-throughput LLM Inference "
        ],
        "shuffle_doc": true,
        "id": "d6c89a48-9f1e-42d3-9a07-ed757ca5dd50",
        "prompt": "[Question]\nWe kindly ask you to thoroughly review the provided papers and construct a citation chain from them. Please adhere to the following instructions strictly while completing the task:\n\n#Task Instructions:\nGiven several papers, you are required to identify and list the longest citation chain, which demonstrates the citation relationship among the provided papers.\n\n#Specific Requirements:\n1.Please present the titles of the papers in the form of a list, as follows: [\"Title of Paper 1\", \"Title of Paper 2\", ..., \"Title of Paper n\"].\n2.Ensure that the citation chain in the list is linear and continuous, meaning that the first paper title in the list (Paper 1) should not cite any other works. Instead, it should be cited by the next paper in the list (Paper 2); subsequently, each paper should then be cited by the next one in the list, continuing up to the last paper (Paper n).\n3.Consider only the citation relationships within the supplied collection of papers, and ensure that the citation chain accurately reflects the sequential citation order among these documents.\n4.Do not take into account any articles not provided, and disregard other non-linear citation relationships.\n\n#Papers Provided:\n{docs}\n\n[Gold Answer]\n['# EvaluATING THE SUSCEPTIBILITY OF PRE-TRAINED LANGUAGE MODELS VIA HANDCRAFTED ADVERSARIAL EXAMPLES ', '# LLM Censorship: A Machine LEarning Challenge OR A COMPUTER SECURITY PROBLEM? ', '# Code Llama: Open Foundation Models for Code ', '# Mistral 7B ', '# PyramidInfer: Pyramid KV Cache Compression for High-throughput LLM Inference ']\n\n[The Start of Assistant's Predicted Answer]\n[\"LLM Censorship: A Machine LEarning Challenge OR A COMPUTER SECURITY PROBLEM?\", \"EvaluATING THE SUSCEPTIBILITY OF PRE-TRAINED LANGUAGE MODELS VIA HANDCRAFTED ADVERSARIAL EXAMPLES\"]\n[The End of Assistant's Predicted Answer]\n\n[System]\nWe would like to request your feedback on the performance of the AI assistant in response to the user question displayed above according to the gold answer. Please use the following listed aspects and their descriptions as evaluation criteria:\n    - Accuracy and Hallucinations: The assistant's answer is semantically consistent with the gold answer; The numerical value and order need to be accurate, and there should be no hallucinations.\n    - Completeness: Referring to the reference answers, the assistant's answer should contain all the key points needed to answer the user's question; further elaboration on these key points can be omitted.\nPlease rate whether this answer is suitable for the question. Please note that the gold answer can be considered as a correct answer to the question.\n\nThe assistant receives an overall score on a scale of 1 to 100, where a higher score indicates better overall performance.\nPlease note that if the assistant's answer and the gold answer fully meet the above criteria, its overall rating should be the full marks (100).\nPlease first provide a comprehensive explanation of your evaluation, avoiding any potential bias.\nThen, output a line indicating the score of the Assistant.\n\nPLEASE OUTPUT WITH THE FOLLOWING FORMAT, WHERE THE SCORE IS A SCALE OF 1 TO 100 BY STRICTLY FOLLOWING THIS FORMAT: \"[[score]]\", FOR EXAMPLE \"Rating: [[100]]\":\n<start output>\nEvaluation evidence: your evluation explanation here, no more than 100 words\nRating: [[score]]\n<end output>\n\nNow, start your evaluation:",
        "generate_response": "[\"LLM Censorship: A Machine LEarning Challenge OR A COMPUTER SECURITY PROBLEM?\", \"EvaluATING THE SUSCEPTIBILITY OF PRE-TRAINED LANGUAGE MODELS VIA HANDCRAFTED ADVERSARIAL EXAMPLES\"]",
        "eval_response": "Evaluation evidence: The assistant's answer does not match the gold answer in terms of the sequence and completeness of the citation chain. The gold answer lists a specific sequence of five paper titles, forming a complete citation chain. In contrast, the assistant's answer only includes two titles and inverts their order, breaking the continuity and accuracy required for the task. This discrepancy indicates a significant error in understanding and executing the task instructions, leading to both inaccuracies and incompleteness.\n\nRating: [[20]]",
        "score": 20
    },
    {
        "level": 3,
        "set": 3,
        "length": 101540,
        "type": "paper",
        "language": "en",
        "question": "Self-Discover: Large Language Models Self-Compose Reasoning Structures",
        "instruction": "We hope you will carefully study the provided papers and determine the citation relationships between them. Please follow the instructions below strictly to complete the task:\n\n#Specific Requirements:\n1. Reference: When a given paper mentions other provided papers, those other papers are considered as \"references\" for the given paper. To summarize in this specific context, references are about what the given paper is using.\n2. Citation: Conversely, when other provided papers mention the given paper in their works, the given paper is being \"cited\" by those other papers. To summarize in this specific context, citations are about who is using the given paper.\n3. Given a paper, you need to determine the citation or reference relationship between this paper and the other papers. Do not consider papers that are not provided.\n3. Please present the paper titles in a json format as follows: {{\"Reference\":[\"Reference Title 1\", \"Reference Title 2\", ..., \"Reference Title n\"], \"Citation\":[\"Citation Title 1\", \"Citation Title 2\", ..., \"Citation Title n\"]}}.\n4. If a paper does not have any references or citations, please leave the corresponding list empty, e.g.{{\"Refernce\":[]}}, {{\"Citation\":[]}}.",
        "prompt_template": "#Papers:\n{docs}\n\n{instruction}\n\n#The paper you need to analyze:\n{question}",
        "doc": [
            "2402.03620.md",
            "2402.15205.md",
            "2405.14804.md",
            "2310.06117.md",
            "2310.14628.md"
        ],
        "answer": {
            "Reference": [
                "# TAKE A STEP BACK: EVOKING REASONING VIA ABSTRACTION IN LARGE LANGUAGE MODELS ",
                "# Plan, Verify and Switch: Integrated Reasoning with Diverse X-of-Thoughts "
            ],
            "Citation": [
                "# Enhancing ICU Patient Recovery: Using LLMs to Assist Nurses in Diary Writing ",
                "# Can LLMs Solve Longer Math Word Problems Better? "
            ]
        },
        "shuffle_doc": true,
        "id": "8bbc51fb-0c0f-4625-a895-3809a8d0265c",
        "prompt": "[Question]\n#Papers:\n{docs}\n\nWe hope you will carefully study the provided papers and determine the citation relationships between them. Please follow the instructions below strictly to complete the task:\n\n#Specific Requirements:\n1. Reference: When a given paper mentions other provided papers, those other papers are considered as \"references\" for the given paper. To summarize in this specific context, references are about what the given paper is using.\n2. Citation: Conversely, when other provided papers mention the given paper in their works, the given paper is being \"cited\" by those other papers. To summarize in this specific context, citations are about who is using the given paper.\n3. Given a paper, you need to determine the citation or reference relationship between this paper and the other papers. Do not consider papers that are not provided.\n3. Please present the paper titles in a json format as follows: {{\"Reference\":[\"Reference Title 1\", \"Reference Title 2\", ..., \"Reference Title n\"], \"Citation\":[\"Citation Title 1\", \"Citation Title 2\", ..., \"Citation Title n\"]}}.\n4. If a paper does not have any references or citations, please leave the corresponding list empty, e.g.{{\"Refernce\":[]}}, {{\"Citation\":[]}}.\n\n#The paper you need to analyze:\nSelf-Discover: Large Language Models Self-Compose Reasoning Structures\n\n[Gold Answer]\n{'Reference': ['# TAKE A STEP BACK: EVOKING REASONING VIA ABSTRACTION IN LARGE LANGUAGE MODELS ', '# Plan, Verify and Switch: Integrated Reasoning with Diverse X-of-Thoughts '], 'Citation': ['# Enhancing ICU Patient Recovery: Using LLMs to Assist Nurses in Diary Writing ', '# Can LLMs Solve Longer Math Word Problems Better? ']}\n\n[The Start of Assistant's Predicted Answer]\n{\"Reference\": [\"BIG-Bench Hard (BBH) (Suzgun et al., 2022)\", \"Thinking for Doing (T4D) (Zhou et al., 2023)\", \"MATH (Hendrycks et al., 2021)\", \"Plan-and-Solve (Wang et al., 2023)\", \"CoT-Self-Consistency (Wang et al., 2022)\", \"LLMs as optimizers (OPRO) (Yang et al., 2023)\", \"Promptbreeder: Self-referential self-improvement via prompt evolution (Fernando et al., 2023)\", \"Strategyllm: Large language models as strategy generators, executors, optimizers, and evaluators for problem solving (Gao et al., 2023a)\", \"Reasoning with language model is planning with world model (Hao et al., 2023)\", \"Plan, verify and switch: Integrated reasoning with diverse x-of-thoughts (Liu et al., 2023)\", \"HELP ME THINK: A simple prompting strategy for non-experts to create customized content with models (Mishra and Nouri, 2023)\", \"Lila: A unified benchmark for mathematical reasoning (Mishra et al., 2022a)\", \"Crosstask generalization via natural language crowdsourcing instructions (Mishra et al., 2022c)\", \"Show your work: Scratchpads for intermediate computation with language models (Nye et al., 2021)\", \"Chatgpt: Optimizing language models for dialogue (OpenAI, 2022)\", \"Json generation mode (OpenAI, 2023a)\", \"Gpt-4 technical report (OpenAI, 2023b)\", \"Is a question decomposition unit all we need? (Patel et al., 2022)\", \"Beyond the imitation game: Quantifying and extrapolating the capabilities of language models (Srivastava et al., 2023)\", \"Beyond chain-of-thought, effective graph-of-thought reasoning in large language models (Yao et al., 2023b)\", \"Large language models as analogical reasoners (Yasunaga et al., 2023)\", \"Take a step back: Evoking reasoning via abstraction in large language models (Zheng et al., 2023)\", \"Adapting language models for zero-shot learning by metatuning on dataset and prompt collections (Zhong et al., 2021)\", \"Least-to-most prompting enables complex reasoning in large language models (Zhou et al., 2022a)\", \"How far are large language models from agents with theory-of-mind? (Zhou et al., 2023)\", \"Large language models are humanlevel prompt engineers (Zhou et al., 2022b)\", \"Elements of a theory of human problem solving (Newell et al., 1958)\", \"Skills, rules, and knowledge; signals, signs, and symbols, and other distinctions in human performance models (Rasmussen, 1983)\", \"How to solve it: A new aspect of mathematical method (Polya, 2004)\", \"Attention is all you need (Vaswani et al., 2017)\", \"Palm 2 technical report (Anil et al., 2023)\", \"Graph of thoughts: Solving elaborate problems with large language models (Besta et al., 2023)\", \"Skills-in-context prompting: Unlocking compositionality in large language models (Chen et al., 2023)\", \"Program of thoughts prompting: Disentangling computation from reasoning for numerical reasoning tasks (Chen et al., 2022)\", \"Branch-solve-merge improves large language model evaluation and generation (Saha et al., 2023)\"], \"Citation\": [\"Language models are few-shot learners (Brown et al., 2020)\", \"Palm: Scaling language modeling with pathways (Chowdhery et al., 2022)\", \"Chatgpt: Optimizing language models for dialogue (OpenAI, 2022)\", \"Crosstask generalization via natural language crowdsourcing instructions (Mishra et al., 2022c)\", \"Refining instructional prompts to gptk's language (Mishra et al., 2022b)\", \"Training language models to follow instructions with human feedback (Ouyang et al., 2022)\", \"Chain-of-thought prompting elicits reasoning in large language models (Wei et al., 2022)\", \"Least-to-most prompting enables complex reasoning in large language models (Zhou et al., 2022a)\", \"Decomposing prompts: A modular approach for solving complex tasks (Khot et al., 2022)\", \"Compositional semantic parsing with large language models (Drozdov et al., 2022)\", \"A Survey on Evaluation of Large Language Models (Chang et al., 2023)\", \"Vicuna: An Open-Source Chatbot Impressing GPT-4 with $90 \\% *$ ChatGPT Quality (Chiang et al., 2023)\", \"ChatLaw: Open-Source Legal Large Language Model with Integrated External Knowledge Bases (Cui et al., 2023)\", \"Compositional Semantic Parsing with Large Language Models (Drozdov et al., 2023)\", \"Open LLM Leaderboard (Edward Beeching et al., 2023)\", \"How Close is ChatGPT to Human Experts? Comparison Corpus, Evaluation, and Detection (Guo et al., 2023)\", \"REALM: Retrieval-Augmented Language Model PreTraining (Guu et al., 2020)\", \"Rethinking with Retrieval: Faithful Large Language Model Inference (He et al., 2022)\", \"Measuring Massive Multitask Language Understanding (Hendrycks et al., 2021)\", \"Atlas: Few-shot Learning with Retrieval Augmented Language Models (Izacard et al., 2022)\"]}}\n[The End of Assistant's Predicted Answer]\n\n[System]\nWe would like to request your feedback on the performance of the AI assistant in response to the user question displayed above according to the gold answer. Please use the following listed aspects and their descriptions as evaluation criteria:\n    - Accuracy and Hallucinations: The assistant's answer is semantically consistent with the gold answer; The numerical value and order need to be accurate, and there should be no hallucinations.\n    - Completeness: Referring to the reference answers, the assistant's answer should contain all the key points needed to answer the user's question; further elaboration on these key points can be omitted.\nPlease rate whether this answer is suitable for the question. Please note that the gold answer can be considered as a correct answer to the question.\n\nThe assistant receives an overall score on a scale of 1 to 100, where a higher score indicates better overall performance.\nPlease note that if the assistant's answer and the gold answer fully meet the above criteria, its overall rating should be the full marks (100).\nPlease first provide a comprehensive explanation of your evaluation, avoiding any potential bias.\nThen, output a line indicating the score of the Assistant.\n\nPLEASE OUTPUT WITH THE FOLLOWING FORMAT, WHERE THE SCORE IS A SCALE OF 1 TO 100 BY STRICTLY FOLLOWING THIS FORMAT: \"[[score]]\", FOR EXAMPLE \"Rating: [[100]]\":\n<start output>\nEvaluation evidence: your evluation explanation here, no more than 100 words\nRating: [[score]]\n<end output>\n\nNow, start your evaluation:",
        "generate_response": "{\"Reference\": [\"BIG-Bench Hard (BBH) (Suzgun et al., 2022)\", \"Thinking for Doing (T4D) (Zhou et al., 2023)\", \"MATH (Hendrycks et al., 2021)\", \"Plan-and-Solve (Wang et al., 2023)\", \"CoT-Self-Consistency (Wang et al., 2022)\", \"LLMs as optimizers (OPRO) (Yang et al., 2023)\", \"Promptbreeder: Self-referential self-improvement via prompt evolution (Fernando et al., 2023)\", \"Strategyllm: Large language models as strategy generators, executors, optimizers, and evaluators for problem solving (Gao et al., 2023a)\", \"Reasoning with language model is planning with world model (Hao et al., 2023)\", \"Plan, verify and switch: Integrated reasoning with diverse x-of-thoughts (Liu et al., 2023)\", \"HELP ME THINK: A simple prompting strategy for non-experts to create customized content with models (Mishra and Nouri, 2023)\", \"Lila: A unified benchmark for mathematical reasoning (Mishra et al., 2022a)\", \"Crosstask generalization via natural language crowdsourcing instructions (Mishra et al., 2022c)\", \"Show your work: Scratchpads for intermediate computation with language models (Nye et al., 2021)\", \"Chatgpt: Optimizing language models for dialogue (OpenAI, 2022)\", \"Json generation mode (OpenAI, 2023a)\", \"Gpt-4 technical report (OpenAI, 2023b)\", \"Is a question decomposition unit all we need? (Patel et al., 2022)\", \"Beyond the imitation game: Quantifying and extrapolating the capabilities of language models (Srivastava et al., 2023)\", \"Beyond chain-of-thought, effective graph-of-thought reasoning in large language models (Yao et al., 2023b)\", \"Large language models as analogical reasoners (Yasunaga et al., 2023)\", \"Take a step back: Evoking reasoning via abstraction in large language models (Zheng et al., 2023)\", \"Adapting language models for zero-shot learning by metatuning on dataset and prompt collections (Zhong et al., 2021)\", \"Least-to-most prompting enables complex reasoning in large language models (Zhou et al., 2022a)\", \"How far are large language models from agents with theory-of-mind? (Zhou et al., 2023)\", \"Large language models are humanlevel prompt engineers (Zhou et al., 2022b)\", \"Elements of a theory of human problem solving (Newell et al., 1958)\", \"Skills, rules, and knowledge; signals, signs, and symbols, and other distinctions in human performance models (Rasmussen, 1983)\", \"How to solve it: A new aspect of mathematical method (Polya, 2004)\", \"Attention is all you need (Vaswani et al., 2017)\", \"Palm 2 technical report (Anil et al., 2023)\", \"Graph of thoughts: Solving elaborate problems with large language models (Besta et al., 2023)\", \"Skills-in-context prompting: Unlocking compositionality in large language models (Chen et al., 2023)\", \"Program of thoughts prompting: Disentangling computation from reasoning for numerical reasoning tasks (Chen et al., 2022)\", \"Branch-solve-merge improves large language model evaluation and generation (Saha et al., 2023)\"], \"Citation\": [\"Language models are few-shot learners (Brown et al., 2020)\", \"Palm: Scaling language modeling with pathways (Chowdhery et al., 2022)\", \"Chatgpt: Optimizing language models for dialogue (OpenAI, 2022)\", \"Crosstask generalization via natural language crowdsourcing instructions (Mishra et al., 2022c)\", \"Refining instructional prompts to gptk's language (Mishra et al., 2022b)\", \"Training language models to follow instructions with human feedback (Ouyang et al., 2022)\", \"Chain-of-thought prompting elicits reasoning in large language models (Wei et al., 2022)\", \"Least-to-most prompting enables complex reasoning in large language models (Zhou et al., 2022a)\", \"Decomposing prompts: A modular approach for solving complex tasks (Khot et al., 2022)\", \"Compositional semantic parsing with large language models (Drozdov et al., 2022)\", \"A Survey on Evaluation of Large Language Models (Chang et al., 2023)\", \"Vicuna: An Open-Source Chatbot Impressing GPT-4 with $90 \\% *$ ChatGPT Quality (Chiang et al., 2023)\", \"ChatLaw: Open-Source Legal Large Language Model with Integrated External Knowledge Bases (Cui et al., 2023)\", \"Compositional Semantic Parsing with Large Language Models (Drozdov et al., 2023)\", \"Open LLM Leaderboard (Edward Beeching et al., 2023)\", \"How Close is ChatGPT to Human Experts? Comparison Corpus, Evaluation, and Detection (Guo et al., 2023)\", \"REALM: Retrieval-Augmented Language Model PreTraining (Guu et al., 2020)\", \"Rethinking with Retrieval: Faithful Large Language Model Inference (He et al., 2022)\", \"Measuring Massive Multitask Language Understanding (Hendrycks et al., 2021)\", \"Atlas: Few-shot Learning with Retrieval Augmented Language Models (Izacard et al., 2022)\"]}}",
        "eval_response": "Evaluation evidence: The assistant's response does not match the gold answer in terms of the specific papers listed as references and citations. The titles mentioned in the assistant's response are entirely different from those in the gold answer, indicating a significant accuracy issue and hallucination of content not present in the gold answer. This discrepancy suggests that the assistant did not correctly identify the citation relationships for the specified paper, leading to a failure in both accuracy and completeness.\n\nRating: [[10]]",
        "score": 10
    },
    {
        "level": 3,
        "set": 2,
        "length": 86433,
        "type": "paper",
        "language": "en",
        "question": "CHAIN-OF-TABLE: EVOLVING TABLES IN THE REASONING CHAIN FOR TABLE UNDERSTANDING",
        "instruction": "We hope you will carefully study the provided papers and determine the citation relationships between them. Please follow the instructions below strictly to complete the task:\n\n#Specific Requirements:\n1. Reference: When a given paper mentions other provided papers, those other papers are considered as \"references\" for the given paper. To summarize in this specific context, references are about what the given paper is using.\n2. Citation: Conversely, when other provided papers mention the given paper in their works, the given paper is being \"cited\" by those other papers. To summarize in this specific context, citations are about who is using the given paper.\n3. Given a paper, you need to determine the citation or reference relationship between this paper and the other papers. Do not consider papers that are not provided.\n3. Please present the paper titles in a json format as follows: {{\"Reference\":[\"Reference Title 1\", \"Reference Title 2\", ..., \"Reference Title n\"], \"Citation\":[\"Citation Title 1\", \"Citation Title 2\", ..., \"Citation Title n\"]}}.\n4. If a paper does not have any references or citations, please leave the corresponding list empty, e.g.{{\"Refernce\":[]}}, {{\"Citation\":[]}}.",
        "prompt_template": "#Papers:\n{docs}\n\n{instruction}\n\n#The paper you need to analyze:\n{question}",
        "doc": [
            "2401.04398.md",
            "2404.10150.md",
            "2404.15676.md",
            "2305.17812.md",
            "2307.03172.md"
        ],
        "answer": {
            "Reference": [
                "# Tab-CoT: Zero-shot Tabular Chain of Thought ",
                "# Lost in the Middle: How Language Models Use Long Contexts "
            ],
            "Citation": [
                "# TabSQLify: Enhancing Reasoning Capabilities of LLMs Through Table Decomposition ",
                "# Beyond Chain-of-Thought: A Survey of Chain-of-X Paradigms for LLMs "
            ]
        },
        "shuffle_doc": true,
        "id": "e26e9bdb-815f-4747-a63f-d367904338e2",
        "prompt": "[Question]\n#Papers:\n{docs}\n\nWe hope you will carefully study the provided papers and determine the citation relationships between them. Please follow the instructions below strictly to complete the task:\n\n#Specific Requirements:\n1. Reference: When a given paper mentions other provided papers, those other papers are considered as \"references\" for the given paper. To summarize in this specific context, references are about what the given paper is using.\n2. Citation: Conversely, when other provided papers mention the given paper in their works, the given paper is being \"cited\" by those other papers. To summarize in this specific context, citations are about who is using the given paper.\n3. Given a paper, you need to determine the citation or reference relationship between this paper and the other papers. Do not consider papers that are not provided.\n3. Please present the paper titles in a json format as follows: {{\"Reference\":[\"Reference Title 1\", \"Reference Title 2\", ..., \"Reference Title n\"], \"Citation\":[\"Citation Title 1\", \"Citation Title 2\", ..., \"Citation Title n\"]}}.\n4. If a paper does not have any references or citations, please leave the corresponding list empty, e.g.{{\"Refernce\":[]}}, {{\"Citation\":[]}}.\n\n#The paper you need to analyze:\nCHAIN-OF-TABLE: EVOLVING TABLES IN THE REASONING CHAIN FOR TABLE UNDERSTANDING\n\n[Gold Answer]\n{'Reference': ['# Tab-CoT: Zero-shot Tabular Chain of Thought ', '# Lost in the Middle: How Language Models Use Long Contexts '], 'Citation': ['# TabSQLify: Enhancing Reasoning Capabilities of LLMs Through Table Decomposition ', '# Beyond Chain-of-Thought: A Survey of Chain-of-X Paradigms for LLMs ']}\n\n[The Start of Assistant's Predicted Answer]\n{\"Reference\": [\"Table-based reasoning with large language models\", \"WikiTQ: A dataset for table-based question answering\", \"FeTaQA: Free-form table question answering\", \"TabFact: A large-scale dataset for table-based fact verification\", \"TaPas: Weakly supervised table parsing via pre-training\", \"TAPEX: Table pre-training via learning a neural SQL executor\", \"Chain-of-Thought prompting elicits reasoning in large language models\", \"Text-to-SQL: Evaluating the text-to-sql capabilities of large language models\", \"Binder: Binding language models in symbolic languages\", \"Dater: Large language models are versatile decomposers\", \"A survey on table question answering: recent advances\", \"A survey on deep learning approaches for text-to-sql\", \"Llama 2: Open foundation and fine-tuned chat models\", \"Chain-of-Thought: A modular approach for solving complex tasks\", \"LEVER: Learning to verify language-to-code generation with execution\", \"Program-aided Language Models (PAL): Mathematical reasoning using large language models\", \"Handling divergent reference texts when evaluating table-to-text generation\", \"Understanding tables with intermediate pre-training\", \"Program-aided language models\", \"Decomposed prompting: A modular approach for solving complex tasks\", \"Large language models are zero-shot reasoners\", \"ROUGE: A package for automatic evaluation of summaries\", \"From zero to hero: Examining the power of symbolic tasks in instruction tuning\", \"Benchmarking large language model capabilities for conditional generation\", \"Large language models still can't plan (a benchmark for llms on planning and reasoning about change)\", \"Tree of thoughts: Deliberate problem solving with large language models\", \"Least-to-most prompting enables complex reasoning in large language models\", \"PASTA: Tableoperations aware fact verification via sentence-table cloze pre-training\", \"Table-to-text generation and pre-training with TabT5\", \"Webtables: Exploring the power of tables on the web\", \"BERT: Pre-training of deep bidirectional transformers for language understanding\", \"Lost in the middle: How language models use long contexts\", \"Evolving Table Design for Prompting Large Language Models\"], \"Citation\": [\"TabSQLify: Enhancing Reasoning Capabilities of LLMs Through Table Decomposition\", \"Beyond Chain-of-Thought: A Survey of Chain-of-X Paradigms for LLMs\"]}\n[The End of Assistant's Predicted Answer]\n\n[System]\nWe would like to request your feedback on the performance of the AI assistant in response to the user question displayed above according to the gold answer. Please use the following listed aspects and their descriptions as evaluation criteria:\n    - Accuracy and Hallucinations: The assistant's answer is semantically consistent with the gold answer; The numerical value and order need to be accurate, and there should be no hallucinations.\n    - Completeness: Referring to the reference answers, the assistant's answer should contain all the key points needed to answer the user's question; further elaboration on these key points can be omitted.\nPlease rate whether this answer is suitable for the question. Please note that the gold answer can be considered as a correct answer to the question.\n\nThe assistant receives an overall score on a scale of 1 to 100, where a higher score indicates better overall performance.\nPlease note that if the assistant's answer and the gold answer fully meet the above criteria, its overall rating should be the full marks (100).\nPlease first provide a comprehensive explanation of your evaluation, avoiding any potential bias.\nThen, output a line indicating the score of the Assistant.\n\nPLEASE OUTPUT WITH THE FOLLOWING FORMAT, WHERE THE SCORE IS A SCALE OF 1 TO 100 BY STRICTLY FOLLOWING THIS FORMAT: \"[[score]]\", FOR EXAMPLE \"Rating: [[100]]\":\n<start output>\nEvaluation evidence: your evluation explanation here, no more than 100 words\nRating: [[score]]\n<end output>\n\nNow, start your evaluation:",
        "generate_response": "{\"Reference\": [\"Table-based reasoning with large language models\", \"WikiTQ: A dataset for table-based question answering\", \"FeTaQA: Free-form table question answering\", \"TabFact: A large-scale dataset for table-based fact verification\", \"TaPas: Weakly supervised table parsing via pre-training\", \"TAPEX: Table pre-training via learning a neural SQL executor\", \"Chain-of-Thought prompting elicits reasoning in large language models\", \"Text-to-SQL: Evaluating the text-to-sql capabilities of large language models\", \"Binder: Binding language models in symbolic languages\", \"Dater: Large language models are versatile decomposers\", \"A survey on table question answering: recent advances\", \"A survey on deep learning approaches for text-to-sql\", \"Llama 2: Open foundation and fine-tuned chat models\", \"Chain-of-Thought: A modular approach for solving complex tasks\", \"LEVER: Learning to verify language-to-code generation with execution\", \"Program-aided Language Models (PAL): Mathematical reasoning using large language models\", \"Handling divergent reference texts when evaluating table-to-text generation\", \"Understanding tables with intermediate pre-training\", \"Program-aided language models\", \"Decomposed prompting: A modular approach for solving complex tasks\", \"Large language models are zero-shot reasoners\", \"ROUGE: A package for automatic evaluation of summaries\", \"From zero to hero: Examining the power of symbolic tasks in instruction tuning\", \"Benchmarking large language model capabilities for conditional generation\", \"Large language models still can't plan (a benchmark for llms on planning and reasoning about change)\", \"Tree of thoughts: Deliberate problem solving with large language models\", \"Least-to-most prompting enables complex reasoning in large language models\", \"PASTA: Tableoperations aware fact verification via sentence-table cloze pre-training\", \"Table-to-text generation and pre-training with TabT5\", \"Webtables: Exploring the power of tables on the web\", \"BERT: Pre-training of deep bidirectional transformers for language understanding\", \"Lost in the middle: How language models use long contexts\", \"Evolving Table Design for Prompting Large Language Models\"], \"Citation\": [\"TabSQLify: Enhancing Reasoning Capabilities of LLMs Through Table Decomposition\", \"Beyond Chain-of-Thought: A Survey of Chain-of-X Paradigms for LLMs\"]}",
        "eval_response": "Evaluation evidence: The assistant's answer does not match the gold answer in terms of the references and citations listed. The assistant's response includes a large number of hallucinated titles that are not present in the gold answer, indicating a significant accuracy issue and the presence of hallucinations. The response fails to accurately capture the specific references and citations related to the paper in question, leading to a lack of completeness.\n\nRating: [[20]]",
        "score": 20
    },
    {
        "level": 3,
        "set": 1,
        "length": 47774,
        "type": "paper",
        "language": "en",
        "question": "AUTORT: EMBODIED FOUNDATION MODELS FOR LARGE SCALE ORCHESTRATION OF ROBOTIC AGENTS",
        "instruction": "We hope you will carefully study the provided papers and determine the citation relationships between them. Please follow the instructions below strictly to complete the task:\n\n#Specific Requirements:\n1. Reference: When a given paper mentions other provided papers, those other papers are considered as \"references\" for the given paper. To summarize in this specific context, references are about what the given paper is using.\n2. Citation: Conversely, when other provided papers mention the given paper in their works, the given paper is being \"cited\" by those other papers. To summarize in this specific context, citations are about who is using the given paper.\n3. Given a paper, you need to determine the citation or reference relationship between this paper and the other papers. Do not consider papers that are not provided.\n3. Please present the paper titles in a json format as follows: {{\"Reference\":[\"Reference Title 1\", \"Reference Title 2\", ..., \"Reference Title n\"], \"Citation\":[\"Citation Title 1\", \"Citation Title 2\", ..., \"Citation Title n\"]}}.\n4. If a paper does not have any references or citations, please leave the corresponding list empty, e.g.{{\"Refernce\":[]}}, {{\"Citation\":[]}}.",
        "prompt_template": "#Papers:\n{docs}\n\n{instruction}\n\n#The paper you need to analyze:\n{question}",
        "doc": [
            "2401.12963.md",
            "2405.11537.md",
            "2307.15818.md"
        ],
        "answer": {
            "Reference": [
                "# RT-2: Vision-Language-Action Models Transfer Web Knowledge to Robotic Control "
            ],
            "Citation": [
                "# VR-GPT: Visual Language Model for Intelligent Virtual Reality Applications "
            ]
        },
        "shuffle_doc": true,
        "id": "2fb7e717-7267-4727-832f-384483046ff3",
        "prompt": "[Question]\n#Papers:\n{docs}\n\nWe hope you will carefully study the provided papers and determine the citation relationships between them. Please follow the instructions below strictly to complete the task:\n\n#Specific Requirements:\n1. Reference: When a given paper mentions other provided papers, those other papers are considered as \"references\" for the given paper. To summarize in this specific context, references are about what the given paper is using.\n2. Citation: Conversely, when other provided papers mention the given paper in their works, the given paper is being \"cited\" by those other papers. To summarize in this specific context, citations are about who is using the given paper.\n3. Given a paper, you need to determine the citation or reference relationship between this paper and the other papers. Do not consider papers that are not provided.\n3. Please present the paper titles in a json format as follows: {{\"Reference\":[\"Reference Title 1\", \"Reference Title 2\", ..., \"Reference Title n\"], \"Citation\":[\"Citation Title 1\", \"Citation Title 2\", ..., \"Citation Title n\"]}}.\n4. If a paper does not have any references or citations, please leave the corresponding list empty, e.g.{{\"Refernce\":[]}}, {{\"Citation\":[]}}.\n\n#The paper you need to analyze:\nAUTORT: EMBODIED FOUNDATION MODELS FOR LARGE SCALE ORCHESTRATION OF ROBOTIC AGENTS\n\n[Gold Answer]\n{'Reference': ['# RT-2: Vision-Language-Action Models Transfer Web Knowledge to Robotic Control '], 'Citation': ['# VR-GPT: Visual Language Model for Intelligent Virtual Reality Applications ']}\n\n[The Start of Assistant's Predicted Answer]\n{\"Reference\":[\"Do as i can and not as i say: Grounding language in robotic affordances\", \"How to prompt your robot: A promptbook for manipulation skills with code as policies\", \"Runaround\", \"Constitutional ai: Harmlessness from ai feedback\", \"Robocat: A self-improving foundation agent for robotic manipulation\", \"RT-1: Robotics transformer for real-world control at scale\", \"RT2: Vision-language-action models transfer web knowledge to robotic control\", \"Universal sentence encoder\", \"Open-vocabulary queryable scene representations for real world planning\", \"Pali: A jointly-scaled multilingual language-image model\", \"Leveraging procedural generation to benchmark reinforcement learning\", \"Robonet: Large-scale multi-robot learning\", \"Palm-e: An embodied multimodal language model\", \"Visual foresight: Model-based deep reinforcement learning for vision-based robotic control\", \"Physically grounded vision-language models for robotic manipulation\", \"Robot learning in homes: Improving generalization and reducing dataset bias\", \"Fleet-dagger: Interactive robot fleet learning with scalable human supervision\", \"BC-z: Zero-shot task generalization with robotic imitation learning\", \"Qt-opt: Scalable deep reinforcement learning for vision-based robotic manipulation\", \"Mt-opt: Continuous multi-task robotic reinforcement learning at scale\", \"Hgdagger: Interactive imitation learning with human experts\", \"Learning hand-eye coordination for robotic grasping with deep learning and large-scale data collection\", \"Code as policies: Language model programs for embodied control\", \"Interactive language: Talking to robots in real time\", \"Scaling robot supervision to hundreds of hours with roboturk: Robotic manipulation dataset through human reasoning and dexterity\", \"Grounding language with visual affordances over unstructured data\", \"Generative agents: Interactive simulacra of human behavior\", \"Supersizing self-supervision: Learning to grasp from 50k tries and 700 robot hours\", \"Grasp learning: Models, methods, and performance\", \"Sayplan: Grounding large language models using 3d scene graphs for scalable task planning\", \"Flexcap: Generating rich, localized, and flexible captions in images\", \"A reduction of imitation learning and structured prediction to no-regret online learning\", \"Multiple interactions made easy (mime): Large scale demonstrations data for imitation\", \"Reflexion: Language agents with verbal reinforcement learning\", \"D4: Improving llm pretraining via document de-duplication and diversification\", \"Chatgpt for robotics: Design principles and model abilities\", \"Voyager: An open-ended embodied agent with large language models\", \"Towards a foundation model for generalist robots: Diverse skill learning at scale via automated task and scene generation\", \"Robotic skill acquistion via instruction augmentation with visionlanguage models\", \"React: Synergizing reasoning and acting in language models\", \"More than a million ways to be pushed: A high-fidelity experimental dataset of planar pushing\"], \"Citation\":[]}\n[The End of Assistant's Predicted Answer]\n\n[System]\nWe would like to request your feedback on the performance of the AI assistant in response to the user question displayed above according to the gold answer. Please use the following listed aspects and their descriptions as evaluation criteria:\n    - Accuracy and Hallucinations: The assistant's answer is semantically consistent with the gold answer; The numerical value and order need to be accurate, and there should be no hallucinations.\n    - Completeness: Referring to the reference answers, the assistant's answer should contain all the key points needed to answer the user's question; further elaboration on these key points can be omitted.\nPlease rate whether this answer is suitable for the question. Please note that the gold answer can be considered as a correct answer to the question.\n\nThe assistant receives an overall score on a scale of 1 to 100, where a higher score indicates better overall performance.\nPlease note that if the assistant's answer and the gold answer fully meet the above criteria, its overall rating should be the full marks (100).\nPlease first provide a comprehensive explanation of your evaluation, avoiding any potential bias.\nThen, output a line indicating the score of the Assistant.\n\nPLEASE OUTPUT WITH THE FOLLOWING FORMAT, WHERE THE SCORE IS A SCALE OF 1 TO 100 BY STRICTLY FOLLOWING THIS FORMAT: \"[[score]]\", FOR EXAMPLE \"Rating: [[100]]\":\n<start output>\nEvaluation evidence: your evluation explanation here, no more than 100 words\nRating: [[score]]\n<end output>\n\nNow, start your evaluation:",
        "generate_response": "{\"Reference\":[\"Do as i can and not as i say: Grounding language in robotic affordances\", \"How to prompt your robot: A promptbook for manipulation skills with code as policies\", \"Runaround\", \"Constitutional ai: Harmlessness from ai feedback\", \"Robocat: A self-improving foundation agent for robotic manipulation\", \"RT-1: Robotics transformer for real-world control at scale\", \"RT2: Vision-language-action models transfer web knowledge to robotic control\", \"Universal sentence encoder\", \"Open-vocabulary queryable scene representations for real world planning\", \"Pali: A jointly-scaled multilingual language-image model\", \"Leveraging procedural generation to benchmark reinforcement learning\", \"Robonet: Large-scale multi-robot learning\", \"Palm-e: An embodied multimodal language model\", \"Visual foresight: Model-based deep reinforcement learning for vision-based robotic control\", \"Physically grounded vision-language models for robotic manipulation\", \"Robot learning in homes: Improving generalization and reducing dataset bias\", \"Fleet-dagger: Interactive robot fleet learning with scalable human supervision\", \"BC-z: Zero-shot task generalization with robotic imitation learning\", \"Qt-opt: Scalable deep reinforcement learning for vision-based robotic manipulation\", \"Mt-opt: Continuous multi-task robotic reinforcement learning at scale\", \"Hgdagger: Interactive imitation learning with human experts\", \"Learning hand-eye coordination for robotic grasping with deep learning and large-scale data collection\", \"Code as policies: Language model programs for embodied control\", \"Interactive language: Talking to robots in real time\", \"Scaling robot supervision to hundreds of hours with roboturk: Robotic manipulation dataset through human reasoning and dexterity\", \"Grounding language with visual affordances over unstructured data\", \"Generative agents: Interactive simulacra of human behavior\", \"Supersizing self-supervision: Learning to grasp from 50k tries and 700 robot hours\", \"Grasp learning: Models, methods, and performance\", \"Sayplan: Grounding large language models using 3d scene graphs for scalable task planning\", \"Flexcap: Generating rich, localized, and flexible captions in images\", \"A reduction of imitation learning and structured prediction to no-regret online learning\", \"Multiple interactions made easy (mime): Large scale demonstrations data for imitation\", \"Reflexion: Language agents with verbal reinforcement learning\", \"D4: Improving llm pretraining via document de-duplication and diversification\", \"Chatgpt for robotics: Design principles and model abilities\", \"Voyager: An open-ended embodied agent with large language models\", \"Towards a foundation model for generalist robots: Diverse skill learning at scale via automated task and scene generation\", \"Robotic skill acquistion via instruction augmentation with visionlanguage models\", \"React: Synergizing reasoning and acting in language models\", \"More than a million ways to be pushed: A high-fidelity experimental dataset of planar pushing\"], \"Citation\":[]}",
        "eval_response": "Evaluation evidence: The assistant's answer is inaccurate and contains hallucinations. It lists numerous references that are not mentioned in the gold answer, indicating a failure to correctly identify the specific references for the given paper. Additionally, it fails to list any citations, whereas the gold answer identifies one citation. This discrepancy shows a lack of accuracy and completeness in the assistant's response.\n\nRating: [[10]]",
        "score": 10
    },
    {
        "level": 3,
        "set": 3,
        "length": 193673,
        "type": "paper",
        "language": "en",
        "question": "Risk Taxonomy, Mitigation, and Assessment Benchmarks of Large Language Model Systems",
        "instruction": "We hope you will carefully study the provided papers and determine the citation relationships between them. Please follow the instructions below strictly to complete the task:\n\n#Specific Requirements:\n1. Reference: When a given paper mentions other provided papers, those other papers are considered as \"references\" for the given paper. To summarize in this specific context, references are about what the given paper is using.\n2. Citation: Conversely, when other provided papers mention the given paper in their works, the given paper is being \"cited\" by those other papers. To summarize in this specific context, citations are about who is using the given paper.\n3. Given a paper, you need to determine the citation or reference relationship between this paper and the other papers. Do not consider papers that are not provided.\n3. Please present the paper titles in a json format as follows: {{\"Reference\":[\"Reference Title 1\", \"Reference Title 2\", ..., \"Reference Title n\"], \"Citation\":[\"Citation Title 1\", \"Citation Title 2\", ..., \"Citation Title n\"]}}.\n4. If a paper does not have any references or citations, please leave the corresponding list empty, e.g.{{\"Refernce\":[]}}, {{\"Citation\":[]}}.",
        "prompt_template": "#Papers:\n{docs}\n\n{instruction}\n\n#The paper you need to analyze:\n{question}",
        "doc": [
            "2401.05778.md",
            "2403.13309.md",
            "2404.01349.md",
            "2404.05264.md",
            "2404.12736.md",
            "2405.11647.md",
            "2310.08256.md",
            "2310.13548.md",
            "2311.05232.md",
            "2311.11855.md",
            "2312.07930.md"
        ],
        "answer": {
            "Reference": [
                "# Impact of Co-occurrence on Factual Knowledge of Large Language Models ",
                "# TOWARDS UNDERSTANDING SYCOPHANCY IN LANGUAGE MODELS ",
                "# Evil Geniuses: Delving into the Safety of LLM-based Agents ",
                "# Towards Optimal Statistical Watermarking "
            ],
            "Citation": [
                "# Mapping LLM Security Landscapes: A Comprehensive Stakeholder Risk Assessment Proposal ",
                "# Large Language Model Supply Chain: A Research Agenda ",
                "# Hummer: Towards Limited Competitive Preference Dataset "
            ]
        },
        "shuffle_doc": true,
        "id": "4e293212-e211-4099-841e-aa06e78db0b0",
        "prompt": "[Question]\n#Papers:\n{docs}\n\nWe hope you will carefully study the provided papers and determine the citation relationships between them. Please follow the instructions below strictly to complete the task:\n\n#Specific Requirements:\n1. Reference: When a given paper mentions other provided papers, those other papers are considered as \"references\" for the given paper. To summarize in this specific context, references are about what the given paper is using.\n2. Citation: Conversely, when other provided papers mention the given paper in their works, the given paper is being \"cited\" by those other papers. To summarize in this specific context, citations are about who is using the given paper.\n3. Given a paper, you need to determine the citation or reference relationship between this paper and the other papers. Do not consider papers that are not provided.\n3. Please present the paper titles in a json format as follows: {{\"Reference\":[\"Reference Title 1\", \"Reference Title 2\", ..., \"Reference Title n\"], \"Citation\":[\"Citation Title 1\", \"Citation Title 2\", ..., \"Citation Title n\"]}}.\n4. If a paper does not have any references or citations, please leave the corresponding list empty, e.g.{{\"Refernce\":[]}}, {{\"Citation\":[]}}.\n\n#The paper you need to analyze:\nRisk Taxonomy, Mitigation, and Assessment Benchmarks of Large Language Model Systems\n\n[Gold Answer]\n{'Reference': ['# Impact of Co-occurrence on Factual Knowledge of Large Language Models ', '# TOWARDS UNDERSTANDING SYCOPHANCY IN LANGUAGE MODELS ', '# Evil Geniuses: Delving into the Safety of LLM-based Agents ', '# Towards Optimal Statistical Watermarking '], 'Citation': ['# Mapping LLM Security Landscapes: A Comprehensive Stakeholder Risk Assessment Proposal ', '# Large Language Model Supply Chain: A Research Agenda ', '# Hummer: Towards Limited Competitive Preference Dataset ']}\n\n[The Start of Assistant's Predicted Answer]\n{\"Reference\":[\"Language models are few-shot learners\", \"GPT-4 technical report\", \"Llama: Open and efficient foundation language models\", \"Llama 2: Open foundation and fine-tuned chat models\", \"GLM-130B: an open bilingual pretrained model\", \"Codet5+: Open code large language models for code understanding and generation\", \"In-context instruction learning\", \"Chain-of-thought prompting elicits reasoning in large language models\", \"Tree of thoughts: Deliberate problem solving with large language models\", \"Graph of thoughts: Solving elaborate problems with large language models\", \"Training language models to follow instructions with human feedback\", \"Introducing chatgpt\", \"March 20 chatgpt outage: Here's what happened\", \"do anything now\": Characterizing and evaluating in-the-wild jailbreak prompts on large language models\", \"A survey on chatgpt: Ai-generated contents, challenges, and solutions\", \"Decodingtrust: A comprehensive assessment of trustworthiness in GPT models\", \"Trustworthy llms: a survey and guideline for evaluating large language models' alignment\", \"From chatgpt to threatgpt: Impact of generative AI in cybersecurity and privacy\", \"A survey of safety and trustworthiness of large language models through the lens of verification and validation\", \"Developing safe \\& responsible ai\", \"Introducing gemini: our largest and most capable ai model\", \"Llama 2 - responsible user guide\", \"AI Research and Products That Put Safety at the Frontier\", \"A survey of large language models\", \"Speech understanding systems\", \"Deep Learning\", \"Hierarchical neural story generation\", \"The curious case of neural text degeneration\", \"GPT3.5 Turbo fine-tuning and API updates\", \"Model Index for Researchers\", \"Scaling laws for neural language models\", \"Attention is all you need\", \"A survey of large language models\", \"Direct preference optimization: Your language model is secretly a reward model\", \"Preference ranking optimization for human alignment\", \"RRHF: Rank responses to align language models with human feedback without tears\", \"Calibrating sequence likelihood improves conditional language generation\", \"Chain of hindsight aligns language models with feedback\", \"Second thoughts are best: Learning to re-align with human values from text edits\", \"Training socially aligned language models in simulated human society\", \"Galactica: A large language model for science\", \"GPTNeoX-20B: An open-source autoregressive language model\", \"RealToxicityPrompts: Evaluating Neural Toxic Degeneration in Language Models\", \"Challenges in detoxifying language models\", \"Process for Adapting Language Models to Society (PALMS) with Values-Targeted Datasets\", \"Exploring the Limits of Domain-Adaptive Training for Detoxifying Large-Scale Language Models\", \"Jailbroken: How Does LLM Safety Training Fail?\", \"Protecting User Privacy in Remote Conversational Systems: A Privacy-Preserving Framework Based on Text Sanitization\", \"Privacy-Preserving Prompt Tuning for Large Language Model Services\", \"Medical Document Anonymization with a Semantic Lexicon\", \"Largescale Evaluation of Automated Clinical Note De-identification and Its Impact on Information Extraction\", \"Deidentification of Patient Notes with Recurrent Neural Networks\", \"Deidentification of Free-Text Medical Records Using Pre-Trained Bidirectional Transformers\", \"Deduplicating Training Data Mitigates Privacy Risks in Language Models\", \"Calibrating Noise to Sensitivity in Private Data Analysis\", \"A Firm Foundation for Private Data Analysis\", \"The Algorithmic Foundations of Differential Privacy\", \"Learning and Evaluating a Differentially Private Pre-Trained Language Model\", \"Differentially Private Decoding in Large Language Models\", \"Differentially Private Fine-Tuning of Language Models\", \"Differential Privacy: Now it's Getting Personal\", \"One-Sided Differential Privacy\", \"Selective Differential Privacy for Language Modeling\", \"Just Fine-tune Twice: Selective Differential Privacy for Large Language Models\", \"Differentially Private Bias-Term Only Fine-Tuning of Foundation Models\", \"Submix: Practical Private Prediction for Large-Scale Language Models\", \"Flocks of Stochastic Parrots: Differentially Private Prompt Learning for Large Language Models\", \"Differentially Private In-context Learning\", \"A Comparative Study of Using Pre-Trained Language Models for Toxic Comment Classification\", \"PromptBench: Towards Evaluating the Robustness of Large Language Models on Adversarial Prompts\", \"Adversarial GLUE: A Multi-Task Benchmark for Robustness Evaluation of Language Models\", \"ANLI: A Benchmark for Evaluating Natural Language Understanding Systems\", \"GLUE-X: Evaluating Natural Language Understanding Models from an Out-of-Distribution Generalization Perspective\", \"BOSS: A Benchmark for Evaluating the Robustness of Large Language Models\", \"HaDes: A Token-Level Reference-Free Hallucination Detection Benchmark for Free-Form Text Generation\", \"WikiBro: A Zero-Resource Black-Box Hallucination Detection Benchmark for Generative Large Language Models\", \"Med-HALT: Medical Domain Hallucination Test for Large Language Models\", \"HaluEval: A Large-Scale Hallucination Evaluation Benchmark for Large Language Models\", \"Zero-Resource Hallucination Prevention for Large Language Models\", \"Explore, Establish, Exploit: Red Teaming Language Models from Scratch\", \"Hatexplain: A Benchmark Dataset for Explainable Hate Speech Detection\", \"Using GPT-4 for Content Moderation\", \"Llama 2 Responsible Use Guide\", \"You Are What You Do: Hunting Stealthy Malware via Data Provenance Analysis\", \"Towards Optimal Statistical Watermarking\", \"Towards Reliable Utilization of AIGC: Blockchain-Empowered Ownership Verification Mechanism\", \"A Survey on Adversarial Attacks and Defences\", \"Red Teaming Language Models from Scratch\", \"Stereoset: Measuring Stereotypical Bias in Pretrained Language Models\", \"Is ChatGPT a Psychopath? Evaluating Large Language Models from a Psychological Perspective\", \"The Self-Perception and Political Biases of ChatGPT\", \"Evaluating ChatGPT's Performance for Multilingual and Emoji-Based Hate Speech Detection\", \"Aligning AI with Shared Human Values\", \"Is ChatGPT Better than Human Annotators? Potential and Limitations of ChatGPT in Explaining Implicit Hate Speech\", \"Societal Biases in Language Generation: Progress and Challenges\", \"Assessing Cross-Cultural Alignment between ChatGPT and Human Societies: An Empirical Study\", \"Knowledge of Cultural Moral Norms in Large Language Models\", \"BiasAsker: Measuring the Bias in Conversational AI System\", \"A Perspectival Mirror of the Elephant: Investigating Language Bias on Google, ChatGPT, Wikipedia, and YouTube\", \"Evil Geniuses: Delving into the Safety of 1LM-based Agents\"], \"Citation\":[\"Large Language Model Supply Chain: A Research Agenda\", \"Unbridled Icarus: A Survey of the Potential Perils of Image Inputs in Multimodal Large Language Model Security\", \"Mapping LLM Security Landscapes: A Comprehensive Stakeholder Risk Assessment Proposal\"]}\n[The End of Assistant's Predicted Answer]\n\n[System]\nWe would like to request your feedback on the performance of the AI assistant in response to the user question displayed above according to the gold answer. Please use the following listed aspects and their descriptions as evaluation criteria:\n    - Accuracy and Hallucinations: The assistant's answer is semantically consistent with the gold answer; The numerical value and order need to be accurate, and there should be no hallucinations.\n    - Completeness: Referring to the reference answers, the assistant's answer should contain all the key points needed to answer the user's question; further elaboration on these key points can be omitted.\nPlease rate whether this answer is suitable for the question. Please note that the gold answer can be considered as a correct answer to the question.\n\nThe assistant receives an overall score on a scale of 1 to 100, where a higher score indicates better overall performance.\nPlease note that if the assistant's answer and the gold answer fully meet the above criteria, its overall rating should be the full marks (100).\nPlease first provide a comprehensive explanation of your evaluation, avoiding any potential bias.\nThen, output a line indicating the score of the Assistant.\n\nPLEASE OUTPUT WITH THE FOLLOWING FORMAT, WHERE THE SCORE IS A SCALE OF 1 TO 100 BY STRICTLY FOLLOWING THIS FORMAT: \"[[score]]\", FOR EXAMPLE \"Rating: [[100]]\":\n<start output>\nEvaluation evidence: your evluation explanation here, no more than 100 words\nRating: [[score]]\n<end output>\n\nNow, start your evaluation:",
        "generate_response": "{\"Reference\":[\"Language models are few-shot learners\", \"GPT-4 technical report\", \"Llama: Open and efficient foundation language models\", \"Llama 2: Open foundation and fine-tuned chat models\", \"GLM-130B: an open bilingual pretrained model\", \"Codet5+: Open code large language models for code understanding and generation\", \"In-context instruction learning\", \"Chain-of-thought prompting elicits reasoning in large language models\", \"Tree of thoughts: Deliberate problem solving with large language models\", \"Graph of thoughts: Solving elaborate problems with large language models\", \"Training language models to follow instructions with human feedback\", \"Introducing chatgpt\", \"March 20 chatgpt outage: Here's what happened\", \"do anything now\": Characterizing and evaluating in-the-wild jailbreak prompts on large language models\", \"A survey on chatgpt: Ai-generated contents, challenges, and solutions\", \"Decodingtrust: A comprehensive assessment of trustworthiness in GPT models\", \"Trustworthy llms: a survey and guideline for evaluating large language models' alignment\", \"From chatgpt to threatgpt: Impact of generative AI in cybersecurity and privacy\", \"A survey of safety and trustworthiness of large language models through the lens of verification and validation\", \"Developing safe \\& responsible ai\", \"Introducing gemini: our largest and most capable ai model\", \"Llama 2 - responsible user guide\", \"AI Research and Products That Put Safety at the Frontier\", \"A survey of large language models\", \"Speech understanding systems\", \"Deep Learning\", \"Hierarchical neural story generation\", \"The curious case of neural text degeneration\", \"GPT3.5 Turbo fine-tuning and API updates\", \"Model Index for Researchers\", \"Scaling laws for neural language models\", \"Attention is all you need\", \"A survey of large language models\", \"Direct preference optimization: Your language model is secretly a reward model\", \"Preference ranking optimization for human alignment\", \"RRHF: Rank responses to align language models with human feedback without tears\", \"Calibrating sequence likelihood improves conditional language generation\", \"Chain of hindsight aligns language models with feedback\", \"Second thoughts are best: Learning to re-align with human values from text edits\", \"Training socially aligned language models in simulated human society\", \"Galactica: A large language model for science\", \"GPTNeoX-20B: An open-source autoregressive language model\", \"RealToxicityPrompts: Evaluating Neural Toxic Degeneration in Language Models\", \"Challenges in detoxifying language models\", \"Process for Adapting Language Models to Society (PALMS) with Values-Targeted Datasets\", \"Exploring the Limits of Domain-Adaptive Training for Detoxifying Large-Scale Language Models\", \"Jailbroken: How Does LLM Safety Training Fail?\", \"Protecting User Privacy in Remote Conversational Systems: A Privacy-Preserving Framework Based on Text Sanitization\", \"Privacy-Preserving Prompt Tuning for Large Language Model Services\", \"Medical Document Anonymization with a Semantic Lexicon\", \"Largescale Evaluation of Automated Clinical Note De-identification and Its Impact on Information Extraction\", \"Deidentification of Patient Notes with Recurrent Neural Networks\", \"Deidentification of Free-Text Medical Records Using Pre-Trained Bidirectional Transformers\", \"Deduplicating Training Data Mitigates Privacy Risks in Language Models\", \"Calibrating Noise to Sensitivity in Private Data Analysis\", \"A Firm Foundation for Private Data Analysis\", \"The Algorithmic Foundations of Differential Privacy\", \"Learning and Evaluating a Differentially Private Pre-Trained Language Model\", \"Differentially Private Decoding in Large Language Models\", \"Differentially Private Fine-Tuning of Language Models\", \"Differential Privacy: Now it's Getting Personal\", \"One-Sided Differential Privacy\", \"Selective Differential Privacy for Language Modeling\", \"Just Fine-tune Twice: Selective Differential Privacy for Large Language Models\", \"Differentially Private Bias-Term Only Fine-Tuning of Foundation Models\", \"Submix: Practical Private Prediction for Large-Scale Language Models\", \"Flocks of Stochastic Parrots: Differentially Private Prompt Learning for Large Language Models\", \"Differentially Private In-context Learning\", \"A Comparative Study of Using Pre-Trained Language Models for Toxic Comment Classification\", \"PromptBench: Towards Evaluating the Robustness of Large Language Models on Adversarial Prompts\", \"Adversarial GLUE: A Multi-Task Benchmark for Robustness Evaluation of Language Models\", \"ANLI: A Benchmark for Evaluating Natural Language Understanding Systems\", \"GLUE-X: Evaluating Natural Language Understanding Models from an Out-of-Distribution Generalization Perspective\", \"BOSS: A Benchmark for Evaluating the Robustness of Large Language Models\", \"HaDes: A Token-Level Reference-Free Hallucination Detection Benchmark for Free-Form Text Generation\", \"WikiBro: A Zero-Resource Black-Box Hallucination Detection Benchmark for Generative Large Language Models\", \"Med-HALT: Medical Domain Hallucination Test for Large Language Models\", \"HaluEval: A Large-Scale Hallucination Evaluation Benchmark for Large Language Models\", \"Zero-Resource Hallucination Prevention for Large Language Models\", \"Explore, Establish, Exploit: Red Teaming Language Models from Scratch\", \"Hatexplain: A Benchmark Dataset for Explainable Hate Speech Detection\", \"Using GPT-4 for Content Moderation\", \"Llama 2 Responsible Use Guide\", \"You Are What You Do: Hunting Stealthy Malware via Data Provenance Analysis\", \"Towards Optimal Statistical Watermarking\", \"Towards Reliable Utilization of AIGC: Blockchain-Empowered Ownership Verification Mechanism\", \"A Survey on Adversarial Attacks and Defences\", \"Red Teaming Language Models from Scratch\", \"Stereoset: Measuring Stereotypical Bias in Pretrained Language Models\", \"Is ChatGPT a Psychopath? Evaluating Large Language Models from a Psychological Perspective\", \"The Self-Perception and Political Biases of ChatGPT\", \"Evaluating ChatGPT's Performance for Multilingual and Emoji-Based Hate Speech Detection\", \"Aligning AI with Shared Human Values\", \"Is ChatGPT Better than Human Annotators? Potential and Limitations of ChatGPT in Explaining Implicit Hate Speech\", \"Societal Biases in Language Generation: Progress and Challenges\", \"Assessing Cross-Cultural Alignment between ChatGPT and Human Societies: An Empirical Study\", \"Knowledge of Cultural Moral Norms in Large Language Models\", \"BiasAsker: Measuring the Bias in Conversational AI System\", \"A Perspectival Mirror of the Elephant: Investigating Language Bias on Google, ChatGPT, Wikipedia, and YouTube\", \"Evil Geniuses: Delving into the Safety of 1LM-based Agents\"], \"Citation\":[\"Large Language Model Supply Chain: A Research Agenda\", \"Unbridled Icarus: A Survey of the Potential Perils of Image Inputs in Multimodal Large Language Model Security\", \"Mapping LLM Security Landscapes: A Comprehensive Stakeholder Risk Assessment Proposal\"]}",
        "eval_response": "Evaluation evidence: The assistant's answer does not match the gold answer in terms of accuracy and completeness. The references and citations listed in the assistant's answer are largely hallucinated and do not correspond to those in the gold answer. The assistant included numerous unrelated titles and failed to accurately identify the specific references and citations from the provided paper. This indicates a significant deviation from the required task.\n\nRating: [[10]]",
        "score": 10
    },
    {
        "level": 3,
        "set": 2,
        "length": 72327,
        "type": "paper",
        "language": "en",
        "question": "Large Language Models for Mathematical Reasoning: Progresses and Challenges",
        "instruction": "We hope you will carefully study the provided papers and determine the citation relationships between them. Please follow the instructions below strictly to complete the task:\n\n#Specific Requirements:\n1. Reference: When a given paper mentions other provided papers, those other papers are considered as \"references\" for the given paper. To summarize in this specific context, references are about what the given paper is using.\n2. Citation: Conversely, when other provided papers mention the given paper in their works, the given paper is being \"cited\" by those other papers. To summarize in this specific context, citations are about who is using the given paper.\n3. Given a paper, you need to determine the citation or reference relationship between this paper and the other papers. Do not consider papers that are not provided.\n3. Please present the paper titles in a json format as follows: {{\"Reference\":[\"Reference Title 1\", \"Reference Title 2\", ..., \"Reference Title n\"], \"Citation\":[\"Citation Title 1\", \"Citation Title 2\", ..., \"Citation Title n\"]}}.\n4. If a paper does not have any references or citations, please leave the corresponding list empty, e.g.{{\"Refernce\":[]}}, {{\"Citation\":[]}}.",
        "prompt_template": "#Papers:\n{docs}\n\n{instruction}\n\n#The paper you need to analyze:\n{question}",
        "doc": [
            "2402.00157.md",
            "2405.17386.md",
            "2405.18272.md"
        ],
        "answer": {
            "Reference": [],
            "Citation": [
                "# MindMerger: Efficient Boosting LLM Reasoning in non-English Languages ",
                "# Metaheuristics and Large Language Models Join Forces: Towards an Integrated Optimization Approach "
            ]
        },
        "shuffle_doc": true,
        "id": "e5d0c0e4-089a-4dc5-b524-186f55e9428c",
        "prompt": "[Question]\n#Papers:\n{docs}\n\nWe hope you will carefully study the provided papers and determine the citation relationships between them. Please follow the instructions below strictly to complete the task:\n\n#Specific Requirements:\n1. Reference: When a given paper mentions other provided papers, those other papers are considered as \"references\" for the given paper. To summarize in this specific context, references are about what the given paper is using.\n2. Citation: Conversely, when other provided papers mention the given paper in their works, the given paper is being \"cited\" by those other papers. To summarize in this specific context, citations are about who is using the given paper.\n3. Given a paper, you need to determine the citation or reference relationship between this paper and the other papers. Do not consider papers that are not provided.\n3. Please present the paper titles in a json format as follows: {{\"Reference\":[\"Reference Title 1\", \"Reference Title 2\", ..., \"Reference Title n\"], \"Citation\":[\"Citation Title 1\", \"Citation Title 2\", ..., \"Citation Title n\"]}}.\n4. If a paper does not have any references or citations, please leave the corresponding list empty, e.g.{{\"Refernce\":[]}}, {{\"Citation\":[]}}.\n\n#The paper you need to analyze:\nLarge Language Models for Mathematical Reasoning: Progresses and Challenges\n\n[Gold Answer]\n{'Reference': [], 'Citation': ['# MindMerger: Efficient Boosting LLM Reasoning in non-English Languages ', '# Metaheuristics and Large Language Models Join Forces: Towards an Integrated Optimization Approach ']}\n\n[The Start of Assistant's Predicted Answer]\n{\"Reference\":[\"A Survey on Evaluation of Large Language Models\", \"Vicuna: An Open-Source Chatbot Impressing GPT-4 with $90 \\% *$ ChatGPT Quality\", \"ChatLaw: Open-Source Legal Large Language Model with Integrated External Knowledge Bases\", \"Compositional Semantic Parsing with Large Language Models\", \"Open LLM Leaderboard\", \"How Close is ChatGPT to Human Experts? Comparison Corpus, Evaluation, and Detection\", \"REALM: Retrieval-Augmented Language Model PreTraining\", \"Rethinking with Retrieval: Faithful Large Language Model Inference\", \"Measuring Massive Multitask Language Understanding\", \"Large language models for mathematicians\", \"Mathematical capabilities of chatgpt\", \"Exploring pre-service teachers' perceptions of large language models-generated hints in online mathematics learning\", \"Reasoning in large language models through symbolic math word problems\", \"Llms as potential brainstorming partners for math and science problems\", \"Proof artifact cotraining for theorem proving with language models\", \"DeBERTa: Decoding-enhanced BERT with Disentangled Attention\", \"Solving math word problems by combining language models with symbolic solvers\", \"Measuring mathematical problem solving with the MATH dataset\", \"Learning to solve arithmetic word problems with verb categorization\", \"Solving general arithmetic word problems\", \"A neural network solves, explains, and generates university math problems by program synthesis and fewshot learning at human level\", \"A causal framework to quantify the robustness of mathematical reasoning with language models\", \"Solving olympiad geometry without human demonstrations\", \"Deep neural solver for math word problems\", \"Math word problem generation with mathematical consistency and problem context constraints\", \"Chain-of-thought prompting elicits reasoning in large language models\", \"CMATH: can your language model pass chinese elementary school math test?\", \"LPML: LLM-Prompting Markup Language for Mathematical Reasoning\", \"Metamath: Bootstrap your own mathematical questions for large language models\", \"How well do large language models perform in arithmetic tasks?\", \"Mammoth: Building math generalist models through hybrid instruction tuning\", \"GLM-130B: an open bilingual pre-trained model\", \"Evaluating and improving tool-augmented computation-intensive math reasoning\", \"Interpretable math word problem solution generation via step-by-step planning\", \"Ape210k: A large-scale and template-rich dataset of math word problems\", \"Minif2f: a cross-system benchmark for formal olympiad-level mathematics\", \"A human-centric benchmark for evaluating foundation models\", \"Solving challenging math word problems using GPT-4 code interpreter with code-based self-verification\", \"Mathattack: Attacking large language models towards math solving ability\", \"Solving math word problems via cooperative reasoning induced language models\", \"Solving math word problems concerning systems of equations with GPT-3\", \"Synthesis of solutions for shaded area geometry problems\", \"Mathqa: Towards interpretable math word problem solving with operation-based formalisms\", \"Does chatgpt comprehend the place value in numbers when solving math word problems?\", \"Palm 2 technical report\", \"Training a helpful and harmless assistant with reinforcement learning from human feedback\", \"Holist: An environment for machine learning of higher-order theorem proving\", \"Solving math word problems with reexamination\", \"Gpt-neo: Large scale autoregressive language modeling with mesh-tensorflow\", \"Unigeo: Unifying geometry logical reasoning via reformulating mathematical expression\", \"A unified benchmark for mathematical reasoning\", \"Rewriting math word problems with large language models\", \"Show your work: Scratchpads for intermediate computation with language models\", \"Training language models to follow instructions with human feedback\", \"Are NLP models really able to solve simple math word problems?\", \"Neural-symbolic solver for math word problems with auxiliary tasks\", \"Language models are unsupervised multitask learners\", \"Math word problem solving by generating linguistic variants of problem statements\", \"Enhancing mathematical capabilities through chatgpt and similar generative artificial intelligence: Roles and challenges in solving mathematical problems\", \"Mathematical discoveries from program search with large language models\"], \"Citation\":[]}\n[The End of Assistant's Predicted Answer]\n\n[System]\nWe would like to request your feedback on the performance of the AI assistant in response to the user question displayed above according to the gold answer. Please use the following listed aspects and their descriptions as evaluation criteria:\n    - Accuracy and Hallucinations: The assistant's answer is semantically consistent with the gold answer; The numerical value and order need to be accurate, and there should be no hallucinations.\n    - Completeness: Referring to the reference answers, the assistant's answer should contain all the key points needed to answer the user's question; further elaboration on these key points can be omitted.\nPlease rate whether this answer is suitable for the question. Please note that the gold answer can be considered as a correct answer to the question.\n\nThe assistant receives an overall score on a scale of 1 to 100, where a higher score indicates better overall performance.\nPlease note that if the assistant's answer and the gold answer fully meet the above criteria, its overall rating should be the full marks (100).\nPlease first provide a comprehensive explanation of your evaluation, avoiding any potential bias.\nThen, output a line indicating the score of the Assistant.\n\nPLEASE OUTPUT WITH THE FOLLOWING FORMAT, WHERE THE SCORE IS A SCALE OF 1 TO 100 BY STRICTLY FOLLOWING THIS FORMAT: \"[[score]]\", FOR EXAMPLE \"Rating: [[100]]\":\n<start output>\nEvaluation evidence: your evluation explanation here, no more than 100 words\nRating: [[score]]\n<end output>\n\nNow, start your evaluation:",
        "generate_response": "{\"Reference\":[\"A Survey on Evaluation of Large Language Models\", \"Vicuna: An Open-Source Chatbot Impressing GPT-4 with $90 \\% *$ ChatGPT Quality\", \"ChatLaw: Open-Source Legal Large Language Model with Integrated External Knowledge Bases\", \"Compositional Semantic Parsing with Large Language Models\", \"Open LLM Leaderboard\", \"How Close is ChatGPT to Human Experts? Comparison Corpus, Evaluation, and Detection\", \"REALM: Retrieval-Augmented Language Model PreTraining\", \"Rethinking with Retrieval: Faithful Large Language Model Inference\", \"Measuring Massive Multitask Language Understanding\", \"Large language models for mathematicians\", \"Mathematical capabilities of chatgpt\", \"Exploring pre-service teachers' perceptions of large language models-generated hints in online mathematics learning\", \"Reasoning in large language models through symbolic math word problems\", \"Llms as potential brainstorming partners for math and science problems\", \"Proof artifact cotraining for theorem proving with language models\", \"DeBERTa: Decoding-enhanced BERT with Disentangled Attention\", \"Solving math word problems by combining language models with symbolic solvers\", \"Measuring mathematical problem solving with the MATH dataset\", \"Learning to solve arithmetic word problems with verb categorization\", \"Solving general arithmetic word problems\", \"A neural network solves, explains, and generates university math problems by program synthesis and fewshot learning at human level\", \"A causal framework to quantify the robustness of mathematical reasoning with language models\", \"Solving olympiad geometry without human demonstrations\", \"Deep neural solver for math word problems\", \"Math word problem generation with mathematical consistency and problem context constraints\", \"Chain-of-thought prompting elicits reasoning in large language models\", \"CMATH: can your language model pass chinese elementary school math test?\", \"LPML: LLM-Prompting Markup Language for Mathematical Reasoning\", \"Metamath: Bootstrap your own mathematical questions for large language models\", \"How well do large language models perform in arithmetic tasks?\", \"Mammoth: Building math generalist models through hybrid instruction tuning\", \"GLM-130B: an open bilingual pre-trained model\", \"Evaluating and improving tool-augmented computation-intensive math reasoning\", \"Interpretable math word problem solution generation via step-by-step planning\", \"Ape210k: A large-scale and template-rich dataset of math word problems\", \"Minif2f: a cross-system benchmark for formal olympiad-level mathematics\", \"A human-centric benchmark for evaluating foundation models\", \"Solving challenging math word problems using GPT-4 code interpreter with code-based self-verification\", \"Mathattack: Attacking large language models towards math solving ability\", \"Solving math word problems via cooperative reasoning induced language models\", \"Solving math word problems concerning systems of equations with GPT-3\", \"Synthesis of solutions for shaded area geometry problems\", \"Mathqa: Towards interpretable math word problem solving with operation-based formalisms\", \"Does chatgpt comprehend the place value in numbers when solving math word problems?\", \"Palm 2 technical report\", \"Training a helpful and harmless assistant with reinforcement learning from human feedback\", \"Holist: An environment for machine learning of higher-order theorem proving\", \"Solving math word problems with reexamination\", \"Gpt-neo: Large scale autoregressive language modeling with mesh-tensorflow\", \"Unigeo: Unifying geometry logical reasoning via reformulating mathematical expression\", \"A unified benchmark for mathematical reasoning\", \"Rewriting math word problems with large language models\", \"Show your work: Scratchpads for intermediate computation with language models\", \"Training language models to follow instructions with human feedback\", \"Are NLP models really able to solve simple math word problems?\", \"Neural-symbolic solver for math word problems with auxiliary tasks\", \"Language models are unsupervised multitask learners\", \"Math word problem solving by generating linguistic variants of problem statements\", \"Enhancing mathematical capabilities through chatgpt and similar generative artificial intelligence: Roles and challenges in solving mathematical problems\", \"Mathematical discoveries from program search with large language models\"], \"Citation\":[]}",
        "eval_response": "Evaluation evidence: The assistant's answer inaccurately lists numerous references that are not mentioned in the gold answer, indicating a significant hallucination issue. The gold answer specifies that there are no references for the paper in question, yet the assistant lists an extensive array of unrelated titles. This error severely impacts the accuracy of the response. Additionally, the citations section in the assistant's answer is empty, which aligns with the gold answer, but the error in the references section is too substantial to overlook.\n\nRating: [[20]]",
        "score": 20
    },
    {
        "level": 4,
        "set": 1,
        "length": 46433,
        "type": "paper",
        "language": "en",
        "question": "",
        "instruction": "We kindly ask you to thoroughly review the provided papers and construct a citation chain from them. Please adhere to the following instructions strictly while completing the task:\n\n#Task Instructions:\nGiven several papers, you are required to identify and list the longest citation chain, which demonstrates the citation relationship among the provided papers.\n\n#Specific Requirements:\n1.Please present the titles of the papers in the form of a list, as follows: [\"Title of Paper 1\", \"Title of Paper 2\", ..., \"Title of Paper n\"].\n2.Ensure that the citation chain in the list is linear and continuous, meaning that the first paper title in the list (Paper 1) should not cite any other works. Instead, it should be cited by the next paper in the list (Paper 2); subsequently, each paper should then be cited by the next one in the list, continuing up to the last paper (Paper n).\n3.Consider only the citation relationships within the supplied collection of papers, and ensure that the citation chain accurately reflects the sequential citation order among these documents.\n4.Do not take into account any articles not provided, and disregard other non-linear citation relationships.",
        "prompt_template": "{instruction}\n\n#Papers Provided:\n{docs}",
        "doc": [
            "2306.11348.md",
            "2311.13503.md",
            "2305.01865.md"
        ],
        "answer": [
            "# The quantum state of light in collective spontaneous emission ",
            "# Non-Gaussian correlations in the steady-state of driven-dissipative clouds of two-level atoms ",
            "# Collective Lamb Shift and Modified Linewidth of An Interacting Atomic Gas "
        ],
        "shuffle_doc": true,
        "id": "5a79fe19-b944-493f-be04-13b3ba24c7c6",
        "prompt": "[Question]\nWe kindly ask you to thoroughly review the provided papers and construct a citation chain from them. Please adhere to the following instructions strictly while completing the task:\n\n#Task Instructions:\nGiven several papers, you are required to identify and list the longest citation chain, which demonstrates the citation relationship among the provided papers.\n\n#Specific Requirements:\n1.Please present the titles of the papers in the form of a list, as follows: [\"Title of Paper 1\", \"Title of Paper 2\", ..., \"Title of Paper n\"].\n2.Ensure that the citation chain in the list is linear and continuous, meaning that the first paper title in the list (Paper 1) should not cite any other works. Instead, it should be cited by the next paper in the list (Paper 2); subsequently, each paper should then be cited by the next one in the list, continuing up to the last paper (Paper n).\n3.Consider only the citation relationships within the supplied collection of papers, and ensure that the citation chain accurately reflects the sequential citation order among these documents.\n4.Do not take into account any articles not provided, and disregard other non-linear citation relationships.\n\n#Papers Provided:\n{docs}\n\n[Gold Answer]\n['# The quantum state of light in collective spontaneous emission ', '# Non-Gaussian correlations in the steady-state of driven-dissipative clouds of two-level atoms ', '# Collective Lamb Shift and Modified Linewidth of An Interacting Atomic Gas ']\n\n[The Start of Assistant's Predicted Answer]\n[\"Coherence in spontaneous radiation processes\", \"Superradiance: An essay on the theory of collective spontaneous emission\", \"Laser-driven superradiant ensembles of two-level atoms near Dicke regime\", \"The quantum state of light in collective spontaneous emission\", \"Quantum information with continuous variables\"]\n[The End of Assistant's Predicted Answer]\n\n[System]\nWe would like to request your feedback on the performance of the AI assistant in response to the user question displayed above according to the gold answer. Please use the following listed aspects and their descriptions as evaluation criteria:\n    - Accuracy and Hallucinations: The assistant's answer is semantically consistent with the gold answer; The numerical value and order need to be accurate, and there should be no hallucinations.\n    - Completeness: Referring to the reference answers, the assistant's answer should contain all the key points needed to answer the user's question; further elaboration on these key points can be omitted.\nPlease rate whether this answer is suitable for the question. Please note that the gold answer can be considered as a correct answer to the question.\n\nThe assistant receives an overall score on a scale of 1 to 100, where a higher score indicates better overall performance.\nPlease note that if the assistant's answer and the gold answer fully meet the above criteria, its overall rating should be the full marks (100).\nPlease first provide a comprehensive explanation of your evaluation, avoiding any potential bias.\nThen, output a line indicating the score of the Assistant.\n\nPLEASE OUTPUT WITH THE FOLLOWING FORMAT, WHERE THE SCORE IS A SCALE OF 1 TO 100 BY STRICTLY FOLLOWING THIS FORMAT: \"[[score]]\", FOR EXAMPLE \"Rating: [[100]]\":\n<start output>\nEvaluation evidence: your evluation explanation here, no more than 100 words\nRating: [[score]]\n<end output>\n\nNow, start your evaluation:",
        "generate_response": "[\"Coherence in spontaneous radiation processes\", \"Superradiance: An essay on the theory of collective spontaneous emission\", \"Laser-driven superradiant ensembles of two-level atoms near Dicke regime\", \"The quantum state of light in collective spontaneous emission\", \"Quantum information with continuous variables\"]",
        "eval_response": "Evaluation evidence: The assistant's answer does not match the gold answer in terms of the titles of the papers listed. The titles in the assistant's answer do not correspond to those in the gold answer, indicating a failure in accurately identifying the correct citation chain from the provided papers. This results in both inaccuracies and incompleteness as the assistant's response does not reflect the correct sequence or titles of the papers that form the longest citation chain as per the task requirements.\n\nRating: [[20]]",
        "score": 20
    },
    {
        "level": 3,
        "set": 2,
        "length": 71197,
        "type": "paper",
        "language": "en",
        "question": "Patchscopes: A Unifying Framework for Inspecting Hidden Representations of Language Models",
        "instruction": "We hope you will carefully study the provided papers and determine the citation relationships between them. Please follow the instructions below strictly to complete the task:\n\n#Specific Requirements:\n1. Reference: When a given paper mentions other provided papers, those other papers are considered as \"references\" for the given paper. To summarize in this specific context, references are about what the given paper is using.\n2. Citation: Conversely, when other provided papers mention the given paper in their works, the given paper is being \"cited\" by those other papers. To summarize in this specific context, citations are about who is using the given paper.\n3. Given a paper, you need to determine the citation or reference relationship between this paper and the other papers. Do not consider papers that are not provided.\n3. Please present the paper titles in a json format as follows: {{\"Reference\":[\"Reference Title 1\", \"Reference Title 2\", ..., \"Reference Title n\"], \"Citation\":[\"Citation Title 1\", \"Citation Title 2\", ..., \"Citation Title n\"]}}.\n4. If a paper does not have any references or citations, please leave the corresponding list empty, e.g.{{\"Refernce\":[]}}, {{\"Citation\":[]}}.",
        "prompt_template": "#Papers:\n{docs}\n\n{instruction}\n\n#The paper you need to analyze:\n{question}",
        "doc": [
            "2401.06102.md",
            "2404.05971.md",
            "2405.20778.md",
            "2311.04897.md"
        ],
        "answer": {
            "Reference": [
                "# Future Lens: Anticipating Subsequent Tokens from a Single Hidden State "
            ],
            "Citation": [
                "# Does Transformer Interpretability Transfer to RNNs? ",
                "# Improved Generation of Adversarial Examples Against Safety-aligned LLMs "
            ]
        },
        "shuffle_doc": true,
        "id": "b3e24075-2d88-460d-8fbb-8cae98fc6b57",
        "prompt": "[Question]\n#Papers:\n{docs}\n\nWe hope you will carefully study the provided papers and determine the citation relationships between them. Please follow the instructions below strictly to complete the task:\n\n#Specific Requirements:\n1. Reference: When a given paper mentions other provided papers, those other papers are considered as \"references\" for the given paper. To summarize in this specific context, references are about what the given paper is using.\n2. Citation: Conversely, when other provided papers mention the given paper in their works, the given paper is being \"cited\" by those other papers. To summarize in this specific context, citations are about who is using the given paper.\n3. Given a paper, you need to determine the citation or reference relationship between this paper and the other papers. Do not consider papers that are not provided.\n3. Please present the paper titles in a json format as follows: {{\"Reference\":[\"Reference Title 1\", \"Reference Title 2\", ..., \"Reference Title n\"], \"Citation\":[\"Citation Title 1\", \"Citation Title 2\", ..., \"Citation Title n\"]}}.\n4. If a paper does not have any references or citations, please leave the corresponding list empty, e.g.{{\"Refernce\":[]}}, {{\"Citation\":[]}}.\n\n#The paper you need to analyze:\nPatchscopes: A Unifying Framework for Inspecting Hidden Representations of Language Models\n\n[Gold Answer]\n{'Reference': ['# Future Lens: Anticipating Subsequent Tokens from a Single Hidden State '], 'Citation': ['# Does Transformer Interpretability Transfer to RNNs? ', '# Improved Generation of Adversarial Examples Against Safety-aligned LLMs ']}\n\n[The Start of Assistant's Predicted Answer]\n{\"Reference\": [\"Understanding intermediate layers using linear classifier probes\", \"Revisiting model stitching to compare neural representations\", \"Probing classifiers: Promises, shortcomings, and advances\", \"Analysis methods in neural language processing: A survey\", \"A Survey on Evaluation of Large Language Models\", \"Vicuna: An open-source chatbot impressing GPT-4 with $90 \\% *$ ChatGPT quality\", \"Eliciting latent predictions from transformers with the tuned lens\", \"Graph of thoughts: Solving elaborate problems with large language models\", \"Pythia: A suite for analyzing large language models across training and scaling\", \"Language models can explain neurons in language models\", \"Sok: Toward transparent ai: A survey on interpreting the inner structures of deep neural networks\", \"The Pile: An 800GB dataset of diverse text for language modeling\", \"Causal abstractions of neural networks\", \"Localizing model behavior with path patching\", \"Distributional vectors encode referential attributes\", \"Does localization inform editing? surprising differences in causality-based localization vs. knowledge editing in language models\", \"In-context learning creates task vectors\", \"Natural language descriptions of deep features\", \"Measuring and manipulating knowledge representations in language models\", \"Measuring massive multitask language understanding\", \"Dissecting recall of factual associations in auto-regressive language models\", \"Dissect: Disentangled simultaneous explanations via concept traversals\", \"Interpretability beyond feature attribution: Quantitative testing with concept activation vectors (tcav)\", \"What's in an embedding? analyzing word embeddings through multilingual evaluation\", \"Decoderlens: Layerwise interpretation of encoder-decoder transformers\", \"Understanding image representations by measuring their equivariance and equivalence\", \"Understanding and patching compositional reasoning in llms\", \"Does circuit analysis interpretability scale? evidence from multiple choice capabilities in chinchilla\", \"ROUGE: A package for automatic evaluation of summaries\", \"Self-refine: Iterative refinement with self-feedback\", \"The expressive power of transformers with chain of thought\", \"Linearly mapping from image to text space\", \"A mechanism for solving relational tasks in transformer language models\", \"Can llms facilitate interpretation of pre-trained language models?\", \"Mapping language models to grounded conceptual spaces\", \"Sentence-bert: Sentence embeddings using siamese bert-networks\", \"Multimodal neurons in pretrained text-only transformers\", \"Explaining black box text modules in natural language with language models\", \"The curious case of hallucinatory (un)answerability: Finding truths in the hidden states of over-confident large language models\", \"A mechanistic interpretation of arithmetic reasoning in language models using causal mediation analysis\", \"Lstmvis: A tool for visual analysis of hidden state dynamics in recurrent neural networks\", \"Bert rediscovers the classical nlp pipeline\", \"Llama: Open and efficient foundation language models\", \"Llama 2: Open foundation and finetuned chat models\", \"Investigating gender bias in language models using causal mediation analysis\", \"Gaussian Process Probes (GPP) for uncertainty-aware probing\", \"Chain-of-thought prompting elicits reasoning in large language models\", \"Tree of thoughts: Deliberate problem solving with large language models\", \"Give me the facts! a survey on factual knowledge probing in pre-trained language models\", \"Interpreting deep visual representations via network dissection\"], \"Citation\": []}\n[The End of Assistant's Predicted Answer]\n\n[System]\nWe would like to request your feedback on the performance of the AI assistant in response to the user question displayed above according to the gold answer. Please use the following listed aspects and their descriptions as evaluation criteria:\n    - Accuracy and Hallucinations: The assistant's answer is semantically consistent with the gold answer; The numerical value and order need to be accurate, and there should be no hallucinations.\n    - Completeness: Referring to the reference answers, the assistant's answer should contain all the key points needed to answer the user's question; further elaboration on these key points can be omitted.\nPlease rate whether this answer is suitable for the question. Please note that the gold answer can be considered as a correct answer to the question.\n\nThe assistant receives an overall score on a scale of 1 to 100, where a higher score indicates better overall performance.\nPlease note that if the assistant's answer and the gold answer fully meet the above criteria, its overall rating should be the full marks (100).\nPlease first provide a comprehensive explanation of your evaluation, avoiding any potential bias.\nThen, output a line indicating the score of the Assistant.\n\nPLEASE OUTPUT WITH THE FOLLOWING FORMAT, WHERE THE SCORE IS A SCALE OF 1 TO 100 BY STRICTLY FOLLOWING THIS FORMAT: \"[[score]]\", FOR EXAMPLE \"Rating: [[100]]\":\n<start output>\nEvaluation evidence: your evluation explanation here, no more than 100 words\nRating: [[score]]\n<end output>\n\nNow, start your evaluation:",
        "generate_response": "{\"Reference\": [\"Understanding intermediate layers using linear classifier probes\", \"Revisiting model stitching to compare neural representations\", \"Probing classifiers: Promises, shortcomings, and advances\", \"Analysis methods in neural language processing: A survey\", \"A Survey on Evaluation of Large Language Models\", \"Vicuna: An open-source chatbot impressing GPT-4 with $90 \\% *$ ChatGPT quality\", \"Eliciting latent predictions from transformers with the tuned lens\", \"Graph of thoughts: Solving elaborate problems with large language models\", \"Pythia: A suite for analyzing large language models across training and scaling\", \"Language models can explain neurons in language models\", \"Sok: Toward transparent ai: A survey on interpreting the inner structures of deep neural networks\", \"The Pile: An 800GB dataset of diverse text for language modeling\", \"Causal abstractions of neural networks\", \"Localizing model behavior with path patching\", \"Distributional vectors encode referential attributes\", \"Does localization inform editing? surprising differences in causality-based localization vs. knowledge editing in language models\", \"In-context learning creates task vectors\", \"Natural language descriptions of deep features\", \"Measuring and manipulating knowledge representations in language models\", \"Measuring massive multitask language understanding\", \"Dissecting recall of factual associations in auto-regressive language models\", \"Dissect: Disentangled simultaneous explanations via concept traversals\", \"Interpretability beyond feature attribution: Quantitative testing with concept activation vectors (tcav)\", \"What's in an embedding? analyzing word embeddings through multilingual evaluation\", \"Decoderlens: Layerwise interpretation of encoder-decoder transformers\", \"Understanding image representations by measuring their equivariance and equivalence\", \"Understanding and patching compositional reasoning in llms\", \"Does circuit analysis interpretability scale? evidence from multiple choice capabilities in chinchilla\", \"ROUGE: A package for automatic evaluation of summaries\", \"Self-refine: Iterative refinement with self-feedback\", \"The expressive power of transformers with chain of thought\", \"Linearly mapping from image to text space\", \"A mechanism for solving relational tasks in transformer language models\", \"Can llms facilitate interpretation of pre-trained language models?\", \"Mapping language models to grounded conceptual spaces\", \"Sentence-bert: Sentence embeddings using siamese bert-networks\", \"Multimodal neurons in pretrained text-only transformers\", \"Explaining black box text modules in natural language with language models\", \"The curious case of hallucinatory (un)answerability: Finding truths in the hidden states of over-confident large language models\", \"A mechanistic interpretation of arithmetic reasoning in language models using causal mediation analysis\", \"Lstmvis: A tool for visual analysis of hidden state dynamics in recurrent neural networks\", \"Bert rediscovers the classical nlp pipeline\", \"Llama: Open and efficient foundation language models\", \"Llama 2: Open foundation and finetuned chat models\", \"Investigating gender bias in language models using causal mediation analysis\", \"Gaussian Process Probes (GPP) for uncertainty-aware probing\", \"Chain-of-thought prompting elicits reasoning in large language models\", \"Tree of thoughts: Deliberate problem solving with large language models\", \"Give me the facts! a survey on factual knowledge probing in pre-trained language models\", \"Interpreting deep visual representations via network dissection\"], \"Citation\": []}",
        "eval_response": "Evaluation evidence: The assistant's answer is inaccurate and contains hallucinations. The provided answer lists numerous references that are not mentioned in the gold answer, indicating a failure to correctly identify the specific references for the paper in question. Additionally, the assistant's answer does not list any citations, whereas the gold answer lists two specific citations. This discrepancy shows a lack of completeness and accuracy in the assistant's response.\n\nRating: [[10]]",
        "score": 10
    },
    {
        "level": 3,
        "set": 3,
        "length": 104208,
        "type": "paper",
        "language": "en",
        "question": "AUtoAcT: Automatic Agent Learning from Scratch for QA via Self-Planning",
        "instruction": "We hope you will carefully study the provided papers and determine the citation relationships between them. Please follow the instructions below strictly to complete the task:\n\n#Specific Requirements:\n1. Reference: When a given paper mentions other provided papers, those other papers are considered as \"references\" for the given paper. To summarize in this specific context, references are about what the given paper is using.\n2. Citation: Conversely, when other provided papers mention the given paper in their works, the given paper is being \"cited\" by those other papers. To summarize in this specific context, citations are about who is using the given paper.\n3. Given a paper, you need to determine the citation or reference relationship between this paper and the other papers. Do not consider papers that are not provided.\n3. Please present the paper titles in a json format as follows: {{\"Reference\":[\"Reference Title 1\", \"Reference Title 2\", ..., \"Reference Title n\"], \"Citation\":[\"Citation Title 1\", \"Citation Title 2\", ..., \"Citation Title n\"]}}.\n4. If a paper does not have any references or citations, please leave the corresponding list empty, e.g.{{\"Refernce\":[]}}, {{\"Citation\":[]}}.",
        "prompt_template": "#Papers:\n{docs}\n\n{instruction}\n\n#The paper you need to analyze:\n{question}",
        "doc": [
            "2401.05268.md",
            "2405.14751.md",
            "2311.11797.md",
            "2401.07324.md"
        ],
        "answer": {
            "Reference": [
                "# Igniting Language Intelligence: The Hitchhiker's Guide From Chain-of-Thought Reasoning to Language Agents ",
                "# Small LLMs Are Weak Tool Learners: A Multi-LLM Agent "
            ],
            "Citation": [
                "# AGILE: A Novel Framework of LLM Agents "
            ]
        },
        "shuffle_doc": true,
        "id": "a8cad78c-7065-4029-8924-e3610c0d3478",
        "prompt": "[Question]\n#Papers:\n{docs}\n\nWe hope you will carefully study the provided papers and determine the citation relationships between them. Please follow the instructions below strictly to complete the task:\n\n#Specific Requirements:\n1. Reference: When a given paper mentions other provided papers, those other papers are considered as \"references\" for the given paper. To summarize in this specific context, references are about what the given paper is using.\n2. Citation: Conversely, when other provided papers mention the given paper in their works, the given paper is being \"cited\" by those other papers. To summarize in this specific context, citations are about who is using the given paper.\n3. Given a paper, you need to determine the citation or reference relationship between this paper and the other papers. Do not consider papers that are not provided.\n3. Please present the paper titles in a json format as follows: {{\"Reference\":[\"Reference Title 1\", \"Reference Title 2\", ..., \"Reference Title n\"], \"Citation\":[\"Citation Title 1\", \"Citation Title 2\", ..., \"Citation Title n\"]}}.\n4. If a paper does not have any references or citations, please leave the corresponding list empty, e.g.{{\"Refernce\":[]}}, {{\"Citation\":[]}}.\n\n#The paper you need to analyze:\nAUtoAcT: Automatic Agent Learning from Scratch for QA via Self-Planning\n\n[Gold Answer]\n{'Reference': [\"# Igniting Language Intelligence: The Hitchhiker's Guide From Chain-of-Thought Reasoning to Language Agents \", '# Small LLMs Are Weak Tool Learners: A Multi-LLM Agent '], 'Citation': ['# AGILE: A Novel Framework of LLM Agents ']}\n\n[The Start of Assistant's Predicted Answer]\n{\"Reference\": [\"A Survey on Evaluation of Large Language Models\", \"REALM: Retrieval-Augmented Language Model PreTraining\", \"Compositional Semantic Parsing with Large Language Models\", \"Open LLM Leaderboard\", \"How Close is ChatGPT to Human Experts? Comparison Corpus, Evaluation, and Detection\", \"Rethinking with Retrieval: Faithful Large Language Model Inference\", \"Measuring Massive Multitask Language Understanding\", \"Atlas: Few-shot Learning with Retrieval Augmented Language Models\", \"Chain-of-thought prompting elicits reasoning in large language models\", \"CAMEL: communicative agents for \\\"mind\\\" exploration of large scale language model society\", \"More agents is all you need\", \"Encouraging divergent thinking in large language models through multi-agent debate\", \"Self-refine: Iterative refinement with self-feedback\", \"Making language models better tool learners with execution feedback\", \"Reasoning with language model prompting: A survey\", \"Toolllm: Facilitating large language models to master 16000+ real-world apis\", \"Rest meets react: Self-improvement for multistep reasoning llm agent\", \"Llama 2: Open foundation and fine-tuned chat models\", \"Stanford alpaca: An instruction-following llama model\", \"Self-instruct: Aligning language models with self-generated instructions\", \"Autoagents: A framework for automatic agent generation\", \"Reconcile: Round-table conference improves reasoning via consensus among diverse llms\", \"Medagents: Large language models as collaborators for zero-shot medical reasoning\", \"Xagent: An autonomous agent for complex task solving\", \"Autogpt: build & use ai agents\", \"AutoAct: Toward language agent fine-tuning\", \"The rise and potential of large language model based agents: A survey\", \"Learning agents with unified data, modular design, and open-source llms\", \"Agenttuning: Enabling generalized agent abilities for llms\", \"Igniting language intelligence: The hitchhiker's guide from chain-of-thought reasoning to language agents\", \"Judging llm-as-a-judge with mt-bench and chatbot arena\", \"Webarena: A realistic web environment for building autonomous agents\"], \"Citation\": []}\n[The End of Assistant's Predicted Answer]\n\n[System]\nWe would like to request your feedback on the performance of the AI assistant in response to the user question displayed above according to the gold answer. Please use the following listed aspects and their descriptions as evaluation criteria:\n    - Accuracy and Hallucinations: The assistant's answer is semantically consistent with the gold answer; The numerical value and order need to be accurate, and there should be no hallucinations.\n    - Completeness: Referring to the reference answers, the assistant's answer should contain all the key points needed to answer the user's question; further elaboration on these key points can be omitted.\nPlease rate whether this answer is suitable for the question. Please note that the gold answer can be considered as a correct answer to the question.\n\nThe assistant receives an overall score on a scale of 1 to 100, where a higher score indicates better overall performance.\nPlease note that if the assistant's answer and the gold answer fully meet the above criteria, its overall rating should be the full marks (100).\nPlease first provide a comprehensive explanation of your evaluation, avoiding any potential bias.\nThen, output a line indicating the score of the Assistant.\n\nPLEASE OUTPUT WITH THE FOLLOWING FORMAT, WHERE THE SCORE IS A SCALE OF 1 TO 100 BY STRICTLY FOLLOWING THIS FORMAT: \"[[score]]\", FOR EXAMPLE \"Rating: [[100]]\":\n<start output>\nEvaluation evidence: your evluation explanation here, no more than 100 words\nRating: [[score]]\n<end output>\n\nNow, start your evaluation:",
        "generate_response": "{\"Reference\": [\"A Survey on Evaluation of Large Language Models\", \"REALM: Retrieval-Augmented Language Model PreTraining\", \"Compositional Semantic Parsing with Large Language Models\", \"Open LLM Leaderboard\", \"How Close is ChatGPT to Human Experts? Comparison Corpus, Evaluation, and Detection\", \"Rethinking with Retrieval: Faithful Large Language Model Inference\", \"Measuring Massive Multitask Language Understanding\", \"Atlas: Few-shot Learning with Retrieval Augmented Language Models\", \"Chain-of-thought prompting elicits reasoning in large language models\", \"CAMEL: communicative agents for \\\"mind\\\" exploration of large scale language model society\", \"More agents is all you need\", \"Encouraging divergent thinking in large language models through multi-agent debate\", \"Self-refine: Iterative refinement with self-feedback\", \"Making language models better tool learners with execution feedback\", \"Reasoning with language model prompting: A survey\", \"Toolllm: Facilitating large language models to master 16000+ real-world apis\", \"Rest meets react: Self-improvement for multistep reasoning llm agent\", \"Llama 2: Open foundation and fine-tuned chat models\", \"Stanford alpaca: An instruction-following llama model\", \"Self-instruct: Aligning language models with self-generated instructions\", \"Autoagents: A framework for automatic agent generation\", \"Reconcile: Round-table conference improves reasoning via consensus among diverse llms\", \"Medagents: Large language models as collaborators for zero-shot medical reasoning\", \"Xagent: An autonomous agent for complex task solving\", \"Autogpt: build & use ai agents\", \"AutoAct: Toward language agent fine-tuning\", \"The rise and potential of large language model based agents: A survey\", \"Learning agents with unified data, modular design, and open-source llms\", \"Agenttuning: Enabling generalized agent abilities for llms\", \"Igniting language intelligence: The hitchhiker's guide from chain-of-thought reasoning to language agents\", \"Judging llm-as-a-judge with mt-bench and chatbot arena\", \"Webarena: A realistic web environment for building autonomous agents\"], \"Citation\": []}",
        "eval_response": "Evaluation evidence: The assistant's answer does not match the gold answer in terms of accuracy. The references listed in the assistant's answer do not include the two titles mentioned in the gold answer, and it incorrectly lists numerous other papers not mentioned in the gold answer. Additionally, the assistant's answer fails to list any citations, whereas the gold answer lists one. This discrepancy indicates issues with both accuracy and completeness, as the assistant neither correctly identifies the references nor the citation.\n\nRating: [[10]]",
        "score": 10
    },
    {
        "level": 3,
        "set": 3,
        "length": 100739,
        "type": "paper",
        "language": "en",
        "question": "Patchscopes: A Unifying Framework for Inspecting Hidden Representations of Language Models",
        "instruction": "We hope you will carefully study the provided papers and determine the citation relationships between them. Please follow the instructions below strictly to complete the task:\n\n#Specific Requirements:\n1. Reference: When a given paper mentions other provided papers, those other papers are considered as \"references\" for the given paper. To summarize in this specific context, references are about what the given paper is using.\n2. Citation: Conversely, when other provided papers mention the given paper in their works, the given paper is being \"cited\" by those other papers. To summarize in this specific context, citations are about who is using the given paper.\n3. Given a paper, you need to determine the citation or reference relationship between this paper and the other papers. Do not consider papers that are not provided.\n3. Please present the paper titles in a json format as follows: {{\"Reference\":[\"Reference Title 1\", \"Reference Title 2\", ..., \"Reference Title n\"], \"Citation\":[\"Citation Title 1\", \"Citation Title 2\", ..., \"Citation Title n\"]}}.\n4. If a paper does not have any references or citations, please leave the corresponding list empty, e.g.{{\"Refernce\":[]}}, {{\"Citation\":[]}}.",
        "prompt_template": "#Papers:\n{docs}\n\n{instruction}\n\n#The paper you need to analyze:\n{question}",
        "doc": [
            "2401.06102.md",
            "2404.05971.md",
            "2405.17653.md"
        ],
        "answer": {
            "Reference": [],
            "Citation": [
                "# Does Transformer Interpretability Transfer to RNNs? ",
                "# InversionView: A General-Purpose Method for Reading Information from Neural Activations "
            ]
        },
        "shuffle_doc": true,
        "id": "f91f5058-b057-4b90-a4f7-b6c005bb459e",
        "prompt": "[Question]\n#Papers:\n{docs}\n\nWe hope you will carefully study the provided papers and determine the citation relationships between them. Please follow the instructions below strictly to complete the task:\n\n#Specific Requirements:\n1. Reference: When a given paper mentions other provided papers, those other papers are considered as \"references\" for the given paper. To summarize in this specific context, references are about what the given paper is using.\n2. Citation: Conversely, when other provided papers mention the given paper in their works, the given paper is being \"cited\" by those other papers. To summarize in this specific context, citations are about who is using the given paper.\n3. Given a paper, you need to determine the citation or reference relationship between this paper and the other papers. Do not consider papers that are not provided.\n3. Please present the paper titles in a json format as follows: {{\"Reference\":[\"Reference Title 1\", \"Reference Title 2\", ..., \"Reference Title n\"], \"Citation\":[\"Citation Title 1\", \"Citation Title 2\", ..., \"Citation Title n\"]}}.\n4. If a paper does not have any references or citations, please leave the corresponding list empty, e.g.{{\"Refernce\":[]}}, {{\"Citation\":[]}}.\n\n#The paper you need to analyze:\nPatchscopes: A Unifying Framework for Inspecting Hidden Representations of Language Models\n\n[Gold Answer]\n{'Reference': [], 'Citation': ['# Does Transformer Interpretability Transfer to RNNs? ', '# InversionView: A General-Purpose Method for Reading Information from Neural Activations ']}\n\n[The Start of Assistant's Predicted Answer]\n{\"Reference\":[\"A Survey on Interpreting the Inner Structures of Deep Neural Networks\", \"Revisiting Model Stitching to Compare Neural Representations\", \"Probing Classifiers: Promises, Shortcomings, and Advances\", \"Analysis Methods in Neural Language Processing: A Survey\", \"Vicuna: An Open-Source Chatbot Impressing GPT-4 with $90 \\% *$ ChatGPT Quality\", \"Towards Automated Circuit Discovery for Mechanistic Interpretability\", \"Similarity and Matching of Neural Network Representations\", \"Analyzing Transformers in Embedding Space\", \"Jump to Conclusions: Short-Cutting Transformers with Linear Transformations\", \"The Pile: An 800GB Dataset of Diverse Text for Language Modeling\", \"Causal Abstractions of Neural Networks\", \"How does GPT-2 Compute Greater-Than?: Interpreting Mathematical Abilities in a Pre-Trained Language Model\", \"Does Localization Inform Editing? Surprising Differences in Causality-Based Localization vs. Knowledge Editing in Language Models\", \"In-Context Learning Creates Task Vectors\", \"Natural Language Descriptions of Deep Features\", \"Measuring and Manipulating Knowledge Representations in Language Models\", \"Linearity of Relation Decoding in Transformer Language Models\", \"Large Language Models Struggle to Learn Long-Tail Knowledge\", \"Interpretability Beyond Feature Attribution: Quantitative Testing with Concept Activation Vectors (TCAV)\", \"What's in an Embedding? Analyzing Word Embeddings through Multilingual Evaluation\", \"Decoderlens: Layerwise Interpretation of Encoder-Decoder Transformers\", \"Understanding Image Representations by Measuring their Equivariance and Equivalence\", \"Understanding and Patching Compositional Reasoning in LLMs\", \"Does Circuit Analysis Interpretability Scale? Evidence from Multiple Choice Capabilities in Chinchilla\", \"ROUGE: A Package for Automatic Evaluation of Summaries\", \"Self-Refine: Iterative Refinement with Self-Feedback\", \"Post-Hoc Interpretability for Neural NLP: A Survey\", \"When Not to Trust Language Models: Investigating Effectiveness of Parametric and Nonparametric Memories\", \"Locating and Editing Factual Associations in GPT\", \"Mass-Editing Memory in a Transformer\", \"Pointer Sentinel Mixture Models\", \"The Expressive Power of Transformers with Chain of Thought\", \"Linearly Mapping from Image to Text Space\", \"A Mechanism for Solving Relational Tasks in Transformer Language Models\", \"Can LLMS Facilitate Interpretation of Pre-Trained Language Models?\", \"Interpreting GPT: The Logit Lens\", \"Future Lens: Anticipating Subsequent Tokens from a Single Hidden State\", \"Mapping Language Models to Grounded Conceptual Spaces\", \"Sentence-BERT: Sentence Embeddings Using Siamese BERT-Networks\", \"Multimodal Neurons in Pretrained Text-Only Transformers\", \"Explaining Black Box Text Modules in Natural Language with Language Models\", \"The Curious Case of Hallucinatory (Un)Answerability: Finding Truths in the Hidden States of Over-Confident Large Language Models\", \"A Mechanistic Interpretation of Arithmetic Reasoning in Language Models Using Causal Mediation Analysis\", \"LSTMVis: A Tool for Visual Analysis of Hidden State Dynamics in Recurrent Neural Networks\", \"BERT Rediscovers the Classical NLP Pipeline\", \"Llama: Open and Efficient Foundation Language Models\", \"Llama 2: Open Foundation and Finetuned Chat Models\", \"Investigating Gender Bias in Language Models Using Causal Mediation Analysis\", \"Gaussian Process Probes (GPP) for Uncertainty-Aware Probing\", \"Chain-of-Thought Prompting Elicits Reasoning in Large Language Models\", \"Tree of Thoughts: Deliberate Problem Solving with Large Language Models\", \"Give Me the Facts! A Survey on Factual Knowledge Probing in Pre-Trained Language Models\", \"Towards Best Practices of Activation Patching in Language Models: Metrics and Methods\", \"MQuake: Assessing Knowledge Editing in Language Models via Multi-Hop Questions\"], \"Citation\":[]}\n[The End of Assistant's Predicted Answer]\n\n[System]\nWe would like to request your feedback on the performance of the AI assistant in response to the user question displayed above according to the gold answer. Please use the following listed aspects and their descriptions as evaluation criteria:\n    - Accuracy and Hallucinations: The assistant's answer is semantically consistent with the gold answer; The numerical value and order need to be accurate, and there should be no hallucinations.\n    - Completeness: Referring to the reference answers, the assistant's answer should contain all the key points needed to answer the user's question; further elaboration on these key points can be omitted.\nPlease rate whether this answer is suitable for the question. Please note that the gold answer can be considered as a correct answer to the question.\n\nThe assistant receives an overall score on a scale of 1 to 100, where a higher score indicates better overall performance.\nPlease note that if the assistant's answer and the gold answer fully meet the above criteria, its overall rating should be the full marks (100).\nPlease first provide a comprehensive explanation of your evaluation, avoiding any potential bias.\nThen, output a line indicating the score of the Assistant.\n\nPLEASE OUTPUT WITH THE FOLLOWING FORMAT, WHERE THE SCORE IS A SCALE OF 1 TO 100 BY STRICTLY FOLLOWING THIS FORMAT: \"[[score]]\", FOR EXAMPLE \"Rating: [[100]]\":\n<start output>\nEvaluation evidence: your evluation explanation here, no more than 100 words\nRating: [[score]]\n<end output>\n\nNow, start your evaluation:",
        "generate_response": "{\"Reference\":[\"A Survey on Interpreting the Inner Structures of Deep Neural Networks\", \"Revisiting Model Stitching to Compare Neural Representations\", \"Probing Classifiers: Promises, Shortcomings, and Advances\", \"Analysis Methods in Neural Language Processing: A Survey\", \"Vicuna: An Open-Source Chatbot Impressing GPT-4 with $90 \\% *$ ChatGPT Quality\", \"Towards Automated Circuit Discovery for Mechanistic Interpretability\", \"Similarity and Matching of Neural Network Representations\", \"Analyzing Transformers in Embedding Space\", \"Jump to Conclusions: Short-Cutting Transformers with Linear Transformations\", \"The Pile: An 800GB Dataset of Diverse Text for Language Modeling\", \"Causal Abstractions of Neural Networks\", \"How does GPT-2 Compute Greater-Than?: Interpreting Mathematical Abilities in a Pre-Trained Language Model\", \"Does Localization Inform Editing? Surprising Differences in Causality-Based Localization vs. Knowledge Editing in Language Models\", \"In-Context Learning Creates Task Vectors\", \"Natural Language Descriptions of Deep Features\", \"Measuring and Manipulating Knowledge Representations in Language Models\", \"Linearity of Relation Decoding in Transformer Language Models\", \"Large Language Models Struggle to Learn Long-Tail Knowledge\", \"Interpretability Beyond Feature Attribution: Quantitative Testing with Concept Activation Vectors (TCAV)\", \"What's in an Embedding? Analyzing Word Embeddings through Multilingual Evaluation\", \"Decoderlens: Layerwise Interpretation of Encoder-Decoder Transformers\", \"Understanding Image Representations by Measuring their Equivariance and Equivalence\", \"Understanding and Patching Compositional Reasoning in LLMs\", \"Does Circuit Analysis Interpretability Scale? Evidence from Multiple Choice Capabilities in Chinchilla\", \"ROUGE: A Package for Automatic Evaluation of Summaries\", \"Self-Refine: Iterative Refinement with Self-Feedback\", \"Post-Hoc Interpretability for Neural NLP: A Survey\", \"When Not to Trust Language Models: Investigating Effectiveness of Parametric and Nonparametric Memories\", \"Locating and Editing Factual Associations in GPT\", \"Mass-Editing Memory in a Transformer\", \"Pointer Sentinel Mixture Models\", \"The Expressive Power of Transformers with Chain of Thought\", \"Linearly Mapping from Image to Text Space\", \"A Mechanism for Solving Relational Tasks in Transformer Language Models\", \"Can LLMS Facilitate Interpretation of Pre-Trained Language Models?\", \"Interpreting GPT: The Logit Lens\", \"Future Lens: Anticipating Subsequent Tokens from a Single Hidden State\", \"Mapping Language Models to Grounded Conceptual Spaces\", \"Sentence-BERT: Sentence Embeddings Using Siamese BERT-Networks\", \"Multimodal Neurons in Pretrained Text-Only Transformers\", \"Explaining Black Box Text Modules in Natural Language with Language Models\", \"The Curious Case of Hallucinatory (Un)Answerability: Finding Truths in the Hidden States of Over-Confident Large Language Models\", \"A Mechanistic Interpretation of Arithmetic Reasoning in Language Models Using Causal Mediation Analysis\", \"LSTMVis: A Tool for Visual Analysis of Hidden State Dynamics in Recurrent Neural Networks\", \"BERT Rediscovers the Classical NLP Pipeline\", \"Llama: Open and Efficient Foundation Language Models\", \"Llama 2: Open Foundation and Finetuned Chat Models\", \"Investigating Gender Bias in Language Models Using Causal Mediation Analysis\", \"Gaussian Process Probes (GPP) for Uncertainty-Aware Probing\", \"Chain-of-Thought Prompting Elicits Reasoning in Large Language Models\", \"Tree of Thoughts: Deliberate Problem Solving with Large Language Models\", \"Give Me the Facts! A Survey on Factual Knowledge Probing in Pre-Trained Language Models\", \"Towards Best Practices of Activation Patching in Language Models: Metrics and Methods\", \"MQuake: Assessing Knowledge Editing in Language Models via Multi-Hop Questions\"], \"Citation\":[]}",
        "eval_response": "Evaluation evidence: The assistant's answer does not match the gold answer in terms of accuracy and completeness. The gold answer specifies two papers that cite the analyzed paper, but the assistant's answer incorrectly lists no citations and includes a long list of references not mentioned in the gold answer. This indicates both inaccuracies and hallucinations in the assistant's response.\n\nRating: [[10]]",
        "score": 10
    },
    {
        "level": 3,
        "set": 2,
        "length": 61336,
        "type": "paper",
        "language": "en",
        "question": "How Johnny Can Persuade LLMs to Jailbreak Them: Rethinking Persuasion to Challenge AI Safety by Humanizing LLMs",
        "instruction": "We hope you will carefully study the provided papers and determine the citation relationships between them. Please follow the instructions below strictly to complete the task:\n\n#Specific Requirements:\n1. Reference: When a given paper mentions other provided papers, those other papers are considered as \"references\" for the given paper. To summarize in this specific context, references are about what the given paper is using.\n2. Citation: Conversely, when other provided papers mention the given paper in their works, the given paper is being \"cited\" by those other papers. To summarize in this specific context, citations are about who is using the given paper.\n3. Given a paper, you need to determine the citation or reference relationship between this paper and the other papers. Do not consider papers that are not provided.\n3. Please present the paper titles in a json format as follows: {{\"Reference\":[\"Reference Title 1\", \"Reference Title 2\", ..., \"Reference Title n\"], \"Citation\":[\"Citation Title 1\", \"Citation Title 2\", ..., \"Citation Title n\"]}}.\n4. If a paper does not have any references or citations, please leave the corresponding list empty, e.g.{{\"Refernce\":[]}}, {{\"Citation\":[]}}.",
        "prompt_template": "#Papers:\n{docs}\n\n{instruction}\n\n#The paper you need to analyze:\n{question}",
        "doc": [
            "2401.06373.md",
            "2405.19103.md",
            "2405.21018.md"
        ],
        "answer": {
            "Reference": [],
            "Citation": [
                "# Voice Jailbreak Attacks Against GPT-4o ",
                "# Improved Techniques for Optimization-Based Jailbreaking on Large Language Models "
            ]
        },
        "shuffle_doc": true,
        "id": "a5fc8bde-2e0f-479f-88e3-a4d250a60bb8",
        "prompt": "[Question]\n#Papers:\n{docs}\n\nWe hope you will carefully study the provided papers and determine the citation relationships between them. Please follow the instructions below strictly to complete the task:\n\n#Specific Requirements:\n1. Reference: When a given paper mentions other provided papers, those other papers are considered as \"references\" for the given paper. To summarize in this specific context, references are about what the given paper is using.\n2. Citation: Conversely, when other provided papers mention the given paper in their works, the given paper is being \"cited\" by those other papers. To summarize in this specific context, citations are about who is using the given paper.\n3. Given a paper, you need to determine the citation or reference relationship between this paper and the other papers. Do not consider papers that are not provided.\n3. Please present the paper titles in a json format as follows: {{\"Reference\":[\"Reference Title 1\", \"Reference Title 2\", ..., \"Reference Title n\"], \"Citation\":[\"Citation Title 1\", \"Citation Title 2\", ..., \"Citation Title n\"]}}.\n4. If a paper does not have any references or citations, please leave the corresponding list empty, e.g.{{\"Refernce\":[]}}, {{\"Citation\":[]}}.\n\n#The paper you need to analyze:\nHow Johnny Can Persuade LLMs to Jailbreak Them: Rethinking Persuasion to Challenge AI Safety by Humanizing LLMs\n\n[Gold Answer]\n{'Reference': [], 'Citation': ['# Voice Jailbreak Attacks Against GPT-4o ', '# Improved Techniques for Optimization-Based Jailbreaking on Large Language Models ']}\n\n[The Start of Assistant's Predicted Answer]\n{\"Reference\":[\"A Survey on Evaluation of Large Language Models\", \"Vicuna: An Open-Source Chatbot Impressing GPT-4 with $90 \\% *$ ChatGPT Quality\", \"ChatLaw: Open-Source Legal Large Language Model with Integrated External Knowledge Bases\", \"Compositional Semantic Parsing with Large Language Models\", \"Open LLM Leaderboard\", \"How Close is ChatGPT to Human Experts? Comparison Corpus, Evaluation, and Detection\", \"REALM: Retrieval-Augmented Language Model PreTraining\", \"Rethinking with Retrieval: Faithful Large Language Model Inference\", \"Measuring Massive Multitask Language Understanding\", \"Atlas: Few-shot Learning with Retrieval Augmented Language Models\", \"Jailbreaker: Automated jailbreak across multiple large language model chatbots\", \"Multilingual jailbreak challenges in large language models\", \"Automatically auditing large language models via discrete optimization\", \"Exploiting programmatic behavior of $11 \\mathrm{~ms}$ : Dual-use through standard security attacks\", \"Certifying $11 \\mathrm{~m}$ safety against adversarial prompting\", \"Your language models can align themselves without finetuning\", \"Generating stealthy jailbreak prompts on aligned large language models\", \"Jailbreaking chatgpt via prompt engineering: An empirical study\", \"Shining a light on dark patterns\", \"Dark patterns at scale: Findings from a crawl of $11 \\mathrm{k}$ shopping websites\", \"Dark patterns: Past, present, and future: The evolution of tricky user interfaces\", \"Evidence-based advertising using persuasion principles: Predictive validity and proof of concept\", \"Persuasion and coercion: a critical review of philosophical and empirical approaches\", \"Llama 2: Open foundation and fine-tuned chat models\", \"Adversarial demonstration attacks on large language models\", \"Jailbreak and guard aligned language models with only few in-context demonstrations\", \"The earth is flat because...: Investigating llms' belief towards misinformation via persuasive conversation\", \"Shadow alignment: The ease of subverting safely-aligned language models\", \"Low-resource languages jailbreak gpt-4\", \"Gptfuzzer: Red teaming large language models with auto-generated jailbreak prompts\", \"Gpt-4 is too smart to be safe: Stealthy chat with $11 \\mathrm{~ms}$ via cipher\", \"Universal and transferable adversarial attacks on aligned language models\"], \"Citation\":[\"Improved Techniques for Optimization-Based Jailbreaking on Large Language Models\"]}\n[The End of Assistant's Predicted Answer]\n\n[System]\nWe would like to request your feedback on the performance of the AI assistant in response to the user question displayed above according to the gold answer. Please use the following listed aspects and their descriptions as evaluation criteria:\n    - Accuracy and Hallucinations: The assistant's answer is semantically consistent with the gold answer; The numerical value and order need to be accurate, and there should be no hallucinations.\n    - Completeness: Referring to the reference answers, the assistant's answer should contain all the key points needed to answer the user's question; further elaboration on these key points can be omitted.\nPlease rate whether this answer is suitable for the question. Please note that the gold answer can be considered as a correct answer to the question.\n\nThe assistant receives an overall score on a scale of 1 to 100, where a higher score indicates better overall performance.\nPlease note that if the assistant's answer and the gold answer fully meet the above criteria, its overall rating should be the full marks (100).\nPlease first provide a comprehensive explanation of your evaluation, avoiding any potential bias.\nThen, output a line indicating the score of the Assistant.\n\nPLEASE OUTPUT WITH THE FOLLOWING FORMAT, WHERE THE SCORE IS A SCALE OF 1 TO 100 BY STRICTLY FOLLOWING THIS FORMAT: \"[[score]]\", FOR EXAMPLE \"Rating: [[100]]\":\n<start output>\nEvaluation evidence: your evluation explanation here, no more than 100 words\nRating: [[score]]\n<end output>\n\nNow, start your evaluation:",
        "generate_response": "{\"Reference\":[\"A Survey on Evaluation of Large Language Models\", \"Vicuna: An Open-Source Chatbot Impressing GPT-4 with $90 \\% *$ ChatGPT Quality\", \"ChatLaw: Open-Source Legal Large Language Model with Integrated External Knowledge Bases\", \"Compositional Semantic Parsing with Large Language Models\", \"Open LLM Leaderboard\", \"How Close is ChatGPT to Human Experts? Comparison Corpus, Evaluation, and Detection\", \"REALM: Retrieval-Augmented Language Model PreTraining\", \"Rethinking with Retrieval: Faithful Large Language Model Inference\", \"Measuring Massive Multitask Language Understanding\", \"Atlas: Few-shot Learning with Retrieval Augmented Language Models\", \"Jailbreaker: Automated jailbreak across multiple large language model chatbots\", \"Multilingual jailbreak challenges in large language models\", \"Automatically auditing large language models via discrete optimization\", \"Exploiting programmatic behavior of $11 \\mathrm{~ms}$ : Dual-use through standard security attacks\", \"Certifying $11 \\mathrm{~m}$ safety against adversarial prompting\", \"Your language models can align themselves without finetuning\", \"Generating stealthy jailbreak prompts on aligned large language models\", \"Jailbreaking chatgpt via prompt engineering: An empirical study\", \"Shining a light on dark patterns\", \"Dark patterns at scale: Findings from a crawl of $11 \\mathrm{k}$ shopping websites\", \"Dark patterns: Past, present, and future: The evolution of tricky user interfaces\", \"Evidence-based advertising using persuasion principles: Predictive validity and proof of concept\", \"Persuasion and coercion: a critical review of philosophical and empirical approaches\", \"Llama 2: Open foundation and fine-tuned chat models\", \"Adversarial demonstration attacks on large language models\", \"Jailbreak and guard aligned language models with only few in-context demonstrations\", \"The earth is flat because...: Investigating llms' belief towards misinformation via persuasive conversation\", \"Shadow alignment: The ease of subverting safely-aligned language models\", \"Low-resource languages jailbreak gpt-4\", \"Gptfuzzer: Red teaming large language models with auto-generated jailbreak prompts\", \"Gpt-4 is too smart to be safe: Stealthy chat with $11 \\mathrm{~ms}$ via cipher\", \"Universal and transferable adversarial attacks on aligned language models\"], \"Citation\":[\"Improved Techniques for Optimization-Based Jailbreaking on Large Language Models\"]}",
        "eval_response": "Evaluation evidence: The assistant's answer does not match the gold answer in terms of accuracy and completeness. The gold answer specifies two citations with no references, while the assistant's answer lists numerous references and only one citation, which is not even mentioned in the gold answer. This indicates a significant deviation from the required task, showing both inaccuracies and hallucinations of data not present in the gold answer.\n\nRating: [[20]]",
        "score": 20
    },
    {
        "level": 3,
        "set": 2,
        "length": 70227,
        "type": "paper",
        "language": "en",
        "question": "LLM Maybe LongLM: SelfExtend LLM Context Window Without Tuning",
        "instruction": "We hope you will carefully study the provided papers and determine the citation relationships between them. Please follow the instructions below strictly to complete the task:\n\n#Specific Requirements:\n1. Reference: When a given paper mentions other provided papers, those other papers are considered as \"references\" for the given paper. To summarize in this specific context, references are about what the given paper is using.\n2. Citation: Conversely, when other provided papers mention the given paper in their works, the given paper is being \"cited\" by those other papers. To summarize in this specific context, citations are about who is using the given paper.\n3. Given a paper, you need to determine the citation or reference relationship between this paper and the other papers. Do not consider papers that are not provided.\n3. Please present the paper titles in a json format as follows: {{\"Reference\":[\"Reference Title 1\", \"Reference Title 2\", ..., \"Reference Title n\"], \"Citation\":[\"Citation Title 1\", \"Citation Title 2\", ..., \"Citation Title n\"]}}.\n4. If a paper does not have any references or citations, please leave the corresponding list empty, e.g.{{\"Refernce\":[]}}, {{\"Citation\":[]}}.",
        "prompt_template": "#Papers:\n{docs}\n\n{instruction}\n\n#The paper you need to analyze:\n{question}",
        "doc": [
            "2401.01325.md",
            "2405.03547.md",
            "2405.04028.md",
            "2310.06825.md",
            "2312.15166.md"
        ],
        "answer": {
            "Reference": [
                "# Mistral 7B ",
                "# SOLAR 10.7B: Scaling Large Language Models with Simple yet Effective Depth Up-Scaling "
            ],
            "Citation": [
                "# Position: Leverage Foundational Models for Black-Box Optimization ",
                "# Masked Graph Transformer for Large-Scale Recommendation "
            ]
        },
        "shuffle_doc": true,
        "id": "49621ee6-a2d8-486c-8b0f-519955658c62",
        "prompt": "[Question]\n#Papers:\n{docs}\n\nWe hope you will carefully study the provided papers and determine the citation relationships between them. Please follow the instructions below strictly to complete the task:\n\n#Specific Requirements:\n1. Reference: When a given paper mentions other provided papers, those other papers are considered as \"references\" for the given paper. To summarize in this specific context, references are about what the given paper is using.\n2. Citation: Conversely, when other provided papers mention the given paper in their works, the given paper is being \"cited\" by those other papers. To summarize in this specific context, citations are about who is using the given paper.\n3. Given a paper, you need to determine the citation or reference relationship between this paper and the other papers. Do not consider papers that are not provided.\n3. Please present the paper titles in a json format as follows: {{\"Reference\":[\"Reference Title 1\", \"Reference Title 2\", ..., \"Reference Title n\"], \"Citation\":[\"Citation Title 1\", \"Citation Title 2\", ..., \"Citation Title n\"]}}.\n4. If a paper does not have any references or citations, please leave the corresponding list empty, e.g.{{\"Refernce\":[]}}, {{\"Citation\":[]}}.\n\n#The paper you need to analyze:\nLLM Maybe LongLM: SelfExtend LLM Context Window Without Tuning\n\n[Gold Answer]\n{'Reference': ['# Mistral 7B ', '# SOLAR 10.7B: Scaling Large Language Models with Simple yet Effective Depth Up-Scaling '], 'Citation': ['# Position: Leverage Foundational Models for Black-Box Optimization ', '# Masked Graph Transformer for Large-Scale Recommendation ']}\n\n[The Start of Assistant's Predicted Answer]\n{\"Reference\": [\"A Survey on Evaluation of Large Language Models\", \"Vicuna: An Open-Source Chatbot Impressing GPT-4 with $90 \\% *$ ChatGPT Quality\", \"ChatLaw: Open-Source Legal Large Language Model with Integrated External Knowledge Bases\", \"Compositional Semantic Parsing with Large Language Models\", \"Open LLM Leaderboard\", \"How Close is ChatGPT to Human Experts? Comparison Corpus, Evaluation, and Detection\", \"Rethinking with Retrieval: Faithful Large Language Model Inference\", \"Measuring Massive Multitask Language Understanding\", \"Coding\", \"Claude1.3-100k\", \"GPT-4-32k\", \"Turbo-16k-0613\", \"Chatglm2-6b-8k\", \"XGen-7b-8k (2k-4k-8k)\", \"Llama2-7b-chat\", \"Longchat1.5-7b-32k\", \"Llama2-13b-chat\", \"Llama2-13b-NTK\", \"Llama2-13b-NTK(Dyn)\", \"Mistral-7b-ins-0.1 w/ SWA\", \"Mistral-7b-ins-0.1 w/o SWA\", \"MistralLite\", \"Phi-2\", \"SOLAR-10.7b-Instruct-v1.0\", \"Mistrallite model\", \"L-eval: Instituting standardized evaluation for long context language models\", \"Long context prompting for claude 2.1\", \"Recent advances in adversarial training for adversarial robustness\", \"Longbench: A bilingual, multitask benchmark for long context understanding\", \"Language models are few-shot learners\", \"Continuous length extrapolation for large language models\", \"Extending context window of large language models via positional interpolation\", \"Efficient fine-tuning of long-context large language models\", \"Think you have solved question answering? try arc, the ai 2 reasoning challenge\", \"Training verifiers to solve math word problems\", \"Transformer-xl: Attentive language models beyond a fixed-length context\", \"Flashattention: Fast and memory-efficient exact attention with io-awareness\", \"A framework for few-shot language model evaluation\", \"LLMTest_NeedleInAHaystack: Doing simple retrieval from llm models\", \"Lm-infinite: Simple on-the-fly length generalization for large language models\", \"Measuring massive multitask language understanding\", \"Phi-2: The surprising power of small language models\", \"Mistral 7b\", \"Rethinking positional encoding in language pre-training\", \"Solar 10.7 b: Scaling large language models with simple yet effective depth up-scaling\", \"Truthfulqa: Measuring how models mimic human falsehoods\", \"Towards out-of-distribution generalization: A survey\", \"Landmark attention: Random-access infinite context length for transformers\", \"Giraffe: Adventures in expanding context lengths in llms\", \"Efficient context window extension of large language models\", \"Train short, test long: Attention with linear biases enables input length extrapolation\", \"Compressive transformers for long-range sequence modelling\", \"Exploring the limits of transfer learning with a unified text-to-text transformer\", \"Code llama: Open foundation models for code\", \"Towards out-of-distribution generalization: A survey\", \"Sparsebert: Rethinking the importance analysis in self-attention\", \"Rectified rotary position embeddings\", \"RoFormer: Enhanced transformer with rotary position embedding\", \"A lengthextrapolatable transformer\", \"Llama 2: Open foundation and finetuned chat models\", \"Attention is all you need\", \"Rethinking and improving relative position encoding for vision transformer\", \"Efficient streaming language models with attention sinks\", \"Effective long-context scaling of foundation models\", \"mt5: A massively multilingual pre-trained text-to-text transformer\", \"Harnessing the power of llms in practice: A survey on chatgpt and beyond\", \"amazon/MistralLite\", \"Big bird: Transformers for longer sequences\", \"Hellaswag: Can a machine really finish your sentence?\", \"When neural networks fail to generalize? a model sensitivity perspective\", \"Opt: Open pre-trained transformer language models\", \"A survey of large language models\"], \"Citation\": []}\n[The End of Assistant's Predicted Answer]\n\n[System]\nWe would like to request your feedback on the performance of the AI assistant in response to the user question displayed above according to the gold answer. Please use the following listed aspects and their descriptions as evaluation criteria:\n    - Accuracy and Hallucinations: The assistant's answer is semantically consistent with the gold answer; The numerical value and order need to be accurate, and there should be no hallucinations.\n    - Completeness: Referring to the reference answers, the assistant's answer should contain all the key points needed to answer the user's question; further elaboration on these key points can be omitted.\nPlease rate whether this answer is suitable for the question. Please note that the gold answer can be considered as a correct answer to the question.\n\nThe assistant receives an overall score on a scale of 1 to 100, where a higher score indicates better overall performance.\nPlease note that if the assistant's answer and the gold answer fully meet the above criteria, its overall rating should be the full marks (100).\nPlease first provide a comprehensive explanation of your evaluation, avoiding any potential bias.\nThen, output a line indicating the score of the Assistant.\n\nPLEASE OUTPUT WITH THE FOLLOWING FORMAT, WHERE THE SCORE IS A SCALE OF 1 TO 100 BY STRICTLY FOLLOWING THIS FORMAT: \"[[score]]\", FOR EXAMPLE \"Rating: [[100]]\":\n<start output>\nEvaluation evidence: your evluation explanation here, no more than 100 words\nRating: [[score]]\n<end output>\n\nNow, start your evaluation:",
        "generate_response": "{\"Reference\": [\"A Survey on Evaluation of Large Language Models\", \"Vicuna: An Open-Source Chatbot Impressing GPT-4 with $90 \\% *$ ChatGPT Quality\", \"ChatLaw: Open-Source Legal Large Language Model with Integrated External Knowledge Bases\", \"Compositional Semantic Parsing with Large Language Models\", \"Open LLM Leaderboard\", \"How Close is ChatGPT to Human Experts? Comparison Corpus, Evaluation, and Detection\", \"Rethinking with Retrieval: Faithful Large Language Model Inference\", \"Measuring Massive Multitask Language Understanding\", \"Coding\", \"Claude1.3-100k\", \"GPT-4-32k\", \"Turbo-16k-0613\", \"Chatglm2-6b-8k\", \"XGen-7b-8k (2k-4k-8k)\", \"Llama2-7b-chat\", \"Longchat1.5-7b-32k\", \"Llama2-13b-chat\", \"Llama2-13b-NTK\", \"Llama2-13b-NTK(Dyn)\", \"Mistral-7b-ins-0.1 w/ SWA\", \"Mistral-7b-ins-0.1 w/o SWA\", \"MistralLite\", \"Phi-2\", \"SOLAR-10.7b-Instruct-v1.0\", \"Mistrallite model\", \"L-eval: Instituting standardized evaluation for long context language models\", \"Long context prompting for claude 2.1\", \"Recent advances in adversarial training for adversarial robustness\", \"Longbench: A bilingual, multitask benchmark for long context understanding\", \"Language models are few-shot learners\", \"Continuous length extrapolation for large language models\", \"Extending context window of large language models via positional interpolation\", \"Efficient fine-tuning of long-context large language models\", \"Think you have solved question answering? try arc, the ai 2 reasoning challenge\", \"Training verifiers to solve math word problems\", \"Transformer-xl: Attentive language models beyond a fixed-length context\", \"Flashattention: Fast and memory-efficient exact attention with io-awareness\", \"A framework for few-shot language model evaluation\", \"LLMTest_NeedleInAHaystack: Doing simple retrieval from llm models\", \"Lm-infinite: Simple on-the-fly length generalization for large language models\", \"Measuring massive multitask language understanding\", \"Phi-2: The surprising power of small language models\", \"Mistral 7b\", \"Rethinking positional encoding in language pre-training\", \"Solar 10.7 b: Scaling large language models with simple yet effective depth up-scaling\", \"Truthfulqa: Measuring how models mimic human falsehoods\", \"Towards out-of-distribution generalization: A survey\", \"Landmark attention: Random-access infinite context length for transformers\", \"Giraffe: Adventures in expanding context lengths in llms\", \"Efficient context window extension of large language models\", \"Train short, test long: Attention with linear biases enables input length extrapolation\", \"Compressive transformers for long-range sequence modelling\", \"Exploring the limits of transfer learning with a unified text-to-text transformer\", \"Code llama: Open foundation models for code\", \"Towards out-of-distribution generalization: A survey\", \"Sparsebert: Rethinking the importance analysis in self-attention\", \"Rectified rotary position embeddings\", \"RoFormer: Enhanced transformer with rotary position embedding\", \"A lengthextrapolatable transformer\", \"Llama 2: Open foundation and finetuned chat models\", \"Attention is all you need\", \"Rethinking and improving relative position encoding for vision transformer\", \"Efficient streaming language models with attention sinks\", \"Effective long-context scaling of foundation models\", \"mt5: A massively multilingual pre-trained text-to-text transformer\", \"Harnessing the power of llms in practice: A survey on chatgpt and beyond\", \"amazon/MistralLite\", \"Big bird: Transformers for longer sequences\", \"Hellaswag: Can a machine really finish your sentence?\", \"When neural networks fail to generalize? a model sensitivity perspective\", \"Opt: Open pre-trained transformer language models\", \"A survey of large language models\"], \"Citation\": []}",
        "eval_response": "Evaluation evidence: The assistant's answer does not match the gold answer in terms of accuracy and completeness. The references listed in the assistant's answer are entirely different from those in the gold answer, indicating a significant hallucination issue. Additionally, the assistant's answer does not include any citations, whereas the gold answer lists two specific citations. This discrepancy shows a lack of completeness and accuracy in the assistant's response.\n\nRating: [[10]]",
        "score": 10
    }
]